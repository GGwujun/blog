exports.category = "frontend";
exports.title = "搞定音频技术";
exports.data = [
  {
    chapterTitle: "开篇词",
    children: [
      {
        title: "开篇词｜实时互动强势发展，如何快速入门音频技术？",
        id: 447712,
        content:
          '<p>你好，我是建元。桃李春风一杯酒，江湖夜雨十年灯。乘着算法的一叶扁舟在多个行业的江、河、湖、海中，我不知不觉已经遨游了将近十年。</p><h2>一名“算法吟游诗人”的 10 年</h2><p>如果用一句话来总结我的经历，我想我应该是个算法吟游诗人，我会随时将灵感运用于我的工作中。而与音频领域的结缘，可以回溯到2011年。</p><p>那时候我还在国内读大学，去工厂实习时，我记得有位老师傅，他只需要用耳朵听一下，就能判断出设备是哪里出了故障。受此启发，我当时是利用人工神经网络和振动传感器，搭建了一套空气压缩机故障自动诊断系统。那时，我惊叹于傅立叶变换的时、频域变换的神奇，人工神经网络的反向传播机制以及算法作为交叉学科实践的魅力。</p><p>后来为了进一步学习，我去了美国芝加哥的 Illinois Institute of Technology，攻读了硕士和博士学位。在硕博期间，我逐渐发现各个领域的算法其实都是相通的。利用信号处理、控制理论、人工智能算法的融合，我陆续研发了一系列包括毛细流体力学建模、智能电网系统、人工智能胰岛系统在内的多个专业的应用，并在 IEEE、AIChE、Journal of Process Control 等一线期刊上发表了20多篇论文。</p><p>也正是因为这些专业的学习，为我后面的工作乃至创业都打下了一个很好的基础。</p><!-- [[[read_end]]] --><p>博士毕业后，我去了University of Illinois at Chicago，担任特聘研究员。这段时间也正好是人工智能网络开始在音视频、自然语言处理等领域崭露头角的时候。那时我正好经手了<strong>两个项目：</strong>一个是基于自然语言处理的医学大数据的智能诊断，另一个是基于图像的食物营养成分的提取。我意识到传统算法可能在AI算法的加持下会迈入一个新的纪元，而且正好国内工业5.0正在如火如荼的改革进程中。于是，我便回国作为合伙人加入了一家做工业音视频智能化改造的创业公司。</p><p><strong>创业期间，我在音频方面做了很多有意思的尝试。</strong>比如，利用多个麦克风阵列矩阵，实现园区的异常声音的预警系统。该系统可以根据麦克风阵列采集的音频信号自动判断园区中哪个区域、哪台设备的什么地方发生了故障，从而实现了全产线的无人化管理。</p><p>后续结合音视频和机器人，我还做了一套自动巡检机器人系统，让机器人像导游一样在区域内巡逻，帮助访客找到自己想去的地方。机器人集合的音频系统包括语音识别、对话交互和语音生成，几乎每个环节都是传统信号处理算法和AI算法的联合应用。现在类似的机器人，我想你在生活中也接触到了。</p><p>创业阶段让我完整地认识了企业的运作和很多不同行业涌现出的新需求、新玩法，这是件好事。但后来我觉得那段时间做的东西过于繁杂了，反而不容易把一件事情做到极致。我就开始思考，<strong>什么样的行业能包含比较多的领域，又不至于失去趣味，同时又可以长期研究、不断精进呢？</strong></p><p>最终，结合之前的经历，我选择了音频，摆脱了创业的繁琐，加入声网——这家把“声音”放入名字的公司。后来在这里，我研发了一系列实时声音美化、实时空间音频渲染、实时变声、实时修音、实时语音变歌唱等算法，并先后带领团队完成并商用了 50 多种可实时使用的美声音效效果。</p><p>之后我又对实时音频链路进行了 AI 化改造，包括基于深度学习的降噪、音频编/解码、音频丢包补偿、回声消除、音源分离、啸叫检测、音乐检测等算法的实现与落地商用，并把这些算法和不同的业务场景相结合提高实时音频质量。同时，我也是国内第一个基于深度学习的语音编解/码器 Silver 的发明人。</p><p>那听了我这么多的故事，我想我们有必要重新认识一下了。你好，我是声网的音频算法专家，你叫我建元就好。那么提到音频技术，我为什么鼓励你深度学习它呢？</p><h2><strong>为什么说音频是实时互动开发中的必备武器？</strong></h2><p>从整个大环境看，音频已然是实时互动开发中的必备武器。</p><p>在新冠疫情的催化下，线上互动已经成为人们工作和生活中的必要交流方式。我记得2020年疫情爆发的时候，大部分的企业都在远程办公，学生们也在接受在线教育，这催生了很多音频技术的进一步完善。比如说，在这些场景中，延迟必须控制在毫秒级别内，实时互动技术成为决定整体体验的关键，从某种层面来说也是考验着大家的底层实时音视频（RTC）技术。</p><p>而在实时互动中，音频承载的信息密度又是巨大的。语言、音乐都需要音频作为载体进行传播。而人耳对声音的感知又是如此敏感，一个音频采样点的错误，人耳就能明显地感知到。那么像噪声、回声、网络抖动等问题，就更需要专业的处理了。随着AI技术的介入，例如基于AI降噪、回声消除、音频编解码技术，互动音频的可靠性已经得到了更多的保障。<strong>学习它，恰逢其时。</strong></p><p>而随着大家对线上体验的增加，人们对互动体验中的音频体验要求也在提升。人们逐渐从听得到、听得清，向好听、音质还原度高等方向发展。那么美声、音效技术就可以使你的声音更好听；高音质实时编解码器搭配优秀的抗弱网策略可以让音乐会、合唱等场景从线下搬到线上；空间音频技术能让你在虚拟世界中实现“声临其境”……可以说，<strong>音频互动技术就像空气和水一样无处不在，且充满了无限的可能。</strong></p><p>近些年，除了短视频、直播等传统方向会用到音频的处理，我们看到在新体验、新玩法的领域中音频也成了一件必备武器。例如，Facebook改名Meta进军元宇宙，音频播客Clubhouse等纯音频社交方式的流行，以及TWS耳机支持了空间音频渲染和主动降噪等等。</p><p>而在未来，可预测的是，人工智能领域在音频技术上会有进一步的突破。大型线上互动、娱乐、会议、教育等行业万象都会朝着音视频实时互联互通的方向转变。市场对于专业音频技术专家、音频工程师的需求缺口也在日渐增大。音频领域和计算机、信号处理、声学、音乐、语言学等都有交集，<strong>你过往的经历都可能派上用处。</strong></p><h2>课程设计</h2><p>音频是一个跨专业领域的有趣学科。而兴趣驱动，永远是最强的动力。所以，无论你是否是专业的音频工程师，有没有强大的技术背景和积累，都不要紧。只要你有想法转行音频领域，甚至只要你有兴趣学习，这门课都能给你创造新的机会。</p><p>因为我们每天都在说话、都在听各种各样的声音。我们平时发音有什么规律？耳朵对不同频段的心理感知有什么区别？乐器又是怎么发音的？我们如何判断音频的质量？以及“好声音”是怎么修出来的……这些知识不但可以让你用于职业工作，也可以加强你对生活中声音的感知，从而知道应该听什么、怎么听，甚至怎么去让声音发生变化，为你打开“百万调音师”的潘多拉宝盒。</p><p>那说了这么多，这门课的具体内容到底是怎样的呢？<strong>我分为了6个模块，设计思路是这样的。</strong></p><p><img src="https://static001.geekbang.org/resource/image/ca/29/ca4e3d8b628a5739a9d11dc8f3220129.jpg?wh=1572x1280" alt="图片"></p><p>首先，知识基础决定上层应用。所以我们的课程会先从音频的基础知识着手，带你了解音频是如何产生的，我们的听觉是怎么感知语音和音乐的，以及我们是如何评价音频质量的好坏的。</p><p>其次，有了基础知识以后，我会带你看看实时音频处理中所有的常见模块，包括：音频降噪、回声消除、音频编解码器的原理和选择，以及音频抗弱网策略等。有了这些知识你将会对整个音频链路有一个完整的认识。其中，我也会分享很多实践案例，帮助你利用算法解决各种可能遇到的音频问题。这里你既能学到很多音频信号处理的算法原理，也能从诸如AI降噪算法的设计中了解到音频处理算法中的前沿方向。</p><p>最后，我将带你看看VR/AR音频中常见的空间音频是如何渲染的，揭秘音效制作的基本原理，以及通过探究AI变声算法原理，带你看看AI究竟在音频领域还有哪些有意思的应用。</p><p><strong>总结来说，通过这门课程的学习，你能知晓音频处理的全套链路和音频发展的脉络、方向。</strong>这里也给你一个学习指导，你可以在学习过程中重点关注。</p><ul>\n<li>对于基础，我会尽量讲解得通俗易懂，但每一讲都需要你加强记忆，后面还会用到；</li>\n<li>对于音频处理，我会抽丝剥茧地带你看技术演进的过程，对比不同技术的优缺点，以便于你今后的应用；</li>\n<li>而对于音频算法和应用，这些都是不断演化的，我会带你了解目前音频前沿领域的新玩法和新探索，让你能从音频领域中一个更靠前的起跑线开始奔跑；</li>\n<li>对于你想进一步了解的内容，我也会通过大量的相关文献链接，以及课后思考题的形式帮助你充分理解。</li>\n</ul><p>最后，我想说音频技术作为一个很贴近生活的专业领域，实用性和趣味性都很出众，且这方面的人才一直十分稀缺。从AI技术的发展和虚拟现实技术的进步来看，音频都是一个充满朝阳的方向。如果你想转型或者拓展你的技术能力，加入我，让我们一起推开音频这扇新世界的大门。</p><p>也欢迎你在留言区中与我交流，不妨说说你对这门课程的期待，或者你在学习音频技术时的经历和痛点，我会更加有针对性地为你答疑解惑！</p>',
        article_title: "开篇词｜实时互动强势发展，如何快速入门音频技术？",
      },
    ],
  },
  {
    chapterTitle: "音频基础",
    children: [
      {
        title: "01｜声音是如何保存成数字信号的？",
        id: 447740,
        content:
          '<p>你好，我是建元。</p><p>作为专栏的第一节课，今天我们来聊聊音频信号中的一些基础知识，带你近距离地了解一下音频这个既熟悉又陌生的领域。这节课我们重点学习一下音频信号的关键指标和常见的音频封装格式，它们算是基础中的基础，希望你能够快速掌握。</p><h2>音频信号的关键指标</h2><p>声音我们每天都会听见，似乎早已习以为常。那么我们是怎么把声音信号转换成数字信号记录下来存储和传输的呢？</p><p>声音是听觉对声波产生的感知，而声波的本质是介质的振动，比如空气的振动。那么我们只需要把这个振动信号记录下来，并用一串数字来表达振动信号振动的快慢和振动的幅度，就可以实现声音的记录。</p><p>如图1所示，以前的留声机就是通过唱片上凹槽的深浅、长短来表征声音的振幅和持续时间。</p><p><img src="https://static001.geekbang.org/resource/image/98/b3/985486bec1b75d7a9db5989a789019b3.png?wh=1476x768" alt="图片" title="图1 留声机与唱片"></p><p>我们现在一般用麦克风来实现声音的采集。<strong>那如何通过麦克风来采集声音呢？</strong></p><p><img src="https://static001.geekbang.org/resource/image/44/6c/441bba7d52yy4bddfb678fe91812ee6c.jpg?wh=1121x239" alt="" title="图2 音频数字信号的生成"></p><p>使用麦克风的音频数字信号采集过程如图2所示：</p><ul>\n<li>首先，声波通过空气传播到麦克风的振膜。</li>\n<li>然后，振膜随空气抖动的振幅大小产生相应的电学信号。<strong>我们把这种带有声学表征的电学信号叫做模拟信号（Analog Signal）</strong>。</li>\n<li>最后，<strong>通过A/DC（模数转换器）将模拟信号转换成数字信号（Digital Signal）</strong>。即通过PCM（Pulse Code Modulation）脉冲编码调制对连续变化的模拟信号进行抽样、量化和编码转换成离散的数字信号。</li>\n</ul><!-- [[[read_end]]] --><p>这样我们就实现了音频信号的采集，我们常说的PCM文件就是未经封装的音频原始文件或者叫做音频“裸数据”。那么具体音频的数字信号是怎么构成的呢？这就涉及到下面的3个基本概念：采样位深、采样率和通道数。</p><p>现在我们先来熟悉一下这3个概念。</p><h3>采样位深</h3><p>采样位深也就是每个采样点用多少bit来表示。比如位深是16就代表每个采样点需要16bit来进行存储。从物理意义上来说，<strong>位深代表的是振动幅度的表达精确程度或者说粒度</strong>。假设数字信号是一个1到-1的区间，如果位深为16bit，那么第1个bit表示正负号，并且剩下的15个bit可以表征0～32767个数，那么振幅就可以精确到1/32768的粒度了。</p><p>我们一般在网络电话中用的就是16bit的位深，这样不太会影响听感，并且存储和传输的耗费也不是很大。而在做音乐或者更高保真度要求的场景中则可以使用32bit甚至64bit的位深来减少失真。</p><p>8bit时失真就比较严重了。早期受到音频技术条件限制，很多音频都是8bit的，声音会显得比较模糊，如今也只有一些电话和对讲机等设备还有使用。但有趣的是，有的音乐就追求这种模糊感，所以“8bit”有的时候也代表一种听感朦胧的音乐艺术类型。</p><h3>采样率</h3><p><strong>采样率就是1秒内采集到的采样点的个数，一般用赫兹Hz来表示。</strong>比如1秒有48000个采样点那么采样率就是48000Hz（48kHz）。</p><p>根据奈奎斯特采样定理在进行模拟/数字信号的转换过程中，当采样频率fs大于信号中最高频率fmax的2倍时（fs &gt; 2fmax），采样之后的数字信号才可以完整地保留原始信号中的信息。也就是说采样率和保留的声音频率基本上是2倍的关系。</p><p>我们可以看看图3的频谱图来对比一下16kHz采样率和48kHz采样率的音频。</p><p><img src="https://static001.geekbang.org/resource/image/9e/d4/9e3cb2f9b3e4d57a02b81fbd6d3c4dd4.png?wh=1896x1162" alt="图片" title=" 图3 不同采样率（ 48kHz(上)和16kHz(下) ）的频谱能量分布"></p><p>由图3我们可以看到，16kHz采样率的音频在8kHz以上的频谱基本是没有能量的（黑色），也就是说这部分高频的信息由于采样率不够已经丢失了。从听感上来说人耳可以听到的频率范围大概是50～20kHz之间。如果采样率不够，那么和实际听感比起来声音就会显得“低沉”或者说“闷”。</p><p><strong>那么采样率是不是越高越好呢？</strong>其实选用什么样的采样率是根据具体用途来决定的。</p><p>如果只是为了听见人声、听懂对方在说什么，那么为了节省传输码率我们可以把采样率降到8kHz（比如打电话）。而在网络音视频会议场景需要平衡音质和传输带宽消耗，我们一般可以使用16kHz或者32kHz的采样率。如果是开线上音乐会或者音乐直播，我们通常会用较高的采样率来保证音质，比如44.1kHz或者48kHz。更极端一点，在音乐制作录音的时候，我们会采用96kHz甚至更高的采样率来方便后续的调音和制作。</p><h3>通道数</h3><p>你可能在平时买音响的时候听过2.1声道或者5.1声道等名词，这些数字代表了有多少个播放单元。比如，2.1声道中的2指的是左右两个音箱，1指的是中间一个低音音箱（如图4所示）。每个音箱都会播放一个单独的音频，这时候就需要同时有3路音频信号同时播放，或者叫通道数为3。</p><p><img src="https://static001.geekbang.org/resource/image/a1/e2/a16d1e30835d37d3268b00385b9e85e2.png?wh=1212x370" alt="图片" title="图4 2.0声道音箱(左)、立体声耳机(中)、2.1声道音箱(右)"></p><p>我们在实时在线互动的时候，由于编/解码器能力的限制（比如使用了单通道编/解码器），或者采集设备能力的限制（只能采集单通道的信号），音频信号通常为单声道的。我们听歌的时候，戴上耳机如果听到左右耳朵是不一样的，能够感觉到声音是从不同方向传过来的，那么就说明这个音频是双声道。我们通常也把这种双声道音频叫做立体声（Stereo）（如图4所示）。</p><p>除了播放需要多声道以外，采集也可能采集到多通道的数据。比如麦克风阵列采集到的原始信号，有多少个麦克风就会有多少个通道的音频信号。因此，<strong>这里通道数的物理含义其实就是同一时间采集或播放的音频信号的总数。</strong></p><p>好了，这里我们已经基本掌握了音频数字信号的基本构成。现在我们来通过一个简单的计算来理解一下。假设我们有一个立体声的PCM音乐文件。它记录了1分40秒的采样率为48kHz的音频。如果这个文件的采样位深是16bit，那么这个立体声文件应该占用多大的存储空间呢？如果不经过压缩实时传输播放，又至少需要多少的带宽呢？</p><p>有了前面的知识我们就可以知道，一个PCM音频文件的存储大小就是采样位深、采样率、通道数和持续时间的累乘。即：</p><p>$${\\rm\\bf存储空间=采样位深\\times采样率\\times通道数\\times时长}=16\\times 48000\\times 2\\times 100=1.53*10^{8}bit=18.31MB$$</p><p>而它实时传输所需的带宽就是它每秒所需的比特带宽。这可以用采样位深、采样率和通道数来得到。即：</p><p>$${\\rm\\bf比特带宽=采样位深\\times采样率\\times通道数}=16\\times 48000\\times 2=1.53*10^{6}bps=1500kbps$$</p><h2>音频的封装</h2><p>我们可以看到上面例子中100秒的PCM音频文件就需要这么大的存储空间，那我们平时经常看到的音频文件格式，比如MP3、FLAC和WAV等，它们有什么区别？它们所需的存储空间一样么？</p><h3>有损和无损音频编码封装格式</h3><p>其实就算不是音频行业的人，也应该或多或少地听过有损和无损的音频编码封装格式。顾名思义，有损的音频封装格式主要是通过压缩算法把文件大小尽量减少，但是在解压缩的时候却无法完美还原音频原来的数据（即有损）。比如MP3、AAC、AMR和WMA等编码封装格式。</p><p>虽然叫做有损音频格式，但其实发展到现在，有损音频格式比如MP3一般可以达到1:10的压缩比，即存储体积为未压缩音频的十分之一。但在听感上和无损格式比起来，如果不是专业人士很难听出区别。</p><p>而无损音频封装则采用可完美还原的压缩算法，比如FLAC和APE等编码封装格式。FLAC与APE的压缩比基本相同，其中FLAC的压缩比为58.70%，而APE的压缩能力则要更高一些，压缩比为55.50%。它们都能压缩到接近源文件一半大小。无损封装甚至还可以不压缩编码，直接加个文件头作为封装，比如WAVE格式的封装。</p><p>其实简单地说，不是为了追求极致的听感，比如听音乐、音频分析，我们可以用有损压缩来节省存储体积。但是如果是拿音频信号做数据分析或者追求极致的听感还原，则必须使用无损压缩的封装格式来避免编码带来的损伤。</p><p>常见的有损和无损封装格式如下表1所示。其实很多编码封装也是支持调节压缩比来平衡音质和存储空间的，比如WMA可以同时支持无损和有损编码，而且MP3也可以调整不同的编码码率来调节音质。</p><p><img src="https://static001.geekbang.org/resource/image/df/22/df99ffdbe842eb4cd3dd4ea9c7ca8122.jpg?wh=1216x206" alt="" title="表1 常见的无损和有损音频编码封装"></p><p>因为无需封装、方便存储，所以音频工程师经常把音频处理的中间变量（未封装的音频数据）存储为PCM文件。但由于PCM文件也就是裸数据文件是无法直接读取播放的。比如把PCM文件拖到音频处理软件Audacity中，会提示你先输入这个PCM文件的采样率、通道数、比特位深等信息才能进行解析播放。这样，一个不知道这些音频信息的人就无法正确地解析播放这些PCM文件了。</p><p>接下来我们主要介绍一个常用的WAVE格式的封装，这样我们在日常处理音频裸数据的时候就可以将其封装。别人在查看、播放时，就可以直接用播放软件打开了。</p><h3>WAVE文件的封装</h3><p>WAVE文件作为多媒体中使用的声波文件格式之一，文件后缀名为wav。它是以RIFF格式为标准的，RIFF是英文Resource Interchange File Format的缩写。因此，每个WAVE文件的头四个字节便是“RIFF”。</p><p><strong>WAVE文件的封装格式十分简单。</strong>WAVE文件由WAVE文件头部分和WAVE文件数据体部分组成，其中0～43字节存放采样率、通道数、数据部分的标识符等头信息，44字节以后的就是数据部分。简单地理解就是PCM文件加一个文件头描述文件的基本信息。具体文件头每个字节的含义可以参考表2。<br>\n<img src="https://static001.geekbang.org/resource/image/7b/29/7b05eda31a7b57d98776848ae9b2e929.jpg?wh=1300x1200" alt="" title="表2 WAVE文件头每个字节的含义"></p><p>从表格2中我们可以看到，WAVE格式支持单、双声道的音频文件的封装，以及采样位深为8bit和16bit这两种格式。那么具体单通道和双通道数据是如何排列的呢？我们可以看一下图5。</p><p><img src="https://static001.geekbang.org/resource/image/9e/55/9e322f4bbdc02f42b891d5c957b47055.jpg?wh=1200x989" alt="" title="图5 WAVE格式中数据的比特分布"></p><p>从图5我们可以看到双声道的时候，是左右声道的数据间隔排布的，这主要是为了方便可以按照时间连续成块地读取数据信息，而不是读完一个声道再读下一个。</p><h2>小结</h2><p>好了，我们今天的课程到这里就要结束了。我们先来简单回顾一下今天讲到的知识点。</p><p>首先，采样位深、采样率、通道数是描述音频信号的关键指标。采样位深越深、采样率越高，则音质越好，但同时消耗的存储和传输的资源也就越多。</p><p>其次，在编码封装时如果需要尽量缩小存储音频的体积，那么可以选用MP3等有损的编码封装格式，而需要无损、高保真的音频时，则可以采用例如FLAC等无损格式来编码封装。其中对于音频开发者而言，WAVE格式由于可以迅速快捷地封装一个音频裸数据文件而被广泛使用。</p><p>最后，在实际使用时我们就可以根据自己的使用场景来选择音频信号的关键指标和封装形式了。</p><p>记住，音频信号主要是用来给人听的，在音频处理时往往需要“眼看为虚，耳听为实”。你可以多听听不同的采样率，不同的采样位深会有什么样的听感。听着听着就会形成记忆，以后解决问题时，你甚至只需要听一耳朵就能发现问题。</p><h2>思考题</h2><p>学习了上面的知识点，现在我有一道思考题留给你。</p><p>为什么有的音频文件，文件显示的是48kHz的采样率和16bit的位深，但听起来还是很闷或者听感很差呢？</p><p>欢迎你在留言区和我分享你的思考和疑惑，你也可以把今天所学分享给身边的朋友，邀请他加入探讨，共同进步。下节课再见。</p>',
        article_title: "01｜声音是如何保存成数字信号的？",
      },
      {
        title: "02｜如何量化分析语音信号？",
        id: 450737,
        content:
          '<p>你好，我是建元。</p><p>上一讲我们了解了音频信号的一些基础知识。因为语音和音乐是最常见的音频信号类型，所以接下来两节课我会分别从这两种类型入手，带你看看如何科学、量化地对音频信号进行分析，从而读懂音频信号所承载的内容和信息，以及了解音频现象产生的原因是什么。这些知识对音频工程师、开发者甚至是调音师、录音师、音乐制作者而言都是很重要的基础，所谓“听其然，更知其所以然”。</p><p>好的，那我们这节课就先从语音信号的分析开始说起吧。</p><h2>语音的基本特征</h2><p>语音按照发音原理可以分为清音和浊音，语音的音调、能量分布等信息可以用基频、谐波、共振峰等特征来分析。为了更好地分析语音，我们先来看看语音是如何产生的？</p><h3>浊音和清音</h3><p>我们可以结合图1的人体发音器官结构示意图来看一下我们的语音是如何产生的。</p><p><img src="https://static001.geekbang.org/resource/image/70/42/70689eb8684f7b51f9bd6181e0a24d42.jpg?wh=10666x6000" alt="" title="图1 人体发音器官结构示意图[br]新闻传播学大辞典：中国大百科全书出版社"></p><p>我们知道，声道就是声音传播所通过的地方。发音的声道主要是指我们的三个腔体，即咽腔、口腔和鼻腔。而语音是由声源和声道共同作用产生的。按照声源的不同我们把语音分成以下两类：</p><ul>\n<li>第一类是<strong>声带振动作为声源产生的声音，我们把它们叫做浊音</strong>。比如拼音中的 “a，o，e” 等。</li>\n<li>第二类是<strong>由气体在经过唇齿等狭小区域由于空气与腔体摩擦而产生的声音，我们把它们叫做清音。</strong>比如拼音中的 “shi、chi、xi” 等。</li>\n</ul><!-- [[[read_end]]] --><p>那么清音和浊音的声源不同在频谱上有什么样的差异呢？我们先来看看语音的频谱图（如图2所示）。</p><p><img src="https://static001.geekbang.org/resource/image/2d/17/2d3c7dddb85c8ccbcc74650da02ab117.png?wh=1006x645" alt="图片" title="图2 语音的频谱图时域图(上)和频域图(下)"></p><p>图2中显示的是“实时音视频互动”这几个字的音频信号的时域图和频域图（频谱图）。<strong>时域就是信号幅度和时间的关系，而频域指的是能量与时间和频率的关系。</strong></p><p>频域更方便我们观察不同频率的能量分布。我们可以看到浊音，比如最后两个字“互动”是明显的有规律的能量分布，即低频能量大于高频且有明显的能量比较集中的地方，如频谱图中的亮线。而“实时”和“视”这几个字，都有 “sh” 这个由牙齿间高速气流产生的清音。清音在频谱上显示为比较均匀的分布。在13kHz的高频，清音也有不少的能量。</p><p>根据这个简单的分布规律我们已经可以<strong>从频谱上分辨清浊音</strong>了。接下来我们再来看看，我们还能从有明显能量分布的浊音的频谱中得到哪些信息。</p><h3>基频</h3><p>在发浊音的时候，声带会振动从而产生一个声波，我们把这个声波叫做基波，并且把<strong>基波的频率叫做基频</strong>（一般用F0来表示）。<strong>这个基频其实就可以对应到我们平时所说的音调。</strong>比如，你唱歌音调比较高，其实就是你的声音基频比较高。</p><p>一般来说，男生的正常说话基频在100～200Hz之间，而女生的则会高一些，在140～240Hz之间。这就是为什么女生的声音听起来比男生的尖锐一些。基频会随年龄变化而变化，比如小孩的基频比较高，可以达到300Hz，而年龄越大则基频会越来越低。基频的能量对应的是浊音频谱中频率最低的亮线。</p><h3>谐波</h3><p>声带振动产生的基波，在传输过程中会在声道表面反复碰撞反射，从而产生许多频率倍数于基频的声波，我们通常把这些声波叫做谐波。按照谐波频率从低到高，我们依次叫1次谐波，2次谐波等等。图3中我们可以看一下基频信号和谐波信号在时域上的样子。</p><p><img src="https://static001.geekbang.org/resource/image/ca/yy/ca20469a5943fa176a12f0d44c83d8yy.jpg?wh=10666x5027" alt="" title="图3 基波与谐波"></p><p><strong>谐波频率和基频是浊音能量集中的地方</strong>，这也就是为什么我们能看见浊音的频谱是一个栅格的形状。</p><h3>共振峰</h3><p>一个200Hz基频的浊音，大部分的能量都分布在200Hz以及200Hz的整数倍的频率上。那么是什么决定了哪个谐波的能量高、哪个谐波的能量低呢？</p><p>由于高次谐波是由低次谐波在腔体表面碰撞反射得到的，并且碰撞反射会导致能量的衰减，但我们在看频谱图的时候发现谐波信号并不是从低到高依次衰减的。这是为什么呢？</p><p>这是因为在这个浊音的产生过程中，声源的振动信号通过声道时，声道本身也会发生共鸣，与声道共振频率相近的能量会被增强，远离声道共振频率的部分则会被衰减，从而谐波的能量就组成了一组高低起伏的形状包络，<strong>我们把这些包络中的巅峰位置叫做共振峰</strong>。</p><p>比如图4中英文单词father中的 “a” 这个音我们可以看到明显的三个共振峰，频率分别为750Hz、1100Hz、2600Hz。</p><p><img src="https://static001.geekbang.org/resource/image/98/3e/98cccc45109c9a28469c3790b8c6333e.jpg?wh=10666x6000" alt="" title="图4 共振峰示意图[br]语音学：标音、产生、声学和感知，中国社会科学出版社"></p><p>频率从低到高我们分别用F1、F2、F3等来表示第一共振峰、第二共振峰、第三共振峰等。</p><p>从图4我们可以看到，发不同的音，比如 “a、i、u” 等，共振峰的位置和峰值都是不一样的。这是因为我们之前说的声道的三个腔体随发音的不同，开合、形状都会发生变化。从而形成了不同的腔体共振频率。所以，共振峰的位置和幅度就和发音可以一一对应起来了。这其实也是语音识别背后的原理之一，即<strong>通过共振峰的位置和能量分布来识别音频代表的语音</strong>。</p><h2>语音信号的分析</h2><p>好的，我们现在对语音是怎么产生的已经能够理解了，那接下来我们分别从时域、频域这两个方面来介绍几个常用的语音分析的方法。因为窗函数常作为时域或频域实时分析的前处理步骤，所以在介绍这些语音分析方法之前，我们先介绍一下窗函数。</p><h3>窗函数</h3><p>我们分析音频时域或频域特征随时间的变化时，需要按照时间把音频截断成一个个小片段，每个小片段也就是我们说的音频帧。比如10ms的切片为1帧。</p><p>但如果直接截断信号则会导致频谱泄漏，即出现不该有的频谱分量。比如，你对一个50Hz的单频信号直接截断，可能会出现60Hz、200Hz的能量分量。因此，<strong>我们一般采用加窗，即在原有信号中乘一个两端为0的窗信号，来减少截断信号时的频谱泄漏。</strong>常用的窗函数有Haning（汉宁窗）、Hamming（汉明窗）、Blackman（布莱克曼窗）等。在时域上加窗（Haning）的过程如图5所示：</p><p><img src="https://static001.geekbang.org/resource/image/a1/ab/a1c2c0342f9b18fd826f9435825c36ab.jpg?wh=10666x5409" alt="" title="图5 时域加窗过程"></p><p>可以看到图5中加窗的过程其实就是输入信号乘以窗函数，得到了一个两边小、中间高的新信号。</p><h3>时域分析</h3><p>好了，我们已经了解了窗函数，现在我们来聊聊时域分析。在时域上我们主要介绍两个指标，短时能量和短时平均过零率。</p><p><strong>短时能量</strong></p><p>由于语音的能量随时间的变化较快，比如能量小的时候可能就是没有在说话，而能量大的地方可能是语音中重读的地方。因此，短时能量常被用来判断语音的起止位置或者韵律。短时能量分析的定义如公式1所示：</p><p>$$E_{n}=\\sum_{m=-\\infty}^{\\infty}{\\left[  x(m)w(n-m)\\right]^{2}}$$</p><p>其中，$x$代表采样点，$w$代表窗函数。<strong>第$n$个点的短时能量$E_n$就是由加窗后的采样信号的平方和来表示的。</strong>由于不涉及频谱分析，因此这里的窗可以使用简单的矩形窗。</p><p><strong>短时能量主要有以下3个方面的应用：</strong></p><ol>\n<li>可以用来区分清浊音。一般来说，清音部分的能量比浊音部分的能量要小很多。</li>\n<li>可以用来区分有声段和无声段。比如，可以设置一个能量阈值作为判断该语音段是否为静音段的条件。</li>\n<li>能量的起伏在语音识别里也被用于判断韵律（比如重读音节）的特征。</li>\n</ol><p><strong>短时平均过零率</strong></p><p>短时平均过零率，顾名思义，就是每帧内信号通过零值的次数。连续的音频信号是围绕0值上下波动的，并且表现为音频信号正负号随时间不断切换。短时平均过零率可以通过公式2和3来计算。</p><p>$$Z_{n}=1/2\\sum_{m=n}^{n+\\text{N}-1}{\\left| sgn[x(m)]-sgn[x(m-1)] \\right|}$$</p><p>$$sgn[x(n)]=\\begin{cases}1,  &amp; {x(n)&gt;= 0} \\\\-1, &amp; {x(n)&lt;0}\\end{cases}$$</p><p>其中，$\\text{N}$为一帧中包含的信号点数，$sgn$为符号函数，$x$为音频采样点。</p><p>如果是<strong>正弦信号</strong>，例如图3中的基频和谐波信号，它们的短时平均过零率，就是<strong>信号的频率除以两倍的采样频率</strong>。</p><p>短时平均过零率在一定程度上可以表示语音信号的频率信息。由于清音的频率集中的范围要高于浊音，所以浊音的过零率要低于清音，从而<strong>我们可以初步用短时平均过零率来判断清浊音</strong>。</p><p>除了判断清浊音。还可以<strong>将短时能量和短时平均过零率结合起来判断语音起止点的位置</strong>。在背景噪声较小的情况下，短时能量比较准确；但当背景噪声比较大时，短时平均过零率有较好的效果。因此，一般的音频识别系统就是通过这两个参数相结合，来判断待检测语音是否真的开始。</p><h3>频域分析</h3><p>上面我们讲了基于时域的两种语音分析方法，接下来我们来学习基于频域的两种语音分析方法：短时傅里叶变换和梅尔谱。</p><p><strong>短时傅里叶变换</strong></p><p>短时傅里叶变换（Short-time Fourier Transform）是音频频域分析最常用的方法之一，简称STFT。那它有什么作用呢？</p><p>我们在分析音频信号时经常会使用到频谱图（例如图2），<strong>那你知道这个频谱图是怎么得到的吗？</strong></p><p><strong>结合短时傅里叶变换的步骤（如图6所示），</strong>也许你就明白了：</p><ul>\n<li>首先，对时域信号加滑动窗，在把音频切成若干个短帧的同时，防止频谱泄漏（窗可以使用汉宁窗）。</li>\n<li>然后，对每一帧做快速傅里叶变换（Fast Fourier Transform，简称FFT），把时域信号转换为复数频域信号。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/f1/c4/f16aef5cbf20fb62cbae2b8abba7c9c4.jpg?wh=10666x5410" alt="" title="图6 短时傅里叶变换示意图[br]图片来源网络"></p><p>图6中的Hop Length代表滑动窗移动一次的距离，并且Overlap Length就是两个相邻滑动窗重叠的范围。</p><p>清楚了这些，我们就可以回答刚才的问题了。其实呢，<strong>我们是把短时傅里叶变换的结果对复数频域信号求模，并取对数转换成分贝（dB），然后用热力图的形式展示出来</strong>，这样就能得到图2中的频谱图。频谱图的横坐标为时间，纵坐标为频率，并且<strong>热力图中的颜色代表每个频点在当前时刻的能量大小</strong>。这样我们就可以通过频谱图来观察每个时刻的语音能量分布了。</p><p><strong>梅尔谱（Mel spectrum）</strong></p><p>上面我们通过短时傅里叶变换得到的频谱图通常也叫做声谱、线性谱或者语谱。</p><p>由于心理和听力系统的构造，其实人耳对以Hz为单位的频率并不是很敏感。比如，人类很难区分500Hz和510Hz的声音。我们平时能区分的音调都是以指数排列的。比如，我们说的高八度其实就是把原有频率乘以2。因此，用对数的频率坐标来表示可以更好地反映人的实际听感。</p><p>除此之外，人耳对不同频率声音大小的感知也是不同的。如图7所示，红线代表人耳感知到的响度和实际声压的对应关系，人耳感知的响度我们一般用phon（方）来表示。</p><p><img src="https://static001.geekbang.org/resource/image/83/14/835448cb9cf92241342b91e69b20dd14.jpg?wh=10666x5421" alt="" title="图7 人耳音量感知等响曲线[br]语音学：标音、产生、声学和感知，中国社会科学出版社"></p><p>由图7可以看到，人类在4kHz的频率对声音的响度比较敏感，而在两端的高频和低频则需要更强的声压，人类才能感知。这其实和人类的进化有关，4kHz多为猛兽的叫声能量分布范围，所以人耳对这类危险的频率较为警觉。</p><p>因此，为了结合人耳对频率的感知。需要使用对数的频率坐标，且通过分配滤波器组对频谱图的能量按照听感重新分配，于是就有了梅尔谱等表示形式。</p><p>Mel谱的计算步骤分为下面几步：</p><ul>\n<li>首先，对语音信号进行预加重（平衡高低频能量）；</li>\n<li>然后，语音信号通过STFT得到频率谱；</li>\n<li>最后，通过三角滤波器组对频率谱逐帧进行滤波。</li>\n</ul><p>三角滤波器组如图8所示。我们可以看到三角滤波器组把频率划分成了若干个频段。敏感的频段滤波器分布比较密集，而不敏感的频段比较稀疏，这样就能更好地表征人耳的实际听感。</p><p><img src="https://static001.geekbang.org/resource/image/13/fd/137a6a039ebbb746a71b9e69b9e351fd.jpg?wh=10666x4794" alt="" title="图8 梅尔谱中的三角滤波器"></p><p>梅尔谱以及对梅尔谱再进一步求倒谱系数得到的MFCC（梅尔倒谱系数），经常被用于语音识别、声音事件识别等领域。其实类似的基于人耳实际听感的表示还有<a href="https://wikimili.com/en/Bark_scale">Bark谱</a>、<a href="https://www.sciencedirect.com/science/article/abs/pii/S0141933116300461">Gamma Tone Filter</a> 等，这里就不一一赘述了。如果你有兴趣可以自行了解一下，并且有问题也可以发表在留言区。</p><h2>小结</h2><p>好了，这节课到这里就要结束了。我们先来回顾一下这节课的重点。</p><p>关于语音是如何产生的、在信号层面有何不同、语音是如何被人耳感知的这些都是这节课的重点。有了扎实的理论基础和分析方法，以后你再碰到语音算法设计、语音特征分析时就可以有针对性地进行选择了。</p><p>现在我给你简单总结一下，我们今天主要讲了哪些内容。</p><ol>\n<li>语音根据发音原理的不同可分为清音和浊音，并且根据它们在频谱图上的分布规律，我们可以从频谱上分辨清浊音。</li>\n<li>只有浊音才有基频和谐波。我们将发浊音时声带振动产生的声音叫基波，并且将基波的频率叫做基频。基频对应我们平时所说的音调，而谐波是频率倍数于基频的声波。</li>\n<li>共振峰表示发音受腔体形状影响。共振峰的位置和幅度可以和发音一一对应，从而我们可以通过共振峰的位置和能量分布来识别音频代表的语音。</li>\n<li>在做音频分析的时候需要对音频信号进行截断，而这会导致频谱泄漏。加窗可防止频谱泄漏。</li>\n<li>短时能量和短时平均过零率是时域分析的常见指标。它们可用于判断清、浊音以及语音的起止位置。</li>\n<li>频域分析常使用短时傅里叶变换和梅尔谱等方法，并且梅尔谱更能反映人耳的实际听感。</li>\n</ol><h2>思考题</h2><p>最后，给你介绍一个常用的Python音频处理工具<a href="http://librosa.org/doc/latest/index.html">librosa</a>，利用它可以快速地画出频谱图和梅尔谱。下面是示例代码，你可以尝试用自己的音频绘制一下频谱图和梅尔图，分析一下清、浊音在两个频谱上有什么不同。</p><pre><code class="language-plain">#绘制STFT\nimport numpy as np\nimport librosa\nimport matplotlib.pyplot as plt\naudio,sr=librosa.load(\'Path\',sr=48000)\nn_fft=1024\nft = np.abs(librosa.stft(y[:n_fft], hop_length = n_fft+1))\nplt.plot(ft)\nplt.title(\'Spectrum\')\nplt.xlabel(\'Frequency Bin\')\nplt.ylabel(\'Amplitude\')\n#绘制梅尔频谱\nmel_spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)\nmel_spect = librosa.power_to_db(spect, ref=np.max)librosa.display.specshow(mel_spect, y_axis=\'mel\', fmax=8000, x_axis=\'time\');\nplt.title(\'Mel Spectrogram\');\nplt.colorbar(format=\'%+2.0f dB\');\n</code></pre><p>好了，这节课就到这里。你可以把你的问题和答案写下来，分享到留言区，与我一起讨论。我们下节课再见。</p>',
        article_title: "02｜如何量化分析语音信号？",
      },
      {
        title: "03｜如何分析与处理音乐信号？",
        id: 452960,
        content:
          '<p>你好，我是建元。</p><p>上节课我们讲了语音信号分析的基础知识和方法，那么在常见的音频信号类型中，还有一种是音乐信号，这节课我们就一起来聊聊它。音乐信号受乐理、乐器发音规律、心理感知等因素的影响，与语音信号在分析和处理方法上不尽相同。而且音乐层面的分析涉及的知识领域比较多，知识容易发散。所以这节课我们会先介绍一些音乐信号的基本概念，然后从我们实时音频互动中，音乐场景遇到的案例出发，带你看看这些问题背后的原理和解决方法。</p><h2>音乐信号中的基础概念</h2><p>我们先来看看音乐信号中的一些基础概念。这里主要介绍一下常见乐器的发音原理，什么是速度和节拍，以及什么是音调和调式。</p><h3></h3><h3>常见的乐器种类和发音原理</h3><p>你还记得语音信号分析中，人的发音原理吗？音乐信号中除了人声歌唱的部分外，主要是乐器的声音，那么乐器是如何发音的，它们有什么规律吗？我们结合常见的乐器种类一起看下。</p><p>日常中常见的乐器主要有三种：打击乐器、弦乐器和管乐器。</p><p>第一种，打击乐器，比如我们常说的“敲锣打鼓”。<strong>这类乐器的发音主要是依靠打击振动</strong>。比如，鼓在鼓皮绷紧的状态下，受外力振动。振动的速度越快、力量越大、振幅越大，发出的声音就会越响亮。</p><p>打击乐器，比如鼓的音调，取决于鼓的尺寸、鼓皮的材质等。一个鼓经过调音，比如调节鼓皮的松紧程度后音调就固定了，所以如果需要同时演奏不同的音调就需要一组鼓，比如图1中的架子鼓。</p><!-- [[[read_end]]] --><p><img src="https://static001.geekbang.org/resource/image/0d/e6/0df957ce565dcb5a20f91a49a9a570e6.jpg?wh=1162x651" alt="" title="图 1 图片来源于网络"></p><p>第二种，弦乐器，包括钢琴、二胡、小提琴等乐器。<strong>这类乐器发音来源于弦的振动</strong>。不同形状的弦发出的声音特点也有所不同：若弦短而细，且绷紧，则乐器发出的音调高；若弦长而粗，且不紧，则乐器发出的音调低。</p><p>乐器的声音大小取决于弦的振幅大小。为了促使乐器能够发出洪亮的声音，一般会采用木制的共鸣箱以达到增强音量的效果。这一过程和人声浊音的发音十分相似，所以<strong>弦乐器也会有基频和谐波</strong>，<strong>弦乐器的谐波也叫做泛音</strong>。比如图2中的大提琴的声音，由于发音的基频和泛音的频率范围和人声相似，所以大提琴也被称为最像人声的乐器。</p><p><img src="https://static001.geekbang.org/resource/image/d3/cc/d30e14df8d7e732603d4b0761ca7d0cc.jpg?wh=2410x1351" alt="" title="图 2 图片来源于网络"></p><p>第三种，管乐器，包括笛子、号、萨克斯等乐器。<strong>这类乐器发音来源于空气柱振动</strong>。当吹奏管乐器时，抬起不同的手指会形成不同长度的空气柱，<strong>若空气柱越长，则音调越低。若空气柱越短，则音调越高</strong>。比如图 3 中的竖笛，越短的笛子发出的声音音调越高。</p><p><img src="https://static001.geekbang.org/resource/image/72/d5/720e2a8576d2de6d79542a4ec6dyyfd5.jpg?wh=2658x1490" alt="" title="图 3 图片来源于网络"></p><h3>速度与节拍</h3><p>知道了乐器的发音原理，接下来我们来看看乐曲是如何演奏的。</p><p><strong>一首曲子的节奏是由速度和节拍决定的。速度我们一般以BPM（beats per minutes）来表示</strong>。我们在演奏或者歌唱练习时，用的节拍器就是按照BPM来打节奏的。比如80BPM，节拍器就会每分钟发出80次“嘀嗒”声，如果是用鼓来打节奏，就每分钟打出80个间隔均匀的鼓点。</p><p>而<strong>节拍用来描述音乐中的进程的规律</strong>。在音乐中，指有一定强弱分别的一系列拍子，在每隔一定时间重复出现。若干个这样有规律的拍子我们叫一个小节，比如常见的四拍一个小节。一首乐曲的节拍是作曲时就固定的，不会改变。一首乐曲可以是由若干种节拍相结合组成的。常见的节拍有四四拍 4/4、四二拍 2/4、四一拍 1/4、四三拍 3/4、八六拍 6/8 等。</p><p>大部分流行歌曲比如老鹰乐队的《Hotel California》、周杰伦的《东风破》都是 4/4 拍，而 3/4 拍的歌曲因为给人一种跳跃的感觉，所以多被用于舞曲，比如约翰·施特劳斯的《蓝色多瑙河》。在流行乐中也有一些慢舞曲，比如朴树的《白桦林》，也为四三拍。</p><p>所以，我们可以看到<strong>乐器的演奏其实就是按照节拍规定的音符演奏顺序，然后按照指定的速度演奏出来</strong>。</p><p><strong>那乐曲中的音调和调式是如何从频率等数学的角度来分析呢？</strong>下面我们来看一下。</p><h3>音调与调式</h3><p><strong>音调</strong></p><p>基频和音调的英文都为pitch，音乐信号中音调其实也是和乐器或者人声中的基频的频率一一对应的。比如，我们给乐器调音中常说的中央C就是基频频率约为261.6Hz，唱名为do，并且它位于乐音体系的最中央的位置，因而得名。中央C在国际标准中为C4，在德国标准中为c1，为了避免混淆，我们这里统一用国际标准来做解读。</p><p>那么我们平时说的C大调、D大调和我们小时候音乐课的do\\re\\mi\\fa\\so\\la\\ti以及基频频率的关系是什么呢？</p><p>do\\re\\mi\\fa\\so\\la\\ti 是唱名，我们平时唱谱就是用这些音来把谱子唱出来的。他们和音名，也就是在音乐中包含的七个基本音调CDEFGAB一一对应。我们以钢琴中的中央音为例，唱名、音名和基频频率之间的对应如表1所示：</p><p><img src="https://static001.geekbang.org/resource/image/78/ae/78f46be932ebd4a434f083a315a464ae.png?wh=946x191" alt="" title="表1 中央C开始的大调唱名、音名和基频频率的关系"></p><p>我们可以看到C5的基频频率正好是C4的两倍，这个其实就是我们说的度的概念，即C5比C4高八度，且一个八度其实就是基频频率相差一倍。高一个八度我们就把国际标准音名里后面的数字加1。所以，比如A的音调从低到高可写为A0、A1、A2一直到A9。</p><p><strong>调式</strong></p><p>我们知道了音调是什么，音乐中C大调的意思就是基础音调为C，调式为大调。那么什么是大调、什么是小调呢？</p><p>我们刚才说的两个八度音之间基频频率是$2^{1}$的关系，而一个八度里有12个半音的音程，这12个半音是按照比值为$2^{\\frac{1}{12}}$的等比数列排布的。音程是指两个音之间的间隔。音程的单位是半音或者全音，2个半音我们就叫1个全音，由此推理可得，高一个全音就需要把基频频率乘以$2^{\\frac{2}{12}}$。</p><p>我们常说的大、小调又叫自然大、小调，都属于7律调式。7律调式说的就是这12个半音里我们只使用其中的7个。大调中，每两个音之间的音程大小，依序为全-全-半-全-全-全-半，这就是大调的组成规则，而小调中每两个音之间的音程大小，依序为全-半-全-全-半-全-全。</p><p>有了调式我们还需要再指定一个基础音调，比如C，代表这个音程规律是从哪里开始计数的，这样就可以把调式中所有的音都固定下来了。这个基础音调也叫起始音调，也就是音程间隔开始计算的那个音调，这个音调可以在一个八度里的任意一个半音位置作为开始，也就是说可以有12种不同的大调和小调。</p><p>你是不是还有点晕？没关系，让我们回过头来再看看表1。表1其实就是C大调中的一个八度，基础音调是C，那么按照大调的“心法口诀”：“全全半，全全全半”那么C4（do）和D4（re）之间相差一个全音，频率差其实就是<br>\n$$261.6*2^{\\frac{2}{12}}-261.6=32.1，$$</p><p>而E4（mi）和F4（fa）之间相差一个半音，频率差是$$329.6*2^{\\frac{1}{12}}-329.6=19.6。$$</p><p>现在的流行歌主要以自然大调和自然小调为主。但调式除了自然大调和自然小调外还有很多。 比如，中国传统的“宫、商、角、徵、羽”就是一个5律调式，分别对应自然大调中的do、re、mi、so、la等等，这里就不一一赘述了。</p><h2>音乐场景问题案例分析</h2><p>好的，有了前面的音乐信号中的基础知识，我们下面用２个案例来看看如何把之前学到的知识用到音乐场景的问题分析中来。</p><h3>案例1：客户投诉音乐教学直播时低音鼓一直听不到声音</h3><p>没有音乐知识的研发同事听到这种投诉，可能会把这个问题当成一个无声问题，从而觉得是不是软件集成哪里出了错误。而我们已经有了前面的音乐知识，看到投诉里提到了低音鼓，就会想到低音鼓是打击乐器，鼓这种乐器一个鼓只有一个音调。而且低音鼓只会发出一个音调的音，且是频率比较低的声音。那么问题就变成了频率比较低的时候听不到声音。</p><p>这个时候我们就可以有目的性地询问客户，他们的低音鼓是哪个音调的？假设客户说是A1音调的低音鼓。那么我们进一步分析出A1比A4低3个八度。也就是说，A1的基础频率是A4的基频频率除以8，也就是55Hz。然后，我们可以先从采集设备能力问一下客户，他们用的是什么设备，这里我们假设客户回答是用的iPad。接下来，我们可以查一下或者自测一下iPad的采集能力。</p><p>实际上苹果公司的iPad和iPhone的最低支持的录音频率是100Hz，也就是说，设备不支持55Hz低频声音的采集。那么这时候我们就可以得出结论，即iPad不支持低音鼓这么低频率声音的采集。因此，我们就建议客户更换一个更专业一些的录音设备，比如外接一个可以采集低频的麦克风，来解决问题。</p><p>可以看到在这个案例的解决中，我们通过更换采集设备解决了问题，而且里面低音鼓的音调理解起到了关键作用，并帮助我们定位问题，而不是在软件集成等方向上浪费时间。</p><p>通过这个问题我们还可以举一反三，解决一系列<strong>乐器基频与采集设备、编解码器<strong><strong>和</strong></strong>播放设备能力不匹配导致的问题</strong>。例如，乐器是有高频泛音的乐器，比如小提琴，而音频的编/解码器只能支持16kHz的采样率，这样高频就缺失了，在远端就会感知到部分高音的缺失，这时就需要更换一个支持采样率更高的音频编/解码器来解决问题。</p><h3>案例2：伴奏升调或者降调</h3><p>有一款K歌的App，有客户询问有没有办法调节一下伴奏，让唱不上高音的人能唱上去，或者让女生唱男生的歌。这类需求在我们线下KTV唱歌的时候也可能会遇到。歌曲在创作的时候伴奏和歌唱的调式都是固定的，有的歌曲高音部分普通人可能唱不到那么高，或者男生的歌的低音部分女生可能唱不了那么低。无论人声唱高了或者低了，伴奏的音调没变，就会出现人声和伴奏的音调不匹配的问题。听起来就会是不和谐的，或者说很容易被听出来“唱走调”。</p><p>这个时候我们就可以利用调式的概念。无论大、小调都有一个固定的音程距离，但起始音阶是可以不同的。那么如果我们把伴奏都下调一些，比如都乘以$2^{\\frac{-1}{12}}$降一个半音，这样不改变音程距离，但实现了降调，并且这样伴奏还是相同的调式，但本来唱不上去的歌就可以也低一个半音，从而就可以和伴奏匹配了。同理，女生唱男生的歌，就可以把伴奏的频率整体提高一些。比如升一个半音，这样女生就可以用比较高的音调唱男生的歌了。</p><p>有了调式的知识，我们就知道调音只有以半音为单位来调节伴奏，才能匹配到正确的调式所需的基频频率。这里还有一个问题要注意，歌曲是有指定的速度和节拍的，如果一个人唱的快了或者慢了，甚至唱多了或漏了几个音，听上去也会感觉是“走调了”，这时改变伴奏的音调并不能解决这种速度或者缺漏音导致的不和谐情况。</p><p>我们刚才提到的变调算法有很多，比如WSOLA，Phase Vocoder等，是音频音效中常见的算法，在后续的课程中我会和你详细聊一下常见的音效算法，这里就不展开了。</p><h2>小结</h2><p>好了，我们来总结一下这节课的内容。音乐中乐器主要包括打击乐器、弦乐器和管乐器，它们的发声原理各不相同。不同的乐器的发音频率、范围也不一样。音乐信号经常会是多个乐器和人声的多信号的混合信号，所以在音乐信号分析时与语音信号分析有所不同。我们通常<strong>需要一个采样率覆盖范围比较广的采集设备和一个采样率支持范围比较广的音频编/解码</strong><strong>器，</strong><strong>来保证音乐信号的频段不会丢失。</strong></p><p>音乐的速度、节拍和调式等都是在音乐创作时就确定了的，任何一个乐器或者人声没有按照指定的基频频率和出现时间就会导致音乐的不和谐，比如“走调”，从而影响听感。</p><p>在音乐场景中需要注意的问题其实还有不少是在降噪、回声消除、编/解码等音频环节出现的，比如降噪算法可能会导致有些音乐信号的失真甚至消失，而回声消除可能会导致音乐的不流畅。这些我会在今后讲音频处理模块的时候给你详细解读。</p><h2>思考题</h2><p>现在我有一个思考题留给你。</p><p>人类可以模仿一些乐器的声音，那么从发音原理上来说，有哪些乐器是人类不太可能模仿出来的呢？你可以给B-box的艺术家出出难题。</p><p>欢迎你在留言区和我分享你的思考和疑惑，你也可以把今天所学分享给身边的朋友，邀请他加入探讨，共同进步。下节课再见。</p>',
        article_title: "03｜如何分析与处理音乐信号？",
      },
      {
        title: "04｜如何评价音频质量的好与坏？",
        id: 459470,
        content:
          '<p>你好，我是建元。</p><p>音频在采集、处理、压缩、传输、播放等过程中，不可避免的会对音频的质量产生影响。这节课我们就一起来看一下，音频质量是如何进行评价的。</p><p>你是否还记得，在之前我们讲音频编码封装的时候，讲过有损编码和无损编码。对于有损编码，有的人听上去几乎和无损一致，有的人却能听出差别。再比如降噪算法，降噪算法可能会对人声产生损伤、也可能有残余噪声影响听感。因此，评判一个降噪算法的好坏也需要一套综合的音频评价体系。</p><p>其实音频的评价方法主要有两种。一种是主观评价，即组织足够数量的人来听被测音频样本，并给每个被测样本打分，最后根据测试人打分的高低来评判音频质量的好坏。<strong>主观测试是音频评价的黄金准则</strong>，这样的评价是最符合人的实际听感的。但是<strong>主观评测费时费力</strong>，在算法迭代、研发等中间过程中不一定是最经济的方案。</p><p>另一种测试方法是客观测试，即通过数学方式计算出一些音频质量评价所需要的指标，比如信噪比（SNR）、频谱差异等。然后综合这些指标去拟合一个主观分数。这样就可以通过数学计算而不是人来给出一个音频质量的评价。</p><p>但无论是主观测试还是客观测试，都需要遵守一套严格的测试评价标准，来保证音频质量评价的准确性。那么下面我们先来看看主观评价有哪些可用的方法。</p><!-- [[[read_end]]] --><h2>音频主观评价方法</h2><p>在介绍主观测试方法前，先介绍一个组织：ITU（国际电信联盟）。ITU是联合国下属的一个专门机构，负责电信、通话等相关标准的制定。其中的无线电通信组（ITU-R）和远程通信标准化组织（ITU-T）为了统一国际的音频质量评价方法制定了一系列的主、客观评价方法。按照ITU的评价方法，你的音频算法或者系统的评价结果才会比较有公信力。好了，下面就让我们以ITU主观评价方法中的MUSHRA为例来介绍一下如何做音频主观评价。</p><h3>MUSHRA</h3><p>MUSHRA（Multi-Stimulus Test with Hidden Reference and Anchor，多激励隐藏参考基准测试方法）属于ITU-R BS.1534中的推荐测试方法。它最早被用于流媒体与通信的相关编码的主观评价，现在也被广泛应用于心理声学相关研究中的音质主观评价。其测试的特点主要是<strong>在测试<strong><strong>语</strong></strong>料中混入无损音源作为参考（上限），全损音源作为锚点（下限），通过双盲听测试，对待测音源和隐藏参考音源与锚点进行主观评分。</strong></p><p>所谓<strong>“双盲”就是测试人和提供测试的人都不知道自己要听的是哪段语料</strong>，这个在测试环节中很重要。比如，如果你提前告诉测试人“你的算法会让声音中的风声不那么刺耳”之类的暗示，或者在测试的时候双号为无损语料，单号为测试语料，这样的操作都会让测试结果不具有参考性。</p><p>MUSHRA的分数是0到100分，按照从高到低的听感描述，如表1所示：</p><p><img src="https://static001.geekbang.org/resource/image/94/4e/9486d05308f3b418e76d6f8ab08dbc4e.png?wh=846x164" alt="" title="表1 MUSHRA分数代表的听感"></p><p>测试规范中有很多测试细节，这里我罗列一下。为了保证测试的有效性，主要需要注意以下４点：</p><ol>\n<li>\n<p>参考的标准音频和被测试音频间隔测试，连续重复4次；</p>\n</li>\n<li>\n<p>音频源采用15～20s；</p>\n</li>\n<li>\n<p>一次完整的测试时间不应超过15～20min；</p>\n</li>\n<li>\n<p>测试成员：专家成员最少10人，非专家20人。</p>\n</li>\n</ol><p>这里我解释一下为什么要注意这些细节。</p><ul>\n<li>首先，测试语料需要重复4次主要是为了，防止只出现一次可能会导致误判。比如，听音者一开始没有集中精神打出了偏低的分数。</li>\n<li>其次，音频源的时间控制是为了防止听音者产生听音疲劳，从而导致判断失误。</li>\n<li>然后，由于集中精神听声音的细节就和瑜伽的正念训练一样，是一件耗费心神的事情，所以不宜时间太长。因此，一般测试时间都控制在15～20min。</li>\n<li>最后，测试人员中要有音频专家，他们知道该听什么，以及该怎么听，这个可以保证测试的可重复性。但也不能都是专家，大部分应该是普通人。这是因为普通人的意见更有普适性。他们对一些明显的缺陷比如“颤音”、“漏音”比较敏感，而对一些声音的细微差异可能会给出相近的分数，然而这样更符合大众的听音习惯。如果都是专家则分数过于严苛反而偏离了人群总体的听感。</li>\n</ul><p>这里举一个用MUSHRA给不同语音合成算法打分的例子，具体如图1所示。图中的REF就是无损的音源，Anchor35就是我们说的锚点（预估只有35分）。这里还把无损音频的采样位深调为8bit，标记为8bit μ-law REF。而其它则是代表不同的语音合成算法。</p><p><img src="https://static001.geekbang.org/resource/image/b6/42/b65877f8c9286fac7bb200264f6c6d42.png?wh=1314x644" alt="" title="图1 不同语音合成方法的MUSHRA评分（Govalkar, Prachi, et al. )"></p><p>我们可以看到WNET的分数最高很接近无损音源，这代表这种算法的语音生成质量最好。锚点一般是质量很差的音频，在图1中甚至有些算法的分数比锚点还低，这代表他们的生成效果极差。这些极差的算法甚至还抬高了锚点的分数，我们可以看到Anchor35最后的MUSHRA评分在50～60分之间。</p><p>为了方便人们测试使用，GitＨub上有基于Ｗeb版的<a href="https://github.com/audiolabs/webMUSHRA">MUSHRA</a> 自动打分工具，你可以自行下载使用。</p><h3>常用的主观评价标准</h3><p>MUSHRA方法的测试面比较广，可以用于编/解码器，语音合成，甚至是耳机测评。除了MUSHRA，<strong>在ITU中还有其它一些针对不同场景的音频测评标准</strong>，比如ITU-T评价标准中的ITU-T P.800《语音质量的主观评价方法》，也就是我们常说的MOS（Mean Option Scores，平均意见分），以及ITU-T P.830《电话和宽带数字语音编码器的主观评价方法》、ITU-T P.805《对话质量的主观评价》等。ITU-R主观评价标准中的ITU-R BS.1116 《音频系统中小损伤主观评价方法》、ITU-R BS.1285 《音频系统中小损伤主观评价的预选方法》等。如果你有兴趣可以上<a href="http://www.itu.int">ITU官网</a>自行查看。</p><h2>音频客观评价方法</h2><p>主观评价的缺点是：人少了、执行不规范都会带来测试偏差。那么有没有什么客观评价的方法，可以给出可复现又贴近主观评价的结果呢？下面我就来为你介绍一下符合这些要求的客观评价方法。</p><p>客观评价主要包括有参考评价和无参考评价。所谓有参考评价就是除了测试音频以外，还需要同时给出一个参考音频做为基准，通过计算测试音频和参考音频的区别来拟合出音频的主观得分。而无参考的客观评价则不需要参考音频，直接根据音频的频谱能量分布、连续性等指标来评分。</p><h3>有参考音频质量评价</h3><p>我们先来看一下有参考的客观评价方法。在2001年，ITU-T P.862标准定义了有参考客观评价算法PESQ（Perceptual Evaluation of Speech Quality，语音质量感知评价），该算法主要用来<strong>评估窄带（8kHz 采样率）及宽带（16kHz采样率）下的编解码损伤</strong>。该算法在过去的二十年中，被广泛的应用于通信质量的评定。</p><p>随着技术的发展，PESQ的应用范围变得越来越窄，于是在2011年，P.863标准定义了一套更全面、更准确的有参考客观评价算法POLQA。相比PESQ，<strong>POLQA可评估的带宽更广，对噪声信号和延时的鲁棒性更好，其语音质量评分也更接近<strong><strong>于</strong></strong>主观的评分</strong>。</p><p>PESQ算法已经开源，而POLQA你需要购买一套专门的设备和授权才能使用。所以目前做一些日常的测试中，PESQ还是用的比较多的方法。这里我们主要介绍一下<strong>PESQ算法的基本原理</strong>。PESQ算法的处理步骤如图2所示：</p><p><img src="https://static001.geekbang.org/resource/image/6f/be/6fafa1df20f14d0b91bea9fcb6ae94be.jpg?wh=1280x720" alt="" title="图2 PESQ处理步骤"><br>\n图2中待测系统就是你的音频系统或者算法，比如一个编/解码器。为了消除系统延迟的影响，首先，将参考信号和系统处理后的信号经过相同的预处理后进行时间对齐；然后，进行听觉变换把音频信号转化为频谱信号；接着，再对能量谱逐帧进行差异处理；最后，取时间平均得到PESQ分数。如果发现有的音频片段差异特别巨大，则表明存在对齐错误，需要对没对齐的片段进行再对齐。</p><p>PESQ的分数范围在0～4.5分，一般音质比较好的编／解码器，比如64kbps比特率的OPUS编解码器，可以达到4.5分，而分数越低则代表音质越差。比如OPUS的码率降到6kbps那PESQ可能就只有不到3的分数。</p><p>值得注意的是，PESQ最多只能评价16kHz采样率的音频。如果要评价一个采样率比较高的音频信号，比如音乐信号，POLQA会比较合适。POLQA最高可以支持48kHz采样率的全带音频的客观质量评价。购买一套POLQA设备的价格都是百万级的，为了方便使用，你还可以考虑一下使用例如<a href="https://arxiv.org/pdf/2004.09584.pdf">ViSQOL</a> 等开源算法，也可以支持48kHz的音频采样率。</p><h3>无参考音频质量评价</h3><p>有的时候我们可能无法获得参考音频，比如在打网络电话时，只有接收到的经过编/解码和网络传输的音频信号，<strong>没有远端的输入信号</strong>。这时候无参考音频质量评价方法就派上用场了。<strong>不需要参考信号，仅通过对输入信号本身或参数的分析即可得到一个质量评分</strong>。比较著名的无参考客观评价方法有ITU-T P.563、ANIQUE+、E-model、ITU-T P.1201等。</p><p>其中，ITU-T P.563于2004年提出，主要是面向窄带语音的质量评估；ANIQUE+于2006年提出，也是面向窄带语音，其评分准确度据作者称超过了PESQ，不过PESQ的测量不能反应网络的延时、丢包等，并不能完美适用于如今基于互联网传输的实时互动场景。E-model于2003年提出，不同于上述两种方法，这是一个基于VoIP链路参数的损伤定量标准，不会直接基于信号域进行分析。ITU-T P.1201系列于2012年提出，对于音频部分，该标准也不对音频信号直接进行分析，而是基于网络状态和信号状态对通信质量进行评分。</p><p>其实在实际使用中由于实时音频处理过程复杂，除了编解码器和网络对音频可能造成损伤，音频的处理步骤比如降噪、回声消除等步骤，也可能对音频的质量造成影响。现有的无参考音频质量评价还不能准确地反映音频的实际质量情况，或者说使用的时候还有很多限制，比如隐私问题无法获得音频信号或者音频链路的信息指标无法准确获得。所以无参考音频质量评价还有很多有待研究的地方。</p><h2>小结</h2><p>好了，为了方便你记忆，这里我用图来总结一下音频质量评价的方法。</p><p><img src="https://static001.geekbang.org/resource/image/4c/d3/4cd71c1d367686ba0a12953e68f90dd3.png?wh=1691x481" alt=""><br>\n注意客观测试目前只能测量音频的损伤程度，比如我们平时在选择编解码器时不知道选择什么类型、多少码率，这时不妨跑一下PESQ或者VisQol看一下分数。而对于一些偏主观的测试类型，比如音乐听感、耳机效果等，则一般还是需要依靠主观测试。主观测试有时候也不需要你自己组织，你也可以采用众包等方式把测试分发出去。针对这种分发式的主观测试，最近的ITU-T P.808也有详细的测试流程和方法，如果你有兴趣，可以自行了解一下。</p><h2>思考题</h2><p>通过这节课的学习，你掌握了多少知识呢？我这里有一道思考题留给你。</p><p>如果我们想对一款在线通话App做实时通话质量打分，我们可以采用什么样的音频评价方法？</p><p>你可以把你的答案和感受写下来，分享到留言区，与我一起讨论。我们下节课再见。</p>',
        article_title: "04｜如何评价音频质量的好与坏？",
      },
    ],
  },
  {
    chapterTitle: "音频降噪",
    children: [
      {
        title: "05｜音频降噪如何对症下药？",
        id: 461590,
        content:
          '<p>你好，我是建元。今天我们来聊聊噪声的分类与常见的降噪方法。</p><p>听到噪声，很多人可能首先想到的是深夜的广场舞曲、呼啸的东北风、车水马龙的呼啸而过。但其实噪声是一个相对的概念。如果你想听的目标只有人们说话的声音，那么所有其它的声音包括音乐、 风声等其就都是噪声，而如果你想听鸟语虫鸣，那人声对你来说也是噪声。</p><p>在这里我们主要聊的是当保留目标是人声时，噪声会有哪些分类，它们有什么特点，以及我们如何更好地保留人声去除噪声。这也是音视频工程师的主要工作场景。</p><h2>噪声的分类</h2><p>从通信系统的角度来说，噪声可以分为<strong>加性噪声和乘性噪声</strong>。</p><p>加性噪声与信号之间满足加性条件，即<strong>加噪信号是由噪声和源信号相加得到的</strong>，这种情况下<strong>信号和噪声是不相关的</strong>，我们常见的自然噪声、人造的噪声如电子元器件发出的热噪声等都是这种。</p><p><strong>乘性噪声则是，噪声和信号是相关联的</strong>，比如信号的衰减、房间的混响、多普勒效应等。这类噪声往往是以信号乘积的形式出现，而且往往是从信道传输中产生，所以也叫信道噪声。<strong>我们这里主要讲的是人声与其它不相关的噪声的处理，所以主要是针对加性噪声来讲</strong>。</p><p>加性噪声的种类如果按照声源，比如风声、汽笛声、键盘敲击声等，种类则成千上万，但从降噪方法的选择角度上来说，我们可以按照噪声是否平稳，把噪声分为两类：<strong>稳态噪声和非稳态噪声</strong>。</p><!-- [[[read_end]]] --><ul>\n<li>稳态噪声：比如手机、电脑之类的设备底噪、电脑散热器的风扇声等等。<strong>它们一直存在且基本上响度、频率分布等声学特性都不随时间变化或者变换缓慢</strong>。</li>\n<li>非稳态噪声：比如开关门的声音、背景的人声、门铃声等等。<strong>这些噪声的统计特性随时间而变化</strong>。就好像你在一家餐馆里很多人在说话，还时不时掺杂着吃饭的时候碗碟碰撞的一些声音。<br>\n非稳态噪声按照是否连续又可以再分为<strong>连续性非稳态噪声和瞬态噪声</strong>，比如持续性的背景人声就是连续的噪声，而一些敲击声只会出现极其短暂的时间则为瞬态噪声。</li>\n</ul><p>稳态噪声和非稳态噪声的时域图和频域图如图1所示：</p><p><img src="https://static001.geekbang.org/resource/image/90/34/9021752f8c9e48e1110df0db0ecbc334.png?wh=864x547" alt="" title="图1 稳态噪声（左），非稳态噪声（右）"></p><p>很显然，稳态噪声由于在时间维度上没有变化，我们很容易通过对之前出现过的噪声进行建模，然后用相同的模型来对以后出现的噪声来进行抑制。而非稳态噪声尤其是瞬态噪声，则需要更多的依靠来区分其和正常语音之间的差异。如果更像是语音，则将其保留，反之则将其抑制。</p><p>值得注意的是，这些噪声往往不是单独存在的，可能你的手机底噪是一直存在的，同时你又在人声鼎沸的地铁站，那这时候稳态和瞬态噪声就都会存在。</p><h2>如何降噪</h2><p>清楚了噪声的分类后，接下来让我们看看目前有哪些常见的降噪算法，以及它们在降噪的能力上有哪些不同。这里我们重点介绍基于统计模型的实时降噪算法，这是因为在实时音频降噪处理时主要应用的就是这种算法。另外，基于机器学习的降噪是基于AI的，因此之后会单独开一讲。至于其他三种算法，这里你有个简单了解就足够了。</p><h3>降噪算法第一招：线性滤波器</h3><p>这在一些音频采集硬件的噪声处理中十分常见，因为硬件厂商知道自己的硬件噪声特性。比如由于电路设计在一些频段会有持续的电流声，这时可以采用一些比如高通滤波器来消除低频噪声、用一些陷波滤波器来消除某些频段的持续噪声。</p><p><strong>线性滤波器的处理方法算力要求十分低，但必须事先知道噪声会在哪个频段出现</strong>。所以在实际使用中一般会先做噪声频段检测，看看噪声出现在哪个或哪些频段，再设计线性滤波器或滤波器组来消除噪声。</p><h3>降噪算法第二招：谱减法</h3><p>谱减法的核心思想是先取一段非人声段音频，记录下噪声的频谱能量，然后从所有的音频频谱中减去这个噪声频谱能量。这种方法对稳态噪声比较有效果。但如果是非稳态噪声就会导致有的地方频谱减少了噪声有残留，有的地方频谱减多了人声有损伤。所以<strong>谱减法一般用于离线稳态噪声的降噪处理</strong><strong>。</strong>离线的时候可以人工对音频进行分片处理，在每一个分片中噪声可以控制成稳态的。而在实时音频处理的时候，噪声状态经常是随时间变化的，我们很难让噪声一直保持绝对稳态。</p><h3>降噪算法第三招：基于统计模型的实时降噪算法</h3><p>这类算法是实时音频降噪时最常用的算法类别。算法的思想就是利用统计的方法估算出音频频谱中每个频点所对应的噪声和语音的分量。基于统计的降噪方法其实都是针对相对平稳的噪声进行去除，且为了方便找出噪声和人声的直观统计区别，一般都需要基于两个假设。</p><p><strong>第一个假设：噪声相对于人声一定是在时域和频域上的声学统计特性都更平稳。</strong></p><p><strong>第二个假设: 所有的噪声都满足加性条件。</strong></p><p>所以基于这两个假设，我们就可以解释很多我们平时在使用这些降噪算法时所遇到的现象。比如噪声中的瞬态噪声很难被抑制，比如敲桌子的声音、键盘声之类的。再比如在一些混响比较大的房间，听不出混响，且人声的失真也比较严重。了解了以上特性之后，让我们看看常见的几种基于统计的降噪。</p><p>这里我主要介绍一下常用的分位数噪声估计和维纳滤波（Quantile Noise Estimation and Winner Filter）以及一些改进方法，比如OMLSA &amp; IMCRA（Optimally Modified Log-Spectral Amplitude Estimator and Improved Minima Controlled Recursive Averaging）。分位数噪声估计和维纳滤波这种方法是WebRTC中自带的降噪算法。</p><p>维纳滤波是根据最小均方误差MMSE准则（滤波器的输出信号与需要信号之差的均方值最小）设计的线性滤波器。我们这里用到的是实时频域维纳滤波器，目标就是求出当前帧每个频点的能量有多少占比是语音，即语音的先验信噪比（SNR）。我们把当前帧的含噪信号与噪声的信噪比叫做后验信噪比，而纯净语音信号与噪声的信噪比叫先验信噪比。</p><p>因为在实时处理时没有纯净的参考信号，所以先验信噪比通常为后验信噪比结合判决引导的方法来估算。那么根据维纳滤波的原理降噪的步骤也就变成了从动态平滑的噪声模型得到噪声信号，然后根据含噪信号和噪声模型经过维纳滤波器进行降噪。</p><p>WebRTC中的降噪流程如图2所示。</p><p><img src="https://static001.geekbang.org/resource/image/47/c4/4752d19a0e784yya8724028f77ec7cc4.jpg?wh=1280x720" alt="" title="图2 WebRTC 中分位数噪声估计和维纳滤波的降噪算法处理流程\n"></p><p>首先，做短时傅里叶变换（STFT），即对带噪信号加窗。接着，做快速傅里叶变换（FFT）再求模得到带噪信号的功率谱。然后，利用功率谱进行分位数噪声估计(Quantile Noise Estimation)、语音存在概率（Speech Presence Probability）估计、噪声更新（Noise Update）以及噪声抑制系数计算（Spectral Gain Computation）。最后，把得到的每个频点的抑制系数乘以带噪信号的频率谱得到降噪后的频率谱。再做逆短时傅里叶变换（ISTFT）即可得到降噪后的时域信号。</p><p>这里就不一一展开了，内容很多，具体每一步怎么做你可以参考Google的<a href="https://patentimages.storage.googleapis.com/a4/d9/d6/f3e557a291b982/WO2012158156A1.pdf">WIPO</a> 专利（Noise Suppression Method and Apparatus Using Multiple Feature Modeling for Speech/noise Likelihood），里面和WebRTC开源库中的代码基本可以一一对应。</p><p>我们这里主要讲一下使用分位数噪声估计和维纳滤波的降噪算法逻辑背后的思考。</p><p>基于统计的降噪最主要的是对噪声进行实时建模，这个建模基于假设一，也就是说只对稳态的噪声进行建模。噪声模型迭代不能太快，比如这里WebRTC所用的分位数噪声估计都是在时频域上进行更新，且其更新周期大概为700ms左右。从听感上来说，如果噪声发生了变化，比如突然变大了，模型可能需要约500ms～4s来收敛到新的噪声模型，在这期间我们可能会听到一些噪声的残余。</p><p>这里降噪的理念和我们之前讲的谱减法有些类似，就是利用无人声段进行噪声的估计。这样在实时处理中就需要在无人声段进行噪声模型的迭代。即更新噪声模型以适应非稳态噪声的时变性。</p><p>那么为了区分人声和非人声就需要做一个人声判别也就是我们常说的VAD（Voice Activity Detection）。这里的VAD就是利用几个人工提取的特征来进行统计得出的语音存在概率来判断的。这几个特征包括：频谱平坦度（Spectral Flatness ）， 频谱差异度（Spectral Difference），以及根据先验、后验信噪比的差异得出的似然因子LR（Likelihood Ratio）Factor。具体如图3所示：<br>\n<img src="https://static001.geekbang.org/resource/image/31/d4/313b8d2b66ff2b14c99b7b2251ac54d4.jpg?wh=1280x528" alt="" title="图3 Speech Probability的计算流程"><br>\n似然因子在频域计算log均值得到indicator 0，而根据频谱平坦度和差异度则可以分别得到indicator 1 和 2。语音概率值就是根据这几个指标的加权平均（对应图中的combination）和当前帧的似然因子来更新的。在实际计算中，当这个概率比较大时，也就是语音存在的可能性比较大，噪声模型的更新就很缓慢，反之则更新速度比较快。</p><p>噪声模型则是通过分位数噪声估计来得到。“Quantile”是英文中1/4的意思。这里也就是通过一个经验假设在噪声能量谱中，里面能量最小的1/4是稳态噪声。我们应该用这个部分来迭代更新初始噪声模型。</p><p>利用初始噪声模型以及含噪语音就可以得到频谱平坦度、频谱差异度，以及对数似然比特征，进而得到语音存在概率。有了语音存在概率，就可以更新噪声模型。有了噪声模型和含噪信号，那么根据加性假设，干净的语谱则是含噪信号减去噪声信号或者说含噪频谱乘以频谱增益。</p><p>了解了算法原理后，我们基本上就可以总结出WebRTC原生降噪算法的3个特点了：</p><ol>\n<li>由于speech probability的判断降噪在有人声的地方基本不会进行噪声模型的更新，从而不会对语音造成损伤也就是说基本不会吃字，但是如果噪声是在说话的时候发生了变化，那么噪声无法被有效消除。</li>\n<li>基于MMSE的维纳滤波器有一个弊端，那就是对于浊音谐波间的噪声可能会有残留。这也就是为什么如果观测频谱的时候在谐波之间会有噪声的能量残留，这种噪声残留会随语音出现，听上去像是给语音加了伴奏。我们通常也把这种残留叫做音乐残留。</li>\n<li>由于在低信噪比的时候，语音存在概率的判断会失效，那么就会产生比较大的语谱损伤。<br>\n现在你在使用WebRTC的时候遇到噪声残留和音乐噪声大致就能判断出原因了。那么有没有什么更好一点的方法可以把WebRTC的原生降噪改造一下呢？</li>\n</ol><p>这里简单介绍一下改进方法OMLSA&amp;IMCRA算法。</p><p>它是由Israel Cohen提出的音频降噪算法。OMLSA是对人声进行估计，通过先验无声概率及先验信噪比SNR的估计来得到有声条件概率，从而实现了对人声谱的估计。IMCRA则是通过信号的最小值跟踪，得到先验无声概率估计和先验信噪比估计，来计算得到条件有声概率，进而获取噪声谱的估计。</p><p>将OMLSA同IMCRA相结合最后相当于是功率谱中最小点的追踪。这样藏在谐波之间的音乐噪声的能量由于明显小于谐波的能量就可以被去除了，这也是为什么这一算法可以有效减少音乐噪声。</p><p>关于WebRTC原生降噪和OMLSA&amp;IMCRA降噪的对比如图4所示：</p><p><img src="https://static001.geekbang.org/resource/image/a9/6e/a9b5558f3b69875ec2f4f0d5c76b576e.png?wh=2264x820" alt="" title="图4 WebRTC原生降噪和OMLSA&amp;IMCRA降噪对比"></p><p>从上图可以看出通过OMLSA&amp;IMCRA降噪比WebRTC原生降噪得到的语谱更干净，残留噪声更少。</p><h3>降噪算法第四招：子空间算法</h3><p>子空间算法主要是针对一些已知噪声类型，量身定做一个降噪算法。其思想就是把噪声和人声投影到一个高纬度的空间，让本来不容易分离的信号变成在高纬度占据一个可分的子空间，从而可分的信号。这类算法包括非负矩阵分解和字典法建模等。</p><p>什么时候会用到这种算法呢？比如你只是要去除风噪这一种噪声，你可以用非负矩阵分解的方式单独为风噪建模，从而模型会自动消除音频中的风噪。这个在去风噪的场景下效果也是不错的。但这类方法缺点也很明显，每一种噪声都得单独建模，在噪声类型不定的情况下就很难穷尽达到好的效果。这里我们可以看一下基于非负矩阵分解的降噪。如图5所示：</p><p><img src="https://static001.geekbang.org/resource/image/36/13/36d76c0d07ce5f654e5b36676b243113.png?wh=1922x824" alt=""><br>\n<img src="https://static001.geekbang.org/resource/image/52/88/52014aeb4334dc5cbb21775a41ba6788.png?wh=1612x672" alt=""><br>\n<img src="https://static001.geekbang.org/resource/image/9d/62/9d2732c2873110bfc610160c8045e962.png?wh=1140x484" alt="" title="图5 非负矩阵分解消除鼠标声（从上到下依次为：含噪信号、非负矩阵分解降噪之后的信号和原始语音信号，Mohammadiha, Nasser, and Simon Dodo，2014.)"></p><p>从上图可以看出通过非负矩阵分解来消除鼠标声，降噪之后的那些鼠标点击产生的黑色竖条就被消除了。</p><h3>降噪算法第五招：基于机器学习的降噪</h3><p>这块最近还是比较火热的。它是通过数据训练的方式，训练人工神经网络来进行降噪。特点是噪声鲁棒性好，能兼顾稳态、非稳态甚至是瞬态噪声。比如图6中咖啡馆的噪声属于混合类型的噪声，传统降噪算法对语谱的损伤就很大，很多高频信息都丢失了，而使用AI算法在保留语谱的同时又起到了比较好的降噪效果。这块我会在下节课带你详细了解一番。</p><p><img src="https://static001.geekbang.org/resource/image/cf/3b/cf46b397df3cbdfc00b0be7463985d3b.png?wh=1564x776" alt="" title="图6 传统的降噪（WebRTC）与深度学习的降噪频谱对比（咖啡馆噪声）"></p><p>这里讲了很多噪声的类型以及如何消灭它们的方法。其实如果宏观地从整条音频链路的角度上来说，我们在实时音频中大部分采集的音频就是单通道了，这里介绍的算法也都是单通道降噪算法。如果采集的时候可以用多个麦克风或者麦克风阵列，则可以使用波束形成的方法先锁定声源方向来收音，比如选择说话人的方向来收音。这样采集来的信号，信噪比就比较高了，再通过单通道降噪就可以事半功倍。</p><h2>小结</h2><p>这里我们用图7来总结一下噪声分类和5种降噪算法及其适用范围：</p><p><img src="https://static001.geekbang.org/resource/image/05/80/055445d8682246b12348052d24ddea80.png?wh=1455x552" alt="" title="图7 课程总结"><br>\n你可以针对不同的噪声类型选择合适的降噪算法。在实际使用的时候还必须结合应用场景的需要来权衡算法的复杂度来解决主要矛盾。比如，现在很多TWS耳机都是自带降噪算法的，但是耳机上的DSP芯片的算力有限，可能就用单个子空间算法来解决一下耳机常见的风噪就可以了。</p><p>还有一些音乐场景我们可能需要牺牲一些降噪性能对降噪的幅度做限制。比如，每个频点最多只能降3dB，这样来保证音乐信号不会被削弱。所以你在实际的使用中可以从全局出发多问问自己：我应用的场景是什么？我最关心的效果是哪些？</p><h2>思考题</h2><p>现在请你思考一下，你在平时的远程会议里，或者微信电话里的噪声是什么类型的？在使用不同的实时互动App的时候，你也可以试试键盘或者鼠标的声音是否还能听得见，是不是可以根据降噪效果猜一下这些App里面用的是什么样的降噪算法？</p><p>你可以把你的答案和感受写下来，分享到留言区，与我一起讨论。我们下节课再见。</p>',
        article_title: "05｜音频降噪如何对症下药？",
      },
      {
        title: "06｜如何将AI技术运用到降噪中？",
        id: 462232,
        content:
          '<p>你好，我是建元。</p><p>上节课我们讲了噪声的分类和一些常见的传统降噪算法。传统算法通过统计的方法对噪声进行估计，并可以对稳态噪声起到比较好的降噪作用，但是<strong>在非稳态噪声和瞬态噪声等噪声类型下，传统降噪算法往往不能起到比较好的效果</strong>。</p><p>最近几年，随着AI技术的不断演进，在降噪等音频处理领域，都出现了很多基于Artificail Intelligence（AI）或者说基于人工神经网络模型的降噪算法。这些AI算法在降噪能力上较传统算法都有很大的提升。但<strong>AI降噪算法和很多其它AI算法一样，在部署的时候也会受到诸如设备算力、存储体积等条件的限制</strong>。</p><p>这节课就让我们看看AI降噪算法是如何一步步实现的，以及在实时音频互动场景中，我们如何解决AI降噪算法的部署难题。</p><h2>AI降噪模型的基础知识</h2><p>AI模型也就是我们经常听到的深度学习模型、机器学习模型或人工神经网络模型。其实AI模型的定义更为广泛，后面的这几种说法都是从不同角度描述了目前常用AI模型的特点。</p><p>AI模型的构建普遍采用<strong>大量数据训练</strong>的方式，来让模型学习到数据内隐含的信息，这就是所谓的机器学习。<strong>在降噪这个领域，模型的输入是带噪的语音信号，模型的输出是纯净的语音信号</strong>，我们通过大量的这样成对的带噪和纯净的语音数据，来训练AI模型，使其具有降噪的能力。</p><!-- [[[read_end]]] --><p>下面我们来看看常见的AI降噪模型的结构，以及AI降噪模型的训练方法。</p><h3>常见模型结构</h3><p>AI模型常采用人工神经网络来模拟人脑神经的记忆和处理信号的能力。常见的人工神经网络类型有深度神经网络（Deep Neural Network，DNN）、卷积神经网络（Convolutional Neural Network，CNN）、循环神经网络（Recurrent Neural Network，RNN）等。</p><p><strong>DNN</strong></p><p>一个典型的DNN网络结构如图1所示：</p><p><img src="https://static001.geekbang.org/resource/image/8d/7c/8d8d5c380e4bf76e1df94135f2dff87c.png?wh=1711x873" alt="图片" title="图１DNN的结构示意图"></p><p>可以看到图1中信号从输入层到输出层中间经历了n个隐藏层。每层都是线性连接，并且每层中的圆圈都代表一个神经元。举个例子，图1中隐藏层1中的第一个数$h_{1}$，就是由输入层的（$x_{1}$，$x_{2}$，$x_{3}$）的线性加权得到的，即</p><p>$$h_{1}=w(1,1)x_{1}+w(2,1)x_{2}+w(3,1)x_{3},$$</p><p>其中$w$就是第一个隐藏层的权重。在DNN的计算中，每个神经元都是前一层的加权平均。这样就可以通过一个多层的线性的网络，来对复杂的信号处理过程建模。</p><p><strong>CNN</strong></p><p>比较典型的CNN网络结构图如图2.1和2.2所示：</p><p><img src="https://static001.geekbang.org/resource/image/4e/33/4ee9a76c1821ab6d3905297418402433.png?wh=1213x713" alt="图片" title="图2.1 一维卷积结构示意图"></p><p><img src="https://static001.geekbang.org/resource/image/2b/21/2byy747cf60d3d3bfccdaa827f567f21.png?wh=1770x723" alt="图片" title="图2.2 二维卷积结构示意图（邱锡鹏 著，神经网络与深度学习）"></p><p>图2.1是CNN中的一维卷积的示意图，这里红黄绿三线代表卷积核为（-1,0,1）的卷积计算过程。每一层输出信号都是输入信号和卷积核卷积的结果。比如，输出层中第一个数为$$1\\times 1+1\\times 0+2\\times -1=-1.$$</p><p>图2.2的二维卷积也是同样的道理，只不过二维卷积中输入、输出和卷积核都是二维的。比如结果里右上角的-1，就是由标红的输入矩阵与卷积核做点乘，然后再把得到的结果做累加得到的。</p><p>CNN网络就是由多个这样的一维或者二维的卷积层串联得到的。一维的CNN网络，可以直接在一维音频信号上使用，而二维的CNN网络最早是用于图像这种二维信号的处理。但其实对音频做了STFT后，得到的频谱图也是二维的。所以在频域上做计算时，可以使用二维的CNN网络结构。</p><p><strong>RNN</strong></p><p>典型的RNN网络结构如图3所示：</p><p><img src="https://static001.geekbang.org/resource/image/c5/fc/c57444c3c568da8d2d45d7d1d2cb19fc.png?wh=1020x489" alt="图片" title="图3 RNN网络结构示意图"></p><p>可以看到RNN网络中隐藏层的每个神经元（粉色圆圈），除了和输入层的信号相关，还和隐藏层本身的状态相关。这种<strong>自回归</strong>的结构是RNN的特点。常见的的RNN网络有<a href="https://arxiv.org/pdf/1503.04069.pdf?fbclid=IwAR377Jhphz_xGSSThcqGUlAx8OJc_gU6Zwq8dABHOdS4WNOPRXA5LcHOjUg">LSTM</a>，<a href="https://arxiv.org/ftp/arxiv/papers/1701/1701.05923.pdf">GRU</a>等，由于篇幅所限，这里我们不再介绍。如果你有兴趣可以自行查看一下文献。</p><p>从物理含义来解释，如果我们把输入从左到右按照时间来排列，那么RNN的自回归特性可以感知信号在时间轴上的特征。换句话说，每个时间点的隐藏特征，除了由自身信号提取，还可以从前后时间点上的信息来得到。所以RNN在时序建模中是常见的方法。</p><h3>模型训练方法</h3><p>好的，知道了AI模型的基本结构，我们来看看AI降噪模型是如何训练的。AI模型中大量的参数，比如DNN、RNN中每个神经元的计算权值，以及CNN中的卷积核，都需要依靠训练来得到。</p><p>所谓训练就是，假设我们给予模型$y=f(x)$一个输入$x$，比如1，然后模型可能会计算出$\\hat{y}=1.5$，接着我们再告诉模型输出应该是2。这时模型的误差为0.5，而模型就会朝着输入为1时结果为2的方向调整模型的参数值。这样经过多次训练模型就可以拟合出$y$和$x$的之间的映射关系。所以我们只需要准备一组$x$、$y$作为输入和标签数据，就可以开始训练模型了。</p><p>其实AI模型训练按照是否有标签数据分类，可分为<strong>有监督的训练和无监督的训练</strong>。而降噪算法万变不离其宗，目的都是将目标信号与噪声信号分离开来。这节课我们讲的<strong>AI降噪主要是消除人声之外的所有其<strong><strong>它</strong></strong>声音</strong>。所以目标信号主要是语音信号。因此，在降噪模型的训练时，<strong>我们一般用<strong><strong>的</strong></strong>是有监督的训练方式。</strong></p><p>在训练数据里我们一般用纯净的语音作为目标或者说标签，然后用纯净语音加入一些噪声生成含噪数据，作为模型的输入。这里的噪声主要是指环境噪声。回想一下上节课的内容，环境噪声一般为加性噪声，所以在准备训练数据时，我们需要先准备一个纯净语音库和一个噪声库，而含噪的数据可以直接把纯净语音和噪声信号相加来得到。<strong>AI降噪模型训练</strong>的步骤如下：</p><ol>\n<li>通过预处理把含噪数据转换为AI模型的输入信号；</li>\n<li>通过AI降噪模型得到估计的纯净语音信号；</li>\n<li>计算模型估计和实际纯净语音信号的差距，也就是常说的Loss；</li>\n<li>Loss通过反向传播，结合梯度下降的方法更新模型的参数；</li>\n<li>重复步骤1～4直至收敛（也就是Loss下降至一个稳定的数值）。</li>\n</ol><p>其中，步骤3里所说的Loss，可以用均方差（Mean Suqared Error ，MSE)等形式。MSE如下所示：</p><p>$$\\text {MSE} =\\sum_{i=0}^{N}\\frac{{(s_{i}-\\hat{s_{i}})^2}}{N}，$$</p><p>其中$s_{i}$和$\\hat{s_{i}}$分别代表纯净语音信号和模型估计的语音信号，$N$表示信号的长度，模型训练的目标就是最小化模型预估和纯净语音信号的差距。<strong>不同Loss的设计会对AI模型的结果产生影响</strong>，而在AI降噪中，还有很多不同Loss的尝试。有兴趣，可以到[参考文献 <a href="http://www.apsipa.org/proceedings/2020/pdfs/0000711.pdf">5</a>]中详细了解。</p><p>这样，我们就通过迭代的方法，不断的训练模型，从而得到一组最佳的模型参数。在实际使用的时候，我们就可以用训练好的模型来进行降噪了。那么有了这些基础知识，让我们总结一下作为<strong>一个AI降噪模型的设计者要设计哪些东西</strong>：</p><ol>\n<li>一个AI降噪的模型，包括模型的预处理和后处理流程；</li>\n<li>一个合适的Loss，用于迭代计算模型的参数；</li>\n<li>一个合适的语音信号和噪声信号的数据库，用于模型训练。</li>\n</ol><p>其实AI降噪模型经过这些年的发展，人们已经总结出了一系列比较成熟的方法。基于时域的AI降噪算法，输入和输出都是时域的音频信号，无需任何预处理和后处理，可以实现我们常说的“端到端”处理。具有代表性的模型结构有基于RNN或CNN的TasNet [参考文献 <a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=59c4974257a4dc741c3145275eedbcf1&tn=SE_baiduxueshu_c1gjeupa&ie=utf-8&site=baike">1</a>,<a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=1f3q0gx08m160as0364500g01k643100">2</a>]等。</p><p>而更多的是基于频域信号，进行建模处理的模型。这类模型是对傅里叶变换后的频域信号进行处理，需要先把原始信号经过STFT转换为频谱，然后通过模型和含噪频谱估计出一个纯净语音的频谱，最后需要通过逆STFT作为后处理，将频谱转换为时域的音频信号。其中的代表有基于RNN的RNNoise [参考文献 <a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=cf4024fe7a244ddd949b195443a86ce1&site=xueshu_se">3</a>]，或者结合CNN和RNN的CRN模型[参考文献 <a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=1w400pe0jn5k0t20tt0b0p30km748020&site=xueshu_se">4</a>]。如果你有兴趣，可以看看附录里的文献。</p><p>了解了AI降噪模型的基础知识，接下来我们主要介绍一种最为常用且效果比较好的方法：基于频域掩码的AI降噪算法。</p><h2>基于频域掩码的AI降噪算法</h2><p>在传统降噪中，我们讲的维纳滤波等方法，都是通过计算先验信噪比，然后在频域上对每一个频谱的频点都乘以一个小于等于1的系数来抑制噪声。<strong>这些在频域上乘的系数我们统称为频域掩码</strong>。而如何计算这个频域掩码就成了解决降噪问题的关键，传统降噪是基于统计的方法来得到这个频域掩码的，而AI算法则是通过人工神经网络模型来对这个频域掩码进行建模的。</p><p><strong>基于频域掩码的AI降噪算法的主要步骤</strong>如下：</p><ol>\n<li>带噪的音频信号经过STFT得到频域信号；</li>\n<li>频域信号作为输入，利用人工神经网络得到频域掩码；</li>\n<li>将第1步中的频域信号乘以频域掩码，得到降噪后的频域信号；</li>\n<li>将降噪后的频域信号做STFT的逆变换得到纯净的语音信号。</li>\n</ol><p>值得一提的是通过STFT后得到的频域信号实际上是复数域的。对复数域的频谱取模就是我们所说的幅度谱（Magnitude Spectrum），它代表不同频点的能量分布。而对复数谱中的实部和虚部的比值求反正切（arctan），可以得到一个取值从-π到+π的相位谱（Phase Spectrum）。如果在频谱上乘以一个0～1的实数频域掩码，则修改的就是幅度谱，而相位谱或者说实部、虚部的比值并没有变化。</p><p><img src="https://static001.geekbang.org/resource/image/71/67/715dbd8471fd055d18d8d5e11efca667.png?wh=778x740" alt="图片" title="图4 基于频域掩码的AI降噪时域和频域对比图\n"></p><p>如图4所示就是一个基于频域掩码的AI降噪后的对比图，我们可以看到在频谱上噪声的部分能量被抑制了，且在降噪后能看到一个比较清晰的语谱能量分布。</p><p>STFT中相位谱没有可准确描述的物理含义，所以对相位谱的建模会比较困难，而人耳对相位不是很敏感。因此，在传统算法和大部分基于频域掩码的AI算法中，都只对幅度谱进行处理，且模型得到的纯净语音和带噪语音的相位还是一样的。虽然人们对相位的差异感知不是很明显，但不改变相位谱的频域掩码就不能做到对纯净语音的完美重建。听感上还是能听出一些不同。</p><p>近些年，人们开始用AI模型来对相位谱或者说对整个复数域的频谱整体进行建模降噪。其中具有代表性的，如微软的PHASEN和2020DNS降噪比赛中夺冠的DCCRN模型等。若你有兴趣，可以到[参考文献<a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=134f0gg0by2q04y04r4c0cs04x447781&site=xueshu_se">6</a>，<a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=1x4e00107v100xf0j01g0810em681402&site=xueshu_se">7</a>]中自行了解一下。但是在实践中，增加相位谱的恢复相比只对幅度谱做修正，需要消耗更多的算力和模型存储空间，这可能会为模型的部署造成困难。</p><h2><strong>AI降噪模型的工程部署</strong></h2><p>通常AI模型在算力和模型参数存储上，都比传统的方法要求要更高一些。现在就让我们看看在实时音频系统中部署一个AI降噪模型都需要注意些什么吧。</p><h3><strong>因果性</strong></h3><p>在RTC等实时音频的应用场景中，降噪处理需要考虑到因果性。也就是说，音频未来的信息是拿不到的。在AI降噪模型的选择中，一些双向的网络结构，比如双向的RNN模型就不能使用。但语音信号是有短时相关性的，如果一点未来的信息都不用，可能会导致模型的降噪能力下降。</p><p>我们可以采用引入一点延迟的方式来提升模型的降噪能力。比如在第i+m帧，输出第i帧的降噪信号，这样就引入了m帧长度的延迟，一般m不超过3。AI模型的输入可以往前看3帧，这种方法也就是我们常说的“look ahead”。</p><h3>AI降噪模型存储空间和算力限制</h3><p>在模型部署的时候，尤其是手机、IOT等移动端的部署，设备的算力和存储空间都会受到限制。这个需要我们在设计模型结构的时候就加以考虑。模型结构、算力复杂度（Computation Complexity）和参数量（Number of Parameters）之间的关系可参考图5：</p><p><img src="https://static001.geekbang.org/resource/image/8e/85/8eb59c0a9f65739c9cb759f0079b0985.png?wh=958x448" alt="图片" title="图5 不同模型结构的算力复杂度和参数量分布"></p><p>在图5中我们可以看到CNN的参数量最小，这是因为<strong>CNN的卷积核是可以复用计算的</strong>。一般基于纯卷积的模型，它的参数量会比较小，而RNN和DNN本质上都是线性层的计算，所以参数量会比较大。因此，在为移动端等存储空间小的设备设计算法时，会尽量选择CNN，或者CNN结合其它结构的形式来压缩参数量。</p><p>另一方面，我们也可以<strong>通过参数量化的方式来对模型进行压缩</strong>。比如，采用int 8bit的形式对本来float 32bit 格式的参数进行量化。注意，参数量化会对模型的精度产生损伤。对于卷积这种可复用的模型结构就不适合做量化，而RNN、DNN等结构做量化时对精度的损失就没有那么敏感。</p><p>在算力限制方面，我们可以从模型的输入特征着手。比如采用比较小的模型输入，如在RNNoise中就是采用BFCC这种压缩后的频谱特征作为输入，这样输入信号小了，计算量也就降下来了。另外，刚才说的量化对计算速度也会产生影响。在计算芯片支持不同精度的计算的情况下，量化后的计算速度会更快。模型计算时还可以通过对模型参数和输入数据，按照内存连续读取最优的方式进行重排，来进行计算加速。</p><p>我们上面说的量化、加速计算等过程除了自己一个个去完善外，我们在工程部署模型的时候也可以使用一些现成的工具，能帮助我们加速AI模型的部署。下表罗列了一些可以使用的AI模型部署工具：</p><p><img src="https://static001.geekbang.org/resource/image/77/33/77d11393e80c32e273d4b016d06bfe33.png?wh=623x283" alt="" title="表1 常用AI模型部署工具"></p><h2>小结</h2><p>好的，让我们来总结一下这节课的内容。AI模型常用的结构包括DNN、CNN和RNN等。AI降噪模型在结构设计时，可以选择其中一种，也可以把这些结构组合使用。AI降噪模型一般采用有监督的训练方式，并以带噪语音作为模型的输入、纯净语音作为训练的目标。利用反向传播结合梯度下降的方法不断提升模型预估和纯净语音的相似程度。这个相似程度我们一般用，例如MSE等形式的Loss来表示，并且Loss越小，模型得到的结果就越接近于纯净语音。</p><p>和传统降噪类似，基于频域掩码的AI降噪模型是目前最为常用的AI降噪设计。纯净语音频谱的获得，需要对幅度谱和相位谱都进行修正。但如果是在移动端部署AI降噪模型，受算力影响，基于幅度谱的AI降噪模型可能是最好的选择。</p><p>在实时音频信号系统中，降噪模型需要考虑到模型的因果性。在移动端部署时，由于算力和存储空间受到限制，我们需要通过对模型的输入进行降维、模型参数进行量化等操作来进行设备适配。当然我们也可以通过一些现成的工具来快速实现AI降噪模型的部署。</p><p>在实践中，如果你要自己训练一个AI降噪模型，那么数据库（语音、噪声）是不可少的。正好在最近的<a href="https://github.com/microsoft/DNS-Challenge">DNS challenge</a>的降噪比赛里，主办方为我们整理了不少语音、噪声等数据库，有兴趣可自行了解一下。</p><h2>思考题</h2><p>其实这里AI降噪的模型就是从音频中提取人声部分，如果除了人声之外还想把音乐也保留，那么我们应该怎么设计AI模型的输入和输出呢？</p><p>你可以把你的答案和感受写下来，分享到留言区，与我一起讨论。我们下节课再见。</p><h2>参考文献</h2><p><a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=59c4974257a4dc741c3145275eedbcf1&tn=SE_baiduxueshu_c1gjeupa&ie=utf-8&site=baike">1、Luo Y, Mesgarani N. Tasnet: time-domain audio separation network for real-time, single-channel speech separation[C]//2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018: 696-700.</a></p><p><a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=1f3q0gx08m160as0364500g01k643100">2、Luo Y, Mesgarani N. Conv-tasnet: Surpassing ideal time–frequency magnitude maskingfor speech separation[J]. IEEE/ACM transactions on audio, speech, and language processing, 2019, 27(8): 1256-1266.</a></p><p><a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=cf4024fe7a244ddd949b195443a86ce1&site=xueshu_se">3、Valin J M. A hybrid DSP/deep learning approach to real-time full-band speech enhancement[C]//2018 IEEE 20th international workshop on multimedia signal processing (MMSP). IEEE, 2018: 1-5.</a></p><p><a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=1w400pe0jn5k0t20tt0b0p30km748020&site=xueshu_se">4、Strake M, Defraene B, Fluyt K, et al. Fully convolutional recurrent networks for speech enhancement[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 6674-6678.</a></p><p><a href="http://www.apsipa.org/proceedings/2020/pdfs/0000711.pdf">5、Ma C, Li D, Jia X. Optimal scale-invariant signal-to-noise ratio and curriculum learning for monaural multi-speaker speech separation in noisy environment[C]//2020 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). IEEE, 2020: 711-715.</a></p><p><a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=134f0gg0by2q04y04r4c0cs04x447781&site=xueshu_se">6、Yin D, Luo C, Xiong Z, et al. PHASEN: A phase-and-harmonics-aware speech enhancement network[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(05): 9458-9465.</a></p><p><a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=1x4e00107v100xf0j01g0810em681402&site=xueshu_se">7、Hu Y, Liu Y, Lv S, et al. DCCRN: Deep complex convolution recurrent network for phase-aware speech enhancement[J]. arXiv preprint arXiv:2008.00264, 2020.</a></p>',
        article_title: "06｜如何将AI技术运用到降噪中？",
      },
    ],
  },
  {
    chapterTitle: "回声消除",
    children: [
      {
        title: "07｜如何通过算法自动快速地消除回声？",
        id: 463779,
        content:
          '<p>你好，我是建元。</p><p>前面几节课，我们讲了实时音频前处理中的降噪算法。从这节课开始，我们将会看看语音交互或者说音频通信领域的另一项不可或缺的技术：声学回声消除（Acoustic Echo Cancellation, 简称AEC)。</p><p>回声消除算法在实时音频互动链路中和很多其它模块以及硬件都会有耦合。这节课我们主要看看实时音频互动链路中回声是如何产生的以及回声消除算法的基本原理。</p><p>需要注意的是，这节课涉及到的公式比较多。不过不用担心，只要你理解了原理，就很容易能明白公式的含义，从而能够记住公式的定义。</p><h2>回声产生的原因</h2><p>回声是如何产生的呢？我们可以通过下面的图来看一下，这是一个出现回声的经典场景。</p><p><img src="https://static001.geekbang.org/resource/image/0b/9b/0b5122405b7baeef466839555b1a109b.jpg?wh=1280x592" alt="图片" title="图1 回声产生的原理"></p><p>图1中B端的人在说话，说话的声音会被B端的麦克风采集。麦克风采集到的语音信号转为数字信号后经过网络传输到A端，在A端的扬声器会把收到的语音信号转成声波播放出来，同时A端的麦克风又会把扬声器的声音采集回来，通过网络又传回给了B端。这时<strong>B端的人就会听到自己发出去的声音，我们把这个声音就叫做回声</strong>。</p><p>在音频实时互动的场景里，回声会严重影响通话体验，自己的声音不断被打断，而且对端的声音可能会和回声混在一起，这样会显著降低语音的可懂度。那么相对地，AEC的任务就是为了消除这个回声。在AB端互通的时候，我们需要使用AEC，在A端把麦克风采集到的信号中和B端相关的声音去掉，只保留A端的音源的声音发给B端。</p><!-- [[[read_end]]] --><p><strong>那么是不是把A端麦克风采集到的信号减去A端扬声器中的信号就能把回声消除了呢？</strong>我们先来看看A端的回声消除的基本原理，再回过头来解答这个问题。</p><h2>回声消除的基本原理</h2><p>在近端接收到的远端的声音信号我们把它叫做参考信号$x(n)$。在经过扬声器播放、空气传播、房间墙体反射、麦克风采集后，参考信号不可避免地会产生很多变化。我们把这个变化用数学的方式来表达就叫做<strong>回声路径的传递函数</strong>，一般记作$f$。那么近端麦克风采集的<strong>回声信号</strong>$echo(n)$可以用公式1来表示：</p><p>$$echo(n)=f(x(n)) \\text{  公式1}$$</p><p>近端除了回声信号还有近端自己的声音，比如近端人说话的声音。那么<strong>近端麦克风收到的信号$z(n)$其实是近端声音$y(n)$和回声信号之和</strong>，如公式2所示：</p><p>$$z(n)=echo(n)+y(n)\\text{  公式2}$$</p><p>而<strong>回声消除算法的目的就是通过算法估计出回声路径的传递函数</strong>$f$，我们把算法估计出的传递函数定义为$f’$,那么经过回声消除后得到的近端信号$z’(n)$为公式3所示：</p><p>$$z’(n)=z(n)-f’(x(n))\\text{  公式3}$$</p><p>将公式1和公式2代入公式3，我们可以得到公式4：</p><p>$$z’(n)=f(x(n))-f’(x(n))+y(n)\\text{  公式4}$$</p><p>我们看到如果估计的传递函数$f’$和真实的传递函数$f$是一致的，那么回声就被完美消除了。但<strong>在真实的场景中传递函数的估计是一件比较困难的事情。</strong></p><p>这是因为AEC算法需要面对复杂的、时变的声学环境。比如，扬声器和麦克风的播放失真、采集失真会给声学信号带来很多非线性的变化，并且设备、系统调度的不稳定可能造成回声和远端接收信号的延迟抖动。同时诸如房间的混响、设备所处位置的变化，都会带来回声路径的变化。</p><p>因此，AEC算法必须能够快速地自适应地去估算出这些回声路径的变化。如果估计不准，就会导致回声泄漏或者近端声音被压制，甚至造成丢字、卡顿等现象，从而严重影响实时音频互动的质量。所以回到之前的问题，我们知道<strong>回声消除是做减法，但又不是直接相减就能解决。</strong></p><p>那么我们是<strong>如何让算法自动快速地进行回声消除呢</strong>？AEC技术经过半个多世纪的发展，在不断的实践中已经摸索出一套<strong>以自适应滤波为基础的回声消除方法</strong>。<strong>自适应滤波的核心思想</strong>就是用实时更新的滤波器的系数来模拟真实场景的回声路径，然后结合远端信号来估计出回声信号，再从近端采集的混合信号中减去估计的回声，从而达到消除回声的目的。</p><h2>自适应滤波器</h2><p>接下来我们就先来看看自适应滤波的基本原理。</p><h3>维纳滤波</h3><p>在一个相对稳定的声学环境中，回声路径中的延迟和房间的混响、音量大小的变化其实都可以看作是对远端信号做了一系列的线性变化。这种线性变化我们可以用一个线性离散的FIR线性滤波器来表示，公式1就变成了公式5：</p><p>$$echo’(n)=\\sum_{k=0}^{\\infty}{w_{k}}x(n-k),n=1,2,3,…\\text{  公式5}$$</p><p>其中$w_{k}$代表第$k$个滤波器系数。如果在近端除了回声信号没有别的声音的时候，那么其实接收到的信号就是回声信号，即$z(n)=echo(n)$。这种情况我们一般叫做“<strong>单讲</strong>”。在这种情况下，我们的回声估计误差$e(n)$可用公式6表示：</p><p>$$e(n)=echo(n)-echo’(n)\\text{  公式6}$$</p><p>你还记得之前降噪讲到过的降噪算法第三招中的<a href="https://time.geekbang.org/column/article/461590">维纳滤波</a>么？其实维纳滤波就是以估计误差e(n)的最小平方作为最优解的线性滤波器。也就是通过计算最小均方差（Mean Square Error，简称MSE）来求取滤波器系数$\\mathbf{W}$（$w_{k}\\in \\mathbf{W}$）。公式7为求解最小MSE的代价函数。</p><p>$$\\mathbf{J}=\\mathbf{E}[e^{2}(n)]\\text{  公式7}$$</p><p>我们知道想要让函数值最小，其实就是让函数的全微分等于0。其求解过程可以用维纳-霍夫方程来表示：</p><p>$$\\mathbf{W}=\\mathbf{R}^{-1}\\mathbf{P}\\text{  公式8}$$</p><p>其中，$\\mathbf R$是参考信号$x(n)$序列的相关矩阵，$\\mathbf{P}$是参考信号和回声信号$echo(n)$的互相关矢量。这样滤波器的系数似乎就可以得到了。但是你试想一下，假设音频的采样率是48kHz，如果只取1秒的信号来求解，那么$\\mathbf R$矩阵的维度就是48000乘以48000。</p><p>显然要实时求一个这么大的矩阵的逆矩阵，算力是不可能支持实时计算的。我们把这种直接求得的解叫做维纳解，虽然它是最准确的，但是计算量过于庞大，而且当回声路径变化的时候我们需要重新计算维纳解。所以很显然<strong>维纳解并不适合在实时音频互动中使用</strong>。</p><p><strong>那么有什么办法能实时求解滤波器系数呢？<strong>其实自适应滤波器的核心思想是在面对回声路径不断变化的场景，比如移动电话等时，我们可以</strong>使用梯度下降法来迭代的计算滤波器系数</strong>。</p><p>在计算代价函数的时候我们让$\\mathbf{W}$系数朝着梯度相反的方向或者说朝着减少代价函数的方向移动。随着迭代次数的增加，$\\mathbf{W}$会逐渐的向维纳解收敛。这样当回声路径发生变化的时候，$\\mathbf{W}$就会重新收敛，从而我们就可以实时的追踪回声路径的变化了。</p><p>那么这个迭代计算具体是怎么实现的呢？下面我们就通过两个算法来看一下迭代计算的过程。</p><h3>LMS、NLSM算法</h3><p><strong>最小均方算法LMS（Least Mean Square）是最早提出，也是最基础的自适应滤波方法。</strong>它的基本原理可以表示为公式9：</p><p>$$\\mathbf{W}(n+1)=\\mathbf{W}(n)+\\mu \\mathbf{X}(n)\\mathbf{e}(n)\\text{  公式9}$$</p><p>其中，$\\mathbf{W}(n)$代表第n次迭代时的滤波器的系数向量，$\\mathbf{X}(n)$是第n次迭代的输入向量，$\\mathbf{e}(n)$是第n次迭代的误差，$\\mu$是步长因子。我们可以看到步长因子$\\mu$决定了滤波器系数的收敛速度，且$\\mu$越大收敛越快。</p><p>最小均方算法的梯度下降是随机的，随着迭代次数的增加它会不断逼近维纳解。但是我们看到公式9里梯度下降也会受到输入向量$\\mathbf{X}(n)$大小的影响。也就是说，如果远端信号音量比较小，那这时系数向量的收敛速度会变得很慢；反过来，$\\mathbf{X}(n)$很大的时候会导致梯度放大，从而系数向量的收敛变快。</p><p><strong>那么怎么解决这个音量变化带来的收敛波动问题呢？</strong></p><p>其实我们可以<strong>通过$\\mathbf{X}(n)$的大小来动态调节步长因子，这样就可以把$\\mathbf{X}(n)$进行归一化</strong>。这就是NLMS算法的由来。<strong>NLMS算法的迭代步骤</strong>如公式10、11所示：</p><p>$$\\mathbf{W}(n+1)=\\mathbf{W}(n)+\\mu(n) \\mathbf{X}(n)\\mathbf{e}(n)\\text{  公式10}$$</p><p>$$\\mu(n)=\\frac{\\tilde{\\mu}}{||\\mathbf{X}(n)||^2+\\delta}\\text{  公式11}$$</p><p>其中，$\\tilde{\\mu}$是一个常量，取值范围在0～2，$\\delta$为一个大于0的常数，主要是为了防止$\\mathbf{X}(n)$过小导致的梯度爆炸。<strong>NLMS相对于LMS通过归一化的方式提升了算法的收敛速度。</strong>目前NLMS算法已经成为AEC算法中最常用的算法之一。</p><h2>线性滤波器的挑战和解决方法</h2><p>那么只有NLMS是不是就足够了呢？其实这里面还有三个很重要的问题没有解决。下面我就来简单介绍一下这三个问题。</p><h3>延迟估计</h3><p><strong>第一个问题是回声延迟</strong>。公式9～11中$\\mathbf{X}(n)$是一段有限长度的输入信号，这个长度也就是我们常说的滤波器的感知长度。如果实际回声信号的传递路径很长，比如有很大的延迟和混响，那么我们就需要用一个很长的$\\mathbf{X}(n)$作为输入才能估计出回声信号的传递函数。然而一个感知长度很大的滤波器需要的算力也会随之增加，这样就会对AEC的实时性造成挑战。</p><p>为了解决这个问题，最先想到的就是把延迟进行单独计算。我们可以看到假设回声信号的延迟为$dn$，那么在公式5里延迟的表示就是$w_{k}=0\\text{，}k\\in[0,dn-1]$。如果我们能够把延迟估计出来，那么权重为0的系数就不需要放到NLMS里去估计了，那么整体的算力就可以降下来。同时有了延迟估计，NLMS只需要估计后面非0部分的权重，从而收敛速度也可以变快。</p><p>延迟估计的方法也比较简单，其实就是移动远端信号的起始位置，然后和回声信号计算互相关性，并找到互相关最大的位置。这个位置就是我们要的延迟。</p><h3>双讲检测</h3><p><strong>第二个问题是双讲</strong><strong>。</strong>前面讲维纳滤波的时候讲到了“单讲”。所谓“双讲”，就是远端和近端同时说话或者说两侧都有明显的声音。那在这个时候麦克风采集的信号除了回声还有混入了近端的声音。又NLMS是依赖于回声信号来进行估计的，而这时如果用麦克风采集的信号作为回声信号，就会导致滤波器无法收敛到正确的位置，从而产生回声泄漏或近端声音被损伤。</p><p>因此，我们一般会利用远端和近端信号先做一个简单的判断，此时是单讲还是双讲状态。如果是单讲，那么滤波器系数照常迭代更新；如果是双讲，则需要通过调节步长因子等方法停止或者减缓滤波器的更新。双讲检测的方法主要是结合能量和远、近端信号的相干性来做一个判断。如果远端和近端能量都比较高但是相干性却不强，那么就说明远端和近端都有声音，也就是双讲的状态。</p><p>这里你可以思考一下，我们在现实生活中可能经常会碰到的一个现象：如果和对方打网络电话的时候，我们从一个房间走到另一个房间，比如从会议室走到走廊，对面反馈说听到了回声。</p><p>这其实就和AEC的双讲时的策略有关，如果你和对端同时说话恰巧在此刻你换了个地方，也就是回声路径发生了改变。但由于是双讲的状态，滤波器没有及时更新，这时候就会漏回声。所以双讲检测可以防止滤波器发散。但这其实也并不是一个完美的解决方案，可能还会导致回声泄漏。只是这种双讲时，恰巧换房间的情况不是那么常见，所以双讲检测依然是回声策略中常见的调整依据。</p><h3>非线性</h3><p>第三个问题是我们看到NLMS等算法中实际上估计的是一个线性的滤波器。但是我们之前有讲到<strong>扬声器、麦克风等都可能会导致一些非线性的变换。</strong>那么这时线性滤波器可能就无法处理了。一般来说一些廉价或者说声学特性比较差的设备导致的非线性失真比较多，所以出现回声的概率也更大。</p><p>在实时音频互动刚开始的时候，其实大部分厂商都还是只有线性的回声消除。但现在我们一般会在线性回声处理之后再集联一个非线性处理，来解决这些线形处理后的残留回声。非线性建模需要兼顾不同设备、环境是一件很有挑战的事情。</p><p>除了传统算法，最近几年也有很多通过机器学习的方式来解决非线性的方案，并起到了比较好的效果。究其复杂性，非线性这块我们将会在下一讲中再继续展开来聊聊。</p><h2>小结</h2><p>好的，我们这里总结一下。由于采集和播放设备的耦合，在实时音频互动领域，回声消除是实时音频链路中重要的一环。常见的回声消除流程包括双讲检测、延迟估计、线性回声消除、非线性回声消除等步骤。这里可以用一个流程图（图2）来总结一下，帮助你整体理解AEC的算法过程。<br>\n<img src="https://static001.geekbang.org/resource/image/18/7a/185e729081ef73b91246c1695e32357a.jpg?wh=1280x484" alt="图片" title="图2 回声消除的基本步骤"></p><p>回声消除发展了几十年，依然还是一个比较热门的研究领域。究其原因还是因为它的复杂性，设备、环境、工程部署的实时性甚至是其它的音频模块都可能会对回声消除的效果产生影响。我们一般把回声消除模块放在紧挨着音频采集模块的位置。也就是说，做完了AEC再做降噪、增益调整等其它的音频模块。这样可以尽量减少音频处理对回声路径的复杂性的增加。</p><p>回声消除算法其实是在已知一个音源信号的条件下，在多音源混合的音频中消除这一音源。所以有的时候<strong>回声消除也被用来做一些音源分离的事情</strong>。比如一首歌你已经有伴奏的情况下，对人声和伴奏混合在一起的歌曲，用回声消除就可以提取到清唱（也就是没有伴奏的纯人声）。</p><h2>思考题</h2><p>有的时候设备或者App在使用过程中还是会频繁地出现回声泄漏，但是带上耳机似乎大部分回声问题就可以解决，这背后的原理是什么呢？</p><p>你可以把你的答案和疑惑写下来，分享到留言区，与我一起讨论。我们下节课再见。</p>',
        article_title: "07｜如何通过算法自动快速地消除回声？",
      },
      {
        title: "08｜回声消除算法实践指南",
        id: 464464,
        content:
          '<p>你好，我是建元。</p><p>上节课我们学习了回声消除算法的基本原理。我们看到，回声消除会受到声学环境、采集播放设备等多种因素的影响。因此，要想实现一个鲁棒、高效的回声消除算法是一件比较有挑战的事情。而在实际的音频实时互动场景中，回声问题可能也是我们碰到的最多的问题之一。</p><p>值得注意的是，音频处理往往是一环套一环的链路式的处理结构，回声消除作为音频前处理链路的一环很可能会对整体的音频体验产生影响。所以这节课我会先带你从整体上，了解一下实时音频互动系统的链路是怎么搭建的。然后我们再从几个案例出发，看看我们是如何改进回声消除算法，以及和其它模块配合，来整体提升实时音频的质量的。</p><h2>实时音频处理链路</h2><p>从贝尔实验室发明电话后，实时音频处理的技术不断发展，至今基于 VoIP（Voice over Internet Protocol，基于IP的语音传输）的实时音频互动已经慢慢成为人们日常音频在线交流的主流方法。图1展示的就是这样一个“现代版”的基于VoIP的实时音频处理链路。</p><p><img src="https://static001.geekbang.org/resource/image/ee/20/ee9809a1f5cf82fb1e8dc4fddf89cb20.jpg?wh=1280x372" alt="" title="图1 实时音频处理链路"></p><p>实时音频可以分为上行链路和下行链路两个部分，其中<strong>上行链路</strong>中的音频处理步骤可以对应图1中的上半部分。我们可以看到，在采集模块（Recording）之后就是我们常说的<strong>“3A”处理，包括了AEC（回声消除）、ANS（降噪）、AGC（音频增益控制）这3个部分。</strong>之后在Voice Beautifier and Effects（美声音效）模块我们可能会对人声做一些美化或者加入一些音效，来提升音频的听感。最后把音频进行编码（Encode）传输，至此上行链路也就结束了。</p><!-- [[[read_end]]] --><p><strong>下行链路</strong>主要的流程是先从网络中获取音频包进行解码（Decode）。由于网路传输过程中可能会出现网络抖动、丢包等现象，而下行链路中的NetEQ模块就是用来解决这些网络带来的问题。随后在播放端可能会需要一些个性化的空间音频渲染（Spartial Audio Rendering ），把音频转换成符合空间听感的多声道音频。最后通过播放模块（Playback）将音频通过播放设备播放出来。</p><p>好的，了解了实时音频的处理步骤，我们再聚焦下AEC的位置。AEC的位置放在紧挨着采集模块的地方。上节课我们介绍过<strong>AEC需要对回声路径做估计，如果有别的模块放在AEC之前就会导致回声路径无法收敛到正确的位置</strong>。比如将AGC放在AEC之前，那么AGC模块导致的音量波动就可能导致回声信号忽大忽小，从而AEC中滤波器的系数就很难收敛了。但这样的顺序安排是否就没问题了呢？</p><p>在链路式的处理方法中上游的算法会对下游的算法产生影响。AEC在回声消除的时候如果对近端的声音也造成了损伤，比如，近端的部分噪声被当做回声消除了。之前降噪的部分我们讲过降噪模块由于也需要输入信号中的噪声来做噪声估计，如果输入信号中的噪声不是连续的（有部分被AEC干掉了），那降噪模块可能就会估计出一个偏小的噪声。这种情况下，回声虽然没有泄漏，但噪声的残余却增加了。</p><p>另外，回声消除的远端信号实际上需要的是扬声器播放的信号，也就是说需要在靠近Playback模块之前获取。如果信号在送往扬声器之后被系统再处理一遍（譬如动态范围控制，简称 DRC），就会干扰到AEC的效果。</p><p>所以音频的体验是一个需要整体考虑的问题，我们在音频模块的改进和修改的时候一定要对音频链路有一个整体的认识和理解，不然很可能会出现一些意想不到的体验问题。</p><h2>回声消除算法案例分析</h2><p>下面我们就结合2个常见的案例来分析分析，了解一下回声消除算法可能出现的问题以及回声消除算法的一些改进方向。</p><h3>案例1：混响</h3><p>现在办公室的房间里有很多周围是玻璃墙的会议室，而在这些房间里回声泄漏的现象出现的概率很高。这是为什么呢？这里需要一点声学知识，即光滑且表面坚硬的墙壁，声波在其表面反射后能量衰减较小。也就是说声波可以在房间墙面多次反射，而能量依旧不会完全衰减。那这些<strong>不断反射的声音就形成了一个持续时间很长的混响</strong>，也就是所谓的“余音绕梁”。</p><p>我们知道，在回声消除中自适应滤波器中滤波器的长度是固定的。如果混响持续的时间超过了滤波器的长度，那么回声路径就不能被完整估计出来，从而就会导致回声泄漏。这种情况下，你可能会想，如果我们发现这些玻璃房间的混响时间太长，是不是把自适应滤波器的长度拉长就可以解决问题了呢？回答这个问题之前，我们先看一下下面的混响衰减曲线图。</p><p><img src="https://static001.geekbang.org/resource/image/23/0c/234fc7270ec0447e7de6a00dcc11be0c.jpg?wh=1280x720" alt="" title="图2 混响衰减曲线图"></p><p>如图2所示，<strong>混响时间我们一般用RT60来表示</strong>，也就是房间混响（Reverberation Time）衰减60dB所需要的时间。普通房间的混响的RT60大约在50～200ms左右，但有的玻璃房间或者大房间的RT60可能高达200ms甚至更多。假设我们音频的采样率是16kHz，也就是说我们的自适应滤波器至少需要3200的长度，如果我们直接使用NLMS来计算，那可能实时性就没有办法得到满足。</p><p>为了克服滤波器长度太长造成的算力过大的问题，我们一般用分块频域自适应滤波器（Partitioned Block Frequency Domain Adaptive Filter， 简称PBFDAF）来解决。<strong>自适应滤波器耗时最长的就是卷积计算</strong>。而<strong>卷积计算是可以采用FFT来进行加速计算</strong>的。</p><p><strong>PBFDAF的思想</strong>就是先把滤波器的权重和输入向量都分成等长的多块，然后通过FFT变换到频域来进行频域滤波，之后再通过IFFT得到滤波后的时域信号，最后把所有小块的结果叠加起来就可以得到滤波器的输出向量了。</p><p>卷积的算力是$O(n^2)$，而FFT的算力是$O(nlog(n))$，这样就实现了卷积计算的加速。实际上现在主流的开源算法像<a href="https://webrtc.org.cn">WebRTC</a> 和<a href="https://www.speex.org">Speex</a>，在计算NLMS时都是采用类似这样的频域计算来对NLMS进行加速的。</p><p>好的，回到我们之前的问题，即混响情况下，如果滤波器长度不够就会导致回声泄漏，但是自适应滤波器的长度太长也会导致收敛缓慢。就算是使用了PBFDAF做加速，滤波器的长度也不能随意设置。因此，在实际中我们可以先估算一下场景中RT60的分布。</p><p>例如，一般办公场景中正常办公室RT60很少超过200ms，但可能会经常出现需要快速收敛的场景，比如房间的切换，那么滤波器可以短一些。但一些课程直播或者娱乐场景中场地有很长的RT60时间，但场景一般不会切换，这时则可以把滤波器长度调整得稍微长一些。</p><h3>案例2：非线性</h3><p>Speex作为较早开源的实时音频库，有不少实时音频系统都在使用它。但我们在实际使用的过程中会发现，<strong>Speex在一些采集播放设备的失真比较大的时候效果就比较差了</strong>。</p><p>在上一讲中我们知道NLMS其实只能解决线性部分的变化，而扬声器、麦克风导致的非线性变化则可能需要非线性处理（Non-linear Process，NLP）来帮助解决。而<strong>Speex并没有NLP模块来处理这种非线性的回声残留</strong>，这也正是Speex的效果在不同设备上表现不稳定的原因。</p><p>相比较于Speex这种主要为DSP设备服务的算法，WebRTC中的AEC由于考虑到要适配不同的设备终端，所以相比于Speex，其主要增加了延迟估计和非线性处理这两个步骤来提高AEC算法的鲁棒性。那么WebRTC中AEC算法里的非线性处理是怎么做的呢？</p><p><img src="https://static001.geekbang.org/resource/image/6a/e0/6ae66aca0a6108ff68c68856b49811e0.png?wh=623x849" alt="图片" title="图3 WebRTC中的NLP处理（实时语音处理实践指南 中国工信出版社）"></p><p>WebRTC AEC代码中的NLP的做法如图3所示，主要是利用信号之间的相干性来做回声抑制的判断。这里的相干性主要是指近端信号$d(n)$和残差信号$e(n)$之间的相干性$c_{de}$，以及近端信号$d(n)$和远端信号$x(n)$之间的相干性$c_{dx}$。</p><p>其实这很容易理解：如果$c_{de}$越大，则代表近端信号和残差信号相似度高，也就是说回声很小，那么就越不需要去抑制；而如果$c_{dx}$越大，则代表近端信号和远端信号很相近，需要更多地抑制回声。</p><p>注意，这里由于非线性部分的回声可能是出现在某些频段上的，所以需要先把频带分解成多个的子带，且在每个子带上都做一个相干性的判断，然后再综合各个子带的相干性来计算回声抑制因子。而根据不同的回声状态，比如双讲情况下线性部分回声泄漏比较大，则需要增加一些overDrive的操作。</p><p>比如，本来可能算出来需要抑制50%的频谱能量，但由于判断此时是回声泄漏比较多的场景，这里再多做个0.2倍，变成抑制60%。这里的overDrive就是作为经验值进一步防止回声泄漏。</p><p>值得注意的是，WebRTC中NLP的处理在双讲情况下很容易把近端的噪声也一起干掉了，从而会影响到后续降噪模块的处理。所以我们一般会在NLP之后再补一个舒适噪声，用来保证降噪模块中噪声估计不会受到太大的影响。</p><h2>小结</h2><p>好的，现在我们来总结一下这节课的内容。回声消除作为实时音频处理链路中的重要的组成部分，和其它模块的表现会相互耦合，并且每个模块摆放的位置、处理的方法都可能对实时音频的整体体验产生影响。而房间的混响、非线性处理等方面都是我们可以改进回声消除效果的方向。</p><p>其实<a href="https://www.speex.org">Speex</a>和<a href="https://webrtc.org.cn">WebRTC</a> 的开源已经让整个AEC的发展前进了一大步。利用起来后就再也不用从零开始搭建回声消除算法了。Speex主要采用了前景滤波器和背景滤波器的双滤波器结构，这样的结构能更好地防止滤波器发散，使得在处理双讲时效果更鲁棒。但是Speex没有NLP部分的处理，所以在面对非线性问题时就会捉襟见肘。</p><p>比较可惜的是Speex目前已经停止更新，所以一般基于Speex的项目都会自行再做一些非线性的处理。而WebRTC的线性部分只有单路滤波器，较Speex略弱，所以主要靠NLP部分的处理来提升整体效果。但其实，最近更新的WebRTC aec3 的算法中也使用了基于NLMS和卡尔曼滤波，双滤波器的结构，这使得线性部分能够得到进一步的加强。</p><p>另外，我们看到非线性部分还需要一些人工的经验数值来做一些状态判断和overDrive等操作。而实际上基于传统算法的AEC很难彻底解决由于环境、设备等时变性、非线形导致的不稳定的问题。</p><p>因此，最近几年基于机器学习的方法被用到了AEC的领域。尤其是在NLP的部分，和之前讲的降噪模型类似，基于机器学习的模型可以把AEC线性部分的结果作为输入，结合近端和远端的信号自动进行残余回声的消除，这里你可以参考一下最近举办的<a href="https://github.com/microsoft/AEC-Challenge">AEC challenge</a>的比赛，里面有很多优秀的论文和数据可以参考和使用。</p><h1></h1><h2>思考题</h2><p>这里给你留个思考题。回声消除其实还是不可避免的会对音质造成一些损伤，那么请你想一想如果是音乐场景，我们在做AEC的时候可以做哪些改进来提升音质呢？提示：音乐场景的音频采样率会比较高。</p><p>欢迎留言和我分享你的思考和疑惑，你也可以把今天所学分享给身边的朋友，邀请他加入探讨，共同进步。下节课再见！</p>',
        article_title: "08｜回声消除算法实践指南",
      },
    ],
  },
  {
    chapterTitle: "音频网络传输",
    children: [
      {
        title: "09｜音频编解码器是如何工作的？",
        id: 465508,
        content:
          '<p>你好，我是建元。今天我们来聊聊什么是音频编/解码器，以及它背后的原理。</p><p>在我们讲音频编/解码器之前，你不妨先设想一下：如果没有音频编/解码器，我们直接给对方发送原始数据，会发生什么事情呢？</p><p>我们假设一个在线会议有10个人，每个人要发给另外9个人的音频信号是48kHz采样率的单通道音频。每个采样点我们用16位的浮点来表示。那么上行通道中我们需要每秒发送48000乘以16 bit的音频信号，也就是大约768kbps；而接收的下行通路的信号是除了你之外的另外9个人的信号，也就是6912kbps。</p><p>所以上行加下行要想在没有音频编/解码的情况下，完成一个10人的在线会议，你需要消耗7.5Mb的带宽在音频信号上，来保证流畅的音频通话，注意这里我们还没有计算发送的包头以及抗弱网所需要的额外带宽。</p><p>这么大的带宽消耗在带来巨大成本的同时，也会在网络条件比较差或者通话人数比较多的时候让实时音频出现卡顿甚至不可用的问题。所以音频编/解码器的任务就是在减少带宽码率消耗的同时，尽量保持音频的音质不会受到损伤。</p><p>那么具体是如何实现的呢？这节课我们就来说道说道。清楚了这些，对于你接下来实践音频编解码，以及选择合适的编解码器都是大有帮助的。</p><!-- [[[read_end]]] --><h2>编/解码器的发展史</h2><p>这里我们从发展的视角看下，音频编/解码器都有哪些类型，就以它的作用来区分吧。</p><p>音频编/解码器包括编码和解码两个部分。编码的过程就是对音频进行压缩，而<strong>压缩的过程是为了保留音频的主要甚至全部信息</strong>。解码就是对压缩后的音频信号进行解码，从而恢复原始的音频信号。</p><p>压缩按照是否可以完美还原，可以分为无损压缩和有损压缩。无损压缩，例如APE、FLAC等格式可以让音频中所有的细节都得到还原，而有损压缩可提供更低的码率，是我们在实时音频中使用更多的压缩类型。</p><p>一段音频包含的信息其实可以有很多。比如，里面可能有语音、乐器、噪声等多种信号，而其中的语音部分，是我们平时实时音频互动中最重要的部分。</p><p>最早的一批实时音频编/解码器，如<strong>基于ITU标准的G.71等就是针对如何保持语音部分而设计的</strong><strong>。</strong><strong>我们把这一类音频编/解码器叫做语音编/解码器</strong>。</p><p>后来为了传输更多的信息，比如包括音乐甚至“噪声”等全部音频信号的编/解码器，例如基于MPEG标准的MP3和AAC也陆续出现。</p><p>随后，<strong>主要用来编码语音信号的语音编/解码器，逐渐向基于时域的线性预测框架的方向演化。</strong>这种编/解码器参考了声道的发音特性，将语音信号分解为主要的线性预测系数和次要的残差信号。而线性预测系数编码所需的比特率非常少，却能高效地构建出语音信号的“骨骼”；残差信号则像是“血肉”，能够补充出语音信号的细节。这种设计大幅提升了语音信号的压缩效率。<strong>但是这种基于时域的线性预测框架在有限的复杂度下无法很好地编码音乐信号。</strong></p><p>因此，<strong>针对音乐信号进行编码的音乐编/解码器走上了另一条演化的道路</strong>。因为相比时域信号，频域信号的信息更多集中在少部分频点上，更利于编码器对其进行分析和压缩。所以<strong>音乐编/解码器基本都会选择在频域上对信号进行频域编码</strong>。比如图1中基本上语音的频谱从下到上是连续的（频谱上呈现为一团红色或者一些横跨不同频率的曲线），音乐信号则在部分频段上有一些持续性的能量（频谱上呈现为一条一条的直线）。</p><p><img src="https://static001.geekbang.org/resource/image/8d/ac/8d2c2ec5c5c9a99507f50bc374212dac.png?wh=864x493" alt="图片" title="图1 人声（左）和音乐（右）波形和频谱图"></p><p>后来，随着技术日趋成熟，<strong>两种编/解码架构又再次走到了一起，即语音音乐混合编码器。</strong>WebRTC中默认使用的编/解码器OPUS就是这类编/解码器。<strong>这类编/解码器的特点是融合了两种编码框架，并针对信号类型自动切换合适的编码框架</strong>。一些国内外知名的产品都会用到OPUS，比如Discord。</p><p>会议等主要以人声为主的场景可以使用语音编解码器省流量，而音乐直播等场景则需要更多的流量来保证音质，或者两者都有的情况下可以选用OPUS这样的来自动调节，具体如何选择编/解码器会在下一讲中详细介绍。这里会先介绍一些通用的方法比如音频数据裁剪和量化，再举例介绍一下语音和音乐编解码器的基本原理。</p><p>编/解码的算法细节比较多，这里我们主要介绍一些常见的方法，看看我们是如何一步步的把音频的码率降下来，而又不损伤音质的。</p><h2>音频数据裁剪和量化</h2><p>还记得我们一开始举的那个例子么，即48kHz、每个采样点用16bit的浮点来表示的音频。实际上，音频压缩的第一步就是从这些音频的参数出发来缩减采样率和采样精度非线性量化。不懂？没关系，继续往下看。</p><p>先了解点基础知识，根据编码音频的带宽，我们可以把音频分为窄带、宽带、超宽带和全通带。下表中我列出了带宽和采样频率的关系。</p><p><img src="https://static001.geekbang.org/resource/image/be/68/bed907472950a2d7619982ee188c5768.jpg?wh=1280x294" alt=""></p><p>举个例子帮助你理解，你可以关注一下打电话这个场景，打电话的时候我们能听得见对方在说什么，但是往往会觉得声音有点闷。这是因为打电话时语音信号的采样率实际上只有8kHz，也就是窄带信号，根据采样定律这里的有效频谱是0～4kHz。所以说，在低采样率的情况下，语音信号中的语音信息被有效保留了，而更高频的音色信息没有被保留。总结来说，就是采样率降低了，每秒发出的数据量也就降低了，这就是通过缩减采样率来压缩了音频。</p><p>那么采样精度非线性量化又是什么意思呢？还是回到最开始的例子，我们每个采样点用16bit的浮点来表示，而如果换做非线性量化来表示可能只需要8bit就可以，从听感上来说却不会有太大的损失。这是为什么呢？</p><p>比如，<strong>G.711中的A-LAW就是采用非线性量化的方式对每个采样点进行压缩。</strong>而在这里用8bit量化的采样点表示就可以比16bit的少一半的数据量。在图1的语音时域信号里我们可以发现，大部分信号的幅度都在比较低的范围内，只有少部分的幅度值会比较大。如果我们给予更多的精度用于描述低幅度的信号，那么压缩后，听感上的损失就会比较小。比如下面A-LAW公式中就是通过固定值$1/A$作为区分幅度大小的的界限，用非线性的对数ln来划分表达的幅度位置，对高、低幅度分别用不同的量化精度对采样点进行量化。</p><p>$$f(x)=\\begin{cases}A(x)/(1+lnA),  &amp; {0&lt;=x&lt;=1/A} \\\\(1+lnAx)/(1+lnA), &amp; {1/A&lt;=x&lt;=1}\\end{cases}\\text{   A-LAW解码}$$</p><h2>语音编/解码器的基本原理</h2><p>上述缩减采样率和采样精度非线性量化都是数据层面的压缩，接下来我们再看看如何从语音合成建模的角度对语音进行编/解码。这里我们常采用线性预测编码（LPC）。</p><p>这里你可以先回想一下我们之前讲的人发声的基本原理。人类发声时，声带振动得到的是浊音，而声带不震动并且通过气流吹过唇齿等部位产生的声音为清音。音调或者说音高就表现为基频F0的频率高低，其中这个基频是声带振动的频率。而音色则是由频谱的包络来决定的。频谱包络反映的是咽喉、嘴、鼻等声道的形状引发的共振信息，所以频谱包络也就是音效中常说的共振峰曲线。</p><p>再联想一下前面我们讲语音信号分析时，我们发啊、哦、额的时候，我们的口型是不是不一样的。因此，在语音的建模中，模拟声道建模的线性预测滤波器就可以派上用处了。线性滤波器的系数（LPC）反映的是各个声道器官的状态。而在10～30ms时间内器官移动的距离有限，所以我们可以认为声道器官的位置基本不变化，这样就可以利用线性滤波器来对语音进行编码。而线性滤波器的系数远比时域对应帧的采样点个数要少很多，比如我们一般用16阶的LPC就可以来表示一个10ms、16kHz采样的160点的帧。LPC的公式如下所示，我们可以看到这是一个自回归的模型，即当前值是过去值的加权预测。</p><p>$$y(n)=a_1y(n-1)+a_2y(n-2)+a_3y(n-3)+…+e(n)\\text{  LPC模型}$$</p><p>基于LPC的编/解码器的结构如图2所示：</p><p><img src="https://static001.geekbang.org/resource/image/50/8c/5099457df5213b9246869a2d253ff68c.jpg?wh=1280x396" alt="" title="图2 基于LPC的编/解码器"></p><p>其中，基音检测是LPC类型的编/解码器必备的模块之一。<strong>基音检测模块会提供两个信息：一个是信号的周期性，也就是区分是清音还是浊音；另一个是如果是浊音那么基频是多少Hz。</strong>如果判断为浊音，那么激励信号产生器会产生一个和基频相同周期的脉冲激励信号；如果是清音则产生一个白噪声，然后由线性预测合成器按照频谱包络来还原出原始音频。</p><p>LPC模型是线性预测，有预测就会有预测误差。值得注意的是在LPC模型的公式里我们可以看到还有一项$e(n)$，我们把这一项叫做噪声或者残差（excitation）。由于音频信号不可能是完全线性的，如果这个部分缺失，你依然可以听清楚一个语音发的是什么音，但声音会非常“生硬”，或者说听着像机器人。因此，我们实际使用LPC模型时还需要对残差部分来进行编码传输。也就是说在编码传输的时候我们需要传递LPC系数和残差这两个部分，才能在解码时将语音比较好的还原出来。</p><p>例如，在实际使用中，由Skype公司发明的基于LPC的SILK编/解码器使用长时预测分析（LPT）来估计残差信号，使用Burgs方法来计算LPC系数。LPC系数被转换为线谱频率（LSF）向量，然后对LSF进行矢量量化（VQ，Vector Quantization），在解码的时候再结合残差信号把LSF转换为LPC系数。这样就实现了残差和LPC系数的编码。如果直接用LPC系数做量化，一两个系数的误差可能会导致某些频段有较大的失真。在这里，LSF是对LPC模型的因式分解形式，在后续做量化的时候可以减少单一频段的失真。</p><p>对于类似LPC的参数编码器，可以注意到如果类似LPC的参数不进行进一步编码，每一帧仍需要传一组LPC参数或LSF参数。如果1秒有100帧，那么这个码率还是有点大的。例如SILK中就使用了多级向量码本的方式来解决这个问题。 我们通常把这个过程叫矢量量化VQ。</p><p><strong>VQ是一种基于块编码规则的有损数据压缩方法。</strong>事实上，在音视频中的JPEG和MPEG-4等多媒体压缩格式里都有VQ这一步。它的基本思想是：将若干个标量数据组构成一个矢量，然后在矢量空间给以整体量化，从而压缩了数据。</p><p>比如<strong>编码器中每一帧都有一组需要编码的参数向量，那这些参数就可以通过有限数量的固定向量来表达。</strong>比如固定1024种参数的组合，这时如果有新的需要量化的参数向量则需要找到和它最接近的那个固定向量来表达。这个固定的1024个参数向量我们把它编成一个码本，且码本的标号1至1024分别代表这1024种参数组合。那么在编码的时候只需要从码本中找到与这个最接近的参数向量的编号，比如16，然后把16发给解码器。解码端有一个同样的码本。解码的时候只需要去找第16个编号代表的参数向量就可以实现解码。这样传输过程中只用到16这一个数，而参数向量一共有1024种可能，所以编码的时候使用11bit来编码就可以了。</p><p><strong>VQ所需的码本是通过大量的离线语料训练得到的。</strong>VQ的训练方法有很多，常见的有LBG-VQ，K-means等，这里我们不再详细介绍。<strong>其中SILK所用的是多级码本</strong>。这里的多级码本是指第一阶段的输入是待量化的矢量，而之后每一级的输入是前一级的输出的量化误差。比如一个三级码本，编码的时候需要传3个编号，解码的时候则分别根据编号查三个码本，然后把三个码本的向量相加，从而得到完整的解码结果。</p><h2>音乐编/解码器的基本原理</h2><p>上面这些主要是语音编码器的基本原理。我们可以看到<strong>语音编码器主要是对语音的发声来建模编/解码</strong>。而音乐编/解码器因为要编码频带更丰富的音乐信号，所以更多的是<strong>从听得清晰的角度利用心理听觉来进行编码</strong>。也就是说我们人耳更敏感的频带需要多耗费一些码率来编码，不敏感的则少耗费一些码率。</p><p>这里我们以CELT编/解码器作为例子。CELT编码全称是Constrained Energy Lapped Transform，主要使用MDCT编码。离散余弦变换（<a href="https://www.egr.msu.edu/waves/people/Ali_files/DCT_TR802.pdf">DCT</a>）在音、视频编码中都是常用的降维方法，对其内部原理有兴趣的同学可以自行了解一下。MDCT其实就是时域重叠的DCT变换，主要是为了消除DCT带来的块效应。MDCT的正、逆变换可以参照图3。</p><p><img src="https://static001.geekbang.org/resource/image/3a/82/3a980a6fb2cc5c51301ae3dae52db282.jpg?wh=1280x720" alt="" title="图3 MDCT和IMDCT"><br>\nMDCT编码主要过程是分帧，然后一次有重叠的取几帧（比如图3是一次取2帧）做MDCT变换得到一个编码信息。解码时，每个编码信息通过IMDCT逆变换还原编码的那几帧的信号，然后再通过滑动叠加的方式得到还原的音频信号。</p><p>之前的课程里我们讲过人对不同的频带的感知是不同的，比如，人对低频较高频的频率变化较为敏感。回想一下我们之前讲过的人耳敏感频响曲线，Bark谱和Mel谱。在音乐这种频带分布比较随机的信号，就需要对不同的频段加以划分，有的频段人的听感比较敏感就需要更细致的编码，相反则可以进行加大程度的压缩。</p><p>比如CELT中的MDCT变换频谱就是基于Bark频带的，并且选了约21个频带进行编码。每个频带都进行分析、量化数据，并通过预测压缩，可以直接将差异传到预测值。Bark谱的频带分布可以参照图4。</p><p><img src="https://static001.geekbang.org/resource/image/bf/d3/bf4a300980645d79349d4yy91a5ba3d3.png?wh=864x114" alt="图片" title="图4 CELT中Bark谱的频带分布"></p><p>从DCT系数中去除未量化频段的能量值，产生的剩余信号的系数部分是通过金字塔矢量量化（PVQ）来编码的。这种编码方式使用固定（可预测）长度的码字，从而对比特位错误有更好的鲁棒性。CELT将频带的能量分为粗粒度能量和细粒度能量，并且两者相加为整体的频带能量。而在编码时可根据想要编的码率来调节粗细力度的码率分配，这样就可以兼容不同的码率设置。具体CELT的编码流程如图5所示：</p><p><img src="https://static001.geekbang.org/resource/image/8f/f9/8f12503780e52a76bf087b1d44d6b2f9.jpg?wh=1280x390" alt="" title="图5 CELT的编码流程"></p><h2>小结</h2><p>这一讲我们主要介绍了一些音频编/解码的基本原理。音频的编/解码主要有语音编/解码、音乐编/解码和混合编/解码。其中通用的编/解码方法有非线性量化、减少采样率、矢量量化等方法。对于语音信号可以使用声道模型建模，然后采用线性预测加残差的方法来进行编/解码，而对音乐信号则需要根据人耳敏感频响曲线分频带来建模。</p><p>其中，在WebRTC 中默认使用的编/解码器 OPUS 就是这类编/解码器。其语音主要采用SILK来编码，音乐则采用CELT来编码。在OPUS内部还有一个基于人工神经网络的音乐判断器来进行人声和音乐的自动切换，从而达到最好的编/解码效果。</p><p>其实在音频编/解码领域有很多成熟而优秀的编/解码器，初学者不要被这些复杂的数学、声学、心理学知识所吓到。我们完全可以在掌握编/解码器原理的基本概念之后根据自己的场景，选择一些成熟的编/解码器来使用，具体怎么选我们将会在下一讲中详细介绍。</p><h2>思考题</h2><p>如果我们人类每分钟最多说200个字，假设一共有4096个常用汉字。设想一下如果采用文字识别作为编码（语音转文字ASR），然后用文字转语音技术（TTS）来解码那这种编/解码器的码率会是多少？提示一下，4096可以用多少bit来表达？</p><p>欢迎留言和我分享你的答案和疑惑。我们下节课再见！</p>',
        article_title: "09｜音频编解码器是如何工作的？",
      },
      {
        title: "10｜如何选择一个适合你的编解码器？",
        id: 467024,
        content:
          '<p>你好，我是建元。</p><p>上节课我们讲了音频编/解码器的基本原理。相信你已经对编/解码器有了一个整体的了解。其实编/解码器中的算法链路还是比较复杂的，自己从头开始设计和调试一个编/解码器的研发成本也是非常巨大的。所以我们一般会选择已有的编/解码器来使用。</p><p>而音频编/解码器经过几十年的发展，其实已经有很多成熟的解决方案可以选择。而且不同的场景对实时音频也有不同的要求。比如，音乐场景要求有比较高的采样率；合唱场景则需要比较低的延迟等。那具体根据什么标准来选择编解码器呢？这正是我们这节课的重点所在。</p><p>这节课我们先来看看编/解码器选择时需要重点看哪些指标，然后从几个应用场景的角度看看如何挑选编/解码器以及如何选择合适的码率来达到我们想要的效果。</p><h2>音频编/解码中常见的指标</h2><p>音频编/解码器需要关注的指标主要包括码率、音质、计算复杂度和延迟这4个大项。你可以先结合图1来大体了解一下，下面我会详细介绍。</p><p><img src="https://static001.geekbang.org/resource/image/cc/b7/cc0e27fc5333df4582c4a5b54cc520b7.jpg?wh=1460x953" alt="" title="图1 实时音频编/解码的常见指标"></p><h3>码率与音质</h3><p>音频编/解码最直观的目的就是节省传输带宽。所以我们第一个要关注的指标就是码率。现在比较常用的编/解码器，比如OPUS、EVS等，都是支持不同码率传输的。而不同的码率一般会对应我们要关注的第二项指标音质。你还记得我们之前讲的音频质量评估么，音质我们可以用主观评测试验，也可以用PESQ、POLQA等客观评测方法来对主观听感进行打分。当然我们也可以从一些直接的音频指标，比如采样率、采样位深、通道数等来大致衡量音频质量的好坏。</p><!-- [[[read_end]]] --><h3>计算复杂度</h3><p>除了码率和音质，我们下一个要关心的是编/解码器的计算复杂度。音频的编码和解码都需要一定的算力支持。你可以回想一下我们之前讲的编/解码器的原理，其实大部分常用的编/解码器解码的计算力会比编码端的计算力要小很多。一般，我们在多人实时音频互动的时候，其实一个设备需要做自己这一路的编码和多路的解码。所以<strong>在看某个音频编/解码是否可用的时候，我们至少需要看看我们的设备是否可以支持实时一路编码和实时多路解码</strong>。</p><p>测试方法也比较简单，就是看看在你需要同时互动的最大数量的情况下，<strong>你的终端播放出来的声音是否会出现卡顿或者无声的情况</strong>。</p><p>这里从经验上来说，目前移动端的硬件设备，比如说笔记本电脑或者手机，常见的编/解码的计算复杂度一般都是可以支持的。但有一些定制化的IOT设备或者后台还有其它应用，在同时运行的时候，我们可能需要关注或者测试一下编/解码器是否支持你的应用场景。</p><h3>延迟</h3><p>在实时音频互动中还有一个需要注意的指标就是延迟。延迟主要包括两个部分：</p><ul>\n<li>一个是编/解码器算法引入的延迟，比如，编码时依赖未来帧的信息对当前帧进行编码；</li>\n<li>另一个是网络发送时组包的延迟，比如，我们把4帧作为一个包来发送，那么延迟就会增加4帧的时长。而其实我们在音频互动中一般会比较关心一个音频“端到端”的延迟是多少。这里说的端到端，就是从你说的话被麦克风采集传到对端的设备，并从扬声器里播放出来的延迟。<strong>这个端到端的延迟包括了设备采集播放的延迟、音频处理算法引入的延迟、编/解码引入的延迟和网络传输的延迟。</strong></li>\n</ul><p>这里我和你分享一些经验：</p><ul>\n<li>一般来说“端到端”的延迟如果超过200ms，人就可以开始感受到音频通话和面对面说话之间的差异。</li>\n<li>而如果延迟超过400ms，那么你可以明显感受到，对面的反映有一种慢半拍的感觉。也就是你说了一句话，对面需要反映一会儿才会给你回应。所以我们在音质、码率等指标都合适的情况下，<strong>如果是需要音频互动的场景，我们一般会选择延迟比较低的编/解码器。</strong></li>\n</ul><p>好的，说了这么多需要关心的指标，我们可以通过图2来看一下不同编/解码器的性能差别。</p><p><img src="https://static001.geekbang.org/resource/image/db/1d/dbafed21804728f2a483a66ff6aa0a1d.png?wh=1297x940" alt="" title="图2 不同编/解码器的性能表现"></p><p>从这张图中我们可以看到，如果从延迟（Delay）来看，OPUS和G.729是所有编/解码器中延迟最小的，可以达到20ms左右的延迟。而OPUS能提供的码率是6～80kbps，覆盖的范围比较大（OPUS实际可支持的的码率可以更高，这里绘图限制在80kbps以下的常用码率范围内方便对比）。但要注意这里红黄绿分别代表窄带、宽带和超宽带，而结合上一讲的知识我们知道这和音频的采样率有关。</p><p>举个例子，如果你选择了OPUS作为编/解码器，码率选择了6kbps，那么你就只能得到一个采样率为8kHz的窄带音频。这里需要提醒一下你，这里说的采样率是有效采样率。什么意思呢？也就是说，你可能发现最后解码出来的音频文件是48kHz，但在频谱图上4kHz以上是没有能量的。</p><p>通过这张图中Speex和OPUS的对比，你不难发现OPUS的延迟要低于Speex，同时音质又高于Speex。可以说Speex已经被OPUS完全超越了，这也是为什么Speex开源项目停止更新的原因。</p><p>最后，让我们再来看看图2中的MP3、AAC这几种编/解码器。它们的延迟都达到了200ms以上。这并不适合实时音频互动，但它们的存储空间比较少，所以<strong>如果我们需要对音频进行录制或者直播，这种对延迟要求不那么高的情况，可以采用这些音频格式。</strong></p><h2>音频编/解码器案例分析</h2><p>音频编/解码器的指标还是比较好理解的。但在实际工作中，我们选择编/解码器还需要考虑到带宽成本、编/解码器是否有开源代码方便集成等问题。所以其实我们做决策的时候，往往需要更为全面地思考问题。这里让我们通过两个真实场景的例子，来看看如何选择一个适合你的编/解码器。</p><h3>案例1：在线会议</h3><p>在线会议，你应该并不陌生。我们来想一下这个场景有什么特点。首先这是一个可能涉及多人的实时互动场景；其次它对音质的要求主要是为了保证语音的通话流畅。</p><p>那么转换为音频编/解码器的指标来理解，也就是音质首先需要保证语音通话的采样率（比如宽带），其次延迟要小。那么在选择编/解码器的时候就可以进行筛选了。</p><p>这里举个例子，结合图2我们可以看到，如果选择中、低等码率的OPUS基本上就可以满足要求了。OPUS在码率超过10kbps之后就可以做到宽带信号的编/解码了。</p><p>那么具体要选择多少码率呢？这个就需要你对场景业务的理解了。</p><ul>\n<li>如果在线会议中可能会有音乐的场景，为了保证音质，你可以把码率调整到比如64kbps这种比较高的码率，来保证音乐信号的保真度。</li>\n<li>如果你主要是为了保证多人同时互动，而且需要降低带宽成本，那么其实比如18kbps的码率，也就可以做到多人语音互动的流畅了。</li>\n</ul><p>好了，选择好了码率，现在你可能会好奇，哪些编/解码器是开源的呢？</p><p>其实为了更好的普及实时音频技术推动标准化的进程。不少行业标准组织、企业联合推出了多款可开源使用的音频编/解码器，这里我主要给你介绍比较常用的三个：OPUS、EVS和AAC。</p><ul>\n<li>其中，OPUS作为互联网行业最常用的编/解码器，是由Xiph.org 基金会、Skype、Mozilla基金会联合出品的。OPUS延迟低、效果也不错，从而在互联网领域有一统江湖之势。并且因为它集成在了WebRTC中，所以如果你的应用需要和Web端互通音频，那么建议你使用OPUS。</li>\n<li>传统的移动通信标准组织3GPP则是EVS的幕后推动者。EVS作为开源的音频编/解码器，在音频编/解码的各项指标上也可以说是表现优秀。它可以支持8kHZ～48kHz的音频编码，并支持5.9kbps到128kbps的码率范围。</li>\n<li>而AAC编/解码器是由MPEG（Moving Picture Experts Group，动态图像专家组）开发的编/解码器，其背后是杜比、Sony等公司的技术集成。是不是听到这些公司的名字你已经有所察觉？没错，AAC编/解码器在音乐编码方面的效果会比较好。AAC的系列有多个种类，比如LC-AAC(低复杂性，普通质量)和HE-AAC(高效性，高质量)等。你可以根据计算复杂度、音质等挑选合适的系列。</li>\n</ul><h3>案例2：在线K歌直播</h3><p>好的，聊完了会议场景，让我们再来看一个音乐场景：在线K歌直播。音乐场景需要高采样率甚至是立体声和多声道。但是不是将码率调高，换一个支持立体声的音频编/解码器就解决问题了呢？</p><p>越简单的答案，越需要小心。如果你的在线K歌直播是在网络覆盖比较差的区域，比如印度，那么你用了一个高码率，比如说128 kbps的EVS编/解码器，来保证音频质量，实际得到的结果可能就是音频卡顿不断，反而影响了效果。这时我们可能会<strong>分地区来使用不同的码率，甚至不同的编/解码器。</strong></p><p>除了需要根据地区网络情况来选择编/解码器之外，空间感在音乐音质中也是很重要的组成部分。而为了保留空间感，我们最少需要两个声道，也就是我们说的立体声来实现。因此，我们假设网络带宽足够的情况下想要播放双声道的声音，那么可以使用LC-AAC、HE-AAC等编/解码器的双声道模式，来实现立体声的传输。</p><p>那么为了追求极限的音质，我们还可以使用哪些编/解码器呢？</p><p>为了能比较好地还原音频的空间感，我们往往需要更多的音轨来做到。比如为了还原环绕声的听感，杜比的AC-3编/解码器可以支持5.1声道编/解码，也就是说通过AC-3可以传输6个声道的数据。这样就可以在杜比数码（Dolby Digital）家庭影院系统中播放环绕声了。</p><p>一个杜比5.1声道的播放系统如图3所示。我们可以看到，6个通道的AC-3编码最后分别由6个音响进行播放，这样你就可以感受到声音从四周不同的方位传来。</p><p><img src="https://static001.geekbang.org/resource/image/2a/0b/2a4d9ccd4a10898956989afbd05aee0b.png?wh=837x674" alt="" title="图3 杜比5.1声道播放系统"></p><p>类似的编/解码器还有DTS格式。比如你在看电影的时候，如果有标记上有DTS，就说明这是一个高保真的音频编/解码格式。从码率上来说，AC-3所支持的码率从32kbps到640kbps。而DTS的码率比较高，通常为768～1536kbps，一般用于DVD、电影等离线场景。你可以看到由于压缩能力好，AC-3则在实时互动中更有优势。</p><h2>小结</h2><p>好的，今天的课到这里就要结束了。我们来回顾一下这节课的内容。</p><p>在你挑选音频编/解码器的时候，常见的需要关心的指标主要是码率、音质、计算复杂度、延迟等。一般来说相同码率的情况下，音频编/解码器的音质越好、复杂度越低、延迟越小，编/解码就越好。但编/解码器的码率和通道数等都是可调节的，且音频编/解码器的选择是需要根据不同场景来综合考虑的。</p><p>文中我给出了两个具有代表性的案例：</p><ul>\n<li>会议场景，主要追求的是互动的流畅，所以我们会选择延迟较小的、音质一般的音频编/解码器，比如OPUS。</li>\n<li>音乐场景，除了高码率保证音质外，还需要多声道来保证音频的空间感，我们可以使用双声道、甚至多声道的编/解码器，例如AAC来实现。<br>\n如果你对音频中如何还原空间感还有疑问，不要着急，我会在后面的课程为你详细介绍。</li>\n</ul><p>其实，最近还有很多基于AI的音频编/解码器相继发布，可以说基于AI技术的编/解码器在如何使得码率更小、音质更高上是一个不错的研究方向。</p><p>例如，Google发布的Lyra，Sound stream，以及微软发布的Satin。另外，我在声网也研发了国内第一个基于AI的语音编/解码器<a href="https://mp.weixin.qq.com/s/b6f6e9B4-AjRVZRHRzxdcQ">Silver</a>，它可以用3.6kbps实现32kHz采样率的语音的编/解码。你可以看到基于AI的编/解码器可以把码率降到一个很低的水平，同时又保持较高的音质。有兴趣你可以通过链接自行了解一下。</p><h2>思考题</h2><p>最后，这里有一道思考题留给你：很多社交场景中语音和音乐是同时存在的，那么我们有没有什么办法能同时满足沟通的流畅和音乐的音质保真呢？</p><p>欢迎你在留言区和我分享你的思考和疑惑，你也可以把今天所学分享给身边的朋友，邀请他加入探讨，共同进步。下节课再见。</p>',
        article_title: "10｜如何选择一个适合你的编解码器？",
      },
      {
        title: "11｜网络差怎么办？音频网络传输与抗弱网策略",
        id: 468054,
        content:
          '<p>你好，我是建元。</p><p>上节课我们讲了如何挑选一个编解码器。其实编解码器就是把音频信息拆解、包装成一个个的数据包，然后通过网络传输到远端。在远端打开数据包，再组装成音频播放出去。</p><p>如果把一个个数据包比喻成一辆辆运送音频货物的小车，而网络则是运输的道路。那么我们会发现：有的时候我们经过的是高速公路，物流十分流畅；但有的时候我们经过的却是崎岖蜿蜒的山路，甚至有的小车在山路上，摔下了悬崖，丢掉了包裹，或者赶上交通高峰期道路十分拥堵，从而小车超过了规定的物流时间，这些情况就是我们说的弱网。</p><p>一般在弱网情况下，音频的体验可能表现为卡顿、杂音。如果情况严重可能会直接导致无法正常通话。这节课我们就来看看音频链路中弱网是怎么形成的，以及我们是如何通过抗弱网策略来解决弱网问题的。</p><h2>实时音频传输</h2><p>在讲弱网之前我们先来看看音频是怎么传输的。在实时音频交互的场景中，为了保证传输的实时性，一般使用基于UDP协议的RTP协议来传输音频数据。<strong>相较于TCP协议，UDP提供了一种无需建立连接，就可以发送封装的 IP 数据包的方法。</strong>所以它的优点是延迟低、速度快，但丢包了、包损坏了的时候也没有重传机制等做保护，可以说它是一种“尽力而为”的协议机制。</p><!-- [[[read_end]]] --><p>而RTP定义了我们音视频的数据包格式，其中包含了RTP版本号、包顺序编号等信息。而音频编码得到的压缩后的音频信息，就对应了数据包最后的Audio Payload，也就是音频负载部分。我们可以通过图1来看看一个完整的音频数据包的组成形式。</p><p><img src="https://static001.geekbang.org/resource/image/8d/8f/8d380583cb948c23c1a1cef67a9e6a8f.png?wh=1394x248" alt="图片" title="图1 音频数据包的组成"></p><h2>弱网是如何形成的</h2><p>现在你了解了音频是怎么传输的，接下来我们来看看弱网是如何形成的。其实弱网状态中有三个常见的问题：丢包（Packet Loss）、延迟（Latency）和抖动（Jitter）。我们挨个来看看它们分别是怎么产生的。</p><h3>丢包</h3><p>我们还是以物流小车为例。“丢包”指的是有的车无法在有效时间内到达终点，甚至可能永远也到不了终点。比如有的小车发生了车祸，或者小车司机罢工了。如果100辆车里有10辆无法到达终点，那么我们就把它叫做丢包率为10%。</p><p>是的，在<strong>网络传输中，数据包会经过很多复杂的路径，有的是在物理传输中发生了丢失，有的是在服务器、路由转发时由于拥堵或等待时间过长被抛弃。</strong>可以说，互联网传输并不是百分百可靠的，总有数据无法按时传输到目的地。</p><h3>延迟和抖动</h3><p>在网络这条公路上，从起点到终点我们有很多不同的路径可以选择。你可以选择走高速但也可能走了乡村小道，这样就会导致包裹到达终点所经历的时间发生变化。而这个从发送到接收经过的时间我们把它叫做延迟。</p><p>那么很显然，音频在发送的时候是按照时间顺序等间隔发送的，但是由于每个数据包经过的路径不同，从而到达目的地的延迟也不一样。这就导致有的时候很长时间都没有一个数据包到达，而有的时候几乎是同时来了好几个数据包。这就是我们常说的抖动。如果我们按照数据包到达的顺序去播放音频，那么音频播放可能是乱序的而发生杂音，也可能是没有数据可以播放，导致卡顿。</p><p>实际上，全球的网络传输环境随区域、时段的不同而不断变化，并且时好时坏。所以丢包、延时和抖动，是基于互联网进行实时传输不可避免的三个问题，不论是在局域网、单一国家地区内传输，还是跨国、跨地区传输，我们都可能会遇到这些问题。</p><p>从声网Agora监控的网络实况来看，以网络相对较好的中国为例，99%的音频互动需要处理丢包、抖动和网络延时等。在这些音频会话中，20%由于网络问题会有超过 3% 的丢包，10%的会话有超过8%的丢包。</p><h2>抗弱网策略</h2><p>既然弱网状态是互联网传输中不可避免的，那么我们有什么办法来解决或者说对抗弱网呢？主要有网络丢包控制这一网络传输条件下的通用解决方法，和NetEQ这种音频独有的抗弱网策略这两块来解决弱网问题。接下来我们逐一看一下。</p><h3>网络丢包控制</h3><p>我们先来看看我们是怎么对抗丢包的。其实抗丢包背后的逻辑比较简单。简单地说，就是同一个包一次多发几个，只要不是都丢了就能至少收到一个包；又或者丢了包就再重传一个，只要速度够快还能赶上正常的播放时间就可以。这两种思想对应我们通常使用的前向纠错FEC（Forward Error Correction）和自动重传请求ARQ（Automatic Repeat-reQuest）这两个纠错算法。</p><p>FEC是发送端通过信道编码和发送冗余信息，而接收端检测丢包，并且在不需要重传的前提下根据冗余信息恢复丢失的大部分数据包。即<strong>以更高的信道带宽作为恢复丢包的开销</strong>。</p><p>这里你需要注意的是：音频前向纠错遵循RFC-2198标准；而视频前向纠错遵循RFC-5109标准。音频由于数据包相比视频要小的多，可以直接用完整的音频包做冗余，而不是像视频用一个分辨率比较差的小数据包做冗余。如图2所示这就是Simple-FEC的原理。</p><p><img src="https://static001.geekbang.org/resource/image/63/d4/6395fde0c1cb5282cb0772de815ca2d4.jpeg?wh=1920x1080" alt="图片" title="图2 正常发包(上)Simple-FEC(下)"></p><p>我们看图2中的FEC就是每次发一个当前时间的数据包和一个上一时刻的冗余包，当其中一个数据包丢失时，我们可以用下一时刻的冗余包把数据恢复起来。显而易见的是，如果同时丢两个包，那么就无法恢复数据了。我们可以看到，在这种多发一倍流量的情况下，连续丢两个包就无法还原了。那么有没有什么办法可以改进这种Simple-FEC呢？</p><p>让我们再看看另一种FEC的方法：<a href="https://datatracker.ietf.org/doc/html/rfc5510#page-15">RS-FEC</a>，RS码即里德-所罗门码（Reed-solomon Code）。这里我们结合图3来看一下。</p><p><img src="https://static001.geekbang.org/resource/image/5b/dd/5bd33ae6fc2899ce198623dyy295e8dd.jpeg?wh=1920x1080" alt="图片" title="图3 RS-FEC抗丢包原理"></p><p>我们假设每m个包（红色方块）进行一次RS-FEC编码得到n个冗余包（绿色方块）。冗余包加上原来的包，也就是我们在m个包的间隔时间里要发送m+n个包。RS-FEC的特点是，我们只需要得到m+n个包中的任意m个包就可以把音频还原出来。在图3中，m=4，n=4，这样即使这8个包里连续丢了4个，也就是丢包率是50%，都可以保证音频的流畅播放。</p><p>好的，介绍了FEC，我们再来看看另一个常用的防丢包策略：ARQ 。其实ARQ的原理非常简单。它就是采用使用确认信息（Acknowledgements Signal，ack），也就是接收端发回的确认信息，表征已正确接收数据包和超时时间。如果发送方在超时前没有收到确认信息ack，那么发送端就会重传数据包，直到发送方收到确认信息 ack 或直到超过预先定义的重传次数。</p><p>可以看到相比ARQ的丢包恢复，由于FEC是连续发送的，且无需等待接受端回应，所以FEC在体验上的延时更小。但<strong>由于不管有没有丢包FEC都发送了冗余的数据包，所以它对信道带宽消耗较多</strong>。而相比 FEC 的丢包恢复，ARQ因为要等待 ack 或者需要多次重传。因此，ARQ 延时较大，带宽利用率不高。</p><p>我们可以看到，这两种算法都增加了网络所需要传输的带宽。那么如果网络带宽本就不够而导致丢包，这时FEC和ARQ不但没能起到抗丢包的作用，而且还可能导致网络堵塞导致丢包更严重。那么除了FEC和ARQ这两种抗丢包手段外，还有没有什么其它的方法可以解决弱网问题呢？</p><h3>NetEQ</h3><p>其实为了解决弱网问题，在接收端音频解码时通常都有一套比较完整的抗丢包策略。实际上，很多音频编解码器或者开源实时音频框架中都自带了抗丢包策略，其中比较典型的是在WebRTC框架中的NetEQ模块。我们可以通过图4来了解一下。</p><p><img src="https://static001.geekbang.org/resource/image/e4/19/e4d130a7d5d316f6dbe686c3de693619.png?wh=1094x807" alt="图片" title="图4 NetEQ处理流程[br]《实时语音处理实践指南》中国工信出版社"></p><p>由图4我们可以看到，NetEQ主要包括两个模块：MCU（Micro Control Unit，微控制单元）和DSP（Digital Signal Prcessing，信号处理单元）。我们知道由于网络传输的不稳定性，虽然我们有FEC和ARQ，但由于延迟或者严重丢包导致的数据包乱序，或者数据包丢失，还是会经常发生的。</p><p>在MCU里的Jitter Buffer（抖动缓存区）或者说Pactet Buffer（数据包缓存区）就是通过开辟一个比较大的缓冲区域，让一段时间内到来的数据包在Jitter Buffer里存储、排序。然后按照播放顺序把数据包交给DSP中的解码器进行解码。</p><p>在DSP模块中，由解码缓冲区得到的音频信号并不是直接交给播放设备播放的。而是需要根据网络状态、缓冲区未处理的数据包长度，以及等待播放的音频长度等参数，来决定使用DSP处理中的五种决策方法中的哪一种来处理音频数据。接下来我们就来看看这五种策略：加速、慢速、正常、融合和丢包补偿背后决策的原理、实现方法和实际听感的效果是什么样的。</p><p>其实NetEQ中主要定义了四种收包的情况：</p><p><strong>1. 过去帧和当前帧都正确收到</strong></p><p>如果过去帧和当前帧都正确接收到了，那么这种情况下只需要考虑网路抖动带来的数据包堆积和数据包接收不足的问题。</p><p>所谓数据包堆积，就是同一时间到达了多个数据包都在等待播放，而这个时候需要使用加速策略（accelerate），即对音频信号采用变速不变调的算法来缩短解码后音频的长度，从而实现快速播放。</p><p>相反的，如果在缓存中的数据就快播放完了但新包还未送达，那么这时候就需要慢速的方法来把音频时长拉长。这里用到的同样是变速不变调的算法，即只改变音频的播放速度而不改变音频的音调。</p><p>WebRTC中使用的是一种叫WSOLA的算法来实现的，这其实是音效算法中变调不变速算法的一种反向应用，更具体地我会在音效算法的一讲中为你详细解读，这里就不再赘述了。</p><p>那快慢放的听感是什么样的呢？在网络有抖动的时候，你可能会感觉对面说话，有的时候会快一点，有的时候会慢一点。这种快慢感在语音的时候可能不是那么容易察觉，这是因为人说话本来就有快有慢。但是在音乐的场景下，因为你对一首歌比较熟悉，所以快慢放就会更容易被察觉。</p><p><strong>2. 当前帧发生丢包或者延迟</strong></p><p>如果当前帧发生了丢包或者延迟导致当前没有音频数据可以播放，这个时候就需要额外的PLC（Package Loss Compensation，丢包补偿）模块来重建音频。你还记得我们在编解码器中讲的LPC算法吗？其实常见的PLC算法就是通过重建或者复用上一帧的LPC系数和残差来还原这一帧的音频数据，从而实现丢包隐藏的。</p><p>这里你需要注意和前面的慢放相区分。慢放虽然也可以增加音频的长度但一个慢放系数比例确定后，慢放所能增加的音频长度也就固定了，所以一般慢放用于解决需预测时间比较短的音频的拉长。而PLC具有可扩展性，所以一般负责整个一帧或者多帧的，长时间的丢包补偿。</p><p><strong>3. 连续多帧丢包</strong></p><p>那么在连续多帧丢包的情况下依靠PLC是不是就可以了呢？答案是否定的，因为PLC补出来的音频很大程度上是上一帧音频的延长。如果长时间使用PLC，声音就会变得失真，从而影响听感。所以如果出现连续多帧丢包，我们就会逐帧递减PLC补出音频的能量增益。这也就是为什么，长时间的丢包后的听感是声音逐渐变小直至没有声音，而不是有一个奇怪的声音一直在延续。</p><p><strong>4. 前一帧丢失，当前帧正常</strong></p><p>最后这种情况前一帧可能存在PLC的补帧操作，那么新来的音频数据和上一帧就会出现不连续的情况，这里我们就会用到融合的操作。操作也比较简单，就是把当前帧的新数据和之前帧的音频做交叉淡化，让它们的连接处能平稳过度。</p><p>交叉淡化的步骤如图5所示，其实就是前一帧信号的末尾取一段逐步衰减至0，然后让后一帧的前端数据从0开始逐步提升。然后把这两帧重叠部分相加就可以实现比较平滑的拼接了。</p><p><img src="https://static001.geekbang.org/resource/image/f8/c8/f86b168234411fbf28abe95e6e69c5c8.jpeg?wh=1920x1080" alt="图片" title="图5 交叉淡化的过程"></p><p>图5中橙色虚线表示交叉淡化用的淡化增益，第一第二行分别表示原始数据和交差淡化衰减后的曲线，最后一行是两帧重叠部分相加、拼接后得到的数据。</p><h2>总结</h2><p>好的，这里让我们来总结一下这节课的内容。</p><p>弱网的情况在实时音频领域中是不可避免的一类问题。它主要表现为丢包、延迟和抖动这三个常见现象。为了能在弱网情况下继续保持音频的流畅播放，我们分别从网络丢包控制这一网络传输条件下的通用解决方法，和NetEQ这种音频独有的抗弱网策略这两块来解决弱网问题。</p><p>其中，网络丢包控制比较常见的方法有前向纠错FEC和自动重传请求ARQ这两种。需要注意的是，这两种方法都是以增加网络带宽消耗的方式来提升弱网的能力的。因此，如果在网络带宽本身比较差的情况下，就可能导致适得其反。而NetEQ则是通过一些音频处理方法，比如快慢放、PLC等方法来解决抖动和丢包的问题。</p><p>我们可以看到，NetEQ中通过多个Buffer缓存以及快慢放的形式引入了延迟，从而提升了抗网络抖动的能力。然后通过PLC的方式解决丢包带来的音频卡顿。这与FEC和ARQ相比无需额外的带宽消耗，但是却增加了延迟。</p><p>实际上抗弱网我们也叫作音频的“最后一公里”(Last-mile)，并且它是保证音频传递的重要组成部分。其实在实际使用中你可能需要针对自己的场景进行一些调整，比如说对于流畅通话比较重要的会议等场景，可以把NetEQ中的缓冲Buffer适量增大，这样可以进一步提升抗网络丢包的能力。但是Buffer也不能太大，这样会导致过多的延迟，从而影响通话效果。</p><p>我们也可以在NetEQ中引入网络抖动情况的估计，比如在网络抖动严重的时候，动态增加NetEQ的Jitter Buffer的大小，而网络情况较好的时候减少一些Jitter Buffer的大小，从而降低延迟，这些都是可以改进的策略。</p><h2>思考题</h2><p>这里，给你留一个思考题。在网络带宽比较差的地方，比如只有3G网络覆盖的地方，我们要如何调整实时音频系统才能让实时音频获得比较流畅的体验呢？</p><p>欢迎你在留言区和我分享你的思考和疑惑，你也可以把今天所学分享给身边的朋友，邀请他加入探讨，共同进步。我们下节课再见。</p>',
        article_title: "11｜网络差怎么办？音频网络传输与抗弱网策略",
      },
    ],
  },
  {
    chapterTitle: "空间音频",
    children: [
      {
        title: "12｜空间音频入门：如何实现“声临其境”？",
        id: 469134,
        content:
          '<p>你好，我是建元。</p><p>前面的课程，我们主要介绍了音频链路中必备的前处理模块，可以说是音频体验的基础设施。在这个基础上我们就可以实现很多有趣的应用，也就是音频应用的上层建筑。这节课，我们先来讲一讲如何重现一个声场环境，让音频能够穿越时空的限制，做到“声临其境”的。</p><p>你可以先闭上眼睛，尝试听一听周围的声音。你是不是可以感受到有的声音是从左边传来，有的声音会从右边，甚至是从后面传来。我们如果身处地铁、小巷、教室或者球场，是不是听到的声音也会有所不同。有的时候，浴室里的声音会比较浑浊，而操场等空旷的场景声音又会显得比较干燥？其实这些都是我们人耳对空间的感知。</p><p><strong>所谓空间音频的技术，就是把现实中这些对声音的感知，能够用空间音频采集设备和播放设备还原出来。</strong>空间音频涉及空间声学、空间声采集、空间声重放等细分领域，内容比较多。这里我会按照空间音频的基本原理以及空间音频的采集和播放，结合已有的解决方案，来给你讲一讲空间音频背后的原理和使用方法，从而让你能够快速步入空间音频这一音频“元宇宙”的入口。</p><h2>方位判断与双耳效应</h2><p>好的，刚才我们说过，我们可以感知到声音是从不同的方向传来的。我们不妨先来看看这所谓的“方向感”是怎么产生的。</p><!-- [[[read_end]]] --><p><img src="https://static001.geekbang.org/resource/image/66/8f/66523b97e3417844b740cf069dc6eb8f.png?wh=600x338" alt="图片" title="图1 耳廓收音示意图"></p><p>我们可以通过图1看到人耳的耳廓在接收不同方向的音源时，会让声波以不同的路径传导至内耳。这样，不同方向的声波传输到内耳的时候，音色就会由于耳廓的形状而产生各向异性。除此之外，由于我们有两个耳朵，所以音源在不同方向时声波到达耳朵的时间也会不同，这一点我们可以结合图2来理解一下。</p><p><img src="https://static001.geekbang.org/resource/image/4d/f0/4d318fc373076a03b0b293b98585f1f0.png?wh=450x235" alt="图片" title="图2 双耳效应示意图"></p><p>其实很简单，如果音源在你的左侧，那么左耳会先接收到声波；相反如果音源在右侧，右耳会先收到声音。同时由于人的头部也会对声音的传播产生影响，如果音源在左侧，那么声波需要越过头部这个“障碍”才能传递到右耳，那么相对于左耳，音色和能量可能都会有所衰减。<strong>这其实就是空间音频里常说的“双耳效应”，即依靠双耳间的音量差、时间差和音色差来判别声音方位的效应。</strong></p><h2>距离感和空间感</h2><p>好的，知道了方向感是怎么产生的，我们再看看我们是如何听出音源所在的不同距离的。距离感，给人的第一感觉是：如果这个声音的音量小，那么一定是因为它离我们比较远，而声音大则是距离近。</p><p>而实际上声音的大小是相对的，比如可以音源离你很近但却只是低声细语，或者离得很远但用一个功率比较大的音箱，大声播放。所以用音量本身来判断声音的距离是不够的。<strong>实际上我们人耳对距离的感知是相对的。</strong>比如声音播放时音量由小变大，我们会感觉声音在靠近。或者同时播放两个吉他的声音，你会感觉声音小的是在远一些的位置。</p><p>除了音量之外，还由于声波在空气传播中会产生衰减。而其中高频的声音衰减快、低频的衰减慢。那么同一个音量的声音，如果高频比较多，我们会觉得它离你更近一些。如图3所示，低频的声音可以传播得更远，而8kHz以上的声音如果超过1千米就很难被听到了。</p><p><img src="https://static001.geekbang.org/resource/image/32/1d/323124fcbe169f409bef60ee598dd51d.png?wh=720x496" alt="图片" title="图3 不同频率的声波空气衰减曲线"></p><p>除了方向和距离，在之前讲回声消除的时候我们提到过混响的概念。<strong>其实混响也是我们感知声场空间大小的重要一环。</strong>简单地说，大房间的混响持续时间长，而小房间的混响持续时间短。我们能通过声音的混响来感知所在空间的大小。当然混响还和房间墙壁的材料、形状以及房间中障碍物的情况有关，所以我们对空间的感知相比方向和距离没有那么准确。</p><h2>空间音频的采集</h2><p>好了，有了空间音频的基础知识，让我们来看看我们是如何把空间音频采集下来的。</p><h3>入耳式麦克风与人工头</h3><p>很显然想要把人耳听到的声音完整的保留，我们可以采用入耳式麦克风，直接把左右耳道接收到的音频给录下来。或者如图4所示，可以使用人工头的方式，通过仿生模型，构建人头和耳廓、耳道等部位，然后<strong>通过人工头上的人工耳中内置的麦克风来采集空间音频。</strong></p><p><img src="https://static001.geekbang.org/resource/image/fb/3f/fbb8e08157a4050b04cfb5e7b02bd33f.png?wh=1362x894" alt="图片" title="图4 人工头示意图"></p><p>入耳式麦克风和人工头采集的区别其实也显而易见的。如果你用入耳式麦克风采集的音频再用入耳式耳机播放，那么基本上可以做到完美还原。而如果是用人工头录制，那么由于耳廓的形状、头的形状等都和你自己有所不同，所以虽然可以做到很大程度上的空间还原，但和你自己实际到场景中去听，还是有一些差别的。</p><p>在实际使用中，每个人的耳朵、头的形状都不一样，但大体的形状和位置是相同的。所以<strong>利用人工头做音频录制在很多影视和游戏音频制作中会经常用到。</strong></p><h3>Ambisonics</h3><p>但拿人工头或者入耳式麦克风采集到的音频都还只是固定方向的立体声还原，且只能还原采集时人头朝向的声音。如果想把整个空间的声场都录下来，从而在回放的时候，你可以转动自己的头聆听任意方向的声音，那么就需要另一套叫做高保真度立体声像复制（Ambisonics）的技术。</p><p>高保真度立体声像复制源于20世纪70年代牛津大学的一个三维空间声场重构技术研究。技术的核心是将远端中能听到的声音通过特制的麦克风录制，比如一阶Ambisonics麦克风（由四个完全相同的麦克风单元构成一个立方体阵列）的方式复制下来，我在图5里展示了几个常见的一阶Ambisonic 麦克风。</p><p><img src="https://static001.geekbang.org/resource/image/61/10/614ayy293712dd676fbe1cfbcace2f10.png?wh=1390x896" alt="图片" title="图5 一阶Ambisonic麦克风"></p><p>这里由一阶Ambisonic麦克风采集的原始数据我们叫A-format，是无法直接播放的，需要按照多通道转码格式先转为4通道的B-format。4通道的B-format也叫作一阶B-format。其中的四个通道分别称为W、X、Y和Z。简单一点来理解，这四个方向分别代表了一个球形声场的中心、左右、前后和上下。B-format的数据就可以用软件渲染成任意播放设备支持的格式，比如立体声、2.1、5.1甚至7.1。</p><p>低阶的Ambisonics麦克风可以还原一个比较小的声场，而如果是飞机场、大型演唱会等场景，则可能需要一个如图6所示的高阶的Ambisonic麦克风。我们可以看到阶数越多需要的麦克风的个数也就越多。</p><p><img src="https://static001.geekbang.org/resource/image/a0/99/a00f792256dacfc3f7aaa644a9fba799.png?wh=910x698" alt="图片" title="图6 高阶Ambisonic麦克风"></p><p>Ambisonics技术在AR、VR等需要转动视角的场景里可以很好地还原整个声场的听感，所以被广泛应用。</p><h2>空间音频的播放</h2><p>好的，讲完了空间音频的采集。我们再来看看我们是如何把采集到的声音回放出来的。空间音频最常用的方式就是使用耳机播放。在空间音频的原理部分我们说过，要想感受空间音频最少需要一个双声道的音频播放，来让左右耳感知音量、音色、时间延迟等差异，从而形成空间感。同时由于耳机的播放单元离人耳比较近，无需引入额外的声波传递带来听感的变化。所以<strong>用耳机来还原空间音频相对比较准确。</strong></p><p>但是耳机由于受限于结构和功率的限制，在低音部分的表现可能就不如音箱来得“震撼”。而且如果需要同时给多人播放体验空间音频，多声道的音箱系统会是我们的另一条可选的播放途径。多通道音箱系统的渲染格式经过多年的发展已经比较标准化了。常见的多通道音箱系统有2.0、2.1、5.1、5.1.2、7.1、7.1.2等等。那么它们分别代表什么含义呢？</p><p>其实这里我们可以把数字分成A.B.C三个部分。其中，A代表有多少个环绕声扬声器（前置、中置和环绕）的数量；B表示有多少个超低音音箱；C代表顶部或向上发声扬声器的数量。</p><p>举个例子，如图7所示，这是一个Dolby 7.1.4声道的全景声音箱系统。其中环绕扬声器是其中长方体表示的左前、中置、右前、左、右、左后、右后共7个环绕扬声器。超低音音箱是左上方的正方形，它一般都是用于增强200Hz以下的超低频率的声音，并且由于低频部分的声波波长较长，在房间内方向的感知不明显，所以方位上来说只需要放在房间中的任意位置即可。而在顶部的扬声器一般在听音位置也就是图7沙发的上方均匀排布（黑色圆形），用于提供来自上方的声音渲染。</p><p><img src="https://static001.geekbang.org/resource/image/af/20/afb9aa3228b6e97df788cb5d12744020.png?wh=704x604" alt="图片" title="图7 Dolby7.1.4家庭影院系统"></p><p>我们可以看到，<strong>不同的声音借由各个扬声器的位置和播放渲染可以实现比较好的声场还原。</strong>但是这种传统的家庭影院的布局并不能完美地还原声场。比如来自下方的声音就不能清晰重现，并且听音者只能在相对固定的位置（比如沙发）上才能获得正确的听感。所以这样的家庭影院看看电影还是不错的，但是玩一些VR游戏之类的就会显得声场渲染不足。</p><p>其实在普通的电影院也只是这种家庭影院的扩展，只是多一些环绕音箱让声音分辨力更强等等。从原理上来说音箱分布得越多、越密集，对声场的还原就越好。但显然这样所需的成本是巨大的，这其实也是多声道影院系统推出这么多年，推广一直受限的主要原因。</p><h2>小结</h2><p>好的，今天的课到这里就要结束了。让我们来总结一下这节课的内容。</p><p>首先，我们对空间内声音的位置的感知是因为我们的耳廓形状和双耳效应。对空间中不同位置传来的声音，左右耳会得到不同的音色、音量以及延迟变化。距离感则主要是声音的相对音量大小以及声波空气衰减时，高频衰减快于低频导致的频率能量变化来感知。而听混响的持续时间则是我们感知所处空间大小的方法。</p><p>空间音频的采集可以使用入耳式麦克风或者人工头的形式来录制可用耳机回放的立体声。而Ambisonics技术则可以采用Ambisonic麦克风来对整体空间声场进行采集，然后通过后期的渲染来自由调整听音的方向。</p><p>这里我想补充一下，在影视制作或者现场录制的时候你可能会采用一些所谓的立体声录制麦克风。比如常见的A-B式麦克风、X-Y式麦克风等，这些录制方式可以采集双声道的立体声信号。这些声音是双声道，虽然你可以听出一些方位感和空间感，但是也可以作为简易的现场录音的方法的，只是这些立体声的录制无法通过后续播放来精准还原现场的声场。</p><p>所以在这节课里我没有把它归入空间音频的录制方法里。但这些立体声麦克风采集的音频，回放空间听感效果还是会优于单声道麦克风，所以在无需完美还原的场景里，也是可以使用的。</p><p>最后，想要感受空间音频逼真的方向感和空间感我们可以使用耳机、环绕声或者全景声音箱的形式来进行回放。其实单声道的声音经过空间音频算法的渲染也可以让人有“声临其境”的感觉，我会在下节课为你详细讲解。</p><h2>思考题</h2><p>好的，这里给你留一个思考题。</p><p>我们在使用蓝牙耳机来听空间音频的时候，为什么有的时候会失去空间感呢？这里提示一下蓝牙耳机有很多协议，其中有的是单向高保真音频协议，例如A2DP、LDAC等；有的是双向低保真音频协议，例如HFP（Hands-Free Profile）或HSP（HeadSet Profile）。</p><p>你可以把你的答案和疑惑写下来，分享到留言区，与我一起讨论。我们下节课再见。</p>',
        article_title: "12｜空间音频入门：如何实现“声临其境”？",
      },
      {
        title: "13｜如何利用HRTF实现听音辨位？",
        id: 470014,
        content:
          '<p>你好，我是建元。</p><p>上节课，我们讲了空间音频的基本概念，以及空间音频是如何采集和播放的。相信你已经基本掌握了空间音频的基本原理。其实在游戏、社交、影视等场景中，空间音频被广泛地应用于构建虚拟的空间环境。</p><p>在空间音频的应用里最常见的一种就是“听音辨位”。比如在很多射击游戏中，我们能够通过耳机中目标的脚步、枪声等信息来判断目标的方向。</p><p>那么这节课我们就来看看，我们是如何利用HRTF（Head Related Transfer Functions）头相关传递函数来实现“听音辨位”的。</p><h2>HRTF简介</h2><p>上节课我们讲的“双耳效应”实际上就是空间中音源的声波从不同的方向传播到左右耳的路径不同，所以音量、音色、延迟在左右耳会产生不同的变化。</p><p>其实这些声波变化的过程就是我们说的声波的空间传递函数，是不是很耳熟？我们在讲回声消除的时候就是通过计算回声的空间传递函数来做回声信号估计的。</p><p>那么如果我们预先把空间中不同位置声源的空间传递函数都测量并记录下来，然后利用这个空间传递函数，我们只需要有一个普通的单声道音频以及这个音源和听音者所在虚拟空间中的位置信息，就可以用预先采集好的空间传递函数来渲染出左右耳的声音，从而实现“听音辨位”的功能了。</p><!-- [[[read_end]]] --><p>而这个和我们头部形状等信息相关的空间传递函数，也就是我们说的<strong>头部相关传递函数（HRTF）</strong>。</p><p>让我们再想一下：音源的声波是如何传递到我们的双耳的？一部分声音没有受到房间的墙壁、地板或者障碍物的干扰，而是直接通过空气传播到我们的双耳，我们把这些直接到达我们耳朵的声音叫做直达声。还有一些声波，经过空间障碍物或者界面的多次反射最后传播到你的耳朵里，从而形成了空间中的折射声或者说混响。很显然，直达声和混响相加就是我们听到的所有的声音了。</p><p>如果我们需要渲染一个真实的空间音频就需要渲染所有的直达声和混响。你有没有发现什么问题？</p><p>没错，直达声和虚拟环境是没有关系的，而混响则和听音者以及音源所处环境的空间布局有关。比如你在一个小房间和荒野大漠中的混响显然是不同的，而且混响还会和音源、听音者以及环境的相对位置有关，比如你在房间的墙角和在房间中央的混响信号，经过的路径显然是不同的。因此<strong>直达声和混响我们通常需要分开来处理</strong>。</p><p>那接下来就让我们先来看看HRTF中和直达声渲染相关的HRIR（Head Related Impulse Response），也就是头部相关冲击响应是如何做直达声渲染的。</p><h2>HRIR与直达声渲染</h2><p>HRIR其实就是预先采集的直达声到达双耳的空间传递函数。为了得到一个线性系统的传递函数，我们通常会用一个脉冲信号作为系统的输入，然后记录这个系统的输出信号。当我们做直达声渲染时，只需要将原始音频卷积对应方向位置的双耳冲击响应就可以得到一个直达声的空间立体声了。</p><p>那么我们是如何采集HRIR的呢？</p><p>为了排除混响只录制HRIR，我们需要在全消实验室录制HRIR。图1就是一个在全消实验室录制HRIR的示意图。在图1中我们看到，全消实验室的墙壁上有很多吸音尖劈，这些材料可以有效地吸收声波，所以在全消实验室里墙壁是不会产生混响的。</p><p><img src="https://static001.geekbang.org/resource/image/e5/46/e5c737657feddb436a159046bf283a46.png?wh=600x468" alt="图片" title="图1 全消实验室真人录制HRIR"></p><p>在采集HRIR时，真人采集就是让人带上入耳式麦克风，然后让扬声器（白色的一圈）在一个球面的不同位置播放脉冲信号。这样入耳式麦克风采集到的就是空间中各个角度的HRIR了。这里的扬声器是一个半弧形阵列，测完一个半弧形后可以改变俯仰角再测一组，直至把所有角度都测完。</p><p>当我们把所有方向的数据都采集完毕后，就可以得到一个球形的数据集。图2就是一个球形HRIR的坐标示意图。我们可以看到图2中央是人头所在的位置，周围的红点就是HRIR采集时音源的方位。</p><p><img src="https://static001.geekbang.org/resource/image/7e/7f/7e08650053855c45271c39111b7e937f.png?wh=760x508" alt="图片" title="图2 HRIR的采集点的空间分布"></p><p>当然让一个真人一动不动地测完全套可以得到比较准确的数据，但是这一整套流程下来时间还是很长的。所以为了方便采集，也可以采用同样的方法，使用人工头来采集HRIR，比如图3就是一个利用人工头来采集HRIR的例子。人工头上的耳廓可以更换，这样就可以实现可定制耳廓的HRIR的采集。</p><p><img src="https://static001.geekbang.org/resource/image/33/5b/33dfc359c01c67b7ac62f35ebf77385b.gif?wh=774x759" alt="图片" title="图3 全消实验室人工头录制HRIR"></p><p>通过采集到的HRIR，我们就可以把单通道的音频分别和某个角度的双耳HRIR做卷积，从而渲染出特定方向的声音。你可能会有疑问，人的耳朵外形轮廓有的不太一样，这会不会影响空间音频的效果呢？</p><p>确实每个人的头部形状可能不尽相同，如果你用别人的HRIR来做空间音频的渲染可能没有你自己平时听得那么自然。从实际使用的结果上来看，<strong>非定制化的HRIR渲染出的音频在角度的判断上差异不是很大，但是渲染后的音色可能会有一些细微的不同。</strong>但目前定制化的HRIR成本还是比较昂贵的，也许在未来人们都进入“元宇宙”，这种定制化的增强听感的需求会催生出一些新的更便宜且准确的定制化方案。</p><p>好，那现在你知道HRIR是怎么采集的了，我再分享几个开源的HRIR的数据集。如果你有兴趣可以拿去试一试。一个是<a href="http://sofacoustics.org/data/database/mit">MIT的人工头数据集</a>，一个是<a href="http://sofacoustics.org/data/database/cipic">CIPIC的真人数据集</a>。</p><p>知道了HRIR的采集和渲染方法之后，我们就可以对任意一个单声道的音频进行直达声方位的渲染了。那么剩下的混响我们要如何去渲染呢？</p><h2>RIR与反射声渲染</h2><p>相较于直达声，反射声或者说混响和空间环境、音源位置、听音者位置都是绑定在一起的。所以如果采用和人工头相似的方法去采集房间冲击响应（RIR），那么理论上一个房间内音源位置和听音位置的组合可以说是无限多的。</p><p>并且就算可以用离散的方式采集一个房间内所有音源位置和听音位置的冲击响应，但是在虚拟环境里房间的形状、大小可以说是千变万化的，不可能每个都采用采样混响的方式来渲染。</p><p>在实际使用中，其实混响的作用不是提供准确的方位感而是提供空间大小，或者说听音者所处环境的感知。实际上根据镜像原理（如图4所示）一个真实声源的声波在经过界面反射后传到听音者的耳中可以等效成一个镜像声源的直达声。</p><p><img src="https://static001.geekbang.org/resource/image/40/7c/4003fa2187d724886f2520ccb84afd7c.png?wh=631x349" alt="图片" title="图4 折射声镜像原理"></p><p>假设我们在一个长方体的房间里唱歌（如图5所示）。那么我们发出的声音经过6个界面的多次反射会形成多个镜像声源。所以其实混响会让对声源位置的判断变得更模糊，但是由于直达声一般来说会比经过反射的混响的音量大一些，所以在一般的房间内不至于让你的听音失去方向感。</p><p>这里值得注意的是，反射声大致可以分为前期反射和后期反射。我们一般把前50ms的反射叫做前期反射，超过50ms的叫做后期反射。一般来说前期反射对我们语音的可懂度是有提升效果的，而后期反射太多则会让声音变浑浊，从而降低语音的可懂度。</p><p><img src="https://static001.geekbang.org/resource/image/4f/55/4f0dd07238bc9df4987yyb49464d1a55.png?wh=1217x679" alt="图片" title="图5 长方体房间内的反射波示意图"></p><p>这里讲的镜像原理其实也就是生成RIR的一种方法。这里我分享一个基于镜像法的<a href="https://github.com/ehabets/RIR-Generator">房间冲击响应的生成器</a>，有兴趣的话你可以自行去了解一下。</p><p>其实镜像法的原理就是给定房间界面的大小、界面的反射吸音系数、空气衰减系数，再通过镜像法建模得到房间中任意位置音源和听音者位置组合的房间冲击响应。<strong>用得到的冲击响应和原始音频做卷积就可以做混响的渲染了。</strong></p><p>在实际使用空间音频的时候，尤其是实时空间音频的渲染时，还有很多需要注意的地方，这里我罗列了几个常见的问题：</p><ol>\n<li>如果我们想要用RIR给声音增加混响，那么我们原始的声音得是“干声”，也就是原始音频不能有混响。这一点是比较苛刻的。如果是在普通的房间录制的声音，比如客厅或者卧室，其实都已经有200ms到1s左右的混响了，再卷积一个RIR那么得到的混响可能比预期的更为浑浊，混响时间也更长一些。<br>\n&nbsp;</li>\n<li>如果音源位置或者听音者位置是移动的，也就是说需要实时生成RIR来对混响进行建模，但镜像法生成混响，尤其在混响时间比较长的时候算力是比较大的。我们有的时候得做出取舍，比如一个房间就使用一个固定的预设RIR来避免算力无法实时计算。这也是很多游戏体验中混响在一个房间内都是一样的原因。<br>\n&nbsp;</li>\n<li>真实的不一定是好听的，在歌曲制作时我们经常会使用一些人工混响效果器来代替真实混响，或者采集一些比较好的固定混响的样本，比如用“维也纳金色大厅”的混响RIR来进行混响的渲染。</li>\n</ol><h2>小结</h2><p>空间音频渲染基本原理中的直达声原理和混响原理我们基本就介绍完了。利用这节课的知识你应该可以自己渲染出一个有方向和空间感的音频了。</p><p>现在我们来总结一下这节课的内容。</p><p>为了实现听音辨位，我们可以使用HRTF对空间音频进行渲染。</p><ul>\n<li>直达声可以采用预先采集的HRIR和原始音频通过卷积的方式来实现。基本上需要哪个方向的声音就去卷积哪个方向的HRIR。HRIR一般需要全消声实验室进行采集，我们可以使用一些开源库中的HRIR来实现。</li>\n<li>反射声或者说混响的渲染则是采用卷积房间冲击响应RIR的形式来实现。由于RIR和房间的大小、材料、听音者和音源的位置都有关系，所以一般采用镜像法模拟的形式来实现。</li>\n</ul><p>在这里我还想补充一点，在实时交互的场景里，空间音频渲染时计算的实时性是很重要的。这里说的卷积和前面回声消除里讲的自适应滤波器一样，都是可以用频域卷积来加速的，比如采样率是48kHz的音频。如果需要和超过64点以上的卷积核做卷积，那么用频域卷积会快于时域卷积。</p><h2>思考题</h2><p>好了，今天的课程就到这里了，最后还有一个思考题留给你。我们如果想用耳机听音一个5.0声道的环绕立体声，我们应该如何用HRTF来实现？</p><p>你可以把你的答案和疑惑写下来，分享到留言区，与我一起讨论。我们下节课再见。</p>',
        article_title: "13｜如何利用HRTF实现听音辨位？",
      },
    ],
  },
  {
    chapterTitle: "音频特效生成与算法",
    children: [
      {
        title: "14｜音效三剑客：变调、均衡器、混响",
        id: 471029,
        content:
          '<p>你好，我是建元。</p><p>你是否很好奇《名侦探柯南》里的变声器在现实中能否实现？百万调音师能让本来唱歌跑调的人的歌声变得好听，这到底是用了什么神奇的方法？其实今天我们要讲的音频中的音效，就是为了实现这些变声、修音等特效而设计的一系列的音频处理算法。</p><p>为了实现某种特定的效果，音效算法的种类有很多，这节课我主要介绍三种常见的音效算法：变调、均衡器和混响的设计和使用方法。</p><h2>变调</h2><p>其实在之前讲弱网对抗部分的时候，我们在做快慢放操作时就使用到了变速不变调算法，这其实是变调算法中的一种用法。在讲算法具体实现之前我们先想一下，变调的物理含义是什么？</p><p>之前我们讲语音信号分析的时候说过，不同的人发音的基频是不一样的。而音调和基频是直接相关的，要变调其实就是要改变基频。而基频的本质是一个信号的循环周期的倒数，比如基频是250Hz，那么当前时间的语音信号就是以4ms为周期的信号。<strong>我们要变调，其实就是把这个循环周期进行扩大或者缩小。</strong></p><p>如图1所示，如果我们把语音信号的基频提升一倍，或者说提升一个八度或者12个半音，那么时域信号（绿线部分，上面为变调前，下面为变调后）语音的波形还是很相似的，只是每一个周期都缩短了一半。</p><p>再看看频域信号（红色部分）是不是变得更稀疏了？最下面的那根亮线代表的基频从250Hz左右提升到了大约500Hz的位置。由于谐波的频率是基频的倍数，所以谐波之间的间隔也变大了。</p><!-- [[[read_end]]] --><p><img src="https://static001.geekbang.org/resource/image/89/61/89e70eyy5132e033ec1e46a0aacbe661.png?wh=702x734" alt="图片" title="图1 升调后的语音时域和频域图"></p><p>那么我们是如何实现音调提升的呢？</p><p>其实方法很简单，就是<strong>把原来的信号进行重采样，但不改变播放信号的采样率</strong>。简单来说，比如把原来20ms的音频每两个点取一个点，然后按照相同的采样率进行播放，这样10ms内需要播放原来20ms的内容。这样一来，原本的信号循环时间周期就变成了二分之一，从而实现了升调。</p><p>但这里有个问题，因为每一段时间内需要播放的音频信号的采样点是固定的。也就是说，通过下采样的方法，音频从原来的20ms缩短成了10ms。直观的感受就是这个人的音调变高了，但说话的语速也变快了，也就是变速又变调。</p><p>但我们想实现的变声只是改变音调，也就是所谓的“<strong>变调不变速</strong>”。其实，实现的方法也很简单，核心思想就是通过把音频中的信号按照一定的规律拼接起来，把音频的长度拉长或者缩短，这就是我们要讲的第一种变调算法OLA（Overlap-and-Add）。</p><p>还记得之前我们讲的STFT吗？<strong>其实OLA的思想和STFT中的Overlap的思想很相似。</strong></p><p>如图2所示，$x$为输入信号、$y$为输出信号。OLA的过程按照图2中b、c、d的顺序：取一帧$x_m$，选择间隔长度$H_a$的下一帧$x_{m+1}$，然后把这两帧加窗（汉宁窗）后，以步长$H_s$把两帧重叠、相加在一起。很显然$H_a$和$H_s$的比值就是原始信号和输出信号长度的比值。这样我们就可以把原始音频拼接成不同长度的音频了，然后再经过重采样把音频恢复成和原始音频相同长度的音频再播放，这样就实现了变速不变调。</p><p><img src="https://static001.geekbang.org/resource/image/92/2f/9229cff9c61f989d2536c6b7d110722f.png?wh=606x455" alt="图片" title="图2 OLA算法的基本原理[br]图片来源 https://www.mdpi.com/2076-3417/6/2/57"></p><p>有趣的是，如果不进行重采样直接播放，由于拼接起来的音频没有改变原始语音的基频周期，只是改变了音频的长短，这就实现了我们弱网对抗中用的“变速不变调”的算法。</p><p>但是采用这种Overlap的方式虽然可以防止连接处的信号产生跳变，但不能保证每一个窗内覆盖的信号都处于周期中的相同相位，或者说<strong>两个窗内信号周期的起始位置不相同</strong>。这就会导致我们常说的“相位失真”。</p><p>如图3所示，拼接的信号会出现时高时低的现象。所以如果我们能实时的根据信号本身的自相关属性，也就是把信号中相似的两段直接拼接在一起，这样就不会有相位的问题了。基于这样的思想，于是就有了波形相似叠加WSOLA（Waveform similarity Overlap-Add）算法。</p><p><img src="https://static001.geekbang.org/resource/image/cb/94/cb0a1479dd41aeebe1f88e2408597f94.png?wh=593x446" alt="图片" title="图3 Overlap导致的幅度变化[br]图片来源 https://www.mdpi.com/2076-3417/6/2/57"></p><p>WSOLA算法的计算步骤如图4所示，其实相比于OLA，WSOLA会在$x_{m+1}$帧的附近寻找和输入信号中如果也移动步长$H_s$的信号$\\tilde{x}_{m}$相似度最高的一段$x’_{m+1}$来做拼接。</p><p><img src="https://static001.geekbang.org/resource/image/dd/b9/dd5b839fee9947397e3314f7b4fe53b9.png?wh=1151x866" alt="图片" title="图4 WSOLA的计算步骤[br]图片来源 https://www.mdpi.com/2076-3417/6/2/57"></p><p>根据相似性原理，其实WSOLA合成出来的变调杂音基本已经没有了，WebRTC中的快慢放用的就是WSOLA。但WSOLA算法在实时变调中有一个问题，那就是每一帧出现的位置由于需要相似性搜索，所以需要更多的未来帧的信息，也就是说，需要引入更多的延迟。</p><p>在时域变调的算法中还有一种<a href="http://articles.ircam.fr/textes/Schnell00a/index.pdf">PSOLA</a>（Pitch Synchronous Overlap and Add），顾名思义需要先计算pitch，然后根据基音周期来改变Overlap的大小，这样就直接实现了变调。但是基频检测的鲁棒性没有WSOLA的相似性搜索高，所以PSOLA的生成可能会出现不稳定的情况，你有兴趣的话可以根据<a href="http://articles.ircam.fr/textes/Schnell00a/index.pdf">链接</a>自行了解一下。</p><p>其实变调算法除了在时域上做拼接，还可以在频域上实现，比如常见的LSEE-MSTFTM、Phase Vocoder等算法。</p><p>我们看到WSOLA等方法通过相似性来寻找拼接对象，但是相似性说到底是通过计算两段时域信号的MSE（mean squared error）取最小值来得到的，这种方法能尽量保持低频相位的连续性，但高频信号的相位差异可能不能确保一致。</p><p>其中Phase Vocoder利用STFT中提供的相位信息，在变调扩展的同时会对每个傅里叶频点做相位修正，生成出的音质会比较高，所以在实时变调中常被使用。有兴趣的话，你可以查看<a href="https://www.jstor.org/stable/3680093">文献</a>了解一下。</p><p>理解了变调，我们就可以通过算法来改变音频的音调了。比如电影《小黄人大眼萌》中“小黄人”的声音，就是通过变调算法把原本男声的音调提高来实现的。</p><p>那么电影《绿巨人》中浩克的那种低沉、怪异的音色又是怎么实现的呢？</p><h2>均衡器</h2><p>我们知道每个人都有自己独特的音色，比如有的人声音比较低沉，有的人声音比较清脆。其实对这些音色的感知主要是由于人们在发音时，频谱中不同频段的能量分布不同而导致的。</p><p>比如声音低沉的人，可能低频分量比较低，而唱高音、音色饱满的人可能高频的能量也能保持得比较多。而其中最直接的可以改变音色，或者说改变声音在不同频率的能量分布的方法就是EQ，也就是均衡器（Equalizer）。</p><p>那么我们是如何实现一个均衡器来对不同频段的能量进行调整的呢？</p><p><strong>其实均衡器就是一组滤波器，比如常见的高通、低通、带通、带阻等形式。</strong>这些可能你之前在大学里的数字信号处理课程里学过。看字面意思应该就可以理解，高通、低通和带通就是让高频、低频或者某个频带的音频保留，而其它的频带都加以削弱，而带阻就是削弱某个频带的音频能量。比如我们觉得人声中齿音太多，想要去齿音，可以在10kHz～14kHz左右加一个带通滤波器，削减一下这部分的能量。</p><p>音频滤波器有很多种，比如常用的FIR（Finite Impulse Response）和IIR（Infinite Impulse Response）Filter。如何根据你想要的频段和增益来设计FIR和IIR滤波器，其实早已编入了教材，不是信号处理专业的同学可以参考《数字信号处理》这本书。</p><p>当然EQ的处理经过多年的发展已经有很多通用的滤波器可以选用了，比如椭圆、切比雪夫、巴特沃斯和贝塞尔滤波器等等。如果你想快速实现一个滤波器看看效果也可以直接使用Matlab中的<a href="https://ww2.mathworks.cn/help/signal/filter-design.html?s_tid=CRUX_lftnav">滤波器设计toolbox</a> 来加速实现进程。</p><p>如果不想自己编程实现，也可以利用一些音频处理软件来进行可视化的处理。比如在Adobe Audition中，我们可以看到各种常见的EQ均衡器。如图5所示，由上到下依次为FFT滤波器、图形均衡器以及科学滤波器。我们在离线自己做一些音频处理时，可以选择其中的一个或者多个串行使用。</p><p><img src="https://static001.geekbang.org/resource/image/d7/87/d7c41d40fd05eb7744255c66009b8a87.png?wh=707x1460" alt="" title="图5 Audition中常见的均衡效果器"></p><h2>混响</h2><p>好的，知道了如何调节音色，我们再来看看和空间相关的混响。</p><p>其实在之前的课程中，我们已经多次提及了混响的概念。在之前讲空间音频时，我们知道可以通过采样的方式或者镜像法得到房间的混响RIR，这样得到的混响叫做采样混响。采样混响真实，但是不一定好听，并且RIR需要和音频信号做卷积才能得到混响信号。当混响时间很长的时候需要的算力也是巨大的。在音乐制作时，为了营造更好的听感经常会使用一些人工混响效果器来产生混响，在实时音效里也可能因为要节省算力而采用人工混响效果器的方式来生成混响。</p><p>简单地理解，混响信号可以看作是直达声和许多逐步衰减、不断延迟的回声信号叠加而成的。假设一个衰减系数$a$和延迟$D$，那么混响信号$y(n)$可以用下面的等比数列来表示：</p><p>$$y(n)=x(n)+ax(n-D)+a^{2}x(n-2D)+…$$</p><p>其中 $x(n)$ 是输入信号，$D$为回声的延迟。而这种形式正是梳状滤波器的形式。如图6所示，所谓梳状滤波器，其实就是因为它的频率响应呈一个梳子的形状。<strong>梳状滤波常被用来消除某些不需要的谐波，但这里主要是利用了它的拖尾效应。</strong></p><p><img src="https://static001.geekbang.org/resource/image/15/ec/152f235816f7128dyy77e47663fb16ec.png?wh=922x347" alt="图片" title="图6 梳状滤波器的频率响应"></p><p>但也如图6所示，梳状滤波器的频谱曲线不平坦，呈现明显的梳状效应。从而对不同的频率成分幅度会产生波动，导致梳状滤波器的拖尾声音带有很强的金属染色效应。而且回声只在延迟为D和D的倍数的时候出现，这就显得过于稀疏了。所以在梳状滤波器的基础上Schroeder使用多个梳状滤波器来解决混响不够密集的问题，然后用全通滤波器（Allpass filter）来消除金属声。</p><p><img src="https://static001.geekbang.org/resource/image/f9/da/f99af8748bd81e2c870b6049663125da.png?wh=1104x338" alt="图片" title="图7 Schroeder混响模型流程[br]图片来源 http://freeverb3vst.osdn.jp/doc/Sun-Schroeders_Reverberator.pdf"></p><p>图7为Schroeder混响模型的结构图，其中每个Comb代表一个梳状滤波器。<strong>但Schroeder依靠全通滤波器生成的混响依旧不够密集。</strong></p><p>后续Moorer又对Schroeder模型进行了改良，把混响的生成拆成了直达声、早期混响、晚期混响这三个部分。加入了FIR模块来模拟早期混响，用6个梳状滤波器和一个全通滤波器来模拟晚期混响，并可以控制各部分的增益。有兴趣的话，你可以通过<a href="http://freeverb3vst.osdn.jp/doc/Elec407-HybridReverb.pdf">链接</a>了解一下。</p><p>其实Moorer模型之后，人们会用各种方式来对混响模型进行改造，现在的混响生成器基本上也都是开放出很多参数，可调节的混响效果器了。比如混响的初始延迟、干湿比、混响RT60时间等。这里介绍一个<a href="http://www.nongnu.org/freeverb3">Freeverb3</a> 开源库，这里有很多基于不同算法的混响效果器实现方法，你可以根据自己的需求了解一下里面的内容。</p><h2>小结</h2><p>好的，今天的课程到这里就要结束了，我们来总结一下。</p><p>改变声音可以从三个基本的方向来修改，也就是改变音调、音色和混响，分别对应了变调、均衡器和混响这三种算法。</p><p>其中变调算法比较常用的有：基于拼接的OLA、WSOLA以及频域的Phase Vocoder等。而均衡器主要是通过一个滤波器组对不同频段的能量来进行调节，成熟的算法有很多，但大多是FIR或者IIR滤波器的组合。人工混响则是可以采用多个梳状滤波器的滤波器组，串联全通滤波器来实现，比较常见的有Scheoeder和Moorer混响模型。</p><p>当然音效的种类还远不止这三种，这节课只能算是音效的常见算法的入门。在这里再和你分享一个音效的网页demo，里面有声网研发的各种声音效果。你可以通过<a href="https://www.agora.io/cn/audio-demo">链接</a>感受一下，看看音效还能做出什么不同的效果。</p><h2>思考题</h2><p>其实知道了这些音效的基本原理，结合常用的一些开源库或者音频处理软件就可以开始对音频进行音频效果调整了。比如我们之前说的绿巨人的音效，实际上就是通过变调、均衡器和混响来整体调节的。绿巨人需要把原来的声音进行降调，比如降八度，然后可以使用10段均衡器，绿巨人的均衡器参数如图8所示，然后再加一个小房间的混响就可以实现了。你可以用Audition的效果模块自己试着调一调。</p><p><img src="https://static001.geekbang.org/resource/image/0f/dc/0f4b314b534dcddd000bff4683e379dc.png?wh=488x261" alt="图片" title="图8 绿巨人均衡器参数"></p><p>期待你的动手实践，过程中遇到任何问题，都可以到留言区中与我交流讨论。也欢迎你把这节课分享给你的朋友，邀请他一同尝试。我们下节课再见！</p>',
        article_title: "14｜音效三剑客：变调、均衡器、混响",
      },
      {
        title: "加餐｜音频技术漫谈之好声音是怎么炼成的？",
        id: 471892,
        content:
          '<p>你好，我是编辑冬青。</p><p>专栏即将接近尾声，不知道你的学习体验如何呢？可以点击右侧的“评价课程”按钮提供更多的建议给我们哦～我和老师会逐一查看大家的反馈。</p><p>作为一堂加餐课，今天我们来学点轻松的，听一场相对简单的分享，看看好声音是怎么炼成的。与建元老师同作为“完美音频”背后默默付出的技术人，我想一定会引起很多共鸣。同时呢，即使脱离音频技术，我想好声音也可以是每个人的标签，不妨看看随着未来音频技术的发展，我们在声音训练上还有哪些可塑的空间吧。</p><p>以下是建元老师在课程直播中所做的技术分享，这里以文字的形式分享给你，相关PPT以及直播回放可以通过文末的链接获取哦！</p><h2>什么是高音质？</h2><p>高音质在后疫情时代被大家频繁提及。你可以回想一下，在你的工作和生活中，疫情期间是不是增加了很多实时音频互动场景，那么在后疫情时代，实时音频发生了哪些改变呢？我们一起看一下。</p><p>首先，这是一个因果导向的事情，由于疫情催化，我们在实时音频的用法和用量上都有了大幅度的增加，以前你可能不会去做一些在线会议、在线教育等等，现在你都要把这些工具用起来。</p><p>在这样的情况下，你对实时音频的质量就会提出更高的要求。比如，日常打电话，可能觉得能听到对方在说什么就OK了，但现在你可能会想去看一些直播以及在线音乐场景，这就需要更多的高保真技术支持。这方面在疫情的催化下，其实是有一个飞快迭代的过程的。</p><!-- [[[read_end]]] --><p><img src="https://static001.geekbang.org/resource/image/ec/ae/ec9bd7b50c45c69bd8acda61c63781ae.jpg?wh=1920x526" alt="图片"></p><p>实时音频技术在不断的演进，到目前为止，我们的直播、在线K歌都可以达到高音质的状态。<strong>那么究竟什么样的音频体验，我们把它定义为高音质呢？</strong></p><p>在实时领域，首先要满足采样率足够高这一条件，要达到48KHz。简单来说就是，你能听到的大部分声音都能进行采样，这样就不会有频带上的损失。</p><p>另一方面，我们现在所使用的一些编解码器能够做到高保真，在4G、5G这样普遍存在的情况下，我们可以使用码率较高的音频编解码器，使音频不会因为编解码导致衰减。</p><p>然后在平时互动中，不知道你有没有注意到，像一些噪声、回声可能都会对音频产生影响。那么在做处理的时候，其实不可避免的会对音质造成一些损伤，比如你降噪的时候可能把一些音乐也当成噪声被消除了。那么在音乐场景，我们就会做一些低损伤的前处理，尽量使音频保真。</p><p>总结来说，<strong>高音质时代就好比你开了一个很清晰的摄像头，你脸上的缺点会暴露无遗。</strong></p><p>在高音质的情况下，我们对卡顿、回声这样的质量问题容忍度会下降。比如，以前你打电话，喂喂喂，没有听到，可能就会重播一遍，现在你可能会经常抱怨网络怎么这么差，为什么老是会卡住，为什么老是能听到自己的声音等等，你会对音质更加敏感。</p><p>然后在听感上也会追求一些细节。因为打电话的时候采样率只有8KHz，高频的信息都已经没有了，听不到也不会去追求那些细节。而如果采样率足够了，又是高保真的，说话的时候如果有一些喷麦、杂音等等，那些高频的信息可能就会更加敏感，你可能会去追求能不能体现出这些声音的细节。比如，在一个房间里，能不能体验出空间感。这些都是对音质的新的追求。</p><h2>什么是实时美声？</h2><p>所以，在这样的情况下，我们就会想能不能让我们的声音更好听呢？当然可以！</p><p>比如在录制一首歌曲的时候，就会有调音师帮忙调音，跑调的地方是不是就可以修正一下，喷麦、齿音不好的地方也会被修正。</p><p>在实时互动场景中也是一样的。实时互联网发展到今天，在音频或者音视频互动场景中我们已经能够把音质做到无损或者半无损，传递到远端。我们对声音细节会有更加苛刻的要求，就像美颜一样，音频上也会有一些美声。</p><p><strong>实时美声可以让你的声音更好听、更动听。</strong>当然了，在算法设计和应用设计上，我们还要考虑很多问题。比如，在实时互动场景中你去美声，就好比你在实时美颜，如果你的美颜有一些偏差，就会暴露自己的本来面貌。实时美声也是这样的，为了满足实时性以及设备低功耗的要求，在算法设计上我们要考虑设备是不是能把这样的算法跑起来，以及算法带来的额外的延迟会不会导致交流的不顺畅等等。这些在做算法设计之前你就要考虑了。</p><p>接下来我们看下，如果你想要做一个实时美声需要做哪些工作呢？我把它称之为<strong>实时美声设计的三驾马车</strong>。这里涉及到的东西会比较多，我在专栏中有做一些知识点的拆解，这里只起到一个先导作用。整体上怎么让声音更好听以及相关细节实现，你可以查看第12～14讲。如果你已经学过这些课程，这里也不妨复习一下。</p><p><img src="https://static001.geekbang.org/resource/image/7b/ec/7b7e4b30784e48deaa8e7fa1c194ebec.jpg?wh=1920x529" alt="图片"></p><h3>数据驱动</h3><p>好听是一个非常主观的描述，每个人的定义和取向都不太一样。那么这个时候，我们就要客观定义一下什么样的声音是好声音了。</p><p>那在我拿到声音变好听这样的任务时，首先我会去网上找一找相关信息，看看大家觉得什么样的人唱歌是好听的，或者什么样的人的声音是好听的，寻找一些这样的标签。还可以去找一找不同的性别，比如男声、女声分别有什么样的声音是好听的，去找一下相对应的形容词，再给形容词做一个分类。比如，你可能听说，这样的男声非常有磁性，或者这个男人说话十分的稳重，女生可能就说这个人说话很温柔或者比较有活力，这样的形容词就是标签。</p><p>在标签的指引下，就会有萝莉音、御姐音这样的分类。加之近些年配音技术的不断成熟，大家对这些名词都很敏感。那像音色方面，可能就会说这个人的音色是比较高亢的、圆润的，其实这些也是标签。</p><p>那么除了音色、年龄、性别分类以外，环境也会对声音产生影响。比如某些空间的混响，你在KTV唱歌和家里就不一样。再比如演讲会的音乐大厅，这些是专门为大规模的管弦乐或交响乐去做的空间，这种声音很需要空间感的塑造，所以混响也是可以划分为一类的。</p><p>而从玩法上来说，声音可以是正常的好听，也可以增加一些好玩的元素。比如用一些电音、自动修音甚至变声，把声音做一些整体的变换，这些都是从玩法上使声音更加好听的一些方向。</p><p><img src="https://static001.geekbang.org/resource/image/c8/ac/c8663fd8faf1438934ef9f6ecebdd8ac.jpg?wh=1920x1095" alt="图片"></p><p>有了这样的一堆标签，我们就可以去网上找一些对应的目标，然后把这些声音下载下来，看看能不能收集一些这样的音乐素材。如果你身边恰好有这样的好声音，也可以录制一下，分析一下他们有什么样的特点。从客观上来说，主要是年龄、性别、风格等等，主观上就是上述标签了。<strong>我们可以按照主观、客观这样的大分类来进行数据收集。</strong></p><p><img src="https://static001.geekbang.org/resource/image/58/fb/589dffda4d93a1dd6c0524byy925dffb.jpg?wh=1920x1095" alt="图片"></p><h3>理论支持</h3><p>有了这样一些数据样本和数据分类，下一步就是寻找理论支持了。很多好声音，其实你知道它好听，却不知道为什么好听，这就需要很多的理论来帮助我们分析好听在哪里。</p><p><strong>覆盖的专业领域有很多。</strong></p><p>像声学，这个人的发音是不是好，就跟发音腔体有关。比如你的发音是不是足够浑厚，你嘴巴的开度是不是足够大，声音的响度如何，还有共振峰决定了发音的音色，基频决定了发音的音调，这些都是在发声时需要注意的一些方向。声学方面还有混响，刚才提到过，不同房间会有不同的混响。</p><p>这块就有很多的理论支持，我们可以提取响度是怎样的，基频是怎样的，混响是怎样的，这样你就可以对声音进行分析了。</p><p>另外一块是语言学，主要和韵律、乐理有关。韵律就是指一个人的抑扬顿挫，从指标上来说就是你的音调变化以及你声音响度的变化，有些重读、重音，或者说你这个字拉得特别长，这些像语速、动态调整、语调的变化就是韵律。还是就是音乐上好不好听，就是说你是不是按照正确唱歌的做法去做的，这就跟乐理有关了，比如你是不是在调上，人声和乐器是不是需要配合，不同的音乐风格也会有所不同。</p><p>还有像心理学这块，是指我们感知声音。其实声音发成什么样子，在我们心理上的感知又是不一样的，我们可以感知到这个声音是冷色调还是暖色调，是有一些情绪标签的。心理上又可以根据双耳效应来感知你这个声音发的位置，不同的声音它的延迟（比如左右耳）是不一样的，那么在心理上就会感觉出这个位置的方向感也是不一样的。这是我们要在最后做的，让声音从心理上也觉得是好听的。</p><p>前面是在塑造你发声的器官是不是正确，空间感是不是正确。其中，语言学决定了你的抑扬顿挫是不是正确，乐理决定了你唱歌是不是正确，心理学决定了你听音是不是正确，甚至和你的播放设备也有关系。</p><p>这块给了我们很多数学的方式，或者说数学描述的特征，去多维度地分析好声音的一般规律。</p><p>举个例子，比如像男性磁性的声音，它其实呈海鸥状，在低频和高频的能量会比较高，中频能量较低，就像一个海鸥展翅的形状，这样的声音往往会表现出比较磁性的特征。再比如说一些温柔的声音，它的节奏就不会那么快，同时咬文嚼字可能也没有那么清楚，这个时候听上去就会比较温柔。</p><p>在有了这些理论支持之后，我们就可以看一下好声音具体是怎么划分的。以下是好声音的金字塔：</p><p><img src="https://static001.geekbang.org/resource/image/3e/df/3e0632a8e084f4e269ab181662c7b4df.jpg?wh=1920x1097" alt="图片"></p><p>首先，我们的好声音一定是分性别的，因为女声的好听和男声的好听其实是不一样的。金字塔的最底端是普通好听，比较明确，这个男女都一样，比如清晰、没有明显的缺陷，这个就是指咬字清楚，没有明显的录音设备导致的缺陷，几乎每个人都能做到。</p><p>再往上就需要一些技巧在里面了。像温柔的声音，它的pitch（音调）变化会比较小，抖动会比较少，停顿会比较多。像一些有活力的声音，男女会各有不同，男的可能稳重点你会觉得好听，但是女生的话，如果你觉得一个女生说话比较磁性，那么不一定是在表扬她，可能她的声音会比较低沉或者沙哑，不一定是好的，所以男女还是要做分类。</p><p>再往上就会考验到我们了。假如你是一个配音演员，那你就会需要这层的技巧，普通人可能不一定能发出这样的声音。比如甜美的声音，各项数值会比较平均，但pitch变化范围却会比较大，像魅惑还会涉及到一些词语、语言方面的选择，会有更高的要求。</p><p>再往上就是专业的配音演员才能达到的层级了。他可以根据不同的场景、不同的情感变化，来自由切换自己的声线，这是最难的，普通人很难实现。</p><p>以上就是好声音的金字塔，你可以对照看看自己在哪一层。这里注意一点，这个金字塔只指你修炼的难易程度，也就是自身靠声学训练或者美声训练去做的难易程度，但实际上如果我们用算法去实现，根据不同的场景、情感去做自由切换，则只要有不同的模式可以自由选择就能实现了。</p><p>反过来说，算法实现不一定很难，只要有足够的理论支持就可以。</p><h3>算法融合</h3><p>在我们有了数据驱动——音频和标签，然后根据理论支持明确了好声音的特征之后（哪些特征是重要的筛选一下，我们把它叫做降维），就需要设计算法去调整声音的细节了。</p><p><img src="https://static001.geekbang.org/resource/image/2b/2b/2b907b4850c1b6e023635b15b513092b.jpg?wh=1920x1097" alt="图片"></p><p><strong>调整的方式主要有这样两种。</strong></p><p>第一种，理论模型。比如说我们要去变调，让这个人发音的调性是正确的，就需要做一些修音，这个时候可以采用一些变调算法。然后你觉得混响空间不够贴合伴奏，比如伴奏是在维也纳金色大厅这种比较大一些的音乐会的混响氛围，而这个人唱歌的时候是在混响比较小的客厅，这时就要改变混响，加入一些混响模型。这种链路式的一个一个模块去改造，根据理论模型就可以实现了，我们也称之为“<strong>链路式的理论模型推导</strong>”。</p><p>第二种，端到端的改变。随着AI技术的发展，我们可以用一些AI的方法，自动提取这个人的风格，而不是提取这样一个一个的链路。比如我们可以做一些整体的变声，整体去改变一个人的音色、音调，以及发音的时长、规律等等。这就是<strong>用AI方法做端到端的调整</strong>，这样我们就可以一次性的把这些工作都完成了。</p><p>从场景上来说，还会有不同的应用，这里我大概介绍一下会有哪些常用的应用场景，结合以下这张图示一起看一下：</p><p><img src="https://static001.geekbang.org/resource/image/8c/61/8cbbcf63af261ac2073f0c572dfc4661.jpg?wh=1920x1100" alt="图片"></p><p>语聊美声主要就是突出男女声不同的声音特点，美声和音效还是有所区别的。语聊美声和歌唱美声主要是在不改变你说话ID的情况下，让你的声音更好听。语聊美声会做些小的细微的调整，比如根据你正常的发音，看看有没有受到设备或者自身状态的影响，导致声音不够饱满，什么意思呢？比如你的高频的谐波有很多的缺失，就可以做一些谐波增强或者加一些EQ的调整，动态调整一下你的频率范围、频率响度，这里把你的高频提升一些声音就会更加好听。这就是语聊美声。</p><p>而歌唱美声则不太一样，它会在歌唱场景下有一个特点，你需要和唱歌、伴奏去融合，刚才提过的混响范围不一样可能就会导致唱歌不是那么好听。</p><p>音效主要是做一些风格转换，可以用于匿名聊天，这块就可以用一些变声的方法。除此之外还有环境的变换，比如虚拟立体声，我们大部分时候用到的都是单通道的声音，也就是指左右耳发出的声音是一样的，而立体声就是双通道的，通过把单通道变成多通道就可以体现出声音所在的方向感，也就是增强空间感。</p><p>那么场景音效，就可以有一些不同的空间场景发出的混响的改变，就像大家普遍会觉得在浴室唱歌特别自信，就是因为加了混响。这和房间的大小、装修材料都是有关系的，都可以通过场景音效模拟实现。</p><p>美音主要对应音色，人的音色调整其实是最直观的，比如感冒期间你可能鼻音比较重，高音的部分由于你的鼻腔共鸣被限制就没有了，反而低音部分被加强了，这个时候利用美音把你的EQ或者整个平响做一些调整，你的声音就可以从一个感冒的状态变成正常说话的状态。</p><p>以上就是这次分享的主要内容，最后还有一些Demo试听，直播回放的效果不是最好的，因为它是经过编解码器以及扬声器播放，再回到麦克风去传播，你可以直接打开PPT进行播放，再对照老师的讲解，整体体验就非常棒啦！</p><p>这里附上直播回放以及PPT的获取链接，也期待你能分享更多好玩的音频知识给我们，我们下节课再见！</p><p>链接：<a href="https://pan.baidu.com/s/1ITgbKfondObV1dQLtpWhFQ">https://pan.baidu.com/s/1ITgbKfondObV1dQLtpWhFQ</a> （密码：und6）</p>',
        article_title: "加餐｜音频技术漫谈之好声音是怎么炼成的？",
      },
      {
        title: "15｜AI变声：音频AI技术的集大成者",
        id: 472580,
        content:
          '<p>你好，我是建元。</p><p>AI技术在音频领域发展十分迅速。除了我们之前讲的降噪、回声消除以及丢包补偿等方向可以用AI模型来提升音质听感之外，AI模型还有很多有趣的应用。其中比较常见的有ASR（Automatic Speech Recognition）可以理解为语音转文字，TTS（Text To Speech）文字转语音和VPR（Voice Print Recognition）声纹识别等。</p><p>在之前讲音效算法的时候，我们知道，要做到变声需要改变整个语音信号的基频，还需要改变语音的音色。传统算法是通过目标语音和原始语音，计算出基频差距和频谱能量分布的差异等特征，然后使用变调、EQ等方法来对语音进行调整，从而实现变声（Voice Conversition，VC）。</p><p>但这些特征的差异，在发不同的音，不同的语境中可能都是不一样的。如果用一个平均值来进行整体语音的调整，你可能会发现有的音变声效果比较贴近目标语音，而有的音，可能会有比较大的偏离。<strong>整体听感上就会觉得变声效果时好时坏。</strong></p><p>甚至由于某些发音在改变了频谱能量分布后，共振峰发生了较大改变，连原本想表达的语意都发生了变化。所以为了获得比较好的变声效果，我们需要实时对语音做动态的调整，而这使用传统算法显然是无法穷尽所有发音、语境的对应变化关系的。</p><!-- [[[read_end]]] --><p>你可能已经发现了，如图1所示，如果我们可以做到语音转文字，文字也可以转语音，那么<strong>结合声纹识别（VPR）把人声的特性加入到TTS之中，是不是就可以实现变声的功能了？</strong></p><p>先别急，接下来我们就基于AI模型的变声算法的角度，来整体认识一下这些常见AI算法背后的原理，以及它们是如何组合、搭配实现变声功能的。</p><p><img src="https://static001.geekbang.org/resource/image/0d/87/0d8f714257c3fff2c621080a31b94c87.jpeg?wh=1920x562" alt="图片" title="图1 基于ASR、TTS、VPR的变声流程示意图"></p><p>我们不妨先从ASR算法来看看我们是如何实现语音转文字的。</p><h2>ASR</h2><p>结合图2，我们可以看一下ASR算法的基本原理。</p><p>语音信号需要转换为频谱信号或者我们之前讲过的MFCC来作为语音特征序列。然后我们根据特征序列推断出对应的音素序列，音素在不同的语言中有很多不同的表达形式，比如中文可以用汉语拼音来表示。最后根据音素和文字的映射字典（lexicon）就可以得到语言对应的文本了。</p><p><img src="https://static001.geekbang.org/resource/image/ed/71/ed7cb9dyy4f11e3e5f8f9e034f098271.jpeg?wh=1771x838" alt="图片" title="图2 ASR算法的基本原理"></p><p><strong>ASR在音频领域的研究一直都是比较火热的方向。</strong>目前在工业界，使用的最多的是基于<a href="https://github.com/kaldi-asr/kaldi">Kaldi</a> 开源框架的算法，有兴趣的话可以通过链接了解一下。Kaldi模型以及其改进版本有很多，这里我主要介绍一下常见的ASR模型的构建方法。</p><p>如图3所示，为了实现比较准确的ASR系统，我们需要构建两个主要的模型：<strong>声学模型（Acoustic Model）和语言模型（Language model）</strong>。然后通过语言解码器和搜索算法（Ligusitic Decoding and search algorithm），结合声学模型和语言模型的结果，综合选择出概率最大的文字序列作为识别的输出。其中声学模型主要是通过语音数据库训练得到，而语言模型则主要是通过文本数据库训练得到。</p><p><img src="https://static001.geekbang.org/resource/image/ea/4e/ea0e354a0e49c17eb4f9883b8432ee4e.jpeg?wh=1920x746" alt="图片" title="图3 ASR模型构建方法"></p><p>这里你可能会有疑问，为什么我们不能从音频的特征信息直接得到文字输出，而需要这么一套相对复杂的模型系统呢？</p><p>这是因为同音字、同音词、谐音、连读等发音特性，可能导致很多容易混淆的结果，从而同一段语音可能会得到多个备选的文字方案。比如图2中根据音素序列可以得到“今天是几号”，也可能是“今天十几号”或者“晴天是几号”等。这时除了声学模型音素读取需要较高的准确性外，还需要语<strong>言模型根据上下文的语境来对ASR的结果进行修正</strong>。</p><p>其实，最近这几年端到端的ASR的研究也有很多不错的进展。比如<a href="https://github.com/espnet/espnet">ESPNet开源项目</a>里就整合了许多基于CTC、Transformer等技术的端到端开源模型，有兴趣的同学可以通过链接自行了解一下。</p><h2>TTS</h2><p>好的，了解了ASR的基本原理后，我们再来看看文字转语音（TTS）是如何实现的。</p><p>其实语音合成作为ASR的逆过程，实现起来主要是先通过一个模型把文字转为语音的特征向量，比如MFCC，或者基频、频谱包络、能量等特征组合的形式，然后再使用声码器（Vocoder）把语音特征转换为音频信号。</p><p>那说到TTS，就不得不提及Google发表的两篇重要论文，一个是<a href="https://arxiv.org/pdf/1609.03499.pdf">WaveNet声码器</a>，另一个是<a href="https://arxiv.org/abs/1806.04558">声纹识别到多重声线语音合成的迁移学习</a>。其中WaveNet声码器首次把语音合成的音频结果提升到了和真人说话一样的自然度，而VPR结合WavNet则是实现了端到端的文字到语音的生成。你可以在链接中找到这两篇论文。</p><p>WaveNet的基本思想是利用因果空洞卷积这种自回归的AI模型，来逐点实现音频时域Wave信号的生成。用通俗一点的话来说就是，<strong>WaveNet是逐个采样点生成音频信号的</strong>，也就是说生成出的第k个点是第k+1个点的输入。这种算法极其消耗算力，比如生成一秒48kHz采样的音频，需要循环调用AI模型计算48000次才能完成，但这样做得到的音频的效果自然度还是很不错的。</p><p>为了缩减算力，同时保持模型生成的计算速度，人们后续对其做了一系列的改进。于是就有了后来的Fast WaveNet、Parallel WaveNet、WaveRNN、WaveGlow和LPCNet等。</p><p>如图4所示，Fraunhofer&nbsp;IIS和International Audio Laboratories Erlangen就曾联合发布过<a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=3b94d755401ea9ace11884cf7bfeafbd">一篇回顾的论文</a>，主要分析了不同的Vocoder的MUSHRA评分。</p><p><img src="https://static001.geekbang.org/resource/image/c7/a5/c7ffbb97e54ec2e3b637e7cbdee2a0a5.png?wh=1446x714" alt="图片" title="图4 不同Vocoder的MUSHRA评分"></p><p>在实际使用中，为了追求效果，可以使用WaveRNN或者WaveGLOW这样效果比较好的模型。但这些模型在服务器端部署还行，在移动端部署算力还是过大。移动端上LPCNet或者基于传统算法的World Vocoder是目前比较可行的TTS实现方式。</p><p>好的，讲完声码器我们再看看<strong>多重声线语音合成的基本原理</strong>。如图5所示，要实现带有任意说话人音色的语音生成，需要依赖3个主要的模块，也就是 Speaker Encoder、Synthesizer和Vocoder这三个模块。</p><p><img src="https://static001.geekbang.org/resource/image/3e/a7/3e5bc072bd28ed0023b344b1d69c4fa7.png?wh=1255x267" alt="图片" title="图5 声纹识别到多重声线语音合成的迁移学习[br]图片来源 https://arxiv.org/abs/1806.04558"></p><p>其中说话人的音色和发音习惯等声纹信息，可以通过一段事先准备好的说话人语料和一个Speaker Encoder（说话人编码器）来提取。说话人编码器其实就是我们之前说的VPR声纹识别技术，常见的VPR技术有I-vector、x-vector、GE2E、Deep Speaker、RawNet等。</p><p><strong>VPR主要目的就是把说话人的声音特点编码成固定长度的向量（SpeakerEmbeding）</strong>。好比我们一般用指纹来验证身份，而VPR得到的声纹也可以用于身份识别。所以VPR技术其实除了可以用于TTS和变声，也可以用于实现身份鉴定、声纹锁等功能。</p><p>Synthesizer（合成器）则是通过音素和声纹信息合成出声码器所需的语谱特征。由于图5中使用的是WaveNet系列的Vocoder，所以需要的是对数梅尔谱（log-mel spectrogram）作为声码器的输入。合成器也有一些现成的框架可以使用，比如Google的Tacotron、微软的Fastspeech等。其实合成器的原理都大致相同，就是<strong>利用AI模型对音素序列进行编码，然后和声纹特征融合在一起，再通过Decoder模型得到声码器需要的输入特征。</strong></p><h2>VC</h2><p>好的，在知道了ASR和TTS的基本原理以后，现在我们来重新审视一下我们要做的AI变声任务。</p><p>假如你想把自己的声音变成目标A的声音，只需要经过以下6个步骤：</p><ol>\n<li>录制一段A的声音；</li>\n<li>通过VPR得到一个A的声纹（Speaker Embeding）；</li>\n<li>录制一段自己的声音；</li>\n<li>用自己的声音通过ASR得到音素序列；</li>\n<li>利用TTS的合成器把A的声纹和ASR得到的音素序列转换为声码器需要的特征；</li>\n<li>利用声码器得到变声后的音频。</li>\n</ol><p>实际上利用上面的技术，你只需要利用VPR提取任意目标的声音，就可以灵活地实现任意目标声音的转换。但要实现这么一整套VC系统，你需要同时实现ASR、TTS这两套可以说是音频领域最为复杂的AI模型。那和很多AI模型一样，你可能会想有没有什么办法可以实现端到端的变声呢？</p><p>答案是有的，尤其是<strong>基于GAN（Generative Adversarial Networks）技术的变声模型</strong>，例如<a href="https://paperswithcode.com/paper/maskcyclegan-vc-learning-non-parallel-voice">Cyclegan</a>、<a href="https://paperswithcode.com/paper/stargan-vc-non-parallel-many-to-many-voice">Stargan</a> 等模型，都在变声领域实现了端到端的变声方案，有兴趣的同学可以通过文稿中的链接了解一下。</p><h2>小结</h2><p>好的，今天的课程到这里就要结束了。我们来回顾一下这节课的内容。</p><p>相比于传统算法，基于AI模型的变声可以动态地根据发音的内容来对语音进行调整，从而实现更为自然且逼真的变声效果。基于AI的变声模型主要包括了三个模块：ASR、TTS和VPR。</p><p>Kaldi是目前最流行的ASR框架。Kaldi利用声学模型和语言模型来综合分析提取语音中的音素和文字信息，从而提升了文字识别的鲁棒性问题。而常见的端到端模型框架可以采用ESPNet等开源框架，在实际使用中，目前同等准确性和内容覆盖率的前提下，端到端模型和Kaldi框架比较起来算力会更大一些。而且端到端模型如果要针对某个场景做定制时，因为要重新训练模型，所以受到数据的限制可能会更大一些，所以端到端模型目前还没有得到大规模的使用。</p><p>TTS技术中，WaveNet声码器已经可以让合成语音得到媲美真人声音的自然度。而基于VPR声纹的TTS技术，可以实现语音音色根据目标声纹来任意替换，这为后续的变声应用提供了一种比较方便的实现方式。</p><p>最后，变声技术把ASR、TTS和VPR技术融合起来，从而实现了高质量的变声系统。在实践中，目前这种基于ASR、TTS和VPR的变声系统，效果要优于传统音效算法和端到端的AI变声算法。但这套系统的算力、存储空间的复杂性还是很高的，在部署和成本方面仍然有很多挑战。</p><h2>思考题</h2><p>这里留给你一道思考题，其实变声技术要实现实时的变声，依旧是一件极具挑战的事情，主要是需要在延迟和效果中做平衡。你可以思考一下，为什么离线的变声效果会优于实时在线变声，或者换句话说，为什么不引入延迟变声的效果就会变差呢？</p><p>你可以把你的答案和疑惑写下来，分享到留言区，与我一起讨论。我们下节课再见。</p>',
        article_title: "15｜AI变声：音频AI技术的集大成者",
      },
    ],
  },
  {
    chapterTitle: "结束语",
    children: [
      {
        title: "结束语｜选择比努力更重要",
        id: 472989,
        content:
          '<p>你好，我是建元。</p><p>这是专栏的最后一讲，恭喜你成功搞定了音频技术基础。也十分感谢各位同学的一路陪伴，希望这场学习之旅能让你不虚此行。最后一节课我们轻松些，我呢也再唠叨几句，和你分享一些我的学习和工作经验。今天没有那么多的公式和理论，泡杯茶边喝边听可能滋味更佳。</p><p>音频可能是为数不多的既可以当做工作，也可以当做兴趣爱好的一个学习方向。在如今这个可能越来越“卷”，越来越需要努力的社会，学点好玩的，做点自己感兴趣的事就显得弥足珍贵。</p><p>下面我放了一个时频图，你猜猜这是什么声音？看时域的波形似乎很有节奏，大概基频会是60Hz左右，好像人类发不出这样的音来。再看频域的能量分布十分均匀，又有点像清音。这其实是猫在开心的时候会发出的一种特有的“咕噜”声。这是我之前在录课程音频的时候，我们家的猫主子一直会过来“骚扰”我，这就是它留下的音频。然后根据频谱图，很容易看出这不是正常的人声，于是在后期剪辑时，就可以很快地挑出来并剪掉了。</p><p><img src="https://static001.geekbang.org/resource/image/bc/78/bc247c2771e1fff6f02635657895fa78.png?wh=776x752" alt="图片" title="图1 猫咕噜的时频图"></p><p>你看，工作和生活其实可以有很多交集。其实现在很多领域的知识也都是互通的。要想把一件事做到极致可能需要很多领域知识的灵活运用。而这就需要我们能够掌握快速学习的能力。所谓“天下武功，无坚不破，唯快不破”，<strong>掌握了正确的学习方法就能事半功倍。</strong></p><!-- [[[read_end]]] --><p>我记得，我刚接触音频的时候，开组会时我都不知道大家在讲的NetEQ、AEC是什么意思。后来我找了一些前辈先疏通了整个音频链路，然后又请教了各个领域的专家，了解每个模块内部的组成和算法原理。这种先面后点的方法是快速学习的关键。这是因为整体了解一个事物之后，你就可以有针对性地去学习你需要知道的知识。吾生也有涯，而知也无涯。人无论天资多高也不可能穷尽所有的知识，所以<strong>有选择的深入就显得尤为重要</strong>。</p><p>再有“工欲善其事必先利其器”，不同的学习途经可能会让你的学习效率千差万别。<strong>如何制定学习计划，选择学习工具，其实是可以通过你想要达到的结果去反推的。</strong>比如你想做世界上最好的降噪系统，你就需要了解目前最好的降噪算法有哪些，它们各有什么优缺点。那么最好就有比赛或者回顾性的论文可以帮助我们快速获取结果，一般来说比赛的结果有比较好的时效性，而回顾性质的论文可以让你了解整个算法以及系统发展的脉络，从而更好地判断算法、系统的优缺点。</p><p>知道了什么是最好以及优缺点之后，就需要我们<strong>快速实现一遍其中你认为比较好的方法</strong>。这里也要感谢一下 GitHub、Paper with Code 这些伟大的工具以及很多无私奉献的社区开源者，基本上大多数的算法实现目前都能找到开源的快速实现方法。</p><p>最后，<strong>你需要对这些知识、方法形成自己的理解</strong>。这点很重要，每个人的学习路径不同、经验不同，给出的结论也不尽相同。只有自己从原理推敲后的理解才最为可靠。就拿我来说吧，有很多做音频的工程师觉得16kHz的采样率对于人声的处理就已经足够了，比如WebRTC中默认就是16kHz的采样率做编解码的。但后来我自己录制了一下人声，绘制一下频谱，却发现正常人说话在8kHz以上的高频还是有很多能量的，要想有一个音色饱满的人声，采样率16kHz其实是不够的。所以有的时候，需要自己做出判断，别人的建议终究只是参考。</p><p>可能有很多人会告诉你：要努力、要坚持、要奋斗。我相信你能抽空完成整个音频课程的学习，本身就已经说明你是一个努力且坚持的人了。可努力的人千千万，成功的人除了努力外还需要多思考当前的行业需要什么，未来发展的趋势可能会遇到什么样的问题和挑战。所谓人无远虑必有近忧，<strong>敏锐的感知和思考是你选择职业、研究方向的必由之径，而选择比努力更重要。</strong></p><p>祝贺你，坚持完成了本专栏的学习。这是终点，也可能只是你学习、工作生涯的一个起点。愿你踏出坚实的每一步，在音频的道路上走得更远。</p><p>最后文末有一份结课问卷，希望你可以花两分钟的时间填写一下。我会认真倾听你对这个专栏的意见或建议，期待你的反馈！</p><p><a href="https://jinshuju.net/f/ULbKTs"><img src="https://static001.geekbang.org/resource/image/a5/60/a537ae76481bcc9a0e3f41690586c460.jpg?wh=1142x801" alt=""></a></p>',
        article_title: "结束语｜选择比努力更重要",
      },
      {
        title: "期末测试｜来赴一场满分之约吧！",
        id: 473014,
        content:
          '<p>你好，我是建元。</p><p>《搞定音频技术》这个专栏到今天就完结了。</p><p>为了帮助你检验自己的学习效果，我特别给你准备了一套结课测试题。这套测试题共有 20 道题目，包括 7 道单选题，13 道多选题，满分 100 分，系统会自动评分。点击下面按钮，马上开始测试吧！</p><p><a href="http://time.geekbang.org/quiz/intro?act_id=1317&exam_id=3454"><img src="https://static001.geekbang.org/resource/image/28/a4/28d1be62669b4f3cc01c36466bf811a4.png?wh=1142*201" alt=""></a></p><!-- [[[read_end]]] -->',
        article_title: "期末测试｜来赴一场满分之约吧！",
      },
    ],
  },
];
