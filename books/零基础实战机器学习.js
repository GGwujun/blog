exports.category = "other";
exports.title = "零基础实战机器学习";
exports.data = [
  {
    chapterTitle: "开篇词",
    children: [
      {
        title: "开篇词｜开发者为什么要从实战出发学机器学习？",
        id: 413037,
        content:
          '<p>你好，我是黄佳。</p><p>去年，我写了一本书，叫《零基础学机器学习》。很多人都挺喜欢这本书，就问我我是怎么入行机器学习的。要说这事儿，得从十几年前开始说起喽。</p><h2>IT老兵和“寒冬”的故事</h2><p>我是一个IT老兵。早在2005年，我就写了本书叫《SAP程序设计》，这个是国内第一本讲SAP的书。后来我还写了很多SAP进阶的书。所以，我算得上是SAP这方面的资深顾问吧。</p><p>可是呢我就发现啊，人他不可能只靠着一种技术就吃一辈子的饭。所以啊，这日子过着过着，我突然发现，曾经那么火爆的、20年前就号称年薪百万的SAP顾问市场变得越来越冷清了。而我所在公司的大客户们的兴趣点，都纷纷转移到了AI、大数据、上云这些新领域上了。所以我只好开始关注数据科学、机器学习和云计算这些技术，关注如何帮他们从数据中挖掘出价值。</p><p>我知道，你也许还没有像我一样，遭遇行业的“寒冬”。可是互联网行业日新月异，我们稍不留神就会被后浪打在沙滩上，所以你可能也会担心自己的未来，也想在自己的主技能之外，学一些新的“保命”技能。</p><p>而机器学习是现在的热点，聊天机器人、人脸识别、自动驾驶以及所有大型电商的精准推荐系统背后都是机器学习。不仅如此，机器学习还能解决很多工作上的问题：</p><!-- [[[read_end]]] --><ul>\n<li>比如说公众号的阅读量一旦超过10万，你就看不到具体数量的统计了。这时候，机器学习就能通过“点赞”量和“朋友在看”量估算出阅读量；</li>\n<li>还有公众号的自动留言回复，机器学习可以为不同类型的留言，选择不同类型的回复，甚至过滤掉恶意的评价；</li>\n<li>又比如说做网络电商，机器学习可以找到我们最需要关注的那一批高价值的用户；</li>\n<li>……</li>\n</ul><p>除此之外，机器学习也还能解决生活中的很多实际问题，比如预测你的女朋友今天开不开心呀，预测明年白菜会不会涨价之类的。考虑到这些优点，你可能也想学点机器学习的知识。可是机器学习的算法又多又难，让你望而却步。</p><p>就比如下面这张机器学习算法图，它还只是机器学习领域的冰山一角。是个人看到这里，都会发出疑问：这怎么学！</p><p><img src="https://static001.geekbang.org/resource/image/d7/14/d7ec5f492d4d1c87eddaf77305281214.jpg?wh=2284x4346" alt=""></p><p>在行业的“寒冬”，面对机器学习这样的“高山”，我开始转型的时候，也和你一样惶恐、不知如何下手。不过，我凭借自己20多年的从业经验，终究还是越过了这座“高山”。我成为了公司的机器学习专家，还写了这本《零基础学机器学习》。而且，在摸索着入了门以后，我只花了几个月的时间，就总结出了一套属于自己的学习秘诀。</p><p>听到这里，你肯定很想知道这个秘诀是啥吧。别着急，我接下来会倾囊相授的。不过，比这个秘诀更重要的是，你要想清楚一个问题：为什么要学机器学习呢？</p><p>你的答案可能有很多，但是不外乎有两大类：一是，我要研究机器学习算法；二是，我要使用机器学习技术。</p><p>如果你的答案是第一类，我的建议是打好数学基础，硬啃机器学习的经典教材，深入研究机器学习的算法，并研读最前沿的论文。而如果你的答案是要使用机器学习的技术，那我觉得对你来说，<strong>最简单的学习方法就是“做中学”</strong>。</p><p>因为我工作的主要内容，就是把机器学习技术用起来，从数据中挖掘出新的价值。所以，我是典型的第二类人，“做中学”的方法就可以让我快速入门机器学习，掌握它的精髓。</p><p>但说实话，我一开始学习的时候，却和第一类人一样，一上手就开始硬啃经典教程。在我刚研究了两个月机器学习的各类图书、论文的时候，公司就来了个项目：客户希望我们用新的AI算法，来帮他提高垃圾邮件分类器的效率。</p><p>怎么办呢？</p><p>那时候我看不懂很多的算法，但项目来了不能不做。所以，我就一边继续研究各种算法，一边收集大量的邮件数据；然后就把那些算法运用到这些数据里，来训练模型；有了模型以后再比较它们的效率。就这样鼓捣了几个月之后，我还真就调出来了个不错的模型。用户一试，分类效率提升近20%！</p><p>做完了这个项目，我也就了解了机器学习从建模到部署的全部流程，也用了深度学习解决了实际的问题。再回过头去看看这个过程，对我来说，了解算法中每一步的实现细节真的那么重要吗？</p><p>其实，对于我们这些机器学习的应用者来说，我们的价值在于，结合每一个业务的实际需要，把算法模型灵活地运用到一个一个的具体场景之中去。</p><p>所以算法原理要了解，但算法的推导细节就没那么重要了。而算法的原理，你只要在“做中学”就好了，而且带着问题去学，更有利于加深你对概念和理论的理解。所以，如果你也希望自己快速上手机器学习的话，我建议你一定要从动手做实际项目开始。</p><h2>怎么才能从实际项目开始呢？</h2><p>不过，你可能仍然会有下面这些想法：</p><ul>\n<li>佳哥，你是在工作中学的，那我在工作中接触不到这样的项目怎么办？</li>\n<li>我是个Java或者其它技术栈的程序员，也没接触过机器学习或者Python，能直接上手实战吗？</li>\n<li>我连机器学习的算法有啥都不知道，怎么找到、选择适合我的项目的算法呢？</li>\n<li>就算找到了算法，Python里有这么多包，我该用哪个呢？</li>\n</ul><p>总而言之，就是我做都不会做，我还能从“做中学”吗？我的答案是：当然能。而且这门课正是为你准备的。</p><p>其实，当初我入门机器学习的时候，就发现现在的机器学习教程，特别不利于我们这种抱着“学点实用技能”的想法，去学习机器学习的人。</p><ul>\n<li>有些资料里充斥着大量算法的解析。这对于研究者来说是必备的，但对于只是想把机器学习当做工具的普通程序员和数据分析人员来说，就没啥必要了。</li>\n<li>还有些教程过于简单，它们很多是用“日漫”风格的文字和插画解释机器学习算法的。你看完这些图，好像明白了什么，但一实践就会发现根本不知道这些算法到底该怎么用。</li>\n<li>还有些资料中理论和实践内容搭配得很合理，但问题在于缺少真实的业务场景。比如，它们大多数用的还是“预测加州的房价”、“给鸢尾花分类”这样的教学型案例，很难给我们更多实用性的启发，遇到实际问题了仍不知怎么去做。</li>\n</ul><p>因此，基于我入门机器学习的痛点问题，我设计的这门课程，就是希望给你一个尽可能接近真实业务场景的项目，让你通过闯业务场景关来学习机器学习。这个过程中，我会手把手教你针对项目，怎么搜寻合适的算法，怎么找到最合适的包来完成项目。</p><p>如果你认认真真地把课程学完、把所有的项目做完，你就能获得一个从具体问题到合适的算法再到所需工具的完整知识图谱。</p><p>同时，为了避免你吃不透这些算法的原理和具体用法，我对步骤的描述是非常详细的。每一步都有代码、有注释、有输出、有结果，只要你有点编程经验，学习起来都是毫无压力的。</p><h2>闯关业务场景玩转机器学习</h2><p>具体来说，我把整个课程划分成了 3 个核心模块。</p><p><strong>第一个模块：准备篇</strong></p><p>在这个模块，我先会讲清楚机器学习究竟是什么；然后，手把手带你部署一个机器学习项目的开发环境；最后，我会通过一次简单的机器学习线性回归实战，也就是估计软文点击率，来教你机器学习的实战5步。</p><p><strong>第二个模块：业务场景闯关篇</strong></p><p>我结合电商“易速鲜花”的运营场景，设置了五大关卡。我会在实战中引导你如何去理解，去利用机器学习基础知识和算法解决实际问题。</p><p>你可以把这个模块当作一个闯关游戏。闯关结束后，你就能灵活地运用机器学习的80%算法了。</p><p><img src="https://static001.geekbang.org/resource/image/c9/27/c9ef72a215ff1141bdc4ecd967911527.jpg?wh=2284x1033" alt=""></p><p>这五大关卡分别是：</p><p><strong>获客关</strong>：获客的本质是找到潜在用户。我会教你如何用机器学习了解你的用户，如何计算他们的RFM值，如何用聚类分析给他们做价值分组画像等。</p><p><strong>变现关</strong>：变现的目标是赚钱和赚更多的钱。我们的具体项目，包括如何预测App用户的生命周期价值，以及如何提升LTV预测准确率。这里你将学到回归算法、决策树算法和随机森林算法，以及一些特征工程和模型性能优化的知识。</p><p><strong>激活关</strong>：激活指的是让潜在用户从“知道”某个产品到“用上”某个产品，并且“爱上”它的过程。我会通过如何给鲜花图片分类来讲一下什么是深度学习，然后教你如何用深度学习预测激活率、如何提高预测准确率。通过这一关，你可以学会使用深度学习中的卷积神经网络CNN和循环神经网络RNN算法。</p><p><strong>留存关</strong>：留存是增加用户的黏性，防止用户的流失。我们会学习如何找出影响用户留存的因子、如何找出高流失风险的用户、预测他们是否会流失。这里你将学到逻辑回归和深度学习在分类问题中的实战应用。</p><p><strong>裂变关</strong>：裂变是让现有用户去自发推广产品。我们会通过XGBoost选择最佳的裂变方案，让你的用户变身为你的推销员。</p><p>你也许会在想，我没接触过运营场景怎么办？其实你完全不用担心，我要讲的这些场景，你零基础也完全能够理解。而且我还会介绍相关的知识，在学机器学习的同时，你还能学到运营的技巧，可谓一举两得。</p><p>而且，虽然我们是借着运营场景来讲解机器学习的，但是我同样会告诉你在别的场景下怎么运用它们，让你能够融会贯通。</p><p><strong>第三个模块：持续赋能篇</strong></p><p>你可能会觉得机器学习的发展日新月异，如果这门课里讲的这9大算法，以后不流行了怎么办呢？一方面，我会把一些机器学习算法的实用知识和学习技巧放到这个模块中；另一方面，我也会不定期地讲解一些最新的落地的算法，持续为你赋能。</p><p><img src="https://static001.geekbang.org/resource/image/92/fc/92dee0118aa4f9508ca09c2880dacffc.jpg?wh=1563x4751" alt=""></p><p>最后，我想说的是，诸多优秀的机器学习算法都已经被封装在各个Python库中了，只等你来调用了。而我也会把项目代码放在GitHub上给你下载。<strong>真正在工作中最有用的东西，其实没有你想象得那么多。</strong>我已经帮你把它们摘出来了，你只要肯动手，就能轻松把它拿下。</p><p>准备好了吗？一起出发吧！</p>',
        article_title: "开篇词｜开发者为什么要从实战出发学机器学习？",
      },
    ],
  },
  {
    chapterTitle: "准备篇",
    children: [
      {
        title: "01｜打好基础：到底什么是机器学习？",
        id: 413057,
        content:
          '<p>你好，我是黄佳。</p><p>欢迎来到《零基础实战机器学习》！在开篇词里面，我说过学机器学习的秘诀是“做中学”。不过呢，即使上来就想要上手做，最最最基础的知识你还是得懂点。</p><p>说是基础知识，其实你也不用怕。我们这一讲里要讲的知识点不多、也不难，主要是想讲讲什么是机器学习和一些最常见的概念。你不知道这些的话，肯定是没法开始实战的。</p><p>除此之外，我想讲讲机器学习的分类。我讲这些，是想让你在实战之前对机器学习的大类心中有数，方便你一边实战，一边构建自己的知识图谱。同时，也可以让你提前了解下我们的实战项目，对我们将来要闯的关卡有个预期。而至于更多的基础知识，我会带你边做项目边学习。</p><p>在开始之前，我们先给这一讲设一个小目标，就是你在学完之后，当别人问你什么是机器学习的时候，你要能够给他解释明白。真能把这个说清楚了，你这节课也就没白学了。</p><h2>什么是机器学习</h2><p>这个问题其实不好回答，因为机器学习涵盖的内容太多了。机器学习之父Arthur Samuel对机器学习的定义是：在没有明确设置的情况下，使计算机具有学习能力的研究领域。国际机器学习大会的创始人之一Tom Mitchell对机器学习的定义是：计算机程序从经验E中学习，解决某一任务T，进行某一性能度量P，通过P测定在T上的表现因经验E而提高。</p><!-- [[[read_end]]] --><p>这两个定义你看了之后可能瞬间就懵了，没关系，这里我用“人话”来和你解释一下。</p><p>现在，请你想象这样一个场景：你周日约了小李、老王打牌，小李先来了，老王没来。你想打电话叫老王过来。小李说：“你别打电话啦，昨天老王喜欢的球队皇马输球了，他的项目在上个礼拜也没成功上线，再加上他儿子期末考试不及格，他肯定没心情来。”</p><p>这种情况下，你觉得老王会来吗？</p><p>一般情况下，我们都会觉得老王大概率不会来了。不过，你有想过我们是怎么得出这个结论的吗？</p><p>实际上我们运用了“推理”。我们人类的大脑做这样的推理似乎是自然而然的事儿。但是，对于计算机来说，如果它也像小李那样有老王的“历史数据”，知道他看皇马，知道他的项目情况，知道他儿子的成绩，那计算机能推出这个结论吗？对于长期以来只能按照人类预设规则解决问题的机器来说，这可并不是件容易的事。</p><p>而机器学习的厉害之处就在于，它能<strong>利用计算机的运算能力，从大量的数据中发现一个“函数”或“模型”，并通过它来模拟现实世界事物间的关系，从而实现预测、判断等目的</strong>。这个过程的关键是建立一个合适的模型，并能主动地根据这个模型进行“推理”，而这个建模的过程就是机器的“学习”过程。</p><p>那么机器学习和我们传统的程序有什么区别呢？实际上，传统程序是程序员把已知的规则定义好后输入给机器的，而机器学习则从已知数据中，通过不断试错、自我优化、自身总结，归纳出规则来。下面这张图，直观地阐述了机器学习和传统程序的区别，你可以看一下。</p><p><img src="https://static001.geekbang.org/resource/image/44/bc/4403627159bf9ff0e14cb55c5f803ebc.jpg?wh=2284x1067" alt=""></p><p>这张图中展示了机器学习的本质特征，就是<strong>从数据中发现规则</strong>。</p><p><strong>虽然说我们希望机器最终能主动地去预测，但在此之前，机器具体选什么模型、如何训练、怎么调参，我们人类还是要在这个过程中给机器很多指导的，这就是我这门课要教你的。</strong></p><p>不过，机器到底怎样建立新的模型呢？我们继续以刚才的场景为例，不过，现在我想请你从函数的角度想一想刚才发生了什么？</p><p>要预测老王的状况，我们就需要建立一个“预测老王会不会来”的函数，而“皇马输赢”、“项目情况”、“儿子成绩”都是输入到这个函数的自变量，我们设为x1, x2, x3。这些自变量每一个发生变化，都会影响到函数的结果，也就是因变量y。</p><p><img src="https://static001.geekbang.org/resource/image/de/ae/de16bc2aa49010cded5cbe8576b388ae.jpg?wh=2284x1285" alt=""></p><p>在机器学习中，这些自变量，就叫做特征（feature），因变量y叫做标签（label）。而一批历史特征和一批历史标签的集合，就是机器学习的数据集。</p><p>理解了这些，我们就可以更加“精准”地定义机器是怎么“学习”的了，就是在已知数据集的基础上，通过反复的计算，选择最贴切的函数去描述数据集中自变量x1, x2, x3, …, xn 和因变量y之间的因果关系。这个过程，就叫做机器学习的训练，也叫拟合。</p><p>基于这一点，我们可以说：<strong>传统程序是程序员来定义函数，而在机器学习中是机器训练出函数。</strong></p><p>最初用来训练的数据集，就是训练数据集（training dataset）。当机器通过训练找到了一个函数，我们还需要验证和评估，也就是说，这时候我们要给机器另一批同类数据特征，看机器能不能用这个函数推出这批数据的标签。这一过程就是在验证模型是否能够被推广、泛化，而此时我们用到的数据集，就叫验证数据集（validation dataset）。</p><p>简单来说，在验证、评估的过程里，我们就是要验证这个函数到底好不好。如果这个函数通过了评估，那就可以在测试数据集（test dataset）上做最终的测试；如果通过不了，就需要继续找新的模型。</p><p>讲到这里，我想你已经对机器学习有了一定的了解。不知道你有没有发现，标签似乎对于机器学习模型有很重要的指导性意义，因为机器必须根据已有的数据来找到特征和标签之间的关系。那么，你可能会问了，机器在训练过程中一定要有标签吗？</p><p>其实，机器学习不一定要有标签，具体我们可以分三种情况来看：</p><ul>\n<li>训练数据集全部有标签，叫监督学习（supervised learning）；</li>\n<li>训练数据集没有标签，叫做无监督学习（unsupervised learning）；</li>\n<li>在训练数据集中，有的数据有标签，有的数据没有标签，我们叫做半监督学习（semi-supervised learning）。</li>\n</ul><p>目前，监督学习是应用最广泛的主流机器学习算法，所以，我们这个课程的重点会放在监督学习上。当然，对于剩下两种，我也会做个简单的介绍。我们先来着重了解一下什么是监督学习。</p><h2>什么是监督学习</h2><p>在监督学习中，我们需要重点关注的是监督学习问题的分类。你可能会想问，佳哥，我为什么要知道它的分类？这是因为，明确要解决的问题是机器学习项目的第一步，也是非常重要的一步。如果我们不了解问题的类型，就无法选择合适的算法。</p><p>根据标签的特点，监督学习可以被分为两类：回归问题和分类问题。</p><p><strong>回归问题的标签是连续数值</strong>。比如，如果我们天天给老王的情绪从1到100打分，那要预测老王今天的情绪，这就是个回归问题。再比如说预测房价，股市，天气情况，这都是回归类型的问题。在我们这个课程里，我特地准备了预测客户的生命周期价值、预测产品转化率等回归项目，你可以在这些项目里学会解决各种回归问题的算法和实战套路。</p><p><strong>分类问题的标签是离散性数值</strong>。比如，预测老王今天会不会来打牌，这就是个分类问题。而我们平时看到的鉴别高欺诈风险的客户、辅助诊断来访者是否患病、人脸识别等等，这些都属于分类问题的应用。在我们的课程中，我也为你设置了对应的分类实战，包括判断客户是否会流失、判断两款裂变模式哪个更有效等，帮你掌握解决各种分类问题的算法和实战套路。</p><p>在这里，我用一张图来给你描绘一下回归和分类的区别：</p><p><img src="https://static001.geekbang.org/resource/image/bc/50/bc3240b75ca5f198e1e4da63bee01150.jpg?wh=2284x1067" alt=""></p><p>好，讲完了监督学习，接下来我们简单了解下无监督学习和半监督学习。无监督学习就是为没有标签的数据而建的模型，目前它大多只应用在聚类、降维等有限的场景中，往往是作为数据预处理的一个子步骤显显身手。不过，由于聚类场景相对更加常见，在这门课中，我会用一个“为用户做分组画像”的项目，带你掌握无监督学习的应用。</p><p>而半监督学习，就是使用大量无标签数据和一部分有标签数据建模。这往往是因为获取数据标签的难度很高。半监督学习的原理、功能和流程与监督学习是很相似的，区别主要在于多了“伪标签的生成”环节，也就是给无标签的数据人工“贴标签”。</p><p>由于半监督学习的落地应用还比较有限，我们的课程中就不专门讲它了，如果你对如何生成伪标签有兴趣的话，可以去研读一些论文。</p><p>其实，还有很多现实问题既没法用监督学习来解决，也没法用无监督学习和半监督学习来解决。比如说你要设计一个机器人来陪你玩牌，怎么办？这个时候，就需要强化学习登场了。</p><h2>什么是强化学习</h2><p>强化学习研究的目标是，智能体（agent）如何基于环境而做出行动反应，以取得最大化的累积奖励。这里的“智能体”，其实我们可以把它理解成一种机器学习模型。</p><p>强化学习和监督学习的差异在于：监督学习是从数据中学习，而强化学习是从环境给它的奖惩中学习。</p><p>强化学习智能体在调整策略的时候需要思路比较长远，它不一定每次都明确地选择最优动作，而是要在探索（未知领域）和利用（当前知识）之间找到平衡。它反复试错、不断收集反馈，收集可供自己学习的信号，每经过一个训练周期，都变得比原来强一点，经过亿万次的训练能变得非常强大。</p><p>强化学习对数学基础的要求是比较高的，考虑到我们这是面向初学者的机器学习课程，所以这个课程中并没有涉及强化学习的内容。你如果有兴趣，我推荐你阅读一下有关强化学习的专业书籍，比如Richard Sutton 的经典教材《Reinforcement Learning：An Introduction》，中译名是<a href="https://book.douban.com/subject/34809689/">《强化学习》第2版</a>。</p><p>从监督学习，到无监督学习，到半监督学习再到强化学习，这就形成了一个完整的闭环。所有机器学习领域中的问题和算法都可以归入其中的某个类别。这里你可能又想问，佳哥，机器学习的分类完全穷尽了吗？我常常听说的深度学习怎么没有在这个分类里呢？别着急，下面我给你解释一下。</p><h2>什么是深度学习</h2><p>其实，深度学习是一种使用深层神经网络算法的机器学习模型，也就是一种算法。这个算法可以应用在监督学习、半监督学习和无监督学习里，也可以应用在强化学习中。</p><p>虽说深度学习中用的算法叫神经网络算法，但是这个“神经网络”（Artificial Neural Network， ANN）和人脑中的神经网络没啥大的关联，它是数据结构和算法形成的机器学习模型。</p><p>我们知道，长期以来，图形图像、自然语言和文本的处理是计算机行业的难题，因为这类信息的数据集，并不是结构化的，需要人工根据信息的类型来选择特征进行提取，这样对于特征的提取是有限的，就拿图像来说，只能提取出一些简单的滤波器。</p><p>而深层神经网络的厉害之处在于，它能对非结构的数据集进行自动的复杂特征提取，完全不需要人工干预。也就是说，深度学习让这个曾经的“难题”一下子变得非常容易。</p><p>因此，针对深度学习，在后面的课程中，我会带你一起操练三个项目：</p><ul>\n<li>使用深层神经网络DNN预测用户是否会流失；</li>\n<li>使用卷积神经网络CNN为鲜花图片分类；</li>\n<li>使用循环神经网络RNN预测App用户的转化率。</li>\n</ul><p>学完这三个项目，面对新的类似场景，你就完全能够选择合适的模型，并搭建起属于自己的深层神经网络了。</p><p>总而言之，机器学习正在突破我们对“计算机能做什么”这个问题的认知极限。因为机器学习的存在，计算机能做的事情越来越多，以后还会更多。</p><h2>总结一下</h2><p>到这里，我们这节课就结束了，不知道你有没有这样一种感受，有些东西乍一听挺唬人的，学了之后发现“嗨，也没啥”。机器学习和深度学习的落地应用，应该可以归为这一类。</p><p>在这节课中，我力图给你讲清楚机器学习最基础的一些概念和应用场景。如果你想用一句话给你的朋友说清机器学习是什么，那么请你告诉他：<strong>机器学习是一种从数据生成规则、发现模型，来帮助我们预测、判断、分组和解决问题的技术。</strong></p><p>机器和传统程序最大的不同就是，机器学习不是程序员直接编写函数的技术，是让机器通过“训练”得出函数。而我们做机器学习项目，就是要选定一个算法，然后用数据训练机器，找到一族函数中最适合的那一个，形成模型。</p><p>具体来看，机器学习分为四大类，分别是监督学习、无监督学习和半监督学习和强化学习。其中，监督学习是我们课程的一个重点。它能解决的两类问题：<strong>回归和分类</strong>。所以，我们在开始一个项目时，一定要首先明确我们要解决的问题属于哪种类型，这对模型的选择十分重要。</p><p>至于深度学习，我们说它是一种使用深层神经网络的模型，可以应用于上述四类机器学习中。深度学习擅长处理非结构化的输入，在视觉处理和自然语言处理方面都很厉害。</p><p>最后，我把机器学习中的各种算法做了分类，放到了下图中。这些算法你现在不需要去记忆，以后在项目里用到的时候，我们会解释它们的。</p><p><img src="https://static001.geekbang.org/resource/image/5e/cc/5e526ee5c3e339719f63658fc9132dcc.jpg?wh=2284x1285" alt=""></p><p>作为过来人，我还想嘱咐一句，我非常理解初学者学机器学习，最怕的就是信息过载，新名词太多，一下子理解不了。其实你不必害怕，我会在后续的课程中，通过实际的项目实战，一步一步手把手，把机器学习中的重要概念和工具陆续教给你，让你在实操过程中把它们搞懂、弄透。</p><h2>思考题</h2><p>这节课就到这里了，最后，我想给你留三个思考题，假如你有一个网店：</p><ol>\n<li>你有用户的历史订单数据，你想要根据这些数据给用户分组，这是什么类型的机器学习问题？</li>\n<li>你有用户的人口统计数据，比如年龄、性别、城市和年收入等等，并且你已经把这些用户已经分成了高、中、低三个价值组。这时拉新团队给了你一批新用户信息，想要判断新用户会属于哪一组，这是什么类型的机器学习问题？</li>\n<li>你有用户的人口统计信息和购买历史记录，你想要预测新注册的用户的预期消费总额，这是什么类型的机器学习问题？</li>\n</ol><p>提示：先思考是有监督还是无监督学习，如果你认为是监督学习，进一步想想是回归问题、还是分类问题。</p><p>欢迎在留言区与我分享你对机器学习的类型的理解，也欢迎你和朋友一起讨论这节课的问题。</p><p><img src="https://static001.geekbang.org/resource/image/05/c5/0553155254b44a41a489309df5d52dc5.jpg?wh=2284x1280" alt=""></p>',
        article_title: "01｜打好基础：到底什么是机器学习？",
      },
      {
        title: "02｜工具准备：安装并使用Jupyter Notebook",
        id: 413648,
        content:
          '<p>你好，我是黄佳。</p><p>欢迎来到零基础实战机器学习。工欲善其事，必先利其器，好的环境让我们事半功倍。今天我要讲的就是怎么来搭建机器学习运行的环境。</p><p>你也许听说过机器学习项目对配置要求高，硬件上需要带GPU的显卡，软件上面需要在Server 中搭建 PyTorch 和 TensorFlow 2.0 什么的。这些东西是不是听起来就挺麻烦的。</p><p>其实没有那么复杂。我觉得对于初学者来说，你不必过于纠结上面的工具。我们只是要进行一些简单的实战项目。而这些简单的项目，在本机上完全可以跑通。所以我建议你就用你手头上的笔记本电脑，装个Jupyter Notebook就足够了。</p><p>你听到这可能会想问什么是Jupyter Notebook呀？你可以把它想象成一个类似于网页的多媒体文档，但是，比普通网页更高一筹的是，它还能直接通过浏览器运行代码块，在代码块下方展示运行结果，就像下图中这样：</p><p><img src="https://static001.geekbang.org/resource/image/a3/eb/a38f6f907c83e1cbb30dec791ef265eb.png?wh=1261x792" alt=""></p><p>Jupyter Notebook可以交互式开发，再加上拥有富文本格式，可以显示图文，非常直观，所以它能迅速地展现数据分析师的想法，是我们上手机器学习的最佳工具。</p><p>选好了工具之后，我们还得确定要用什么语言。我们课程选的是Python，所有源代码都是用Python实现的。之所以选Python是因为Python在AI开发中是最常用的语言了，而且Python非常简单，只要你稍微有些编程方面的基础知识，就能看懂我们的代码。而我们推荐使用的工具Jupyter Notebook可以支持多种语言，其中就包括Python。</p><!-- [[[read_end]]] --><p>在这一讲，我会带着你把Jupyter Notebook安装好，并把其中的Python程序跑起来。这个Python程序，我不会带着你跑Hello World，那个太过于简单了，我会带着你画个图，让你了解一下Jupyter Notebook的运行过程，方便你之后上手用它。</p><h1><strong>安装和启动Jupyter Notebook</strong></h1><p>要在本机搭建Jupyter Notebook平台进行机器学习项目，有一种最简单的方法，就是下载并安装Anaconda。Anaconda是一个免费的开发环境，能帮你管理众多的Python库，并支持Jupyter Notebook、JupyterLab、Spyder等工具。</p><p>Python + Anaconda + Jupyter Notebook这三样东西加在一起，基本上就是机器学习工程师的标配。如果你还是对这哥仨的关系不太明白，可以看看下面的图：</p><p><img src="https://static001.geekbang.org/resource/image/56/72/56e2b3f30dc856867f58578db2171d72.jpg?wh=2284x1285" alt=""></p><p>虽然我说了我们需要的是这哥仨，但是别紧张，其实你只需要安装Anaconda就行啦，剩下的两样是Anaconda打包自带的。</p><p>Anaconda的安装其实非常简单。首先，我们到<a href="https://www.anaconda.com/products/individual">Anaconda官网</a>下载最新的个人版本。对于我们初学者来说，个人版本就够用了。选择好适合本机电脑系统的版本下载后，我们直接选择所有的默认选项就行了。Anaconda是跨平台的，Windows，Linux，Mac OS上都可以装。</p><p>成功安装之后，我们就可以在Anaconda中启动Jupyter Notebook了。启动Jupyter Notebook有两种方法：你可以直接Anaconda初始界面里启动它；也可以在开始菜单栏的Anaconda3目录中找到Jupyter Notebook来启动它。这两种启动如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/46/f2/46cd4e2cb7d8727dd2bcb37589dfd2f2.jpg?wh=2284x1038" alt=""></p><p>在上面这两种启动方式中，我推荐第二种方法。因为直接启动Jupyter Notebook可以占用较少系统资源，因此启动起来也比较快。</p><p>Jupyter Notebook是我们课程的主要编辑工具。你初次接触它，可能对它还比较陌生。没关系，接下来，我会通过画一个图，来给你讲讲Jupyter Notebook是怎么运行的。其中涉及的一些操作和工具，我们会在后续的项目中频繁使用。</p><h1><strong>实战Jupyter Notebook</strong></h1><p>我想带你画的图是这样一个漏斗图：</p><p><img src="https://static001.geekbang.org/resource/image/72/6f/725fc62ed91abc471f8e61e74b1bf36f.png?wh=1007x460" alt=""></p><p>“漏斗图”听起来好像有点专业，但它其实就是一个呈现用户流失情况的图。上面这个图的最上面一层，展现的是有多少人浏览这个网页，接下来的几层分别展示的是有多少人下载这个App、注册App、搜索商品、下单购买鲜花。每层的人数是我们早就统计好的。</p><p>如果你没听懂漏斗图是做什么的，其实没关系。我们是想通过画这个图来学习怎么使用Jupyter Notebook的，所以你只要知道我们是要通过一些已知数据来画这个图的就行了。</p><p>那么怎么画出这个漏斗？</p><p>你可能会觉得我们需要写很复杂的代码，去定义这个漏斗绘图的规则。其实是不用的。因为我们用的是Python，而Python的一大特点就是可以通过import语句导入各种各样的包和模块，来帮助我们用他人已经整理好的函数、方法和算法解决问题。Python里已经有画图的包了，我们直接调用就好了。</p><p>所以画漏斗图听起来很复杂，但总结起来就三步：先安装一个可以画漏斗图的包，然后导入它，最后把数据放到这个包里运行就行啦。而在这个过程中，我想教你的是怎么安装包，调用包。这些都是我们未来会常常用到的操作。</p><p>当然了，在做任何项目之前，你都需要新建一个Notebook（这里我们就把Jupyter Notebook文件简称为Notebook吧）。所以我们先来看看怎么新建Notebook。</p><p>新建Notebook很简单，启动Jupyter Notebook后，在你的默认浏览器（我用的是Chrome）中就会显示本机默认目录页面，就像下图中这样，选择任意一个你想放Notebook的目录，就可以新建了。</p><p><img src="https://static001.geekbang.org/resource/image/5b/74/5bae160f13cd7dfe7a62065ce0c83174.png?wh=982x676" alt=""></p><p>除了新建自己的Notebook，你也可以把我在<a href="https://github.com/huangjia2019/geektime/blob/main/%E5%87%86%E5%A4%87%E7%AF%8702/%E6%BC%8F%E6%96%97%E5%88%86%E6%9E%90.ipynb">GitHub上面的Notebook</a>，也就是后缀名为ipynb的文件下载下来，直接修改它。</p><p>接下来，我们正式开始画图，我们的第一步就是安装可以画漏斗的包。这里我们选择的包是plotly，它是一个数据可视化工具包，里面含有漏斗图的绘制工具。在Anaconda中，很多机器学习相关的工具包都是预装好了的，但是Plotly不在其中，所以需要我们手动安装下。那具体怎么安装呢？pip install是我们会经常使用的包安装语句：</p><pre><code>pip install plotly #安装Plotly包\n</code></pre><p>在Jupyter Notebook中，代码是以一个个单元格的形式组织的，所以在Notebook的第一个单元格中写好这个pip install语句后，我们选择红框中所示的Run按钮，就能执行这段代码了。</p><p><img src="https://static001.geekbang.org/resource/image/7b/e3/7b4b463cb5ab064ba0d40c0a0739bae3.png?wh=947x308" alt=""></p><p>我们知道，Python代码和输出都是一段段的，我们随时能查看每段代码的运行结果。运行这句之后，从上图的运行结果中可以看到，在我的安装语句后，输出显示这个包已经装过了，不需要重复安装。如果你是第一次用，就需要安装。那么在后续课程中，不是Anaconda默认预装的工具包，我都会提醒你先用pip install来安装。</p><p>因为我们只需要安装一次，所以这个语句跑完之后，就可以在前面加一个“#”把它注释掉。</p><pre><code>#pip install plotly #安装Plotly包\n</code></pre><p>安装好了以后，第二步就该导入Plotly包了。这时候你要注意，Plotly包里有很多模块，全部导入的话，会占用更多的内存，所以我们直接导入这里要用的模块express。express是个简易的绘图模块，导入它要通过Import语句来实现。在下面的语句中，我把express命名为px：</p><pre><code>import plotly.express as px #导入Plotly.express工具，命名为px\n</code></pre><p>再次选择Run按钮运行这段代码后，就会看到下图所示的情况：</p><p><img src="https://static001.geekbang.org/resource/image/26/97/262a80d1479c863f0449d3f359b72997.png?wh=897x420" alt=""></p><p>你会发现，这段代码并没有输出结果。其实只要没有报错，就说明我们成功导入了plotly.express工具。不过，你可能会问我，除了刚才的两段代码之外，Notebook中标下划线的那一段字是怎么回事呢？</p><p>其实，这段文字是我在Notebook的代码之间添加的一段笔记。Jupyter Notebook有两种主要类型单元格，选中当前单元格之后，就可以通过上图红框中的选项进行单元格类型的切换：</p><ul>\n<li>Code类型的单元格里是可执行的程序代码，它的输出会显示在本单元格下方。</li>\n<li>Markdown类型的单元格里是富文本格式的内容，你可以在这随时选择添加代码的说明文字或者学习心得。</li>\n</ul><p>你可能会想说，那我为什么要了解它的单元格类型呢？这是因为，如果不了解单元格的类型，把一段代码不小心弄成了富文本格式，那就跑不出来结果了。这两种单元格类型你可以看看下图：</p><p><img src="https://static001.geekbang.org/resource/image/df/fe/df6377e389d264dyy1c13a583a73effe.png?wh=883x408" alt=""><br>\n<img src="https://static001.geekbang.org/resource/image/24/2a/24ab3ce88205513b059eb1d3f78d492a.png?wh=886x405" alt=""></p><p>那如果你刚用Jupyter Notebook，还是有点搞不明白你编辑的单元格是程序还是富文本的话，你看看单元格左边，<strong>如果没有In字，那就是富文本了</strong>。</p><p>如果选择命令行中的双箭头图标从头执行代码，代码会按照单元格的顺序由上至下执行。不过，我们也可以单独执行某一个单元格中的代码，因为单元格是可以反复运行的。所以，Code类型的单元格左侧[ ]中的编号，才表示真正的执行顺序。假如我现在选择单箭头Run重新执行In [1]的单元格，那么这个单元格的执行顺序编号就会变为[3]，如下图所示：<br>\n<img src="https://static001.geekbang.org/resource/image/70/c3/70d6de03e16a4a6ce91de1055ef7c7c3.png?wh=901x441" alt=""></p><p>好，下面再回到我们刚才的项目。</p><p>在完成了express模块的导入之后，第三步，我们要开始使用刚才导入的plotly.express绘制漏斗了，就是要把我们的数据放进去运行起来。这个过程分为3个小步骤：建立数据、调用漏斗和显示漏斗。</p><p>下面的代码里，我创建了一个字典格式的Python数据对象，并且把我统计好的“访问数”、“下载数”、“注册数”、“搜索数”、“付款数”都放进了这个字典之中；然后，我用px.funnel调用漏斗工具，并把这些数据传入漏斗；最后，又用了fig.show显示漏斗。</p><pre><code>data = dict( #准备漏斗数据\n    number=[59, 32, 18, 9, 2],\n    stage=[&quot;访问数&quot;, &quot;下载数&quot;, &quot;注册数&quot;, &quot;搜索数&quot;, &quot;付款数&quot;])\nfig = px.funnel(data, x=\'number\', y=\'stage\') #把数据传进漏斗图\nfig.show() #显示漏斗图\n</code></pre><p>运行后输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/ec/32/ec6c383b1b8932649eyyafe879414b32.png?wh=1152x875" alt=""></p><p>就这么简单的几行代码，就成功地把我们的数据进行了非常漂亮的可视化！别小看这个漏斗，它能够让我们看出每个环节，客户流失的多少。</p><p>比如说，你可以分析一下，为什么18个用户注册了App，只有9个人进行了鲜花的搜索，是不是App的搜索页设计的不好呢？这就是漏斗图的意义，<strong>它能启发你聚焦于某个具体的运营环节去思考</strong>。</p><p>上面的整个过程，完全没有任何难于理解的地方。不过，如果你细心地观察上面的图片，你可能还会想问，为什么执行顺序为[3]的单元格有一个绿框呢？</p><p>这是一个很好的问题，你会看到选中一个单元格时，不仅有时边框是绿色，还有时是蓝色呢。其实这里的绿色和蓝色，代表Jupyter Notebook的两种输入模式。</p><p>那么为什么要有两种模式呢？简单地说，绿色编辑模式是为了写代码，而蓝色的命令模式是为了使用单元格键盘操作快捷键。你了解这两种模式后就能在之后的编程中方便地用键盘快捷键了。</p><p>比如说，在命令模式里，你可以用M键，把单元格就变成了Markdown单元格，也可以用Y键把它变成代码单元格。还有下面这些常用的快捷键，也只能在命令模式下使用：</p><p><img src="https://static001.geekbang.org/resource/image/5d/26/5d3a0449cyy1f962614f6c171407d426.jpg?wh=2284x1285" alt=""></p><p>但是，也有一些在命令模式或编辑模式下都可以直接使用的快捷键：</p><p><img src="https://static001.geekbang.org/resource/image/86/1d/86fac26dc0a456a24cfe7dd046c0331d.jpg?wh=2284x795" alt=""></p><p>这些快捷键，你不用刻意去背，点击运行界面上的键盘图标就可以随时查询它们。这里面写着“command mode ”的快捷键就是专门在命令模式下使用的。</p><p><img src="https://static001.geekbang.org/resource/image/7b/34/7b065283535ab1d97e2b1273e6eb5434.png?wh=1023x319" alt=""></p><p>除此之外，在编辑模式下，你如果在对象后面输入点号，再按下Tab键，还会出现这个对象的所有API，也就是方法和函数的列表供你选择。就像下图这样：</p><p><img src="https://static001.geekbang.org/resource/image/c6/12/c68383255e4a4109a0976436f929b612.png?wh=332x241" alt=""></p><p>单击单元格的内部输入区域，你就可以进入编辑模式，也可以通过Enter键进入编辑模式。而单击单元格外侧区域或者按Esc键，你就可以进入命令模式了。有了这些键盘快捷键，你编辑Notebook的速度就会更快。</p><h1><strong>总结一下</strong></h1><p>好了，关于Jupyter Notebook的知识我就介绍到这里。今天我给你讲了Python + Anaconda + Jupyter Notebook这三样工具，你可以通过Anaconda同时安装它们。然后我带着你创建并运行了第一个Jupyter Notebook，我带着你用pip install安装了Plotly包，import了包中的模块，并画了个简单的漏斗图。</p><p>在使用Jupyter Notebook时，需要你注意单元格的类型是代码还是Markdown文档。运行代码时，Jupyter Notebook会给出每个单元格的执行顺序，而且各单元格都可以反复执行。最后我还给你介绍了Jupyter Notebook的输入模式以及几个重要的键盘快捷键。</p><p>因为Jupyter Notebook很容易用，掌握了上面的内容，你就应该算得上是半个专家了，而轻量级的机器学习实战，Jupyter Notebook完全搞得定。</p><p>当然，如果你真的接触到比较复杂的深度学习，要构建深度网络处理大量的数据的时候，你确实会需要配置要求更高的环境。不过针对这个情况，我也有个简单的解决方案。</p><p>事实上，有很多平台会给你提供这方面的工具。比如说Kaggle在线Jupyter Notebook平台，还有Google Colaboratory，以及国内的阿里云天池实验室，就都提供GPU甚至TPU给你免费使用。所以你现在不用担心，以后学到深度学习的时候，我会带着你去撸一撸这些网站的羊毛的。</p><h1><strong>思考题</strong></h1><p>今天的内容超级简单，但是需要动动手，我这里给你一个小项目。我在前面画的漏斗，没有显示出性别，也就是文章开始是有男生，女生信息的那种组合型漏斗：</p><p><img src="https://static001.geekbang.org/resource/image/28/58/288e103ca59f473be2fc1a1dc1361758.png?wh=1033x455" alt=""></p><p>我给你准备了数据，你能不能想想怎么画出这种细分漏斗呢？</p><p>数据：</p><p>男生：“访问数-30”, “下载数-15”, “注册数-10”, “搜索数-6”, “付款数-1”<br>\n女生：“访问数-29”, “下载数-17”, “注册数-8”, “搜索数-3”, “付款数-1”</p><p>部分提示代码：</p><pre><code>import pandas as pd # 导入Pandas\nstages = [&quot;访问数&quot;, &quot;下载数&quot;, &quot;注册数&quot;, &quot;搜索数&quot;, &quot;付款数&quot;] # 漏斗的阶段\n# 漏斗的数据\ndf_male = pd.DataFrame(dict(number=[30, 15, 10, 6, 1], stage=stages))\ndf_male[\'性别\'] = \'男\'\n...省略部分代码...\ndf = pd.concat([df_male, df_female], axis=0) # 把男生女生的数据连接至一个新的Dataframe对象df\nfig = px.funnel(df, x=\'number\', y=\'stage\', color=\'性别\') # 把df中的数据传进漏斗\n...省略部分代码...\n</code></pre><p>我这个提示已经太给力了，其中的重点是两个DataFrame对象通过pd.concat()的连接部分，下面看你的了！</p><p>欢迎你在留言区里分享你画这个细分图时的收获和遇到的问题，我在留言区等你。如果这节课帮到了你的话，也欢迎你把这节课分享给自己的朋友。</p>',
        article_title: "02｜工具准备：安装并使用Jupyter Notebook",
      },
      {
        title: "03｜实战5步（上）：怎么定义问题和预处理数据？",
        id: 414504,
        content:
          '<p>你好，我是黄佳。</p><p>在<a href="https://time.geekbang.org/column/article/413057">《打好基础：到底什么是机器学习？》</a>中，我和你说了到底什么是机器学习，你还记得我们的结论吗？机器学习是一种从数据生成规则、发现模型，来帮助我们预测、判断、分析和解决问题的技术。现在，你是不是跃跃欲试，准备动手开跑机器学习程序了？</p><p>不要着急，在实战之前你还需要掌握最后一个知识点，也就是机器学习项目分为哪些步骤，你才好跟着一步步走。一个机器学习项目从开始到结束大致分为5步，分别是<strong>定义问题、收集数据和预处理、选择算法和确定模型、训练拟合模型、评估并优化模型性能</strong>。这5步是一个循环迭代的过程，你可以参考下面的图片：</p><p><img src="https://static001.geekbang.org/resource/image/41/9a/41ca9369d516198b33d736462fd8bd9a.jpg?wh=1920x1047" alt=""></p><p>我们所有的项目都会按照这5步来做，我把它简称为实战5步。为了让你更深地理解这5步，在后面能更快地上手，我会带你做一个项目，我会给你清楚解释每一个步骤的目的和背后的原理。我会把这个项目分成两节课，今天我们先来一起搞定前两步，也就是<strong>定义问题和数据的预处理。</strong></p><p>好了，我们正式开始吧！</p><h2>第1步 定义问题</h2><p>我们先来看第一步，定义问题。在定义问题这个环节中，我们要剖析业务场景，设定清晰的目标，同时还要明确当前问题属于哪一种机器学习类型。如果不搞清楚这些，我们后面就无法选择模型了。</p><p>所以首先，我们先得来了解一下我们这个项目的业务场景。假设你已经入职了“易速鲜花”的运营部，正在对微信公众号推广文案的运营效率进行分析。你收集了大量的软文数据，包括点赞数、转发数和浏览量等等，就像下面这样：</p><!-- [[[read_end]]] --><p><img src="https://static001.geekbang.org/resource/image/a6/f7/a68bc4218bb6112b1a6c50742c7cd6f7.png?wh=401x165" alt=""></p><p>因为微信公众号阅读量超过10万之后，就不能显示它的具体阅读量了。所以针对这个问题，我们项目的目标就是，建立一个机器学习模型，根据点赞数和转发数等指标，估计一篇文章能实现多大的浏览量。</p><p>因为要估计浏览量，所以在这个数据集中：点赞数、转发数、热度指数、文章评级，这4个字段都是特征，浏览量就是标签。这里我们已经有要估计的标签了，所以这是一个监督学习问题。再加上我们的标签是连续性的数值，因此它是一个回归问题。</p><p>不难看出，在这个数据集中，特征和标签之间明显呈现出一种相关性：点赞数、转发数多的时候，往往浏览量也多。但是，这种相关性可以通过哪个具体的函数来描绘呢？目前我们还不知道，所以我们在这个项目中的任务就是找到这个函数。</p><p>这样，我们就已经把问题定义好了，紧接着下一步就是数据收集和预处理。</p><h2>第2步 收集数据和预处理</h2><p>“数据的收集和预处理”在所有机器学习项目中都会出现，它的作用是为机器学习模型提供好的燃料。数据好，模型才跑得更带劲。这步骤看似只有一句话，其实里面包含了好几个小步骤，完整来讲有6步：</p><ul>\n<li>收集数据；</li>\n<li>数据可视化；</li>\n<li>数据清洗；</li>\n<li>特征工程；</li>\n<li>构建特征集和标签集；</li>\n<li>拆分训练集、验证集和测试集。</li>\n</ul><p>你可能一眼看上去不太明白这6个步骤的意思。不要着急，接下来我会继续结合“易速鲜花”这个项目挨个解释的。</p><ol>\n<li>\n<h3>收集数据</h3>\n</li>\n</ol><p>首先是收集数据，这一步又叫采集数据。在我们的项目中，我已经把它做好了，你可以在<a href="https://github.com/huangjia2019/geektime/tree/main/%E5%87%86%E5%A4%87%E7%AF%8703">这里</a>下载现成的数据集。</p><p>不过，在现实中，收集数据通常很辛苦，要在运营环节做很多数据埋点、获取用户消费等行为信息和兴趣偏好信息，有时候还需要上网爬取数据。收集数据不是我们的课程重点，你有兴趣的话我向你推荐陈旸老师的<a href="https://time.geekbang.org/column/intro/147">《数据分析实战45讲》</a>，里面有很多收集数据的方法。</p><p>有了数据集，接下来我们要做的是数据可视化的工作，也就是通过可视化去观察一下数据，为选择具体的机器学习模型找找感觉。</p><ol start="2">\n<li>\n<h3>数据可视化</h3>\n</li>\n</ol><p>数据可视化是个万金油技能，能做的事非常多。比如说，可以看一看特征和标签之间可能存在的关系，也可以看看数据里有没有“脏数据”和“离群点”等。</p><p>不过在正式可视化之前，我们需要把收集到的数据导入运行环境。数据导入我们需要用到Pandas数据处理工具包。这个包可是操作数据的利器，我们未来每个项目中都会用到。现在我们用import语句导入它：</p><pre><code>import pandas as pd # 导入Pandas数据处理工具包\n</code></pre><p>然后，我们通过下面的代码把这个项目的数据集读入到Python运行环境，用DataFrame的形式呈现出来：</p><pre><code>df_ads = pd.read_csv(\'易速鲜花微信软文.csv\') # 读入数据\ndf_ads.head() # 显示前几行数据\n</code></pre><p>DataFrame是机器学习中常见的二维表格类型数据结构。在上面的代码中，我用read_csv API把CSV格式的数据文件，读入到Pandas的DataFrame中，把它命名为了df_ads。这段代码输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/cd/16/cd6332eyyb97f956fa5e603f96f40816.png?wh=294x285" alt=""></p><p>这样就完成了数据的导入了，接着我们就可以正式进入“可视化”了。根据经验，我们猜测“点赞数”最有可能和“浏览量”之间存在线性关系。那是不是真的这样呢？我们可以画出图来验证一下。</p><p>在这个“验证”环节中，我们需要用到两个包：一个是Python画图工具库“Matplotlib ”，另一个是统计学数据可视化工具库“Seaborn”。这两个包都是Python数据可视化的必备工具包，它们是Anaconda默认安装包的一部分，不需要pip install语句重复安装。</p><p>在导入这两个包时，我们依旧用import语句。请你注意，为了节省代码量，我并没有导入完整的matplotlib包，而是只导入了matplotlib包中的绘图模块pyplot：</p><pre><code>#导入数据可视化所需要的库\nimport matplotlib.pyplot as plt # Matplotlib – Python画图工具库\nimport seaborn as sns # Seaborn – 统计学数据可视化工具库\n</code></pre><p>因为线性关系可以简单地用散点图来验证一下。所以下面我们用matplotlib包中的plot API，绘制出“点赞数”和“浏览量”之间的散点图，看看它们的分布状态。</p><pre><code>plt.plot(df_ads[\'点赞数\'],df_ads[\'浏览量\'],\'r.\', label=\'Training data\') # 用matplotlib.pyplot的plot方法显示散点图\nplt.xlabel(\'点赞数\') # x轴Label\nplt.ylabel(\'浏览量\') # y轴Label\nplt.legend() # 显示图例\nplt.show() # 显示绘图结果！\n</code></pre><p>输出的结果如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/24/7d/24e97d5a919c0e03b924270233a3b67d.png?wh=399x260" alt=""></p><p>从这张图中我们可以看出来，这些数据基本上集中在一条线附近，所以它的标签和特征之间，好像真的存在着线性的关系，这可以为我们将来选模型提供参考信息。</p><p>接下来，我要用Seaborn的boxplot工具画个箱线图。来看看这个数据集里有没有“离群点”。我这里随便选择了热度指数这个特征，你也可以为其它特征试试绘制箱线图。</p><pre><code>data = pd.concat([df_ads[\'浏览量\'], df_ads[\'热度指数\']], axis=1) # 浏览量和热度指数\nfig = sns.boxplot(x=\'热度指数\', y=&quot;浏览量&quot;, data=data) # 用seaborn的箱线图画图\nfig.axis(ymin=0, ymax=800000); #设定y轴坐标\n</code></pre><p>下图就是我们输出的箱线图：</p><p><img src="https://static001.geekbang.org/resource/image/9b/e7/9b5fc402a8b2b4dc8482a96e1c304ee7.png?wh=399x264" alt=""></p><p>箱线图是由五个数值点组成，分别是最小值(min)、下四分位数(Q1)、中位数(median)、上四分位数(Q3)和最大值(max)。在统计学上，这叫做五数概括。这五个数值可以清楚地为我们展示数据的分布和离散程度。</p><p>这个图中下四分位数、中位数、上四分位数组成一个“带有隔间的盒子”，就是所谓的箱；上四分位数到最大值之间建立一条延伸线，就是所谓的线，也叫“胡须”；胡须的两极就是最小值与最大值；此外，箱线图还会将离群的数据点单独绘出。</p><p>在上面这个箱线图中，我们不难发现，热度指数越高，浏览量的中位数越大。我们还可以看到，有一些离群的数据点，比其它的文章浏览量大了很多，这些“离群点”就是我们说的“爆款文章”了。</p><p>到这里，数据可视化工作算是基本完成了。在数据可视化之后，下一步就是数据的清洗。</p><ol start="3">\n<li>\n<h3>数据清洗</h3>\n</li>\n</ol><p>很多人都把数据清洗比作“炒菜”前的“洗菜”，也就是说数据越干净，模型的效果也就越好。清洗的数据一般分为4种情况：</p><p>第一种是处理缺失的数据。如果备份系统里面有缺了的数据，那我们尽量补录；如果没有，我们可以剔除掉残缺的数据，也可以用其他数据记录的平均值、随机值或者0值来补值。这个补值的过程叫数据修复。</p><p>第二个是处理重复的数据：如果是完全相同的重复数据处理，删掉就行了。可如果同一个主键出现两行不同的数据，比如同一个身份证号后面有两条不同的地址，我们就要看看有没有其他辅助的信息可以帮助我们判断（如时戳），要是无法判断的话，只能随机删除或者全部保留。</p><p>第三个是处理错误的数据：比如商品的销售量、销售金额出现负值，这时候就需要删除或者转成有意义的正值。再比如表示百分比或概率的字段，如果值大于1，也属于逻辑错误数据。</p><p>第四个是处理不可用的数据：这指的是整理数据的格式，比如有些商品以人民币为单位，有些以美元为单位，就需要先统一。另一个常见例子是把“是”、“否”转换成“1”、“0”值再输入机器学习模型。</p><p>那么如何看数据集中有没有脏数据呢？</p><p>就我们这个项目的数据集来说，细心的你可能在DataFrame图中已经发现，行索引为6的数据中“转发数”的值是“NaN”，意思是Not A Number。在Python中，它代表无法表示、也无法处理的值。这是典型的脏数据。</p><p><img src="https://static001.geekbang.org/resource/image/cd/16/cd6332eyyb97f956fa5e603f96f40816.png?wh=294x285" alt=""></p><p>我们可以通过DataFrame的isna().sum()函数来统计所有的NaN的个数。这样，我们就可以在看看有没有NaN的同时，也看看NaN出现的次数。如果NaN过多的话，那么说明这个数据集质量不好，就要找找数据源出了什么问题。</p><pre><code>df_ads.isna().sum() # NaN出现的次数\n</code></pre><p>输出结果如下:</p><pre><code>点赞数      0\n转发数     37\n热度指数     0\n文章评级     0\n浏览量      0\ndtype: int64\n</code></pre><p>输出显示我们的数据集中“转发数”这个字段有37个NaN值。对于上千条数据的数据集，这还不算很多。那么该如何处理呢？也很简单。我们可以用dropna()这个API把出现了NaN的数据行删掉。</p><pre><code>df_ads = df_ads.dropna() # 把出现了NaN的数据行删掉\n</code></pre><p>你可能会觉得，我们刚才通过箱线图找到了离群点（爆款文章），这些算是脏数据吗？这是个很好的问题，而且这个问题并没有固定的答案。</p><p>删去了离群点，模型针对普通的数据会拟合得比较漂亮。但是现实生活中，就是存在着这样的离群点，让模型不那么漂亮。如果把这里的离群点删掉，那模型就不能工作得那么好了。所以，这是一个平衡和取舍的过程。</p><p>我们可以训练出包含这些离群点的模型，以及不包含这些离群点的模型，并进行比较。在这里，我建议保留这些“离群点”。</p><p>现在，我们就完成了对这个数据的简单清洗。不同类型的数据有不同的清洗方法，我们这里就不一一介绍了。后续项目中，针对具体项目和数据集，我们还会再细讲的。我们继续讲下一个步骤，把数据转换成机器所更易于读取的格式，也就是特征工程。</p><ol start="4">\n<li>\n<h3>特征工程</h3>\n</li>\n</ol><p>特征工程是一个专门的机器学习子领域，而且我认为它是数据处理过程中<strong>最有创造力的环节</strong>，特征工程做得好不好，非常影响机器学习模型的效率。</p><p>我举个例子来解释下什么是特征工程。你知道什么是BMI指数吗？它等于体重除以身高的平方，这就是一个特征工程。</p><p>$$BMI=\\frac{Weight（kg）}{\\left[ Height(m) \\right]^{2}}$$</p><p>什么意思呢？就是说经过了这个过程，BMI这一个指数就替代了原来的两个特征——体重和身高，而且完全能客观地描绘我们的身材情况。</p><p>因此，经过了这个特征工程，我们可以把BIM指数作为新特征，输入用于评估健康情况的机器学习模型。</p><p>你可能会问这样做的好处是什么？以BMI特征工程为例，它降低了特征数据集的维度。维度就是数据集特征的个数。要知道，在数据集中，每多一个特征，模型拟合时的特征空间就更大，运算量也就更大。所以，<strong>摒弃掉冗余的特征、降低特征的维度，能使机器学习模型训练得更快。</strong></p><p>这只是特征工程的诸多的妙处之一，此外特征工程还能更好地表示业务逻辑，并提升机器学习模型的性能。</p><p>由于我们这个项目的问题相对简单，对特征工程的要求并不高，这里暂时不做特征工程。后面我会用一节课专门讲解各类特征工程的应用。下面，我们直接进入下一个子步骤，也就是构建机器学习的特征集和标签集。</p><ol start="5">\n<li>\n<h3>构建特征集和标签集</h3>\n</li>\n</ol><p>我们说过，特征就是所收集的各个数据点，是要输入机器学习模型的变量。而标签是要预测、判断或者分类的内容。对于所有监督学习算法，我们都需要向模型中输入“特征集”和“标签集”这两组数据。因此，在开始机器学习的模型搭建之前，我们需要先构建一个特征数据集和一个标签数据集。</p><p>具体的构建过程也很简单，我们只要从原始数据集删除我们不需要的数据就行了。在这个项目中，特征是点赞数、转发数、热度指数和文章评级，所以只需要从原始数据集中删除“浏览量”就行了。</p><pre><code>X = df_ads.drop([\'浏览量\'],axis=1) # 特征集，Drop掉标签相关字段\n</code></pre><p>而标签是我们想要预测的浏览量，因此，我们在标签数据集中只保留“浏览量”字段：</p><pre><code>y = df_ads.浏览量 # 标签集\n</code></pre><p>下面我们再看看特征集和标签集里面都有什么数据。</p><pre><code>X.head() # 显示前几行数据\n</code></pre><pre><code>y.head() #显示前几行数据\n</code></pre><p>因为Notebook一个单元格只能有一个输出。所以我把显示两个数据的代码放在了不同的单元格中。它们的输出结果如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/a8/07/a8b2dddyy331e2f7586c55d7b18f6c07.jpg?wh=2284x546" alt=""></p><p>可以看到，除了浏览量之外，所有其它字段仍然都在特征数据集中，而只有浏览量被保存在了标签数据集中，也就是说原始数据集就被拆分成了机器学习的特征集和标签集。</p><p>这里我想请你思考的是，无监督学习算法需要这个步骤吗？没错，答案是不需要。因为无监督算法根本就没有标签。</p><p>不过，从原数据集从列的维度纵向地拆分成了特征集和标签集后，还需要进一步从行的维度横向拆分。你可能想问，为啥还要拆分呀？因为我们在第一讲<a href="https://time.geekbang.org/column/article/413057">《打好基础：弄懂到底什么是机器学习》</a>里就讲过，机器学习并不是通过训练数据集找出一个模型就结束了，我们需要用验证数据集看看这个模型好不好，然后用测试数据集看看模型在新数据上能不能用。</p><p>那么具体该怎么拆分呢？这就是我们接下来要解决的问题了。</p><ol start="6">\n<li>\n<h3>拆分训练集、验证集和测试集</h3>\n</li>\n</ol><p>在拆分之前，我先说明一点，对于学习型项目来说，为了简化流程，经常会省略验证的环节。我们今天的项目比较简单，所以我们也省略了验证，只拆分训练集和测试集，而此时的测试集就肩负着验证和测试双重功能了。</p><p>拆分的时候，留作测试的数据比例一般是20%或30%。不过如果你的数据量非常庞大，比如超过1百万的时候，那你也不一定非要留这么多。一般来说有上万条的测试数据就足够了。这里我会按照80/20的比例来拆分数据。具体的拆分，我们会用机器学习工具包scikit-learn里的数据集拆分工具train_test_split来完成：</p><pre><code>#将数据集进行80%（训练集）和20%（验证集）的分割\nfrom sklearn.model_selection import train_test_split #导入train_test_split工具\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                   test_size=0.2, random_state=0)\n</code></pre><p>这里请你注意一下，虽然是随机分割，但我们要指定一个random_state值，这样就保证程序每次运行都分割一样的训练集和测试集。训练集和测试集每次拆分都不一样的话，那比较模型调参前后的优劣就失去了固定的标准。</p><p>现在，训练集和测试集拆分也完成了，你会发现原始数据现在变成了四个数据集，分别是：</p><ul>\n<li>特征训练集（X_train）</li>\n<li>特征测试集（X_test）</li>\n<li>标签训练集（y_train）</li>\n<li>标签测试集（y_test）</li>\n</ul><p>至此，我们全部的数据预处理工作就结束了。</p><h2>总结一下</h2><p>现在，我来给你总结一下。</p><p>这节课我们介绍了机器学习实战5步中的前两步：定义问题以及数据的收集和预处理。不明确定义要解决的问题，我们就没办法有的放矢地选择模型。</p><p>而数据的收集和预处理虽然看起来没有模型的选择和优化那么“吸引眼球”，但它其实才是机器学习项目成败的关键。这一步可以分为下图中的6个小步：</p><p><img src="https://static001.geekbang.org/resource/image/90/fd/9065b2d14d1416197ab841025d59d7fd.jpg?wh=2284x1165" alt=""></p><p>这6步中，尤其是数据可视化和特征工程，因为无定法可循，所以很考验经验，它既是我们对着已有数据找感觉的过程，又是下一步把数据“喂”给模型之前的必要准备。</p><p>除此之外，这里我还希望你注意两点：</p><p>第一点是，<strong>这些子步骤中的次序并不是固定的</strong>，比如数据可视化和特征工程，很多时候是先可视化，发现了一些特征工程的思路，然后做特征工程，然后再次可视化。而且有的特征工程比如特征缩放，还必须在拆分数据之后做；</p><p>第二点，这些子步骤<strong>在一个特定的机器学习项目中，可能不需要全部用到</strong>。比如说无监督学习项目，就不需要创建特征集和标签集这个步骤，一般也不需要用到验证集和测试集。</p><p>好的，那么这一讲就到这里，我把这节课使用的代码放在<a href="https://github.com/huangjia2019/geektime/tree/main/%E5%87%86%E5%A4%87%E7%AF%8703">这里</a>了。下一讲，我们要开始选择机器学习模型了，敬请期待！</p><h2>思考题</h2><p>这节课到这我们就讲完了，我来给你留两道练习题。</p><ol>\n<li>今天我们显示了特征和标签的散点图，还显示了热度指数特征的箱线图。你能不能试着显示一下其它特征之间的散点图，或者箱线图呢？</li>\n<li>在数据清洗部分，我们用dropna()这个API，把出现了NaN的数据行删掉了。如果你出于保留更多数据的考虑，应该为该字段补值，你会怎么做呢？你能想到哪些补值方法？</li>\n</ol><p>提示一种方法：</p><pre><code>df_ads[\'转发数\'].fillna(df_ads[\'转发数\'].mean(), inplace=True) # 用均值补值\n</code></pre><p>欢迎你在留言区里分享你做这两个题时的收获和遇到的问题，我在留言区等你。如果这节课帮到了你的话，也欢迎你把这节课分享给自己的朋友。</p><p><img src="https://static001.geekbang.org/resource/image/1f/47/1fe8181d62cdef912a6bd829f3582647.jpg?wh=2284x1136" alt=""></p>',
        article_title: "03｜实战5步（上）：怎么定义问题和预处理数据？",
      },
      {
        title: "04｜ 实战5步（下）：怎么建立估计10万+软文点击率的模型？",
        id: 415149,
        content:
          '<p>你好，我是黄佳。欢迎来到零基础实战机器学习。</p><p>上一讲，我们通过一个项目讲解了“实战5步”的前两步。在第一步“定义问题”中，我们定义了要处理的问题，也就是根据点赞数和转发数等指标，估计一篇文章能实现多大的浏览量。同时我们还将它归类为回归问题；在第二步“收集数据和预处理”中，我们做好了数据的预处理工作，还把数据集拆分成了这四个数据集：</p><ul>\n<li>特征训练集（X_train）</li>\n<li>特征测试集（X_test）</li>\n<li>标签训练集（y_train）</li>\n<li>标签测试集（y_test）</li>\n</ul><p>有了这些数据集后，我们就可以开始考虑选什么算法，然后建立模型了。所以，今天这节课我们继续完成“实战5步”中的后三步：选择算法并建立模型、训练拟合模型和评估并优化模型性能，来把这个项目做完。下面，我们先看看怎么选择算法并建立模型。</p><h1>第3步 选择算法并建立模型</h1><p>在这一步中，我们需要先根据特征和标签之间的关系，选出一个合适的算法，并找到与之对应的合适的算法包，然后通过调用这个算法包来建立模型。</p><p>选算法的过程很考验数据科学家们的经验，不过，你也无需担心自己没有经验，在这个课程中，我会给你讲清楚每一个实战中所用的算法的原理，帮助你建立起选算法的直觉。</p><p>具体到我们这个项目里，在上一讲中我说过，我们这个数据集里的某些特征和标签之间，存在着近似线性的关系。而且，这个数据集的标签是连续变量，因此，适合用回归分析来寻找从特征到标签的预测函数。</p><!-- [[[read_end]]] --><p>所谓回归分析(regression analysis)，就是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析，说白了就是当自变量变化的时候，研究一下因变量是怎么跟着变化的，它可以用来预测客流量、降雨量、销售量等。</p><p>不过，回归分析的算法有很多种，比如说线性回归，多项式回归，贝叶斯回归等等，那具体该选哪个呢？其实，这是根据特征和标签之间的关系来决定的。我们在上一讲的可视化过程中，推测特征和标签可能存在线性关系，并且用下面这个散点图简单做了验证。</p><p><img src="https://static001.geekbang.org/resource/image/24/7d/24e97d5a919c0e03b924270233a3b67d.png?wh=399x260" alt=""></p><p>所以这里，我们就选择用线性回归算法来建模。</p><p>线性回归算法是最简单、最基础的机器学习算法，它其实就是给每一个特征变量找参数的过程。我想你一定熟悉一元线性回归的公式：</p><p>$$ y = a*x + b $$</p><p>对于一元线性回归来说，它的内部参数就是未知的斜率和截距。只不过在机器学习中，我们把斜率a叫做权重（weight），用英文字母w代表，把截距b叫做偏置（bias），用英文字母b代表。所以机器学习中一元线性回归的公式也写成：</p><p>$$ y = w*x + b $$</p><p>而在我们这个项目中，数据集中有4个特征，所以就是：</p><p>$$ y = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + w_{4}x_{4} + b $$</p><p>因此，我们的模型就会有5个内部参数，也就是4个特征的权重和一个偏置（截距）需要确定。不过这些公式的具体代码实现，都不用我们自己完成，它们全部封装在工具包里了。你只要对算法的原理有个印象就行了。</p><p>确定好算法后，我们接着来看一下调用什么样的算法包建立模型比较合适。</p><p>对于机器学习来说，最常用的算法工具包是scikit-learn，简称sklearn，它是使用最广泛的开源Python机器学习库，堪称机器学习神器。sklearn提供了大量用于数据挖掘的机器学习工具，覆盖数据预处理、可视化、交叉验证和多种机器学习算法。</p><p>虽然我们已经选定使用线性回归算法，但是在sklearn中又有很多线性回归算法包，比如说基本的线性回归算法LinearRegression，以及在它的基础上衍生出来的Lasso回归和Ridge回归等。</p><p>那哪一个才是适合我们这个项目的算法包呢？其实，我们一般选算法包的方法是从能够解决该问题的最简单的算法开始尝试，直到得到满意的结果为止。对于这个项目，我们选择LinearRegression，它也是机器学习中最常见、最基础的回归算法包。其它回归算法包，未来我会慢慢给你介绍。</p><p>调用LinearRegression建立模型非常简单，两行代码就可以搞定：</p><pre><code class="language-plain">from sklearn.linear_model import LinearRegression # 导入线性回归算法模型\nlinereg_model = LinearRegression() # 使用线性回归算法创建模型\n</code></pre><p>可以看到，我把这个线性回归模型命名为了“linereg_model”。那到这里，我们算不算是建立好模型了呢？是的，模型已经创建出来了，我们可以开始训练它了。不过，有一点需要指出，建立模型时，你通常还需要了解它有哪些外部参数，同时指定好它的外部参数的值。</p><p>模型的参数有两种：内部参数和外部参数。内部参数是属于算法本身的一部分，不用我们人工来确定，刚才提到的权重w和截距b，都是线性回归模型的内部参数；而外部参数也叫做超参数，它们的值是在创建模型时由我们自己设定的。</p><p>对于LinearRegression模型来讲，它的外部参数主要包括两个布尔值：</p><ul>\n<li>fit_intercept&nbsp;，默认值为True，代表是否计算模型的截距。</li>\n<li>normalize，默认值为 False，代表是否对特征X在回归之前做规范化。</li>\n</ul><p>不过呢，对于比较简单的模型来说，默认的外部参数设置也都是不错的选择，所以，我们不显式指定外部参数而直接调用模型，也是可以的。在上面的代码中，我就是在创建模型时直接使用了外部参数的默认值。</p><p>还有一点需要说明的是，我们课程中不会去详细介绍每一个参数的意义，因为这不是我们的重点。但是对于一些特殊情况下需要调整外部参数的，在后面的课程里我会告诉你调参的重要技巧。</p><p>好，现在我们已经创建好线性回归模型linereg_model，接下来我们就可以进入机器学习的核心环节“训练拟合机器学习模型”了。</p><h1>第4步 训练模型</h1><p>训练模型就是用训练集中的特征变量和已知标签，根据当前样本的损失大小来<strong>逐渐拟合函数，确定最优的内部参数，最后完成模型</strong>。虽然看起来挺复杂，但这些步骤，我们都通过调用fit方法来完成。</p><p>fit方法是机器学习的核心环节，里面封装了很多具体的机器学习核心算法，我们只需要把特征训练数据集和标签训练数据集，同时作为参数传进fit方法就行了。</p><pre><code class="language-plain">linereg_model.fit(X_train, y_train) # 用训练集数据，训练机器，拟合函数，确定内部参数\n</code></pre><p>运行该语句后的输出如下：</p><pre><code class="language-plain">LinearRegression()\n</code></pre><p>这样，我们就完成了对模型的训练。你可能会觉得很奇怪，既然训练模型是机器学习的核心环节，怎么只有一句代码？其实这就是我反复强调过的，由于优秀的机器学习库的存在，我们可以用一两行语句实现很强大的功能。所以，不要小看上面那个简单的fit语句，这是模型进行自我学习的关键过程。</p><p>在这个过程里，<strong>fit的核心就是减少损失，使函数对特征到标签的模拟越来越贴切</strong>。那么它具体是怎么减少损失呢？这里我画了一张图片展示模型从很不靠谱到比较靠谱的过程。</p><p><img src="https://static001.geekbang.org/resource/image/d7/08/d711fe6c1ace614dfe4ef1268498d608.jpg?wh=2284x1032" alt=""></p><p>这个拟合的过程，同时也是机器学习算法优化其内部参数的过程。而优化参数的关键就是减小损失。</p><p>那什么是损失呢？它其实是<strong>对糟糕预测的惩罚</strong>，同时也是对模型好坏的度量。损失也就是模型的误差，也称为成本或代价。名字虽多，但都是一个意思，就是当前预测值和真实值之间的差距的体现。它是一个数值，表示对于单个样本而言模型预测的准确程度。如果模型的预测完全准确，则损失为0；如果不准确，就有损失。</p><p>在机器学习中，我们追求的当然是比较小的损失。不过，模型好不好，还不能仅看单个样本，还要针对所有数据样本，找到一组平均损失“较小”的函数模型。样本的损失大小，从几何意义上基本可以理解为预测值和真值之间的几何距离。平均距离越大，说明误差越大，模型越离谱。在下面这个图中，左边是平均损失较大的模型，右边是平均损失较小的模型，模型所有数据点的平均损失很明显大过右边模型。</p><p><img src="https://static001.geekbang.org/resource/image/1e/1f/1e07d5c3fe9a82b4f2c1d83c6fcd591f.jpg?wh=2284x868" alt=""></p><p>因此，针对每一组不同的参数，机器都会基于样本数据集，用损失函数算一次平均损失。而机器学习的最优化过程，就是逐步减小训练集上损失的过程。具体到我们今天这个回归模型的拟合，它的关键环节就是<strong>通过梯度下降，逐步优化模型的参数，使训练集误差值达到最小</strong>。这也就是我们刚才讲的那个fit语句所要实现的最优化过程。</p><p>在这里面，线性回归中计算误差的方法很好理解，就是数据集中真值与预测值之间的残差平方和。那梯度下降又是怎么一回事呢？为了让你直观地理解，我用一张图来展示一下，梯度下降是怎么一步一步地走到损失曲线中的最小损失点的。</p><p><img src="https://static001.geekbang.org/resource/image/12/1a/128eec906305b3be97989f28cec1961a.jpg?wh=2284x1285" alt=""></p><p>就像图里这样，梯度下降其实就和下山一样。你可以想象一下，当你站在高处，你的目标就是找到一系列的参数，让训练数据集上的损失值最小。那么你该往哪走才能保证损失值最小呢？关键就是通过求导的方法，找到每一步的方向，确保总是往更小的损失方向前进。</p><p>所以，你可以看出方向是有多么的重要。机器学习最优化之所以能够拟合出最佳的模型，就是因为能够找到前进方向，你看，<strong>不仅我们人需要方向，连AI也需要正确的方向</strong>。</p><p>关于梯度下降，你理解这些就已经差不多了。至于具体的细节，我们在这个课程中并不会通过公式和代码实现，这也是基于不重复造轮子的原则。那到这里为止，我们已经完成了模型的建立和训练，接下来，我们一起看看怎么对这个训练好的模型进行评估和优化，让它尽可能精准地估计出我们的文章浏览量。</p><h1>第5步 模型的评估和优化</h1><p>我们刚才说，梯度下降是在用训练集拟合模型时最小化误差，这时候算法调整的是模型的内部参数。而<strong>在验证集或者测试集进行模型效果评估的过程中，我们则是通过最小化误差来实现超参数（模型外部参数）的优化</strong>。</p><p>对此，机器学习工具包（如scikit-learn）中都会提供常用的工具和指标，对验证集和测试集进行评估，进而计算当前的误差。比如$R^{2}$或者MSE均方误差指标，就可以用于评估回归分析模型的优劣。</p><p>不过呢，在开始评估模型之前，我想请你思考一下：在我们这5个实战步骤里面，并没有“使用模型预测浏览量”这个环节，这是为什么呢？其实这个环节已经包含在第5步“模型性能的评估和优化”之中了，并且是我们在第5步中首先要去实现的。</p><p>具体来说，在“模型的评估和优化”这一步中，当我们预测完测试集的浏览量后，我们要再拿这个预测结果去和测试集已有的真值去比较，这样才能够求出模型的性能。而这整个过程也同样是一个循环迭代的过程，我把这个循环过程总结成了下面的图，你可以看看：</p><p><img src="https://static001.geekbang.org/resource/image/69/67/691da4be2681f183f396bfb0e5b74867.jpg?wh=2284x1285" alt=""></p><p>对于我们这个项目来说，预测测试集的浏览量，只需要用训练好的模型linereg_model中的predict方法，在X_test（特征测试集）上进行预测，这个方法就会返回对测试集的预测结果。</p><pre><code class="language-typescript">y_pred = linereg_model.predict(X_test) #预测测试集的Y值\n</code></pre><p>在几乎所有的机器学习项目中，你都可以用predict方法来进行预测，它就是用模型在任意的同类型数据集上去预测真值的，可以应用于验证集、测试集，当然也可以应用于训练集本身。</p><p>这里我要说明一下，为了简化流程，我们并没有真正进行验证和测试的多重循环。因此，在这个项目中，X_test既充当了测试集，也充当了验证集。</p><p>拿到预测结果后，我们再通过下面的代码，把测试数据集的原始特征数据、原始标签真值，以及模型对标签的预测值组合在一起进行显示、比较。</p><pre><code class="language-typescript">df_ads_pred = X_test.copy() # 测试集特征数据\ndf_ads_pred[\'浏览量真值\'] = y_test # 测试集标签真值\ndf_ads_pred[\'浏览量预测值\'] = y_pred # 测试集标签预测值\ndf_ads_pred #显示数据\n</code></pre><p>输出结果如下:</p><p><img src="https://static001.geekbang.org/resource/image/6f/66/6fa715a600aab30ddefb792409ed5366.png?wh=432x346" alt=""></p><p>可以看出，浏览量预测值是比较接近于真值的。而且对于一些文章，这个模型的预测得非常准确，比如编号第1387号数据，其实际浏览量为213501，预测浏览量为216491。这就是一个很棒的结果。</p><p>如果你想看看现在的模型长得什么样？你可以通过LinearRegression的coef_和intercept_属性打印出各个特征的权重和模型的偏置来。它们也就是模型的内部参数。</p><pre><code>print(\'当前模型的4个特征的权重分别是: \', model.coef_)\nprint(\'当前模型的截距（偏置）是: \', model.intercept_)\n</code></pre><p>输出如下：</p><pre><code>当前模型的4个特征的权重分别是:  [   48.08395224    34.73062229 29730.13312489  2949.62196343]\n当前模型的截距（偏置）是:  -127493.90606857178\n</code></pre><p>这也就是说，我们现在的模型的线性回归公式是：</p><p>$$ yy=48.08x_{1}（点赞）+34.73x_{2}（转发）+29730.13x_{3}（热度）+2949.62x_{4}（评级）-127493.91 $$</p><p>不过到这里，整个机器学习项目并没有结束，我们最后还要给出当前这个模型的评估分数：</p><pre><code class="language-typescript">print("线性回归预测评分：", linereg_model.score(X_test, y_test)) # 评估模型\n</code></pre><p>在机器学习中，常用于评估回归分析模型的指标有两种：$R^{2}$分数和MSE指标，并且大多数机器学习工具包中都会提供相关的工具。对此，你无需做过多了解，只需要知道我们这里的score这个API中，选用的是$R^{2}$分数来评估模型的就可以了。</p><p>最后我们得到这样的结果：</p><pre><code class="language-plain">线性回归预测评分： 0.7085754407718876\n</code></pre><p>可以看到，$R^{2}$值约为0.708。那这意味着什么呢？</p><p>一般来说，$R^{2}$的取值在0到1之间，$R^{2}$越大，说明所拟合的回归模型越优。现在我们得到的$R^{2}$值约为0.708，在没有与其它模型进行比较之前，我们实际上也没法确定它是否能令人满意。</p><p>因为分数的高低，与数据集预测的难易程度、模型的类型和参数都有关系。而且，$R^{2}$分数也不是线性回归模型唯一的评估标准。关于模型的评估和优化，我们就先讲到这里，更多内容，我们还会在以后逐步深入讲解。</p><p>不过你需要知道的是，如果模型的评估分数不理想，我们就需要回到第3步，调整模型的外部参数，重新训练模型。要是得到的结果依旧不理想，那我们就要考虑选择其他算法，创建全新的模型了。如果很不幸，新模型的效果还是不好的话，我们就得回到第2步，看看是不是数据出了问题。</p><p><img src="https://static001.geekbang.org/resource/image/ba/0c/ba6f2b4e7fb781ecb238af3325c1b40c.jpg?wh=2284x2341" alt=""></p><p>这也是为什么，我<strong>一直强调机器学习项目是一个循环迭代的过程，优秀的模型都是一次次迭代的产物</strong>。</p><p>当模型通过了评估，就可以去解决实际问题了，机器学习项目也算是基本结束。你或许会问，为什么“实战5步”中没有模型的上线和部署？这是因为在通常的机器学习项目中，机器学习工程师并不负责这一块，而且，各个公司把机器学习部署到生产环境的方法也大不相同。所以，我们没有把部署模型放在我们的实战5步里。不过，具体怎么部署模型，我在后面会讲到。</p><h1>总结一下</h1><p>这一讲的内容到此就全部结束了，现在你已经拥有了一次完整的机器学习实战经历。怎么样，并没有你想象得那么难吧？</p><p>在这一讲和上一讲中，我们通过一个预测软文浏览量的实战项目，了解了机器学习项目要经历5个步骤。第一步就是通过定义问题来明确我们的项目目标；第二步是数据的收集和预处理，这一步的重点是把数据转换成机器学习可处理的格式，这样我们就可以在第三步中针对问题选定适宜的算法，来建立模型了。</p><p>有了模型后，我们要在接下来的第四步中训练模型、拟合函数。最后，再对训练好的模型进行评估和优化。对于这最后一步，也就是第5步，我们的重点是反复测评，找到最优的超参数，确定最终模型。</p><p><img src="https://static001.geekbang.org/resource/image/e8/02/e8b5a33fa7b0576f95f97f627f600402.jpg?wh=2284x1245" alt=""></p><p>在这“实战5步”中，我们每一步做了些什么、机器学习模型做了些什么，都需要你好好地理解和体会。从下一讲开始，我们就要进入更接近实际业务场景的项目实战了，我会引导着你进一步加深对数据和机器学习概念的理解，挖掘出数据之中的价值。</p><p>准备好了吗？让我们继续上路，开始正式闯关，在动手实践中把机器学习知识用起来！</p><h1>思考题</h1><p>这节课讲完了，我给你留的思考题是这样的：</p><p>在我们这个项目中，使用的是线性回归模型LinearRegression的默认外部参数。此时，两个参数的默认值分别是，fit_intercept&nbsp;= True，normalize = False。尽管解释算法中每一个参数的具体意义并非我们的课程重点，但是阅读算法和参数的说明文档，然后调整参数，优化模型，却是机器学习项目的日常工作。</p><p>因此，请你用fit_intercept&nbsp;和normalize这两个参数练练手，自己调整它们的值，形成各种组合，重新训练出其它外部参数组合的模型。我们可以大概算一下，两个布尔类型参数最多就形成4种组合，而且我们的默认参数已经训练出了其中的一种，所以你可以尝试其它三种。</p><p>另外，你也可以看一看，对于这个数据集来说，取哪组fit_intercept和normalize值，你的模型测试结果更优？</p><p>最后，给你一个小提示，如果你希望对变量X做“规范化”，在建立模型的同时就要指定外部参数normalize的值：</p><pre><code class="language-plain">from sklearn.linear_model import LinearRegression # 导入线性回归算法模型\nlinereg_model = LinearRegression(normalize&nbsp;=&nbsp;True) # 使用线性回归算法创建模型,并指定外部参数normalize\n</code></pre><p>欢迎你在留言区分享你的想法和收获，我在留言区等你。如果这节课帮到了你，也欢迎你把这节课分享给自己的朋友。</p><p><img src="https://static001.geekbang.org/resource/image/56/81/5647a4e7bc72e040d8654df556f72081.jpg?wh=2284x1136" alt=""></p>',
        article_title:
          "04｜ 实战5步（下）：怎么建立估计10万+软文点击率的模型？",
      },
    ],
  },
  {
    chapterTitle: "业务场景闯关篇",
    children: [
      {
        title: "05 | 数据探索：怎样从数据中找到用户的RFM值？",
        id: 415910,
        content:
          '<p>你好，我是黄佳。</p><p>今天我们正式进入“业务场景闯关篇”模块。我在开篇词中介绍过，在这个模块中，我会围绕电商场景下的运营环节，带你挑战5个关卡：获客关、变现关、激活关、留存关和裂变关，帮你逐步掌握机器学习的相关知识和实操技巧。今天，我们就从第一关“获客关”开始！</p><p><img src="https://static001.geekbang.org/resource/image/76/08/76466435764dc810ab26d93a8a2b5a08.jpg?wh=2284x1033" alt=""></p><p>人们常说移动互联网的运营已经进入了下半场，几乎所有的企业都希望能用更优质的产品和更精准的服务留住用户，这就需要制定出合适的获客策略。而要做到这一点，前提就是为用户精准画像，也就是根据用户的人口统计信息和消费行为数据，给用户分组，然后推测出用户的消费习惯和价值高低。</p><p>所以，为用户分组、画像，找到不同用户的特点，进而挖掘出哪些才是最有价值的用户，是目前互联网大厂中的数据分析师和机器学习工程师常做的工作。既然如此，那么我们就在获客关，结合一个具体的电商项目，来看看怎么根据用户的基本信息和消费行为数据，给用户分组画像。</p><h1>定义问题</h1><p>按照我们前两讲所说的机器学习“实战5步”，我们首先要做的就是，把项目的问题定义清楚。在我们这个项目中，你可以想象自己就职于一家名为“易速鲜花”的创业公司，担任这家公司的运营团队机器学习工程师。你现在要接手的第一个项目就是为公司的用户分组画像。</p><!-- [[[read_end]]] --><p>现在，你们的运营部门已经准备好了过往用户的基本信息和消费行为数据，你可以在<a href="https://github.com/huangjia2019/geektime/tree/main/%E8%8E%B7%E5%AE%A2%E5%85%B305">这里</a>下载获取它，然后用Excel打开，就能看到这个数据集中所包含的详细信息。</p><p><img src="https://static001.geekbang.org/resource/image/c1/c6/c1b4f484eb7884a2b44961d9abe842c6.png?wh=838x192" alt=""></p><p>既然我们要从这份销售订单数据中，为用户分组画像，那么有一个关键的问题就是，我们以什么为依据，给用户做分组？</p><p>我们知道，用户的消费行为本身是不能直接用于数据分析的，这就需要我们把用户的行为转化成具体的数值，也就是可量化的指标，让我们对用户有更直观的认识。而且我们还可以将这些指标用在数据分析、广告精准投放、产品推荐系统等多个运营场景，来提升我们产品和服务的精准度。</p><p>这个具体的数值呢，就是RFM（Recency、Frequency、Monetary ），它是用户画像的衍生品，也是目前很多互联网厂商普遍采用的分析方式。具体来看：</p><ul>\n<li><strong>R是新进度，代表自用户上次消费以来的天数。</strong>这个指标其实也就是用户的热乎度，比如说你上次使用App是一个月前，你的系统里R值就是30。如果你今天又用了一次App，那R值就会立刻被更新为1；</li>\n<li><strong>F是消费频率，代表用户是否频繁使用服务。</strong>这也是用户黏性的风向标；</li>\n<li><strong>M是消费金额，代表用户在一段时间内消费的总金额，</strong>这个不难理解。</li>\n</ul><p>通过用户消费行为的新进度、消费的总体频率，还有消费总金额这三项指标，我们可以将用户划分为不同的类别或集群，来直观地描述用户的价值。什么意思呢？简单来说就是，只要我们从用户的基本信息和消费行为数据中得出RFM值，就可以根据它对用户分组画像了。</p><p>所以，我们这个项目整体上可以分为两个阶段：第一个阶段是求出RFM值，第二个阶段就是利用RFM值，给用户分组画像，进而绘制出高价值、中等价值和低价值用户的分布情况。我们今天这节课的目标就是解决第一阶段的问题。</p><p>搞清楚问题后，现在我们来看看这份数据的整体状况，根据需要对数据进行一个初步的预处理。</p><h1>数据预处理</h1><p>由于已经完成了数据收集工作，我们现在直接用Pandas中的read_csv工具，把这个原始数据集读入到Pandas的DataFrame中，用Dataframe形式呈现出来。关于Pandas包及其导入方式，如果你感觉比较陌生，可以再回顾一下<a href="https://time.geekbang.org/column/article/414504">第三讲</a>的内容。</p><pre><code class="language-typescript">import pandas as pd #导入Pandas\ndf_sales = pd.read_csv(\'易速鲜花订单记录.csv\') #载入数据\ndf_sales.head() #显示头几行数据&nbsp;&nbsp;\n</code></pre><p>运行这段代码后，输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/73/ce/735b13c7baff56803b115a72fbdee8ce.png?wh=540x183" alt=""></p><p>从图中，你可以看到这个数据集主要包含了订单号、产品码、消费日期，产品说明、数量、单价、用户码和城市等字段。因为我们求的是用户消费行为的新进度（R）、消费的总体频率（F），还有消费总金额（M），所以在这些信息中，我们重点关注的是以下几个数据：</p><ul>\n<li>用户码</li>\n<li>单价</li>\n<li>（订单中产品的）数量</li>\n<li>消费日期</li>\n</ul><p>为了对公司运营团队交给我们的数据集有一个宏观上的了解，下面我们先做一个整体的数据可视化，看看这个数据集所覆盖的消费日期跨度是怎样的。</p><h2>1.数据可视化</h2><p>我们直接来看具体的实现代码：</p><pre><code>import matplotlib.pyplot as plt #导入Matplotlib的pyplot模块\n#构建月度的订单数的DataFrame\ndf_sales[\'消费日期\'] = pd.to_datetime(df_sales[\'消费日期\']) #转化日期格式\ndf_orders_monthly = df_sales.set_index(\'消费日期\')[\'订单号\'].resample(\'M\').nunique() #每个月的订单数量\n#设定绘图的画布\nax = pd.DataFrame(df_orders_monthly.values).plot(grid=True,figsize=(12,6),legend=False)\nax.set_xlabel(\'月份\') # X轴label\nax.set_ylabel(\'订单数\') # Y轴Label\nax.set_title(\'月度订单数\') # 图题\n#设定X轴月份显示格式\nplt.xticks(\n    range(len(df_orders_monthly.index)), \n    [x.strftime(\'%m.%Y\') for x in df_orders_monthly.index], \n    rotation=45)\nplt.show() # 绘图\n</code></pre><p>在这段代码中，你需要注意的是，我们这里使用了Pandas的to_datetime这个API，把原始消费日期转换成了能处理的格式。而在df_orders_monthly中，则是求出了每一个月的订单数量。再往后的代码是具体的绘图工具使用，我就不过多说明了。</p><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/74/9a/74193758e9c81523e1eb9826eda6949a.png?wh=719x406" alt=""></p><p>我们看到，这个数据集收集了“易速鲜花”公司一整年的订单量。所以啊，我们要求的消费额M，实际上是每个用户一整年的总消费额。</p><p>你可能已经注意到，在最后一个月，也就是2021年6月，订单量突然大幅下降。其实这是因为运营人员拉这个表的时候，正是6月的第一个礼拜。所以，6月的数据虽然不全，但并不会影响我们对用户RFM值的分析。</p><h2>2.数据清洗</h2><p>完成了初步可视化之后，我们来清洗一下数据。请你注意，在刚才的可视化过程中，我们已经完成了对消费日期的观察，并没有发现什么异常。所以，现在我们重点要处理的是用户码、单价和（订单中产品的）数量。</p><p>首先，我们用Pandas中的drop_duplicates方法把完全相同的重复数据行删除掉。</p><pre><code class="language-typescript">df_sales = df_sales.drop_duplicates() #删除重复的数据行\n</code></pre><p>我之前介绍过，你还可以用DataFrame的isna().sum()函数，来统计NaN的个数。当然，如果你尝试了这个方法，会发现这个数据集中没有NaN值。</p><p>此外，对于数量、金额等类型的数据，我们还常常会使用describe方法来查看这些字段的统计信息是否有脏数据。</p><pre><code>plain\ndf_sales.describe() #df_sales的统计信息\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/82/b9/820ab32985e7c576aae570128d931ab9.png?wh=322x240" alt=""></p><p>在图中你可以看到这个数据集中，共有9万多行的数据（count统计数据条目的数量），每条数据的平均采购数量是10（mean统计均值），商品平均单价是3.575元左右。</p><p>在概览中我们发现，（订单中产品的）数量的最小值（min）是一个负数（-9360），这显然是不符合逻辑的，所以我们要把这种脏数据清洗掉。具体的处理方式是，用loc属性通过字段名（也就是列名）访问数据集，同时只保留“数量”字段大于0的数据行：</p><pre><code class="language-plain">df_sales = df_sales.loc[df_sales[\'数量\'] &gt; 0] #清洗掉数量小于等于0的数据\n</code></pre><p>在DataFrame对象中，loc属性是通过行、列的名称来访问数据的，我们做数据预处理时会经常用到；还有一个常被用到的属性是iloc，它是通过行列的位置（也就是序号）来访问数据的。</p><p>由于数据行的名称往往也是数值，这两者非常容易被弄混。所以，在这里我特地给你做了一张图，来说明什么是行列名、什么是行列序号，帮你弄清楚它们的区别。</p><p><img src="https://static001.geekbang.org/resource/image/e4/de/e4165807e638ac602fb64fab28a0b8de.jpg?wh=2284x1165" alt=""></p><p>这个语句执行后，如果你再用一次describe方法，就会看到新的最小数量为1，这就对了。</p><p><img src="https://static001.geekbang.org/resource/image/04/b1/0425e6877dbf579eyy74f69a73c809b1.png?wh=323x240" alt=""></p><p>到这里，我们就完成了数据清洗工作，这份数据集中已经没有可以剔除的脏数据了。接下来，我们就看看怎么从中求得RFM值。</p><h1>求RFM值</h1><p>不过在计算RFM之前，我想请你再回顾一下我们整个项目的目标。我们的目标就是建立一个机器学习模型，给用户做价值分组。而其中，RFM实际上就是我们构建模型所需的关键特征数据。</p><p>讲到这里，我想你应该很清楚了，虽说我们这节课要求出RFM的值，但实际上，我们是在完成机器学习项目中的“特征工程”环节，也就是对原始数据集中的信息进行选择、提取、合并、加工、转换，甚至是基于原始信息构建出新的、对于模型的训练更具有意义的特征。</p><p>明白了这一点，我们就可以继续往下走了。那怎么计算这个RFM值呢？</p><p>其实，这里的R值（最近一次消费的天数）和F值（消费频率），我们通过数据集中的消费日期就能得到，但是对于M值（消费金额），你会发现数据集中有采购数量，有单价，但是还没有每一笔消费的总价。</p><p>因此，我们通过一个语句对原有的数据集进行一个小小的扩展。在df_sales 这个DataFrame对象中增加一个数据列计算总价，总价等于由单价乘以数量：</p><pre><code class="language-plain">df_sales[\'总价\'] = df_sales[\'数量\'] * df_sales[\'单价\'] #计算每单的总价\ndf_sales.head() #显示头几行数据&nbsp;&nbsp;&nbsp; &nbsp; &nbsp;\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/c9/ae/c91d515ac9e38b4a59aac6ba89717fae.png?wh=581x181" alt=""></p><p>现在，在这个数据集中，<strong>用户码</strong>、<strong>总价</strong>和<strong>消费日期</strong>这三个字段，给我们带来了每一个用户的R、F、M信息。其中：</p><ul>\n<li>一个用户上一次购物的日期，也就是最新的消费日期，就可以转化成这个用户的R值；</li>\n<li>一个用户下的所有订单次数之和，就是消费频率值，也就是该用户的F值；</li>\n<li>把一个用户所有订单的总价加起来，就是消费金额值，也就是该用户的M值。</li>\n</ul><p>不过，我们目前的这个数据集是一个订单的历史记录，并不是以用户码为主键的数据表。而R、F、M信息是和用户相关的，每一个用户都拥有一个独立的R值、F值和M值，所以，在计算RFM值之前，我们需要先构建一个用户层级表。</p><ol>\n<li>构建用户层级表</li>\n</ol><p>构建用户层级表，简单来说就是生成一个以用户码为关键字段的 Dataframe对象df_user，然后在这个Dataframe对象中，逐步加入每一个用户的新近度（R）、消费频率（F）、消费金额（M），以及最终总的分组信息。</p><p>在代码实现上，我们用Dataframe的unique() 这个API，就能创建出以用户码为关键字段的用户层级表df_user，然后我们再设定字段名，并根据用户码进行排序，最后显示出这个表，就可以了：</p><pre><code class="language-plain">df_user = pd.DataFrame(df_sales[\'用户码\'].unique()) #生成以用户码为主键的结构df_user\ndf_user.columns = [\'用户码\'] #设定字段名\ndf_user = df_user.sort_values(by=\'用户码\',ascending=True).reset_index(drop=True) #按用户码排序\ndf_user #显示df_user\n</code></pre><p>请你注意，在上述语句中，reset_index(drop=True)的意思是重置索引，生成新的默认数值类型索引，并且不保留原来的索引。运行这段代码后，我们得到这样的结果：</p><p><img src="https://static001.geekbang.org/resource/image/46/b9/46d77e14780785ef546daa64c3568ab9.png?wh=183x439" alt=""></p><p>可以看到，我们一共有980个用户的数据。有了用户层级表，现在我们依次求出RFM，让这个用户层级表的结构越来越完整。</p><ol start="2">\n<li>求出R值</li>\n</ol><p>我们知道，R值代表自用户上次消费以来的天数，它与最近一次消费的日期相关。所以，用表中最新订单的日期（拉出来这张表的日期）减去上一次消费的日期，就可以确定对应用户的R值。</p><p>下面是具体的代码实现，在每行代码中都给出了详细的注释，而且Pandas的语句也都不难理解，你可以看一看：</p><pre><code class="language-plain">df_sales[\'消费日期\'] = pd.to_datetime(df_sales[\'消费日期\']) #转化日期格式\ndf_recent_buy = df_sales.groupby(\'用户码\').消费日期.max().reset_index() #构建消费日期信息\ndf_recent_buy.columns = [\'用户码\',\'最近日期\'] #设定字段名\ndf_recent_buy[\'R值\'] = (df_recent_buy[\'最近日期\'].max() - df_recent_buy[\'最近日期\']).dt.days #计算最新日期与上次消费日期的天数\ndf_user = pd.merge(df_user, df_recent_buy[[\'用户码\',\'R值\']], on=\'用户码\') #把上次消费距最新日期的天数（R值）合并至df_user结构\ndf_user.head() #显示df_user头几行数据\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/3e/30/3e46f92305d4d9bf3cd1aa172ed19230.png?wh=142x213" alt=""></p><p>R值越大，说明该用户最近一次购物日距离当前日期越久，那么这样的用户就越是处于休眠状态。从表中可以看出来，编号为14682的用户已经有187天没有购物了。所以我们就可以判断这个用户呈现休眠态，很可能已经被别的购物平台所吸引了，也就是流失了。</p><ol start="3">\n<li>求出F值</li>\n</ol><p>类似地，我们还可以求出F值，并把F值添加至用户层级表：</p><pre><code class="language-plain">df_frequency = df_sales.groupby(\'用户码\').消费日期.count().reset_index() #计算每个用户消费次数，构建df_frequency对象\ndf_frequency.columns = [\'用户码\',\'F值\'] #设定字段名称\ndf_user = pd.merge(df_user, df_frequency, on=\'用户码\') #把消费频率整合至df_user结构\ndf_user.head() #显示头几行数据\n</code></pre><p>这段代码的核心就是通过给消费日期做count()计数，来求出每一个用户的消费次数。最后，我们得到的输出结果如下：</p><p><img src="https://static001.geekbang.org/resource/image/4f/62/4f02132dcfa59e87143ca3d513de9c62.png?wh=181x207" alt=""></p><ol start="4">\n<li><strong>求出M值</strong></li>\n</ol><p>M值很容易求出，它就是用户消费的总和：</p><pre><code class="language-plain">df_revenue = df_sales.groupby(\'用户码\').总价.sum().reset_index() #根据消费总额，构建df_revenue对象\ndf_revenue.columns = [\'用户码\',\'M值\'] #设定字段名称\ndf_user = pd.merge(df_user, df_revenue, on=\'用户码\') #把消费金额整合至df_user结构\ndf_user.head() #显示头几行数据\n</code></pre><p>这段代码的核心是通过给用户每张订单的总价字段做sum()计数，来求出每一个用户的消费总和。最后，输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/13/c2/13bc1375b789e5173ce3016b84a01dc2.png?wh=260x208" alt=""></p><p>到这里，我们就求出了每一个用户的R、F、M值。不过，这只是完成了特征工程环节，在下节课，我们要根据这三个维度的值给用户分组，这就需要聚类算法大显身手了。</p><h1>总结一下</h1><p>现在，我们来回顾一下这节课的重点。</p><p>我们这个项目的目标是为用户分组画像。要做分组画像，RFM分析是一个不错的方法，它也是目前很多互联网厂商普遍采用的分析方式。</p><p>我们今天的重点是，根据用户码、总价和消费日期这三个字段，从消费历史数据中求出每位用户的R、F、M的值，这就好像给用户贴上了一堆数字化的标签。同时，<strong>用户行为的量化，也可以视作是进一步做机器学习项目之前的特征工程</strong>。这也为我们后续给用户做聚类分组做好了准备。</p><p>理解RFM分析的精髓，并利用好RFM三个指标，是我们这个项目实战的核心价值所在。求出了RFM值，就恭喜你闯过了我们获客阶段的数据探索这一关。</p><p>虽然我把实战的具体过程讲得很细，但我更希望你理解的是<strong>“数据处理之道”</strong>， 而不是只学几个Python语句。Python语句都很简单，但是怎么灵活应用数据，是我们一开始就应该领悟的“道”，因为只有对思路的深入理解，才能让我们走得更高、更远。</p><h1>思考题</h1><p>好，这节课就到这里了，最后，我给你留个思考题：</p><p>RFM模型是衡量客户价值和客户创利能力的重要工具和手段，而且也被广泛应用在企业的获客、促销、客户关系管理等诸多营销、推广环节。请你谈一谈你对RFM的理解，以及这些指标可以应用于哪些业务场景。</p><p>欢迎你在留言区和我分享你的观点，如果你认为这节课的内容有收获，也欢迎把它分享给你的朋友，我们下一讲再见！</p><p><img src="https://static001.geekbang.org/resource/image/80/45/800f834531ddbf7686054d0c4cce2445.jpg?wh=2284x736" alt=""></p>',
        article_title: "05 | 数据探索：怎样从数据中找到用户的RFM值？",
      },
      {
        title: "06 | 聚类分析：如何用RFM给电商用户做价值分组画像？",
        id: 416824,
        content:
          "<p>你好，我是黄佳。欢迎来到零基础实战机器学习。</p><p>在上一讲中，我们从一份互联网电商“易速鲜花”的历史订单数据集中，求出了每一个用户的R、F、M值。你可能会问，从这些值中，我们又能看出什么有价值的信息呢？</p><p>别着急，在这一讲中，我们继续往前走，看看如何从这些枯燥且不容易观察的数据中，得到更为清晰的用户分组画像。通过这节课，我希望你能理解聚类算法的原理和最优化过程，这可以帮你建立起针对问题选择算法的直觉。</p><h1>怎么给用户分组比较合适？</h1><p>这是我们在上节课中得出的用户层级表，表中有每位用户的R、F、M值。</p><p><img src=\"https://static001.geekbang.org/resource/image/13/c2/13bc1375b789e5173ce3016b84a01dc2.png?wh=260x208\" alt=\"\"></p><p>这里，我们希望看看R值、F值和M值的分布情况，以便为用户分组作出指导。代码是接着上一讲的基础上继续构建，我就不全部贴上来了，完整的代码和数据集请你从<a href=\"https://github.com/huangjia2019/geektime/tree/main/%E8%8E%B7%E5%AE%A2%E5%85%B306\">这里</a>下载。</p><pre><code class=\"language-typescript\">df_user['R值'].plot(kind='hist', bins=20, title = '新进度分布直方图') #R值直方图\n</code></pre><pre><code class=\"language-typescript\">df_user.query('F值 &lt; 800')['F值'].plot(kind='hist', bins=50, title = '消费频率分布直方图') #F值直方图\n</code></pre><pre><code class=\"language-typescript\">df_user.query('M值 &lt; 20000')['M值'].plot(kind='hist', bins=50, title = '消费金额分布直方图') #M值直方图\n</code></pre><!-- [[[read_end]]] --><p>分别输出如下结果：</p><p><img src=\"https://static001.geekbang.org/resource/image/ce/0d/ced0dbdyy646yy9ac0963a325ab7ba0d.png?wh=383x262\" alt=\"\"><img src=\"https://static001.geekbang.org/resource/image/14/19/14100de8f515c80181bb703cb9663a19.png?wh=383x262\" alt=\"\"><img src=\"https://static001.geekbang.org/resource/image/0b/d5/0bf0ddf6fb046dd5fd7867f3ee4b99d5.png?wh=383x262\" alt=\"\"></p><p>可以看到，我们求出的R值、F值和M值的覆盖区间都很大。就拿R值来说，有的用户7天前购物，R值为7；有的用户70天前购物，R值为70；还有的用户187天前购物，R值为187。</p><p>那现在问题来了，如果说我们的目标是根据R值把用户分为几个不同的价值组，那么怎么分组比较合适呢？</p><p>其实，这个问题又可以拆分为两个子问题：</p><ol>\n<li>分成多少个组比较好？</li>\n<li>从哪个值到哪个值归为第一组（比如0-30天是一组），从哪个值到哪个值归为第二组（比如30天-70天是一组）？</li>\n</ol><p>对于这两个问题的答案，有人肯定会说，可以凭借经验来人为确定。比如说，把用户分为高、中、低三个组，比如R值为0到50的分为一个组，50到150的分为一组，150天以上的归为一组。</p><p>这样的人为分组似乎也可以，但它存在一些弊端：首先，分组的准确性完全取决于人的经验，如果分得不准，效果就不好。其次，人为分组是静态的，如果用户情况变化了，我们还是用同样的区间来分组，就不是很合适。</p><p>那怎么办呢？我想你已经猜到了，其实该怎么分组，我们说了不算，要数据说了算。所以，要解决这个问题，还是要通过机器学习算法，根据数据的实际情况来动态地确定分组。因为只有这样的模型才是动态的，才能长期投入使用。</p><p>坚定了这一点后，我们考虑一下选什么算法来建立模型。</p><h1>聚类算法中的K-Means算法</h1><p>首先，我们要搞清楚，给用户做分组画像属于监督学习问题，还是无监督学习问题？我们要通过历史订单数据来给用户分组，这是没有任何已知标签可以做参照的，数据集中并没有一个字段指明用户的价值是“高”还是“低”，所以这显然是一个无监督学习问题。</p><p>在无监督学习中，聚类和降维是两种最常见的算法，不过它们应用场景很不一样。聚类我们说过了，主要可以用来做分组；而降维，则是通过数学变换，将原始高维属性空间转变为一个低维“子空间”，它本质上是通过最主要的几个特征维度实现对数据的描述。很显然，我们的问题适合用聚类算法来解决。</p><p>聚类算法可以让机器把数据集中的样本按照特征的性质分组，不过它只是帮我们把数据特征彼此邻近的用户聚成一组（这里的组称为聚类的簇）。而这里说的“特征彼此邻近”，指的这些用户的数据特征在坐标系中有更短的向量空间距离。也就是说，<strong>聚类算法是把空间位置相近的特征数据归为同一组。</strong></p><p><img src=\"https://static001.geekbang.org/resource/image/04/89/04df8ab44fd06507930b312bf3647889.png?wh=287x295\" alt=\"\" title=\"聚类算法把空间位置相近的数据归为同一组\"></p><p>不过，请你注意，聚类算法本身并不知道哪一组用户是高价值，哪一组用户是低价值。分完组之后，我们还要根据机器聚类的结果，人为地给这些用户组贴标签，看看哪一组价值高，哪一组价值低。我这里把这种人为贴标签的过程称为“<strong>聚类后概念化</strong>”。等你学完这节课，就能更清楚我为什么要做“聚类后概念化”了。</p><p>搞清楚问题适合用聚类算法解决还不够，因为聚类的算法可不止一种，我们还要进一步确定采用哪一个算法。这里我直接选用K-Means（K-均值）算法了，因为这个算法不仅简洁，而且效率也高，是我们最常用的聚类算法。像文档归类、欺诈行为检测、用户分组等等这些场景，我们往往都能用到。</p><p>说到这里，你也许很疑惑：在监督学习中，模型都是要依赖于标签才能创建出来，这无监督学习怎么就这么聪明，能在没有标签的情况下自动给数据点分组？下面我就带你探寻其中的奥秘。</p><p>在K-Means算法中，“K”是一个关键。K代表聚类的簇（也就是组）的个数。比如说，我们想把M值作为特征，将用户分成3个簇（即高、中、低三个用户组），那这里的K值就是3，并且需要我们人工指定。</p><p>指定K的数值后，K-Means算法会在数据中随机挑选出K个数据点，作为簇的质心（centroid），这些质心就是未来每一个簇的中心点，算法会根据其它数据点和它的距离来进行聚类。</p><p>挑选出质心后，K-Means算法会遍历每一个数据点，计算它们与每一个质心的距离（比如欧式距离）。数据点离哪个质心近，就跟哪个质心属于一类。</p><p>遍历结束后，每一个质心周围就都聚集了很多数据点，这时候啊，算法会在数据簇中选择更靠近中心的质心，如果原来随机选择的质心不合适，就会让它下岗。</p><p>在整个聚类过程中，为了选择出更好的质心，“挑选质心”和“遍历数据点与质心的距离”会不断重复，直到质心的移动变化很小了，或者说固定不变了，那K-Means算法就可以停止了。</p><p>我用下面的图来帮助你理解质心在聚类过程中逐渐移动到最佳位置，以及簇的形成过程：</p><p><img src=\"https://static001.geekbang.org/resource/image/e9/0b/e9042e10d793c15de377e6d904f7280b.jpg?wh=2284x1165\" alt=\"\" title=\"聚类中心（质心）的移动和簇形成的过程\"></p><p>理解了聚类算法的原理，我们继续来思考一个问题：我们前面说K值需要人工指定，那怎么在算法的辅助下确定K值呢？</p><h1>手肘法选取K值</h1><p>其实，在事先并不是很确定分成多少组比较合适的情况下，“手肘法”（elbow method）可以帮我们决定，在某一批数据点中，数据分为多少组比较合适。这里我要特别说明一下，尽管我们前面说要把用户分为高、中、低三个价值组，但是R、F、M的值却可以分成很多组，并不一定都是3组。</p><p>手肘法是通过聚类算法的损失值曲线来直观确定簇的数量。损失值曲线，就是以图像的方法绘出，取每一个K值时，各个数据点距离质心的平均距离。如下图所示，当K取值很小的时候，整体损失很大，也就是说各个数据点距离质心的距离特别大。而随着K的增大，损失函数的值会在逐渐收敛之前出现一个拐点。此时的K值就是一个比较好的值。</p><p><img src=\"https://static001.geekbang.org/resource/image/9b/66/9ba43dc8c0dc4060eb281e7dbf253d66.jpg?wh=2284x783\" alt=\"\" title=\"手肘法——确定最佳K值\n\"></p><p>你看图中，损失随着簇的个数而收敛的曲线大概像个手臂，最佳K值的点像是一个手肘，这就是为什么我们会叫它“手肘法”的原因。</p><p>下面我们就用代码找出R值的手肘点。请你注意，这里我会先定义一个找手肘点的函数，因为后面在对R值、F值和M值聚类的过程中，我们都要用到这个函数。</p><pre><code class=\"language-plain\">from sklearn.cluster import KMeans #导入KMeans模块\ndef show_elbow(df): #定义手肘函数\n&nbsp;&nbsp;&nbsp; distance_list = [] #聚质心的距离（损失）\n&nbsp;&nbsp;&nbsp; K = range(1,9) #K值范围\n&nbsp;&nbsp;&nbsp; for k in K:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kmeans = KMeans(n_clusters=k, max_iter=100) #创建KMeans模型\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; kmeans = kmeans.fit(df) #拟合模型\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; distance_list.append(kmeans.inertia_) #创建每个K值的损失\n&nbsp;&nbsp;&nbsp; plt.plot(K, distance_list, 'bx-') #绘图\n&nbsp;&nbsp;&nbsp; plt.xlabel('k') #X轴\n&nbsp;&nbsp;&nbsp; plt.ylabel('距离均方误差') #Y轴\n&nbsp;&nbsp;&nbsp; plt.title('k值手肘图') #标题\n</code></pre><p>在这段代码中，核心部分是拟合kmeans模型之后，通过&nbsp;kmeans.inertia_计算损失值。损失会随着K值的增大而逐渐减小，而那个拐点就是手肘。</p><p>然后我们调用下面这个函数，显示R值、F值和M值聚类的K值手肘图：</p><pre><code class=\"language-typescript\">show_elbow(df_user[['R值']]) #显示R值聚类K值手肘图\n</code></pre><pre><code class=\"language-typescript\">show_elbow(df_user[['F值']]) #显示F值聚类K值手肘图\n</code></pre><pre><code class=\"language-typescript\">show_elbow(df_user[['M值']]) #显示M值聚类K值手肘图\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/01/c8/01ae89f69241886271d539632b95d9c8.jpg?wh=2284x592\" alt=\"\" title=\"R、F和M值的K值手肘图\"></p><p>可以看到，R、F、M值的拐点大概都在2到4之间附近，这就意味着我们把用户分成2、3、4个组都行。这里我选择3作为R值的簇的个数，选择4作为F值的簇的个数，选择3作为M值的簇的个数。</p><p>那到这里为止呢，我们已经选定好了算法，并确定了R、F、M每个特征下簇的个数，也就是K值。接下来我们就可以开始创建聚类模型了。</p><h1>创建和训练模型</h1><p>前面说了，我们在手肘附近选择3作为R的K值，所以我们创建模型是把n_clusters参数，也就是簇的个数指定为3。这样，聚类算法会把用户的R值分为三个层次。对于F、M，我们也根据对应的K值做类似的操作：</p><pre><code class=\"language-plain\">from sklearn.cluster import KMeans #导入KMeans模块\nkmeans_R = KMeans(n_clusters=3) #设定K=3\nkmeans_F = KMeans(n_clusters=4) #设定K=4\nkmeans_M = KMeans(n_clusters=4) #设定K=4\n</code></pre><p>这样，我们就在程序中创建了一个K-Means聚类模型。<br>\n创建好模型后，我们借助fit方法，用R值的数据，训练模型。</p><pre><code class=\"language-plain\">kmeans_R.fit(df_user[['R值']]) #拟合模型\nkmeans_F.fit(df_user[['F值']]) #拟合模型\nkmeans_M.fit(df_user[['M值']]) #拟合模型\n</code></pre><p>我们不是第一次见到fit这个方法了，fit，翻译成中文就叫做拟合模型。基本上所有的机器学习模型都是用fit语句来进行模型训练的。</p><h1>使用模型进行聚类，并给用户分组</h1><p>模型训练好了，现在我们就用它给R、F、M值聚类。</p><ol>\n<li><strong>给R、F、M值聚类</strong></li>\n</ol><p>我们先用kmeans模型中的predict方法给R值聚类。“predict”翻译成中文是“预测”，不过作为无监督学习方法，它其实就是使用模型进行聚类，而且，也不需要进一步的评估过程。这也是监督学习和无监督学习不一样的地方。</p><pre><code class=\"language-plain\">df_user['R值层级'] = kmeans_R.predict(df_user[['R值']]) #通过聚类模型求出R值的层级\ndf_user.head() #显示头几行数据\n</code></pre><p>这段代码的输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/11/54/11162b81068b36511b6120e7dfbbff54.png?wh=167x163\" alt=\"\"></p><p>输出显示，这个聚类结果被附加到了用户层级表中，也就是说在用户层级表中的“用户码”、“R值”字段后面出现了“R值层级”这个字段，也就是将R值聚类后各个簇的号码。</p><p>下面我们用groupby语句来看看0、1、2这几个簇的用户基本统计数据：</p><pre><code class=\"language-plain\">df_user.groupby('R值层级')['R值'].describe() #R值层级分组统计信息\n</code></pre><p>这段代码的输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/6a/39/6a4375097ce237d709c9d1882697b039.png?wh=523x147\" alt=\"\"></p><p>这里有一个奇怪的现象，不知道你有没有观察到？</p><p>如果你注意看0、1和2这三个簇，也就是三个组，就会发现形成的簇没有顺序。你看，0群的用户最多670个人，均值显示他们平均购物间隔是31天，上次购物距今是0天到94天，这是相对频繁的购物用户群。</p><p>1群的用户平均购物间隔为295天，上次购物距现在是231天到372天，这是在休眠中的用户；而2群的用户平均购货间隔则变成了157天，介于两者之间，他们上次购物距今是从95天到225天。你会发现这个从０到２的顺序既不是升序，也不是降序。</p><p>这其实是聚类这种算法本身的问题。聚类，作为一种无监督学习算法，是不知道顺序的重要性的，它只是盲目地把用户分群（按照其空间距离的临近性），而不管每个群的具体意义，因此也就没有排序的功能。这也就是我前面说的“聚类后概念化”的具体意思。聚类并不知道那组人的价值高低，所以也就无法确定顺序，需要我们人为来排序。</p><ol start=\"2\">\n<li><strong>为聚类的层级做排序</strong></li>\n</ol><p>那么，下面我们就用一段代码，把聚类的结果做一个排序，让0、1、2这三个组体现出价值的高低。这段代码稍微有点长，不过我给出了详细的注释，你可以看一下：</p><pre><code class=\"language-typescript\">#定义一个order_cluster函数为聚类排序\ndef order_cluster(cluster_name, target_name,df,ascending=False):\n&nbsp;&nbsp;&nbsp; new_cluster_name = 'new_' + cluster_name #新的聚类名称\n&nbsp;&nbsp;&nbsp; df_new = df.groupby(cluster_name)[target_name].mean().reset_index() #按聚类结果分组，创建df_new对象\n&nbsp;&nbsp;&nbsp; df_new = df_new.sort_values(by=target_name,ascending=ascending).reset_index(drop=True) #排序\n&nbsp;&nbsp;&nbsp; df_new['index'] = df_new.index #创建索引字段\n&nbsp;&nbsp;&nbsp; df_new = pd.merge(df,df_new[[cluster_name,'index']], on=cluster_name) #基于聚类名称把df_new还原为df对象，并添加索引字段\n&nbsp;&nbsp;&nbsp; df_new = df_new.drop([cluster_name],axis=1) #删除聚类名称\n&nbsp;&nbsp;&nbsp; df_new = df_new.rename(columns={\"index\":cluster_name}) #将索引字段重命名为聚类名称字段\n&nbsp;&nbsp;&nbsp; return df_new #返回排序后的df_new对象\n</code></pre><p>在上述代码中，为聚类做排序的是order_cluster函数。那么接下来，我们再调用这个order_cluster函数，把用户表重新排序。我们知道，消费天数间隔的均值越小，用户的价值就越高，所以我们在这里采用降序，也就是把ascending参数设为False：</p><pre><code class=\"language-typescript\">df_user = order_cluster('R值层级', 'R值', df_user, False) #调用簇排序函数\ndf_user = df_user.sort_values(by='用户码',ascending=True).reset_index(drop=True) #根据用户码排序\ndf_user.head() #显示头几行数据\n</code></pre><p>此时，各用户的层级值就发生了变化，比如用户14688的簇编号从1变成了2，因为这个用户7天前曾经购物，其R值相对偏低，放在高分的2层级是合适的。其实，上面的代码中，我们并没有改变用户的分组，而只是改变了每一个簇的编号，这样层级关系就能体现出来了。</p><p>下面我们重新显示各个层级的信息：</p><pre><code class=\"language-typescript\">df_user.groupby('R值层级')['R值'].describe() #R值层级分组统计信息\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/93/7c/93b9d8c72da873dc6fae6643f5d6787c.png?wh=477x136\" alt=\"\"></p><p>你会看到，此时各个簇已经形成了次序。0层级的用户，平均新近度是298天，1层级的用户，平均新近度是157天，而R值最高的用户组（2层级），平均新近度仅有32天。这说明用户上一次消费距今的天数越少，其R值的价值越高。</p><p>R值聚类做好后，我们按照同样的方法可以根据用户购买频率给F值做聚类，并用刚才定义的order_cluster函数为聚类之后的簇进行排序，确定层级。因为消费次数越多，价值越高，所以我们把order_cluster 函数的ascending参数设定为True，也就是升序：</p><pre><code class=\"language-typescript\">df_user['F值层级'] = kmeans_F.predict(df_user[['F值']]) #通过聚类模型求出F值的层级\ndf_user = order_cluster('F值层级', 'F值',df_user,True) #调用簇排序函数\ndf_user.groupby('F值层级')['F值'].describe() #F值层级分组统计信息\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/b1/a9/b17366087ca2f0f32eba40cd5d78c9a9.png?wh=534x165\" alt=\"\"></p><p>上图显示，经过了排序的层级中，0级用户的购买频率均值为32次，1级的用户消费频率均值为153次，2级用户消费频率均值达到416次，而2级用户的消费频率均值高达1295次，不过这个簇中只有7个用户。</p><p>还是一样，我们重新为用户层级表排序，并显示df_user对象，也就是用户层级表的当前状态。</p><pre><code class=\"language-typescript\">df_user = df_user.sort_values(by='用户码',ascending=True).reset_index(drop=True) #根据用户码排序\ndf_user.head()\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/98/73/98c934ee43fe2a944153152dc9a85c73.png?wh=250x161\" alt=\"\"></p><p>最后，我们依葫芦画瓢，给M值做聚类，并且对聚类的结果做排序，分出层级。因为代码和R值、F值聚类十分相似，我就直接给出所有代码，不再说明了。</p><pre><code class=\"language-typescript\">df_user['M值层级'] = kmeans_M.predict(df_user[['M值']]) #通过聚类模型求出M值的层级\ndf_user = order_cluster('M值层级', 'M值',df_user,True) #调用簇排序函数\ndf_user.groupby('M值层级')['M值'].describe() #M值层级分组统计信息\ndf_user = df_user.sort_values(by='用户码',ascending=True).reset_index(drop=True) #根据用户码排序\ndf_user.head() #显示头几行数据\n</code></pre><p>最终结果：</p><p><img src=\"https://static001.geekbang.org/resource/image/53/12/5361ee46fcfyya3dbd39e54281e33c12.png?wh=383x159\" alt=\"\"></p><p>好，那到这里为止，R、F、M的聚类工作就全部完成了，并且我们还划分了层级！在当前的用户层级表中，已经包含了这三个维度的层级，最终的用户分层就可以以此为基础来确定了。</p><h1>为用户整体分组画像</h1><p>我们这里采用简单叠加的方法把R、F、M三个层级的值相加，用相加后得到的值，作为总体价值，来给用户进行最终的分层。当然了，如果你对其中某一个指标看得比较重，也可以加权之后再相加。</p><p>具体来讲，我们用下面的代码来创建相加之后的层级，即总分字段。</p><pre><code class=\"language-typescript\">df_user['总分'] = df_user['R值层级'] + df_user['F值层级'] + df_user['M值层级'] #求出每个用户RFM总分\n</code></pre><p>因为R值有3个层级（0，1，2），F值有4个层级（0，1，2，3），M值有4个层级（0，1，2，3），我们把三个维度的值相加，那每一个用户的得分有可能是0到8当中的某一个值，也就是说出现了9个层次。</p><p>我这里就按照下面的规则，来确定用户最终的价值分层。当然了，你也可以尝试用其它的阈值来确定你的价值分层。</p><ul>\n<li>0-2分，低价值用户</li>\n<li>3-4分，中价值用户</li>\n<li>5-8分，高价值用户</li>\n</ul><p>什么意思呢？举例来说，就是如果一个用户在R值拿到了2分，在新近度这个维度为高价值用户，但是在消费频率和消费金额这两个维度都只拿到0分，那么最后得分就为2，总体只能评为低价值用户。</p><p>下面这段代码便是根据总分，来确定出每一个用户的总体价值的。</p><pre><code class=\"language-typescript\">#在df_user对象中添加总体价值这个字段\ndf_user.loc[(df_user['总分']&lt;=2) &amp; (df_user['总分']&gt;=0), '总体价值'] = '低价值'&nbsp;\ndf_user.loc[(df_user['总分']&lt;=4) &amp; (df_user['总分']&gt;=3), '总体价值'] = '中价值'&nbsp;\ndf_user.loc[(df_user['总分']&lt;=8) &amp; (df_user['总分']&gt;=5), '总体价值'] = '高价值'\ndf_user #显示df_user\n</code></pre><p>我们再次输出df_user对象，看一看最终的用户层级表：</p><p><img src=\"https://static001.geekbang.org/resource/image/8b/aa/8b98da066d42e514b4d54f4c4ec62daa.png?wh=484x348\" alt=\"\"></p><p>此时，980个用户的R、F、M层级，还有总体价值的层级都非常清楚了。对于每一个用户，我们都可以迅速定位到他的价值。那到这里，我们就成功地完成了为“易速鲜花”公司做用户价值分组的工作。</p><p>现在，有了用户的价值分组标签，我们就可以做很多进一步的分析，比如说选取R、F、M中任意两个维度，并把高、中、低价值用户的散点图进行呈现：</p><pre><code class=\"language-typescript\">#显示高、中、低价值组分布散点图（F值与M值）\nplt.scatter(df_user.query(\"总体价值 == '高价值'\")['F值'],\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df_user.query(\"总体价值 == '高价值'\")['M值'],c='g',marker='*')\nplt.scatter(df_user.query(\"总体价值 == '中价值'\")['F值'],\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df_user.query(\"总体价值 == '中价值'\")['M值'],marker=8)\nplt.scatter(df_user.query(\"总体价值 == '低价值'\")['F值'],\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df_user.query(\"总体价值 == '低价值'\")['M值'],c='r')\n各价值组的用户分布散点图如下图所示：\n\n</code></pre><p><img src=\"https://static001.geekbang.org/resource/image/28/d5/286a4d8050b94f746ac61533372d11d5.png?wh=555x345\" alt=\"\"></p><p>借此，我们可以发现，高价值用户（绿色五星）覆盖在消费频率较高的区域，和F值相关度高。而在总消费金额大于5000元的用户中，中高价值的用户（绿色五星和红色圆点）都有。</p><p>当然，作为运营部门的一员，你还可以通过对新老用户的价值分组，制定出更有针对性的获客、营销、推广等运营方案。</p><h1>总结一下</h1><p>好啦，到这里，我们就成功闯过了“获客”这个关卡。现在来回顾一下这一讲中的重点内容。</p><p>在这一讲中， 我们理解了聚类算法的原理，学会了如何用手肘法选择K值，以及如何用K-均值算法来给R值、F值和M值做聚类。</p><p>我要再次强调的是，聚类只是负责把空间距离相近的数据点分成不同的簇，它并不知道每一个簇代表的具体含义。比如说，我们用K-Means算法给R值分成三个簇，这并不表示0比1价值低，1比2价值低。此时的0、1、2都是聚类随机分配的编号，具体分组排序的工作我们还要单独去完成。</p><p>在K值的选择方面呢，手肘法可以帮我们直观地显示出聚类过程中整体损失的“拐点”，我们可以在拐点或者拐点附近选择K值，确定把数据分成多少个“簇”（也就是多少个组）。</p><p>最后，我们还讲到用K-均值算法来给R值做聚类，这也非常简单，就是创建模型、拟合模型、用模型进行聚类，这些过程加一块也就是几行代码的事儿，你不用有负担。</p><p>掌握了上述这些内容，你就可以用K-Means算法这种无监督学习算法给任何数据集做聚类，来解决其它类似的问题了，比如根据学生的考试成绩，为学生分组聚类等等。</p><h1>思考题</h1><p>这节课就到这里了，我给你留两个思考题：</p><ol>\n<li>对于K-Means算法，X特征数据集的输入，可以不止一个维度。为了给R、F、M分别分层，在这节课中我给这三者单独做了聚类。你能不能试着把R、F、M三个特征同时输入K-Means算法，为用户整体做聚类呢？此外，你还能不能想到些其它的为用户分组画像的方法呢？</li>\n<li>聚类算法的应用场景其实很广，包括给图像的颜色簇量化分组、给文本分组等等，在你的工作和生活中，你还能够想到，或者曾用过哪些可以通过聚类解决的问题？请你分享一下。</li>\n</ol><p>欢迎你在留言区和我分享你的观点，如果你认为这节课的内容有收获，也欢迎把它分享给你的朋友，我们下一讲再见！</p><p><img src=\"https://static001.geekbang.org/resource/image/81/c1/814f8483b103c69295e54c9d87c7d6c1.jpg?wh=2284x1136\" alt=\"\"></p>",
        article_title: "06 | 聚类分析：如何用RFM给电商用户做价值分组画像？",
      },
      {
        title: "07｜回归分析：怎样用模型预测用户的生命周期价值？",
        id: 417479,
        content:
          "<p>你好，我是黄佳。</p><p>首先，恭喜你成功通过“获客关”。在获客关中，我们把互联网电商“易速鲜花”的用户们分成了高、中、低三个价值组，你还记得这个项目是属于监督学习还是无监督学习吗？没错，是无监督学习。</p><p>今天，我们开启“变现关”的挑战。而且我们将进入更为常见、更主流的机器学习应用场景，监督学习的实战。更确切地说，这是用监督学习解决回归问题的一次实战。</p><p><img src=\"https://static001.geekbang.org/resource/image/fb/aa/fb47103938cab174b7093479f02485aa.jpg?wh=2284x1033\" alt=\"\"></p><p>监督学习，主要应用于回归和分类两大场景，分别可以用来预测数值和进行分类判断，这两类问题也是我们课程的两大重点。在这节课中，你就能够学到用回归来进行数值预测的方法了。因为这是监督学习项目，所以，我们会完全跟着前面讲的“机器学习实战5步”来走。</p><h1>定义问题</h1><p>请你想象一下，你刚为客户做了分组画像，老板推门而入：“价值分组这个项目做得不错嘛！现在有这么一个新需求，你看看你这边还有什么idea。”</p><p>他继续说道：“你也知道，现在流量太贵了，拉新成本平均下来每注册一个用户我几乎要花接近500元。我是这么想的。500元，说贵也贵，说不贵也不贵，关键还是要看这些用户能给我们带来多大价值、多大回报，你说对吧？要是多数人用我们的App，用几次就不用了，订花的总消费甚至比500元还少，那就没什么意思了。所以，你看能不能根据历史数据，预测一下新用户未来一两年的消费总额？”</p><!-- [[[read_end]]] --><p>好，那现在我们来分析一下这个问题。其实，这个问题的实质是计算一个用户使用某产品的过程中消费总量是多少。比如某类手机App，用户的平均使用长度是两年左右，那么两年内用户在App上消费所产生的总收益就是用户的生命周期价值，英文是Lifetime Value，简称LTV，有时我们也叫CLV（Customer Lifetime Value）。</p><p>请你想想看，如果你得到一个新用户的成本是500元，看上去是很贵，但如果这个人在你的店铺买花的钱预计会超过万元，那么扣除进货成本和获客成本，你还是赚到了，ROI高嘛。</p><p><img src=\"https://static001.geekbang.org/resource/image/c0/b7/c0267aa171088a27f6c00470749749b7.png?wh=415x239\" alt=\"\" title=\"用户的LTV越高，我们ROI越高\"></p><p>所以，我们的目标就是通过现有数据，找到一个能预测出用户生命周期价值的模型，来指导我们获客的成本，避免超出回报的盲目投入。</p><p>那对于这个问题，我们还是会使用上一个项目中的原始数据集，你在<a href=\"https://github.com/huangjia2019/geektime/tree/main/%E5%8F%98%E7%8E%B0%E5%85%B307\">这里</a>就能下载到。拿到数据集后，我们就可以开始数据的预处理工作了。</p><h1>数据预处理</h1><p>通过前面的课程，我想你应该已经想到：“用户生命周期价值”是一种连续性数值，对这种连续性数值的预测，属于一个回归问题。那么，在数据预处理阶段，我们就要确定把哪些特征字段输入到回归模型中。</p><p><img src=\"https://static001.geekbang.org/resource/image/7b/e9/7bf08c35eecd0e311942269c801296e9.png?wh=709x161\" alt=\"\"></p><p>我们看到，这个数据集的字段包括订单号、产品码、消费日期、产品说明、数量（订单）、单价、用户码和城市。那么，哪些字段和用户的LTV相关呢？</p><p>很显然，像订单号、用户码、产品说明这些信息，肯定和用户的LTV值是不相关的。你可能会说，用户所在的“城市”这个字段，也许和用户的消费能力有一定的关联。</p><p>这看起来有一定的道理。不过，像“北京”、“上海”这样的文本字段肯定不能直接被输入回归模型，如果要考虑城市信息，也应该转换为0、1的哑变量值，再输入模型。也就是说转换成“是否北京”(值是0或1)，“是否上海”（值是0或1），“是否深圳”（值是0或1）……</p><p>你看，这样一下子就增加了好多个特征，而且对这个数据集来说，城市对于LTV值的影响其实并不大。综合这些因素，在这个项目中，我们就不考虑“城市”这个字段了。</p><p>看到这，你也许会想：那这么说的话，用户的消费金额肯定和LTV非常相关，可是，用户的消费金额不就是LTV本身嘛，用自己去预测自己，这样的模型有什么意义呢？</p><p>这个想法没错，用户的消费金额确实是和LTV最为相关的变量。不过，我们要做两个小调整，来解答你的困惑。</p><p>第一个调整是，我们可以考虑用头3个月的消费金额，而不是全部一整年的消费金额，来预测用户后续一年或两年的“价值”。这样，根据历史数据搭建起模型后，对于新注册的用户，我们只需要观察其头3个月的表现，就能够预测他今后一、两年的消费总量。如果某类App用户平均使用长度是一年或两年的话，这也就是该用户的生命周期价值。</p><p>第二个调整是，用我们在前两节课中学到的R、F、M值来做特征变量，这就避免了单一维度建模的局限性。在RMF用户分组中，我们不仅可以得到消费金额，还能得到新近度、消费频率。这些层级把消费频率、最近消费日期这些非数值变量转化成了数值变量，而且这些数值与用户的LTV也都密切相关。</p><p>因此，<strong>在我们的模型中，可以用头3个月的R、F、M这3个数值作为特征，也就是回归模型的自变量。而回归模型所要预测的因变量，即数据集的标签，就是一年的总消费额，你可以认为它就是用户的LTV。</strong></p><p>我要说明一下，这里的3个月、12个月都只是思路上的示意，我们不去考虑用户平均会使用该App一年还是两年、三年。在不同业务场景中，计算RFM特征值的时间区间和LTV的时间区间可以视情况而定。</p><p>下面我们要做的就是数据清洗，这其中包括删除不符合逻辑的负值、查看有没有缺失值、添加每个订单的总价字段等。这部分内容和上一讲的类似，我就不再重复了，你如果不清楚，可以去回顾<a href=\"https://time.geekbang.org/column/article/416824\">上一讲</a>的内容。</p><p>在数据预处理阶段，唯一不同的就是，当前项目需要显示数据集的时间跨度，因为我们要拆分出头3个月的数据作为输入特征，并且只考虑12月的总消费金额作为LTV，所以我们要通过数据集的时间跨度来把它分为两部分，一部分用来构建RFM特征（头３个月），另一部分用来构建LTV这个标签（整个1２个月）。</p><ol>\n<li><strong>整理数据集记录的时间范围</strong></li>\n</ol><p>通过这段代码，我可以知道当前数据集一共覆盖了多长的时间。</p><pre><code class=\"language-typescript\">import numpy as np #导入NumPy\nimport pandas as pd #导入Pandas\ndf_sales = pd.read_csv('易速鲜花订单记录.csv') #载入数据\nprint('日期范围: %s ~ %s' % (df_sales['消费日期'].min(), df_sales['消费日期'].max())) #显示日期范围（格式转换前）\ndf_sales['消费日期'] = pd.to_datetime(df_sales['消费日期']) #转换日期格式\nprint('日期范围: %s ~ %s' % (df_sales['消费日期'].min(), df_sales['消费日期'].max()))#显示日期范围\n</code></pre><p>输出如下：</p><pre><code class=\"language-typescript\">日期范围（格式转化前）: 1/1/2021 10:11 ~ 9/9/2020 9:20\n日期范围（格式转化后）: 2020-06-01 09:09:00 ~ 2021-06-09 12:31:00\n</code></pre><p>结果显示，数据集中的时间跨度是从2020年6月到2021年6月9号。</p><p>因为我们希望求的是整年的LTV，所以，这里我们把不完整的2021年6月份的数据删除：</p><pre><code class=\"language-typescript\">df_sales = df_sales.loc[df_sales['消费日期'] &lt; '2021-06-01'] #只保留整月数据\nprint('日期范围: %s ~ %s' % (df_sales['消费日期'].min(), df_sales['消费日期'].max())) #显示日期范围\n</code></pre><p>输出如下：</p><pre><code class=\"language-typescript\">日期范围（删除不完整的月份）: 2020-06-01 09:09:00 ~ 2021-05-31 17:39:00\n</code></pre><p>目前的数据集中，共包含了整整12个月的数据。下面，我们开始构建机器学习数据集的特征和标签字段。</p><ol start=\"2\">\n<li><strong>构建特征和标签</strong></li>\n</ol><p>基于前面的分析，我们用前3个月的R、F、M值作为特征字段，然后把整个12个月的消费金额视为LTV，作为标签字段。</p><p>首先，我们把头3个月的销售数据拆分出来，形成独立的df_sales_3m对象。这部分数据将是对用户LTV预测的依据。</p><pre><code class=\"language-typescript\">df_sales_3m = df_sales[(df_sales.消费日期 &gt; '2020-06-01') &amp; (df_sales.消费日期 &lt;= '2020-08-30')] #构建仅含头三个月数据的数据集\ndf_sales_3m.reset_index(drop=True) #重置索引\n</code></pre><p>接下来，我们创建以用户码为主键的df_user_LTV对象，利用头3个月的数据，构建R、F、M层级，形成新特征。具体的思路和步骤，我们在<a href=\"https://time.geekbang.org/column/article/415910\">第5讲</a>中讲解过，我就不啰嗦了：</p><pre><code class=\"language-typescript\">df_user_LTV = pd.DataFrame(df_sales['用户码'].unique()) #生成以用户码为主键的结构\ndf_user_LTV.columns = ['用户码'] #设定字段名\ndf_user_LTV.head() #显示头几行数据\ndf_R_value = df_sales_3m.groupby('用户码').消费日期.max().reset_index() #找到每个用户的最近消费日期，构建df_R_value对象\ndf_R_value.columns = ['用户码','最近购买日期'] #设定字段名\ndf_R_value['R值'] = (df_R_value['最近购买日期'].max() - df_R_value['最近购买日期']).dt.days #计算最新日期与上次消费日期的天数\ndf_user_LTV = pd.merge(df_user_LTV, df_R_value[['用户码','R值']], on='用户码') #把上次消费距最新日期的天数（R值）合并至df_user结构\ndf_F_value = df_sales_3m.groupby('用户码').消费日期.count().reset_index() #计算每个用户消费次数，构建df_F_value对象\ndf_F_value.columns = ['用户码','F值'] #设定字段名\ndf_user_LTV = pd.merge(df_user_LTV, df_F_value[['用户码','F值']], on='用户码') #把消费频率(F值)整合至df_user结构\ndf_M_value = df_sales_3m.groupby('用户码').总价.sum().reset_index() #计算每个用户三个月消费总额，构建df_M_value对象\ndf_M_value.columns = ['用户码','M值'] #设定字段名\ndf_user_LTV = pd.merge(df_user_LTV, df_M_value, on='用户码') #把消费总额整合至df_user结构\ndf_user_LTV #显示用户表结构\n</code></pre><p>最后，我们输出显示df_user_LTV对象，就会看到头三个月的R值、F值、M值都已经作为特征，存到我们的数据集df_user_LTV中了。到这里，特征构建完毕。</p><p><img src=\"https://static001.geekbang.org/resource/image/43/y2/430c2cdae13627cf5be68674cc1d4yy2.png?wh=211x167\" alt=\"\"></p><p>下面我们再来看怎么构建数据集的标签。</p><p>我们说过，标签，就是我们需要去预测或者判断的东西。而机器学习，就是通过已知来预测未知，通过训练数据集来寻找规律，发现特征和标签之间的联系。所以，我们下一步要做的，就是把LTV值加入到df_user_LTV中，这样数据集才完整。</p><p>我们先根据一整年的数据计算出每一个用户的LTV值，也就是12个月的总消费金额：</p><pre><code class=\"language-typescript\">df_user_1y = df_sales.groupby('用户码')['总价'].sum().reset_index() #计算每个用户整年消费总额，构建df_user_1y对象\ndf_user_1y.columns = ['用户码','年度LTV'] #设定字段名\ndf_user_1y.head() #显示头几行数据\ndf_LTV = pd.merge(df_user_LTV, df_user_1y, on='用户码', how='left') #构建整体LTV训练数据集\ndf_LTV #显示df_LTV\n</code></pre><p>然后，再把得到的LTV值整合到之前构建的df_user_LTV中，就形成了完整的、带标签的LTV数据集。</p><p><img src=\"https://static001.geekbang.org/resource/image/93/ea/93e42f2785abde4cdfbac43a48a088ea.png?wh=280x351\" alt=\"\"></p><p>现在，在这个数据集中，R、F、M值来自于头3个月收集的数据，是模型的特征；LTV值来自于整年的数据，是模型的标签。这非常符合我们的目标：<strong>用短期数据，来预测用户的长期价值</strong>。</p><p>数据集形成之后，你会发现用户的数量从原来的981个减少到了361个，这是因为在头三个月出现过消费行为的用户数就只有361个。所以，我们后续基于这361个用户的数据来开展机器学习建模就可以了。</p><ol start=\"3\">\n<li><strong>创建特征集和标签集</strong></li>\n</ol><p>刚刚我们把特征和标签整合在一起，是为了形成完整的数据集。不过，标签集和特征集要分别输入机器学习模型，所以要分别创建。</p><p>我们先来构建特征集X：</p><pre><code class=\"language-typescript\">X = df_LTV.drop(['用户码','年度LTV'],axis=1) #特征集\nX.head() #显示特征集\n</code></pre><p>在这段代码中，我们除了移除了LTV值之外，还移除了用户码字段。因为用户码对于回归模型的训练毫无意义，而且用户码也是数字，会对模型形成干扰。如果不移除的话，机器就会把它也视作一个变量，认为15291比15100大，这显然不合逻辑。<br>\n然后，我们再来构建标签集y。这里多说一句，在机器学习中，特征集的X大写，标签集的y小写，似乎是个惯例。这可能是因为通常情况下，X是一个向量，而y是一个数值。</p><pre><code class=\"language-typescript\">y = df_LTV['年度LTV'] #标签集\ny.head() #显示标签集\n</code></pre><p>构建好特征集和标签集后，我们就可以把它们拆分为训练集、验证集和测试集了。</p><ol start=\"4\">\n<li><strong>拆分训练集、验证集和测试集</strong></li>\n</ol><p>我们用scikit-learn工具包中的拆分工具train_test_split，进行拆分：</p><pre><code class=\"language-typescript\">from sklearn.model_selection import train_test_split\n# 先拆分训练集和其它集\nX_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.7,random_state = 36)\n# 再把其它集拆分成验证集和测试集&nbsp;\nX_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5,random_state = 36)\n</code></pre><p>请你注意，这里我做了两重的拆分，至于为什么要这样做，你可以回顾下<a href=\"https://time.geekbang.org/column/article/414504\">第三讲</a>的内容。</p><p>最后，我们得到的数据集X_train、X_valid和X_test的字段，与X中的字段还是一样，y_train、y_valid、y_test中的字段和y的也一样，只是它们的行数发生了改变：</p><ul>\n<li>X_train是288行×4列；</li>\n<li>y_train是288行×1列；</li>\n<li>X_valid是73行×4列；</li>\n<li>y_valid是73行×1列；</li>\n<li>X_test是73行×4列；</li>\n<li>y_test是73行×1列。</li>\n</ul><p>好，到这里，我们的数据准备工作就全部完成啦。在这个项目中，这部分工作几乎占了大头，好在我们已经攻克，下面我们一起进入选算法并创建模型的环节。</p><h1>选择算法创建模型</h1><p>因为这是一个回归问题，所以，在模型类型的选择方面，我们肯定使用的是回归算法。这是基于问题本身的性质而确定的，毋庸置疑。</p><p>不过我们说过，在机器学习中，能够解决回归问题的常见算法有不少：</p><p><img src=\"https://static001.geekbang.org/resource/image/84/34/844348a55550d08968ffb1d0dcaf3a34.jpg?wh=2248x1265\" alt=\"\"></p><p>一般来说，我们在解决具体问题的时候，会选择多种算法进行建模，相互比较之后，再确定比较适合的模型。由于篇幅所限，我们不会使用上述全部算法建立模型，这里我会带你比较3种算法的效率：最基本的线性回归模型、决策树模型和随机森林模型（你可以自己试着使用其它的算法创建别的模型）。</p><p>线性回归我们已经用过了，它是通过梯度下降找到最优的权重和模型偏置的最基本的回归算法。这里，我会用它做为一个基准模型，把其它模型的结果与其相比较，来确定优劣。</p><p>而决策树算法，简单地说是从样本数据的特征属性中，通过学习简单的决策规则，也就是我们耳熟能详的IF ELSE规则，来预测目标变量的值。这个算法的核心是划分点的选择和输出值的确定。</p><p>下面，我给你画了一张图，来帮你理解决策树是怎么进行判断预测的。</p><p><img src=\"https://static001.geekbang.org/resource/image/be/db/be2728ac9936a4c2dd1692c227dfffdb.jpg?wh=2248x2885\" alt=\"\"></p><p>你可以看到，这种算法是根据两个特征$x_{1}$和$x_{2}$的值，以及标签y的取值，来对二维平面上的区域进行精准分割，以确定从特征到标签的映射规则。根据树的深度和分叉时所选择的特征的不同，我们可以训练出很多棵不一样的树来。</p><p>而随机森林呢，就是由多棵决策树构成的集成学习算法。它既能用于分类问题，也能用于回归问题。而且无论是解决哪类问题，它都是相对优秀的算法。在训练模型的过程中，随机森林会构建多个决策树，如果解决的是分类问题，那么它的输出类别是由个别树输出的类别的众数而定；如果解决的是回归问题，那么它会对多棵树的预测结果进行平均。</p><p>关于集成学习，我后面还会单独拿出来给你讲解。现在你只需要知道，随机森林纠正了决策树过度拟合其训练集的问题，在很多情况下它都能有不错的表现。这里的“过拟合”，其实就是说模型对训练集的模拟过头了，反而不太适合验证集和测试集。</p><p>下面我们导入并创建线性回归模型、决策树模型和随机森林模型。</p><pre><code class=\"language-typescript\">from sklearn.linear_model import LinearRegression #导入线性回归模型\nfrom sklearn.tree import DecisionTreeRegressor #导入决策树回归模型\nfrom sklearn.ensemble import RandomForestRegressor #导入随机森林回归模型\nmodel_lr = LinearRegression() #创建线性回归模型\nmodel_dtr = DecisionTreeRegressor() #创建决策树回归模型\nmodel_rfr = RandomForestRegressor() #创建随机森林回归模型\n</code></pre><p>在代码中，有几个缩写我解释一下：</p><ul>\n<li>lr是Linear Regression（线性回归）的缩写；</li>\n<li>dtr是Decision Tree Regresssor（决策树回归）的缩写；</li>\n<li>rfr是Random Forest Regressor（随机森林回归）的缩写。</li>\n</ul><p>对于决策树和随机森林算法来说，它们既有回归算法（Regressor），也有分类算法（Classifer）。以后，我们用到分类模型的时候，我就会把决策树分类模型命名为model_dtc，把随机森林分类模型命名为model_rfc，其中的“c”就代表Classifer。</p><p>创建好模型之后，我们就可以开始训练机器了。</p><h1>训练模型</h1><p>我们直接对线性回归、决策树模型和随机森林模型进行训练、拟合：</p><pre><code class=\"language-typescript\">model_lr.fit(X_train, y_train) #拟合线性回归模型\nmodel_dtr.fit(X_train, y_train) #拟合决策树模型\nmodel_rfr.fit(X_train, y_train) #拟合随机森林模型\n</code></pre><p>你不要小看上面这几个简单的<strong>fit语句，这是模型进行自我学习的关键过程</strong>。我们前面说了，在线性回归算法中，机器是通过梯度下降，逐步减少数据集拟合过程中的损失，让线性函数对特征到标签的模拟越来越贴切。而在决策树模型中，算法是通过根据特征值选择划分点来确定输出值的；在随机森林算法中，机器则是生成多棵决策树，并通过Bagging的方法得到最终的预测模型。</p><p>不过，拟合之后的模型是否有效，我们还无法确定，需要进行验证集上的预测并验证预测结果。</p><h1>评估模型</h1><p>下面我们用这三种模型对验证集分别进行预测。</p><pre><code class=\"language-typescript\">y_valid_preds_lr = model_lr.predict(X_valid) #用线性回归模型预测验证集\ny_valid_preds_dtr = model_dtr.predict(X_valid) #用决策树模型预测验证集\ny_valid_preds_rfr = model_rfr.predict(X_valid) #用随机森林模型预测验证集\n</code></pre><p>为了看看这些模型预测的LTV值是否大体上靠谱，我们先来随机选择其中一行数据，看看模型的预测结果。</p><pre><code class=\"language-typescript\">X_valid.iloc[2] #随便选择一个数据\n</code></pre><p>这行数据的特征如下：</p><pre><code class=\"language-typescript\">R值       1.00\nF值     153.00\nM值    1413.83\nName: 163, dtype: float64\n</code></pre><p>然后，我们再显示一下三个模型对这一行数据所预测的LTV值，以及该用户的LTV真值。</p><pre><code class=\"language-typescript\">print('真值:', y_valid.iloc[2])&nbsp; #真值\nprint('线性回归预测值:', y_valid_preds_lr[2])&nbsp; #线性回归模型预测值\nprint('决策树预测值:', y_valid_preds_dtr[2])&nbsp; #决策树模型预测值\nprint('随机森林预测值:', y_valid_preds_rfr[2]) #随机森林模型预测值\n</code></pre><p>输出：</p><pre><code class=\"language-typescript\">真值: 4391.9399999999905\n线性回归预测值: 7549.22894678151\n决策树预测值: 4243.209999999997\n随机森林预测值: 4704.671799999999\n</code></pre><p>可以看到，相对而言，对这个数据点来说，决策树和随机森林所预测的y值更接近真值。</p><p>当然，一个数据点接近真值完全不能说明问题，我们还是要用$R^2$、MSE等评估指标在验证集上做整体的评估，比较模型的优劣。</p><p>下面我们用$R^2$指标，来评估模型的预测准确率：</p><pre><code class=\"language-typescript\">from sklearn.metrics import r2_score,   median_absolute_error #导入Sklearn评估模块\nprint('验证集上的R平方分数-线性回归: %0.4f' % r2_score(y_valid, model_lr.predict(X_valid)))\nprint('验证集上的R平方分数-决策树: %0.4f' % r2_score(y_valid, model_dtr.predict(X_valid)))\nprint('验证集上的R平方分数-随机森林: %0.4f' % r2_score(y_valid, model_rfr.predict(X_valid)))\n</code></pre><p>输出如下：</p><pre><code class=\"language-plain\">验证集上的R平方分数-线性回归: 0.4333\n验证集上的R平方分数-决策树: 0.3093\n验证集上的R平方分数-随机森林: 0.4677\n</code></pre><p>我们把这个结果用图表来显示一下，会更加直观：</p><p><img src=\"https://static001.geekbang.org/resource/image/47/69/477cb3f996975aa3475c6397bab40569.png?wh=384x267\" alt=\"\"></p><p>我们可以看到，在都没有经过任何参数设定的情况下，和线性回归、决策树相比，随机森林算法显示出了更好的预测能力。</p><p>最后，我们在随机森林上面运行测试集，并绘制出预测值和真值之间的散点图：</p><pre><code>y_test_preds_rfr = model_rfr.predict(X_test) #用模型预随机森林模型预测验证集\nplt.scatter(y_test, y_test_preds) #预测值和实际值的散点图\nplt.plot([0, max(y_test)], [0, max(y_test_preds)],   color='gray', lw=1, linestyle='--') #绘图\nplt.xlabel('实际值') #X轴\nplt.ylabel('预测值') #Y轴\nplt.title('实际值 vs. 预测值') #标题\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/ff/96/ffa9daf7d1b7c0f9697c08bb6a1f3696.png?wh=1000x699\" alt=\"\"></p><p>我们希望实际值和预测值基本上是相等的（预测值越接近真值，则误差越小）。举例来说，图中一个全年消费12000元的用户，所预测出来的LTV值，也在12000元左右。这样的情况越多，就表明模型越准确。</p><p>现在有了这个机器学习模型，我们再回过头看一下，在这一讲的开始，老板提出的问题：如何判断获客成本是否过高？根据模型预测结果，我们可以进一步观察处于R、F、M各个层级中的用户，看他们的LTV值大概是多少，这样就不难得知每个层级的获客成本应该控制在什么范围了。</p><p>对于高RFM价值的客户来说，我们可以适当多投入获客成本；而对于低RFM价值的客户，我们就要严格控制获客成本了。所以，根据这个模型，我们可以得出一个<strong>获客成本的指导区间</strong>。而且，通过该模型，我们<strong>还可以便捷地计算出每个新用户的LTV值。</strong></p><h1>总结一下</h1><p>好，今天这一讲到这里就结束了，我们来回顾一下你在这节课中学到了什么。</p><p>在这一讲中，我们应用机器学习的实战5步，解决了一个回归问题。在这一过程中，最重要的部分是构建特征，也就是把原始数据，转化成R值、F值和M值，来作为新特征进行机器学习。而<strong>这个过程本身就是一个很有意思的特征工程</strong>。</p><p><img src=\"https://static001.geekbang.org/resource/image/77/9f/7780d2ec6af77dd8f482d9551e65d49f.png?wh=616x424\" alt=\"\"></p><p>在模型选择的方面，我们使用普通的线性回归算法作为基准模型。然后，再拿其它的算法（这里我们选择的是决策树和随机森林）与之比较，从而找出更优的算法。请你注意，这里所谓的更优，仅针对于当前的场景而言，并不是说随机森林算法就一定优于线性回归算法。</p><p>当然，一般来说，随机森林简单且容易解释。如果对于任何一个特定问题，你能找到比随机森林还好的算法，那么就可以说是相当成功了。</p><p>在这次实战中，我们只是简单地调用模型，还并没有进行任何的参数优化步骤。以后，我们还会对随机森林算法做调优的工作。</p><h1>思考题</h1><p>这节课就到这里了，最后，我给你留3个思考题：</p><ol>\n<li>在这次实战中，我们放弃了用户所在的“城市”这个信息，请你使用Pandas中的get_dummies这个工具，来添加“城市”相关的哑变量，然后添加到特征集中，输入模型。</li>\n</ol><p><strong>提示</strong>：</p><pre><code class=\"language-typescript\">city = pd.get_dummies(df_sales.城市, prefix='城市')\ndf_sales = pd.concat([df_sales, city], axis=1)\n</code></pre><ol start=\"2\">\n<li>其实，SVM和朴素贝叶斯也可以解决回归问题，请你使用这两种算法（或其它回归算法）来尝试解决这个问题，然后比较各个算法的优劣。</li>\n</ol><p><strong>提示</strong>：</p><pre><code class=\"language-plain\">from sklearn.svm import SVR\nfrom sklearn.linear_model import BayesianRidge\n</code></pre><ol start=\"3\">\n<li>在验证时，我选择了$R^2$作为回归问题的评估指标，你能否尝试使用均方误差、中值绝对误差等评估指标，来验证我们的模型呢？</li>\n</ol><p><strong>提示</strong>：除了$R^2$是越大越好之外，其它评估指标都是越小越好。</p><pre><code class=\"language-plain\">from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import median_absolute_error\n</code></pre><p>欢迎你在留言区分享你的想法和收获，我在留言区等你。如果这节课帮到了你，也欢迎你把这节课分享给自己的朋友。我们下一讲再见！</p><p><img src=\"https://static001.geekbang.org/resource/image/5a/fa/5af212b016be19f8742bafbdd35b26fa.jpg?wh=2284x1149\" alt=\"\"></p>",
        article_title: "07｜回归分析：怎样用模型预测用户的生命周期价值？",
      },
      {
        title: "08 | 模型优化（上）：怎么用特征工程提高模型效率？",
        id: 418354,
        content:
          '<p>你好，我是黄佳。欢迎来到零基础实战机器学习。</p><p>经历过了前面几个项目实战，你是不是想告诉我说，佳哥，机器学习的流程也很简单，似乎只要选个模型并重复5个步骤，就可以搞定任何数据集。</p><p>看起来是这样，不过也没有这么简单。模型，谁都可以构建，但是，如何让模型的性能更优，才是我们真正的考验。今天，我们就来谈一个与模型优化相关的重要内容，也就是特征工程。</p><p>人们常说，<strong>数据和特征决定了机器学习的上限，而模型和算法只是无限逼近这个上限而已</strong>。请你想一想，在那些给定数据集的机器学习竞赛中，高手们为什么能在数据集相同、模型也类似的前提下，让模型达到一个很高的预测准确率？其实，就是因为他们大都通过漂亮的特征工程，提高了机器学习的上限。</p><p>特征工程说起来很简单，就是指<strong>优化数据集的特征，使机器学习算法更起作用的过程，</strong>但用好特征工程并不容易。对于很多初学者来说，常常感觉特征工程实现起来种类繁多，五花八门，不知道怎么下手。今天这节课我就来带你解决这一难题。</p><p>根据我这么多年的经验和理解，特征工程其实是有章可循的。总结起来，几乎所有的特征工程，都逃不开三个基本的思路：<strong>特征选择，特征变换和特征构建。</strong>只要你掌握了它们，以后每拿到一个新的问题和新的数据集时，都可以从这三个维度去分析。这样，你就不至于无从下手，你的特征工程也不会有大的偏差。</p><!-- [[[read_end]]] --><p>那你肯定很想了解它们都是怎么回事儿，别着急，现在我就跟你一一道来。</p><h1>特征选择</h1><p>其实，在一个数据集中，每个特征在标签预测或分类过程中发挥的作用其实都不同。对于那些没作用和作用小的数据，我们就可以删掉，来降低数据的维度，节省模型拟合时的计算空间。这就是特征选择。</p><p>那么，怎么看哪个特征作用大，哪个作用小？我给你介绍一种常用的方法：相关性热力图。就拿我们上节课的项目来说，在我们预测LTV的数据集中，一共有3个特征，分别是“R值”、“F值”和“M值”，还有1个标签“LTV”。这时候，我们就可以通过相关性热力图来看看，哪些特征和标签的相关性更高。</p><p>具体的实现过程，就是用corr()方法求出数据集中所有字段之间的相关性，然后用seaborn的heatmap方法，将它以热力图的形式呈现出来：</p><pre><code class="language-plain"># 对所有的标签和特征两两显示其相关性热力图(heatmap)\nimport seaborn as sns\nsns.heatmap(df_LTV.corr(), cmap="YlGnBu", annot = True)\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/86/fe/869fe6d5eb03a8b3014dafa8c12b39fe.jpg?wh=2000x1005" alt=""></p><p>那么怎么去看这个相关性热力图呢？</p><p>首先，我们只要聚焦于LTV标签和R、F、M三个特征之间的关系就可以了，用户码这个字段和LTV肯定是不相关的。</p><p>然后我们再来看这张图中方格里的数字，这类数字叫做皮尔逊相关系数，表示两个变量间的线性相关性，数值越接近1，就代表相关性越大。那么，自己和自己的相关性当然就是1了。</p><p>接下来，你需要知道的是，相关性是有正负的。正值说明是正相关，负值说明是负相关，从图中我们看到，R值和年度LTV的相关性为-0.23，就表明这个数据集中新进度（R）数字越大，年度LTV就越小。你可以想一想这是为什么。</p><p>此外，我们还会发现，M值、F值和LTV的相关度比较高，而R值和LTV的相关度比较低。这时候，我们就可以选择丢弃“R值”这个字段。这就是一个特征选择。</p><pre><code class="language-plain">X_train_less_feature = X_train.drop([\'R值\'], axis=1) #特征训练集\nX_valid_less_feature = X_valid.drop([\'R值\'], axis=1) #特征验证集\nmodel_lr_less_feature = LinearRegression() #创建线性回归模型\nmodel_lr_less_feature.fit(X_train_less_feature, y_train) #拟合线性回归模型\nprint(\'测试集上的R平方分数-线性回归: %0.4f\' % r2_score(y_valid, model_lr.predict(X_valid)))\nprint(\'测试集上的R平方分数-少R值特征的线性回归: %0.4f\' % r2_score(y_valid, model_lr_less_feature.predict(X_valid_less_feature)))\n</code></pre><p>输出结果如下：</p><pre><code class="language-typescript">测试集上的R平方分数-线性回归: 0.4410\n测试集上的R平方分数-少R值特征的线性回归: 0.4356\n</code></pre><p>在这段代码中，我们用同样的线性回归模型进行拟合、评估，唯一的区别就是特征减少了一个R值。你会发现，在这种情况下训练出来的模型，得分会低一点点，不过变化并不太显著，这就说明“R值”这个特征对函数最终的性能影响确实不大。但是，如果你丢掉的是F值或M值，那么模型的分数将显著降低。</p><p>尽管如此，我还是要说明一下，在这个项目的模型训练中，我并不建议你进行这样的特征选择。因为原始的特征数目已经很少了，要是再把R值这样的特征丢弃，显然不合逻辑。我刚才做这样的尝试，只是想为你展示特征选择的思路。</p><p>在上面的过程中，我们通过“相关性热力图”，观察了每个特征和标签之间的关系，并手工选择了特征。那么你可能会问，难道没有工具可以帮我们自动进行特征选择吗？当然有。</p><p>在sklearn的feature_selection模块中，有很多自动特征选择工具。这里我给你介绍一个常用的单变量特征选择工具，SelectKBest。SelectKBest的原理和使用都非常简单，它是对每个特征和标签之间进行统计检验，根据X 和 y 之间的相关性统计结果，来选择最好的K个特征，并返回。</p><p>对于我们的LTV预测数据集来说，我们可以这样调用SelectKBest来选择特征。</p><pre><code class="language-typescript">from sklearn.feature_selection import SelectKBest, mutual_info_regression&nbsp; #导入特征选择工具\nselector = SelectKBest(mutual_info_regression, k = 2) #选择最重要的两个特征\nselector.fit(X, y) #用特征选择模型拟合数据集\nX.columns[selector.get_support()] #输出选中的两个特征\n</code></pre><p>输出如下：</p><pre><code class="language-plain">Index([\'F值\', \'M值\'], dtype=\'object\')\n</code></pre><p>在调用SelectKBest的过程中，我们是指定了参数score_func = mutual_info_regression，其中，mutual_info_regression是用于对连续型标签进行特征评分。如果标签是离散型的，那么就是分类问题，我们就要用过mutual_info_classif来评分了。</p><p>从输出的结果来看，F值和M值的分数比R值高。所以，在K=2的情况下，我们就需要在这3个特征中舍弃一个，这时候R值就要被舍弃。这个结果和我们前面观察到的一致。</p><p>总体来说，单变量特征选择比较直接，速度快，并且独立于模型。但是，你有没有想过这种做法可能会有一个问题？那就是，如果一个特征表面上和标签不大相关，但是在与另一个特征组合之后，才会具有信息量，那么上面的做法可能会让你错误地舍弃这个特征。</p><p>那么，有没有一种考虑了各个特征之间的组合后，才做出特征选择的工具呢？有。不过，这样的多变量组合型特征选择工具必须结合具体的模型存在，也就是说，当所有特征都被输入模型进行拟合时，在这个过程中才能评估出最不重要的特征，然后再进行特征选择。</p><p>在sklearn的feature_selection里，有下列几种在拟合特定模型时进行特征选择的工具，你可以选择使用：</p><p><img src="https://static001.geekbang.org/resource/image/2a/72/2ayy82ffecbe26bce60ec753f2463672.jpg?wh=2248x1265" alt=""></p><p>除了这些特征选择工具，我认为“数据降维”也可以归入到特征选择中。当然，也有人把降维视为一种独立的特征工程类型。</p><p>那什么是数据降维呢? 其实就是通过特定算法，把多维特征压缩成低维的特征，也就是通过算法实现特征选择，减少特征的数目。常见的降维算法有两种：主成分分析法（PCA）和线性判别分析（LDA）。PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。出于咱们的课程定位，这些降维算法我们并不会过多涉及。</p><p>另外，还有一种方式，就是用深度学习来进行特征选择，这在计算机视觉领域十分普遍。因为深度学习具有自动学习特征的能力。就拿图片识别来说，当图片被输入到卷积神经网络后，网络会自动形成一条条特征通道，选择提取有用的信息来进行最终的目标模型训练。</p><p>不过，神经网络中的这些特征通道是怎么形成的，我们目前还没有很好的方式用数学手段去推导、确定，它对于我们来说就像一个黑箱。</p><p>总而言之，特征选择的目标是，尽量保持对模型训练有效的信息，与此同时减少数据集的特征数量。</p><p>除了特征选择外，还有一种特征工程是在原有特征的基础上进行加工，没有减少特征数，这就是特征变换。</p><h1>特征变换</h1><p>特征变换的整体目标是让原始特征变得机器学习模型可用，甚至是更好用。</p><p>要搞清楚特征变换，我们就得先了解一下特征的类型。其实，特征和标签一样，也分为连续和离散两类。像分数、点击量这些，就属于连续特征（continuous feature）；而离散特征（discrete feature），也叫类别特征（categorical feature），像男生、女生，或者说北京、上海、深圳、广州等等，这些都属于离散特征。</p><p>不同的特征类型，有不同的特征变换方式。对连续特征来说，我们最常见的特征变换就是特征缩放（feature scaling），也就是改变特征的分布或者压缩特征的区间。因为有很多模型都喜欢范围比较小、分布有规律的数据，比如采用梯度下降方法求解最优化的模型，较小的数值分布区间能够提升收敛速度，还有SVM、KNN、神经网络等模型，都要求对特征进行缩放。</p><h2>对数值型特征的缩放</h2><p>在sklearn中，特征缩放方法很多：</p><p><img src="https://static001.geekbang.org/resource/image/b5/d6/b5083f1d8e253120b8c27bd940597bd6.jpg?wh=2248x1168" alt=""></p><p>下面是各个特征缩放方法从原始特征到新特征的缩放效果图，你可以看看：</p><p><img src="https://static001.geekbang.org/resource/image/55/a6/55a8yy57a5e66f82171b0f53b17d50a6.png?wh=853x468" alt="" title="四种特征缩放器效果图"></p><p>下面，我们选择其中最常见的两个特征缩放工具StandardScaler和MinMaxScaler，来预测用户LTV，比较一下它们的优劣。你要注意，这里所谓的优劣，仅针对于这个特定数据集和特定问题而言。</p><p>由于大部分的代码和实战中介绍过的相同，我这里就不再重复了。下面我只是展示和特征工程相关的代码，你可以在<a href="https://github.com/huangjia2019/geektime/tree/main/%E5%8F%98%E7%8E%B0%E5%85%B308">这里</a>下载到完整的代码。</p><p>首先，我们调用StandardScaler，进行数值特征的标准化缩放，这个缩放将形成正态分布的新特征。</p><pre><code class="language-typescript">from sklearn.preprocessing import StandardScaler #导入标准化缩放器\nscaler = StandardScaler() #创建标准化缩放器\nX_train_standard = scaler.fit_transform(X_train) #拟合并转换训练集数据\nX_valid_standard = scaler.transform(X_valid) #转换验证集数据\nX_test_standard = scaler.transform(X_test) #转换测试集数据\n</code></pre><p>在上面的代码中，你要特别注意，在创建标准化缩放器之后，我们对于训练集使用了fit_transform这个API，这是fit和transform两个API的整合，它的意思是先根据训练集拟合数据，找到合适的标准化参数，然后再把参数应用在训练集上，给数据做缩放。</p><p>而在验证集和测试集上，我们只使用transform这个API，来通过从训练集中得到的参数，来转换数据，这个步骤中就没有fit的过程，这是因为对于模型来说，验证集和测试集是新的看不见的数据，因此在训练之前或训练期间它是不应该被访问的，使用来自验证集和测试集的任何信息都属于数据泄露，会导致评估性能的潜在偏差。</p><p>这也就是为什么特征缩放必须在拆分完训练集和测试集后进行，因为它的参数只能来自训练集的数据。</p><p>然后，我们用缩放后的特征训练模型，得到一个随机森林模型model_rfr_standard。这里的rfr是一个缩写，代表random forest refressor，也就是随机森林回归。</p><pre><code class="language-typescript">model_rfr_standard = RandomForestRegressor() #创建随机森林回归模型\nmodel_rfr_standard.fit(X_train_standard, y_train) #拟合随机森林模型\n</code></pre><p>讲完了StandardScaler，我们再用MinMaxScaler来预测用户LTV。进行数值特征的归一化缩放。所谓“归一化”，就是把特征的值压缩到给定的最小值和最大值，比如0-1之间。如果有负值的话，那就是-1到1之间。</p><p>与标准化的过程相同，在对验证集和测试集进行归一化时，我们也应该按原样，使用先前从训练集中获得的归一化参数，而不是在验证集和测试集上重新计算它们。</p><pre><code class="language-typescript">from sklearn.preprocessing import MinMaxScaler #导入归一化缩放器\nscaler = MinMaxScaler() #创建归一化缩放器\nX_train_minmax = scaler.fit_transform(X_train) #拟合并转换训练集数据\nX_valid_minmax = scaler.transform(X_valid) #转换验证集数据\nX_test_minmax = scaler.transform(X_test) #转换测试集数据\n</code></pre><p>然后，我们用缩放之后的特征训练模型，得到一个随机森林模型model_rfr_minmax。</p><pre><code class="language-typescript">model_rfr_minmax = RandomForestRegressor() #创建随机森林回归模型\nmodel_rfr_minmax.fit(X_train_minmax, y_train) #拟合随机森林模型\n</code></pre><p>那到这里，我们就得到了两个随机森林模型：model_rfr_standard和model_rfr_minmax。我们拿这两个模型，和不使用特征缩放训练出来的模型，整体做一个对比，看看这三种模型在验证集上的分数怎么样。这里我把评估的结果用柱状图来展示了（对应的代码你可以参见这个<a href="https://github.com/huangjia2019/geektime/tree/main/%E5%8F%98%E7%8E%B0%E5%85%B308">链接</a>）：</p><p><img src="https://static001.geekbang.org/resource/image/6a/7d/6aecaa6a4e0fcf968c76c0aaacca617d.png?wh=384x265" alt=""></p><p>可以看到，对于预测用户LTV这个项目来说，采用特征缩放，尤其是MinMaxScaler，模型在验证集上会得到较优的结果。</p><p>你也许会问，数值型特征常见的变换是特征缩放，那么类别型特征呢？其实对于类别型特征来讲，最常见的变换方式是通过虚拟变量（也叫哑编码或者哑变量）把机器不能读取的类别文本，转换成一个个的0、1值。</p><h2>对类别型特征的变换：虚拟变量和独热编码</h2><p>把类别特征转换成机器可以读取的编码，也非常简单。如果数据表中的字段值是“男”、“女”这样的数据，我们就用下面的代码直接转换成0、1值即可。</p><pre><code class="language-typescript">df[\'性别\'].replace(to_replace=\'女\', value=0, inplace=True)\ndf[\'性别\'].replace(to_replace=\'男\', value=1, inplace=True)\n</code></pre><p>对于两个以上的分类，pandas编码有一种非常简单的方法，就是get_dummies函数。get_dummies函数会自动变换所有具有对象类型（比如字符串）的列或所有分类的列，而分类特征的每个可能取值都会被扩展为一个新特征，并且每一个新特征只有0、1两种取值。这个过程就是虚拟变量的生成过程，你看看这张图就会比较清楚了。</p><p><img src="https://static001.geekbang.org/resource/image/23/60/23d727fc9715919d7ebd40678af92d60.png?wh=2808x710" alt="" title="对于自然语言处理中的分词过程，这样的编码转换是必须的"></p><p>下面我们就把上一个项目中的订单数据集里的“城市”转化为虚拟变量。对于这一步，其实我在之前的课程中给你留过作业，并给出了“提示”，这里我就用和提示不大一样的代码，来实现相同的功能。</p><pre><code class="language-typescript"># 把多分类字段转换为二分类虚拟变量&nbsp;\ncategory_features = [\'城市\'] #要转换的特征列表\ndf_sales = pd.get_dummies(df_sales, drop_first=True, columns=category_features) #创建哑变量\ndf_sales #显示数据\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/10/05/10742aa69896ebd349121ae93b50dd05.png?wh=1125x424" alt=""></p><p>可以看到，原来的城市字段，变成了多个二分类字段，比如城市_北京、城市_成都等，并且每个字段只有0、1值，这样的编码格式就可以被机器识别。</p><p>这里呢，我要请你特别注意一下drop_first这个参数。这个参数实际上控制着get_dummies函数返回的结果，值为True返回结果是虚拟变量（Dummy Variable），值为False则返回独热编码（&nbsp;One Hot Encoding）。</p><p>独热编码与虚拟变量非常相似，它们的不同之处在于：在虚拟编码方案中，当特征具有 m 个不同类别标签时，我们将得到&nbsp;m-1&nbsp;个二进制特征，作为基准的特征被完全忽略；而在独热编码方案中，我们将得到&nbsp;m&nbsp;个二进制特征。因此，你仔细观察上面get_dummies的结果，就会发现“城市_上海”这个特征没有被生成。而如果你把drop_first设为False，“城市_上海”就会出现。</p><p>那么，为什么要有这样的区别呢？主要有两个原因。第一个原因是，如果有N个特征，并且前N-1个特征的值是已知的，那么第N个特征的值也就知道了。因此，独热编码有冗余，虚拟变量没有冗余。举个例子，对于用户来说，如果其它所有城市的值都是0，唯独“上海”这个字段的值不知道，我们自然就能推知这个用户是属于上海这个城市，因为这是仅有的可能性了。</p><p>第二个原因是，独热编码的这种冗余会导致共线性问题，也就是自变量之间存在高度相关关系，不总是相互独立的，从而使模型参数估计不准确。不过，这也意味着，独热编码可以直接从“１”值看出所对应的类别，而虚拟变量需要进行推理。因此独热编码比较直观，虚拟变量没有那么直观。</p><p>那我们应该在什么时候使用虚拟变量，在什么时候使用独热编码呢？如果线性模型有截距项，就使用虚拟变量；如果线性模型无截距项，那么使用独热编码。</p><p>此外，在线性模型有截距项的情况下，如果使用正则化，那么也推荐使用独热编码，因为正则化能处理多余的自由度，可以约束参数；如果不使用正则化，那么就使用虚拟变量，这样多余的自由度都被统摄到截距项intercept里去了。关于正则化的进一步说明，我会在下一讲详细介绍。</p><p>上面所说的虚拟变量和独热编码是对类别型特征的处理，不过在某些情况下，我们还需要把数值型特征离散化。</p><h2>对数值型特征的离散化：分桶</h2><p>你可能会奇怪为什么要对特征做离散化处理？这是因为，当特征的数量级跨度过大，而且与标签的关系又非线性的时候，模型可能只对大数量级的特征值敏感，也就是说模型可能会向大特征值的那一侧倾斜。另外，模型的运算速度也会受到影响。</p><p>比如说，一个特征的数据值为[3.5, 2.7, 16.9, 5.5, 98]，这里的“98”明显偏离其他数据，很容易影响到模型的效果。这时候我们就可以把这些数值做个离散化，比如把小于5的值记为“0”，大于5且小于10的值记为“1”，大于10的值记为“2”，那么[3.5, 2.7, 16.9, 5.5, 98]就可以变成0、1、2这样的离散值，也就是[0，0，2，1，2]，这不仅能减弱像“98”这样的异常数据的权重，还能提高模型的运算速度。</p><p>我们在这里用的方式呢，就叫做分桶，分桶也叫分箱，就是指将连续型特征离散化为一系列的离散值。而这里的0、1、2呢，就是我们说的“桶”或者“箱”了。</p><p>对数值进行分桶，我们首先要确定两点：一是桶的数量，二是把什么范围的数据分成一个桶，也就是桶的宽度是多少。在这个过程中，常规的做法有三种：</p><ol>\n<li>\n<p>等距分桶：每个桶的宽度固定，如 0-99，100-199，200-299等。这种方式适合样本分布比较均匀的情况，如果样本分布不均，就容易出现有的桶多、有的桶少的情况。</p>\n</li>\n<li>\n<p>等频分桶：每个桶有一样多的样本，但可能出现数值相差很大的样本放在同一个桶的情况。</p>\n</li>\n<li>\n<p>模型分桶：利用模型找到最佳分桶。比如决策树模型，它本身就具有对连续型特征切分的能力，因此我们可以利用分割点进行特征离散化。再比如聚类模型，可以将连续性特征分成多个类别。在之前的用户画像项目中，我们对R值、F值、M值进行聚类，就是一种模型分桶操作，即把连续的数值切分成这三维度的离散类别。</p>\n</li>\n</ol><p>到这里，我们就了解了两种特征工的基本思路，一种是通过特征选择减少特征的维度，另一种是通过特征变换加工原始特征。你可能会问我，有没有什么方法能根据原始数据创造出新的特征呢？</p><p>自然是有的，这就是第三种特征工程类型：特征构建。</p><h1>特征构建</h1><p>特征构建是整个特征工程领域最具创造力的部分，也是我觉得在数据预处理环节中最有意思的地方。因为它完全没有一定之规，全凭借你的经验、领域知识和创造力。就像我们在获客关的项目实战里，从原始的订单数据集中，我们经过整理和组合原始数据，求出了三个完全不存在于原始数据集中的新值：R值、F值和M值，<strong>这就是一个典型的特征构建</strong>。</p><p>为什么我们知道要构建这三个值？就是因为我们具有运营领域的经验和相关知识，知道R值、F值和M值对用户分组画像是重要的因子，能够输入到聚类模型和回归模型中去。</p><p>所以，要想把原始数据集中的特征工程做好，就要不断积累经验和知识，展开自己的想象力，看看可以构建出什么样有用的新特征，能让机器学习模型学得更好。</p><p>这里我再给你举一个例子。假设我们现在有一个航班的旅客订单信息数据集，记录了几年内每一天航班的旅客订票情况。通过这个数据集，我们要帮航空公司构建一个模型，来预测未来某天的客流量。那么，你能想到什么特征工程方法，有可能提高模型的效率？</p><p>其实，我们可以根据“订单日期”这个字段，再人工添加一个新字段，来<strong>标明每一天的具体航班是在国家公休假日的之前、之中还是之后，或并不靠近公休假日</strong>。这个方法，其实是把与预测客流量这个任务的相关先验知识编码到了特征中，以辅助机器学习算法理解为什么流量会出现可能的波动。</p><p>当然，添加一个新特征并不意味着机器学习算法肯定会用到它，即使模型发现假日信息和客流量没啥关联，那也无伤大雅。但是，如果真的有关联，这个新特征就会提高模型的预测准确率，尤其是当预测未来公休假日附近的客流量时，会更准。</p><p>为了让你进一步理解特征构建的作用，我再用一个例子，通过程序代码和示意图来直观地给你进行展示。这次，我们读入一个简单的数据集“特征构建示意数据集”（你可以在<a href="https://github.com/huangjia2019/geektime/tree/main/%E5%8F%98%E7%8E%B0%E5%85%B308">这里</a>下载数据集），来看看特征X和标签Y之间的关系，然后思索一下怎么拟合二者。</p><pre><code class="language-typescript">import pandas as pd #导入Pandas\nimport numpy as np #导入NumPy\nimport matplotlib.pyplot as plt #导入绘图工具\ndf = pd.read_csv(\'特征构建示意数据集.csv\') #导入数据\nplt.scatter(df.X, df.Y) #X，Y散点图\n</code></pre><p>输出显示，这个数据集中的特征X和标签y呈现一种符合回归模型的趋向。</p><p><img src="https://static001.geekbang.org/resource/image/d5/59/d527c42f8b67ef0ef9136cc818113e59.png?wh=394x260" alt=""></p><p>如果我们用线性回归进行建模，就会得到一个线性回归模型：</p><pre><code class="language-typescript">from sklearn.model_selection import train_test_split #导入train_test_split&nbsp;\ndf_x = df[\'X\'].to_frame() #特征集\ndf_y = df[\'Y\'] #标签集\nX_train, X_test, y_train, y_test = train_test_split(df_x,df_y,test_size=0.3, random_state = 0) #拆分数据集\nfrom sklearn.linear_model import LinearRegression #导入LinearRegression\nmodel = LinearRegression() #创建模型\nmodel.fit(X_train, y_train) #拟合模型\nplt.scatter(df.X, df.Y) #散点图\nplt.plot(X_test, model.predict(X_test),linestyle=\'--\', color = \'red\') #显示拟合曲线\n</code></pre><p>输出这个模型，可以看到这个回归线并不能很好地拟合从X到Y的关系：</p><p><img src="https://static001.geekbang.org/resource/image/9d/34/9d285514e52432fd7b8723d03d3c8234.png?wh=394x260" alt=""></p><p>那怎么办呢？其实，我们从图中可以观察到，X-Y之间的关系其实更接近多项式回归（polynomial regression）中的二项式回归，也就是说，Y是X的二次函数。那么，我们就在X的基础上做个平方，构建出一个新特征。</p><pre><code class="language-typescript">X2_train = X_train.copy() #新特征训练集\nX2_test = X_test.copy() #新特征测试集\nX2_train[\'X2\'] = X2_train[\'X\']**2 #构建新特征X2\nX2_test[\'X2\'] = X2_test[\'X\']**2 #构建新特征X2\nmodel2 = LinearRegression() #创建新模型\nmodel2.fit(X2_train, y_train) #拟合新模型\nplt.scatter(df.X, df.Y) #散点图\nplt.scatter(X_test, model2.predict(X2_test), linestyle=\'--\', color = \'red\') #新拟合函数曲线\n</code></pre><p>这里，我们还是用一样的模型来拟合，并显示原始特征X和预测值之间的关系。这时候我们发现，拟合曲线更趋近于X和Y的真实关系：</p><p><img src="https://static001.geekbang.org/resource/image/3e/yy/3edaa26d8236d26a594d0dc216928cyy.png?wh=394x260" alt=""></p><p>是不是很奇妙呢？希望通过这个小例子，你能理解：在实践中，我们所使用的特征，以及特征与方法之间的匹配，通常都是让机器学习模型能够有良好表现的最重要因素。</p><h1>总结一下</h1><p>好了，这节课到这里就结束了，我给你总结了三类重要的特征工程。其中，特征选择可以减少特征的维度，特征变换则能把原始特征塑造的更容易被机器学习模型使用，特征构建则需要你激发极大的创造力，以原始数据为基础去发现对建模有用的新特征。</p><p><img src="https://static001.geekbang.org/resource/image/8d/b6/8df9c3f583cd31d341f2053349ac75b6.jpg?wh=2248x916" alt=""></p><p>在这一讲中，我特别为你示范了不少精巧的特征工程小示例，所有的代码你都可以在<a href="https://github.com/huangjia2019/geektime/tree/main/%E5%8F%98%E7%8E%B0%E5%85%B308">这里</a>找到。不过师傅领进门，修行在个人，更多特征工程的知识和应用，还有待于你在实践中去继续挖掘。</p><h1>思考题</h1><p>那么，这一讲我给你留三道思考题：</p><p>1.我在前面使用了StandardScaler工具和MinMaxScaler工具做特征缩放，请你用一下别的特征缩放器，看看适不适合我们这个数据集。</p><p>2.Sklearn的OneHotEncoder工具和Pandas的get_dummies类似，也可以把整个数据集的字段或者指定的特征字段转换为虚拟变量。请你试一试这个工具。</p><p><strong>提示</strong>：这个题目较难，涉及数组形状变换。如果遇到麻烦，可以试着上网搜索一下解决方案。</p><pre><code class="language-typescript">from sklearn.preprocessing import OneHotEncoder #导入OneHotEncoder工具\nencoder = OneHotEncoder(sparse=False)\n...\ndf_city = pd.DataFrame(df_city, columns=encoder.get_feature_names())\n...\n</code></pre><p>OneHotEncoder输出的结果：</p><p><img src="https://static001.geekbang.org/resource/image/bd/9f/bdaf8741df246413f036c1ff050fbc9f.png?wh=550x242" alt=""></p><p>3.你在工作中用特征工程构建过新特征吗？有没有比较巧妙的特征工程可以给大家分享呢？如果你还没有做过特征工程，那么请你谈一谈学了这节课之后，你对特征工程有怎样的理解。</p><p>欢迎你在留言区分享你的想法和收获，我在留言区等你。如果这节课帮到了你，也欢迎你把这节课分享给自己的朋友。我们下一讲再见！</p><p><img src="https://static001.geekbang.org/resource/image/4d/14/4d4505f3261a8f69dc4c8b0d44d31514.jpg?wh=2284x1149" alt=""></p>',
        article_title: "08 | 模型优化（上）：怎么用特征工程提高模型效率？",
      },
      {
        title: "09｜模型优化（中）：防止过拟合，模型也不能太精细",
        id: 419218,
        content:
          '<p>你好，我是黄佳。欢迎来到零基础实战机器学习。</p><p>前几天，有个同学学完了<a href="https://time.geekbang.org/column/article/417479?cid=100085501">第7讲</a>关于怎样用回归模型预测用户的LTV后，就问我说，佳哥啊，我跟着你的思路绘制出了各种回归模型在训练集和验证集上的评分，并进行了比较，有个发现吓了我一跳，你看这个决策树模型，为什么在训练集上得了满分1.0，而在验证集上的得分却如此低呢？</p><p><img src="https://static001.geekbang.org/resource/image/89/28/89edb029f0577f197d56bdc09669be28.png?wh=384x249" alt=""></p><p>我一看，果然如此。</p><p>我告诉他，这就是机器学习中十分著名的“过拟合”现象。可以说，我们用机器学习建模的过程，就是和过拟合现象持续作斗争的过程。那么，什么是过拟合？我们怎么防止模型出现过拟合的问题呢？当你学完了这一讲，自然就有了答案。</p><h2>什么是过拟合</h2><p>在我们解释概念前，请你先看一下某相亲平台给出的相亲结果的数据集示意图。</p><p><img src="https://static001.geekbang.org/resource/image/80/06/80029885fd5ca057ce70d5627c679006.jpg?wh=2000x1106" alt=""></p><p>现在，我们假设你的机器学习模型根据这个数据集，给了你一个非常漂亮的函数。这个函数是：<strong>如果你不富也不帅，那么你在这个平台相亲就会失败；如果是其它任何一种情况，那你一定会成功</strong>。</p><p>你有没有感觉到哪里不对？其实，这个模型有一个很严重的问题，就是<strong>它过于准确了，犯了在训练集上过拟合的问题。</strong></p><p>下面这张图片是嘲讽过拟合现象的，很有意思。 最左边是一堆数据点，中间的模型试图在这些杂乱无章的数据点中挖掘出一些规律，而最右边的模型精准地穿越了每个数据点，最后画出了一只猫。</p><!-- [[[read_end]]] --><p><img src="https://static001.geekbang.org/resource/image/dy/e4/dyy20a82c4b8db4f428d1d6588b439e4.png?wh=744x391" alt=""></p><p>最右边这个模型，对于当前训练集的挖掘不可谓不精准，但是请你想一想，用这么精细的模型，去套另外一组数据，不大可能会套出另外一只猫，因此这个模型无法泛化到其它数据，它当然就是一个很失败的猫分类模型。</p><p>而且，你可别以为模型对训练集的预测达到百分之百准确，是一件多难的事。它其实并不难，只要模型足够复杂，训练集上的精度可以想多高就多高。比如我们常见的决策树模型，它对回归问题和分类问题，都能达到100%的预测或分类准确率。因为决策树的原理就是在训练集上持续优化分割边界，直到完全正确为止。所以，就像我们开头在图中看到的那样，它在训练集上$R^2$的分数能达到满分。</p><p>这对我们来说是一个非常失败的模型，因为我们不是要一个复杂的、在训练集上预测完全精准的模型，而是想要<strong>构建一个泛化能力尽可能高的模型</strong>。如果一个模型能够对它没见过且性质相似的数据做出准确预测，那我们就可以认为它具有不错的泛化能力，这个模型就是相对成功的。</p><p>那为什么模型会出现过拟合的状态呢？其实，这是因为机器和我们程序员一样，都有强迫症。模型在针对训练集的优化过程中不找到最小的误差点，它们是不会停的。我们说，过拟合是机器学习模型在训练数据集上，通过拟合函数形成的。在过拟合的形成过程中，一般会经历三个阶段：<strong>欠拟合、平衡点和过拟合。</strong></p><p>在初期的时候，模型对训练集的拟合还未完善，能力不够强， 偏差也比较大。此时的模型就处于欠拟合的状态，把模型应用于测试集数据，效果肯定也不好，所以还需要继续训练。</p><p>随着训练次数增多，以及模型自身的调整优化，模型的拟合能力变得越来越强，就差不多可以用了。此刻，实际上已经出现了欠拟合和过拟合之间的平衡点。</p><p>然而，如果训练继续进行，模型经过充分训练后，就会完全拟合训练集数据。如果把此时的模型应用于测试集，我们并不一定能得到好的效果，很容易出现很高的误差。这就是过拟合的状态。我们开头提到的决策树模型就明显处于这种状态。</p><p>这里我说明一下，对于过拟合的讨论，我们多限于监督学习的应用范围，也就是回归和分类两大类问题。当然，也有人认为无监督学习中也存在过拟合现象，但是无监督学习中的过拟合被讨论的不多。所以，我们重点来看看回归和分类问题中的过拟合。</p><p>对于回归问题来说，你可以通过下面的图理解过拟合：</p><p><img src="https://static001.geekbang.org/resource/image/55/37/557e4ff908038d3d43e5653903e1ee37.png?wh=865x273" alt=""></p><p>对于分类问题来说，你可以通过下面的图理解过拟合：</p><p><img src="https://static001.geekbang.org/resource/image/fe/7b/fea2f60375fdabdb10070d0a21a6d57b.png?wh=922x328" alt=""></p><p>那么，解决过拟合问题的方法都有哪些呢？这里有三个思路：</p><ol>\n<li>\n<p>一般来说，数据点少，模型很容易就能完全描绘出从特征到标签的映射。所以，增加数据集中数据的数量可以在一定程度上防止过拟合。</p>\n</li>\n<li>\n<p>如果特征工程做得不好，数据集中的无用信息多，就会放大过拟合的影响，因为模型描绘出来的是无用信息和标签之间关系，当然泛化能力就受影响。所以，进行优质的特征工程，筛除无用的特征，去芜取精，去伪存真，有利于避免过拟合。</p>\n</li>\n<li>\n<p>我们前面提到，模型比较复杂时，它在训练集上的精度可以想多高就多高，这是因为从数学的角度，多次函数的图像远远比一次、二次函数复杂。模型越复杂，所能够覆盖的特征空间就越大。所以，如果数据集较小，或者说问题比较简单，我们尽量选择简单的模型。凡事选择简单的解决方案，这个道理人称<strong>奥卡姆剃刀定理</strong>。</p>\n</li>\n</ol><p>因为数据集很难收集，特征工程又很考验机器学习工程师的水平，所以，防止过拟合的最佳策略就是要在模型的简单和复杂之间寻找一个平衡点。</p><p>不知道你还记不记得，在我们<a href="https://time.geekbang.org/column/article/417479?cid=100085501">第7讲</a>中用过的3个模型：决策树、线性回归和随机森林。相对而言决策树和线性回归模型比较容易出现过拟合问题，而随机森林则本身就可以基于决策树的过拟合问题实现优化，因此，下面我会带你看看怎么把决策树模型和线性回归模型简单化，来帮你进一步理解如何防止过拟合。</p><h2>决策树模型的构造和剪枝</h2><p>决策树这个算法的原理很好理解，它就是将一大堆的if…else 语句进行连接，直到最后得到想要的结果。算法中的各个节点是根据训练数据集中的特征形成的。在对特征节点的选择不同时，就可以生成很多不一样的决策树。</p><p>下图所示是一个相亲数据集和根据该数据集而形成的决策树。此处我们随机设定一个根节点——身高，作为我们决策的起点，从该点出发，根据数据集中的特征和标签值给树分叉。</p><p><img src="https://static001.geekbang.org/resource/image/6f/0d/6ffc4542e07df7b2483973facf6ca00d.jpg?wh=2000x1244" alt=""></p><p><img src="https://static001.geekbang.org/resource/image/a8/01/a81ee237665c0e86ffc74171ef218901.jpg?wh=2000x951" alt=""></p><p>生成一个决策树有两个阶段，分别是构造和剪枝。</p><p>那么什么是构造呢？构造就是选择什么特征作为节点生成决策树的过程。在构造过程中，有三种节点：</p><ul>\n<li><strong>根节点</strong>：就是树的最顶端节点。在上图中，“高不高”就是根节点；</li>\n<li><strong>内部节点</strong>：就是树中间的那些节点，比如说“富不富”、“美不美”；</li>\n<li><strong>叶节点</strong>：就是树最底部的节点，也就是决策结果“同意”或者“不同意”。</li>\n</ul><p>在这些节点之间存在一种父子关系。比如根节点会有子节点，子节点还有子节点，到了叶节点就停止了，不再有子节点。在构造过程中，我们可以根据信息增益的程度来选择哪个属性作为根节点；哪些属性作为子节点；什么时候停止并得到目标状态，也就是叶节点。</p><p>那么什么是信息增益呢，就是数据集的熵和选定某个特征的条件熵之差，而熵就是对随机变量不确定性的度量，熵越大不确定性越大。我们应该选择熵比较小的特征作为根节点，来增加信息的增益。</p><p>如果我们在构造的过程中，选择不同的特征作为根节点，然后根节点下面选择不同的特征形成内部节点，就可以得到另外一棵决策树，也就是一个新的模型：</p><p><img src="https://static001.geekbang.org/resource/image/ef/27/ef65edb62c32a6a91fa82d90612eb927.jpg?wh=2000x951" alt=""></p><p>由于if…else 可以无限制地写下去，对于任何训练集，只要树的深度足够，决策树肯定能够达到100% 的准确率。我们在前面讲过，这并不是一件好事，因为这样的模型认为其它所有数据的特点都和训练集中的数据完全一样的，所以它的泛化能力会很差。</p><p>对于这种情况，我们要在构造过程中对决策树进行剪枝，让树的深度不要太深，以免决策树太过于和精确。这样“模糊”一些的决策树，虽然在训练集上面的分数会降低，但是能够得到更强的泛化能力。</p><p>所以，针对具体数据集为模型选择适合的深度非常重要。下图所展示的就是一棵经过剪枝的决策树，剪枝之后树的深度从原来的3层变成了2层。</p><p><img src="https://static001.geekbang.org/resource/image/7d/73/7dd4786141dd59590f51fc595d690973.jpg?wh=2000x951" alt=""></p><p>那上面这个数据集实际上是在用决策树解决分类问题，用的是决策树分类模型。我们再来看看下面这个决策树回归模型，对某个数据集中数据点的拟合示意图，相信你更能明白为什么我说树太深，就会对训练数据集拟合的太准，随之而来的也就会出现过拟合的现象了。</p><p><img src="https://static001.geekbang.org/resource/image/4a/d7/4aed06404b4ccf8b5469a5d008fb50d7.png?wh=640x480" alt=""></p><p>在这张图中，绿色线是当决策树深度为5是的模型拟合状态，蓝色线是决策树深度为2时的模型拟合状态。很明显，经过深度为5的模型训练集损失小，但深度为2的模型，泛化能力会更好。</p><p>其实，决策树的深度是一个可调的超参数，也就是max_depth。这个超参数能限制树的最大深度，把超过设定深度的树枝全部剪掉，这也是最常见的剪枝策略。</p><p>那现在，我们就针对<a href="https://time.geekbang.org/column/article/417479?cid=100085501">第7讲</a>中LTV预测的问题，在这里创建两棵不同决策树模型：一棵是默认的决策树，这个决策树的深度不加限定，将一直分叉直到完全的拟合所有数据点；另一棵是深度为3的决策树。我们分别拟合两棵树，来比较一下它们在训练集和测试集上的分数。示例代码可以在<a href="https://github.com/huangjia2019/geektime/tree/main/%E5%8F%98%E7%8E%B0%E5%85%B309">这里</a>下载。</p><pre><code>from sklearn.tree import DecisionTreeRegressor #导入决策树回归模型\nmodel_dtr = DecisionTreeRegressor() #创建决策树回归模型\nmodel_dtr_cut = DecisionTreeRegressor(max_depth=3) #创建深度为3的决策树回归模型\nmodel_dtr.fit(X_train, y_train) #拟合决策树模型\nmodel_dtr_cut.fit(X_train, y_train) #拟合深度为3的决策树模型\ny_valid_preds_dtr = model_dtr.predict(X_valid) #用决策树模型预测验证集\ny_valid_preds_dtr_cut = model_dtr_cut.predict(X_valid) #用深度为2的决策树模型预测验证集\nfrom sklearn.metrics import r2_score,   median_absolute_error #导入Sklearn评估模块\nprint(\'训练集上的R平方分数-决策树: %0.4f\' % r2_score(y_train, model_dtr.predict(X_train)))\nprint(\'训练集上的R平方分数-深度为3的决策树: %0.4f\' % r2_score(y_train, model_dtr_cut.predict(X_train)))\nprint(\'测试集上的R平方分数-决策树: %0.4f\' % r2_score(y_valid, model_dtr.predict(X_valid)))\nprint(\'测试集上的R平方分数-深度为3的决策树: %0.4f\' % r2_score(y_valid, model_dtr_cut.predict(X_valid)))\n</code></pre><p>经过训练，并在训练集和测试集上进行评估之后，我们得到$R^2$分数输出如下：</p><pre><code>训练集上的R平方分数-决策树: 1.0000\n训练集上的R平方分数-深度为3的决策树: 0.8045\n测试集上的R平方分数-决策树: 0.2857\n测试集上的R平方分数-深度为3的决策树: 0.4870\n</code></pre><p>我们做成直方图，对比更清晰：</p><p><img src="https://static001.geekbang.org/resource/image/32/0c/32ed53c5538e629f5f66c7a90ddb560c.png?wh=384x262" alt=""></p><p>可以看到，未经剪枝的决策树在训练集上的分数是满分，但是在验证集上的得分低得十分离谱。而设定了深度的决策树（也就是剪枝），它的训练集上的分数有所下降，可在验证集上的分数是大幅提升的，体现出了更好的泛化能力。所以，我们这次的“剪枝”相当成功！</p><p>讲完了决策树的剪枝，我们再来看看怎么防止线性回归模型的过拟合现象。</p><h2>线性回归模型的正则化</h2><p>如果你回忆一下我们在<a href="https://time.geekbang.org/column/article/415149">第4讲</a>中讲过的线性回归的原理和公式，就会记起，线性回归模型，其实就是通过梯度下降确定参数的过程。如果数据集中有3个特征，公式就是：</p><p>$$y = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b$$</p><p>那么线性回归模型中的过拟合是如何产生的呢？如果在$x_{1}$、$x_{2}$、$x_{3}$这些特征中，有某个特征的值域较大，而如果模型拟合的结果中，其权重参数 w 的值也比较大，那么这个特征就会占据“主导”地位，使模型往这些较大值的位置偏移，形成了对这些值的“过拟合”。</p><p>那么，如果我们能让这类特征项的权重参数变小，也许就可以得到更为平衡的模型，来防止过拟合现象的出现。这种在一定程度上减小这些参数的值的方法，就是机器学习中的正则化（regularization）。具体来说，在损失函数当中加入的正则项也叫惩罚项，也就是给需要训练的函数加上一些规矩、一些限制，让它们不要自我膨胀。</p><p>线性回归的损失函数的正则项有两种添加方法，分别叫做L1正则项和L2正则项。</p><p>添加L1正则项之后，一些特征的权重会变小，一些绝对值较小的系数甚至直接变为0（相当于抛弃了一些特征），来增强模型的泛化能力。这种回归也叫Lasso回归。</p><p>添加L2正则项之后，模型在不抛弃任何一个特征的情况下，会缩小回归系数，也就是某些特征的权重，让模型相对稳定，通常模型的泛化能力也更好。这种回归也叫Rigde回归。</p><p>下面，我们就使用Sklearn包中的Lasso回归和Ridge回归模型，比较一下L1和L2正则化方法的是否会给我们的LTV预测带来更好的结果。创建模型、训练模型并评估模型的代码如下：</p><pre><code>from sklearn.linear_model import LinearRegression #导入线性回归模型\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nmodel_lr = LinearRegression() #创建线性回归模型\nmodel_lasso = Lasso() #创建Lasso回归模型\nmodel_ridge = Ridge() #创建Ridge回归模型\nmodel_lr.fit(X_train, y_train) #拟合线性回归模型\nmodel_lasso.fit(X_train, y_train) #拟合Lasso回归模型\nmodel_ridge.fit(X_train, y_train) #拟合Ridge回归模型\nfrom sklearn.metrics import r2_score,   median_absolute_error #导入Sklearn评估模块\nprint(\'训练集上的R平方分数-线性回归: %0.4f\' % r2_score(y_train, model_lr.predict(X_train)))\nprint(\'训练集上的R平方分数-Lasso回归: %0.4f\' % r2_score(y_train, model_lasso.predict(X_train)))\nprint(\'训练集上的R平方分数-Ridge回归: %0.4f\' % r2_score(y_train, model_ridge.predict(X_train)))\nprint(\'测试集上的R平方分数-线性回归: %0.4f\' % r2_score(y_valid, model_lr.predict(X_valid)))\nprint(\'测试集上的R平方分数-Lasso回归: %0.4f\' % r2_score(y_valid, model_lasso.predict(X_valid)))\nprint(\'测试集上的R平方分数-Ridge回归: %0.4f\' % r2_score(y_valid, model_ridge.predict(X_valid)))\n</code></pre><p>分数输出如下：</p><pre><code>训练集上的R平方分数-线性回归: 0.6732\n训练集上的R平方分数-Lasso回归: 0.6731\n训练集上的R平方分数-Ridge回归: 0.6354\n测试集上的R平方分数-线性回归: 0.4719\n测试集上的R平方分数-Lasso回归: 0.4737\n测试集上的R平方分数-Ridge回归: 0.4992\n</code></pre><p>还是一样，我们用表格来呈现输出结果：</p><p><img src="https://static001.geekbang.org/resource/image/d4/cd/d4984e208bdd278f3d23bc5d449e15cd.png?wh=384x264" alt=""></p><p>你会发现，从普通线性回归到Lasso回归，再到Ridge回归，训练集分数呈现下降的趋势，这就是防止过拟合的惩罚项发挥了作用；在测试集上，分数则正好相反，从普通线性回归和Lasso回归，再到Ridge回归，反而呈现略微升高的趋势。不过，整体来说，差异似乎不是很大，说明目前这个线性回归模型的过拟合现象其实并不明显。</p><p>好啦，线性回归正则化的内容到这里就讲完了。不过，我要提醒你一点，正则化可以应用于诸多模型中，比如以后我们要用到的逻辑回归中的C值，也是一个正则化相关的参数。</p><p>其实，我们前面对决策树进行剪枝，避免过拟合，也可以称得上是一种正则化，因为剪枝过程也是对模型进行约束，不让树任意生长。如果多棵经过剪枝的决策树集合而成随机森林模型，那么这个模型其实也是一种正则化了的模型。</p><h2>总结一下</h2><p>好，今天这一讲到这里就结束了。我们学到了机器学习中非常重要的一个概念，就是“过拟合”。过拟合就是指模型过度地适应了训练集，反而影响其泛化到测试集上的能力。理解过拟合很重要，可以说，我们在机器学习性能调优的过程中，就一直在和过拟合现象做斗争。</p><p>那要防止过拟合现象，也有多种方法，包括增加数据量、特征工程以及选择更简单的模型等。不过，因为数据收集很难，特征工程又很考验机器学习工程师的水平，所以，防止过拟合的最佳策略就是要通过正则化，让模型变得尽量简单。</p><p>此外，我还给你讲了两个具体的示例，第一个例子是对决策树模型进行剪枝，我们通过剪枝可以控制决策树生成时的深度，防止出现对于训练集数据点切分过于精准，导致模型无法泛化到训练集的问题；另外，我们还对线性回归模型进行了正则化，我们选择使用了L1和L2正则项的Lasso回归和Ridge回归，这两个模型通过引入惩罚项减少某些特征的权重值，从而避免过拟合，提升模型的性能。</p><h2>思考题</h2><p>这节课就到这里了，最后，我给你留两个思考题：</p><ol>\n<li>在决策树的剪枝过程中，除了max_depth之外，还有min_samples_leaf、min_samples_split等超参数也可用于“剪枝”。其中，min_samples_leaf限定一个结点在分支后，每个子结点都必须包含至少N个训练样本，这个参数搭配max_depth使用，在回归树中会让模型变得更加平滑。而min_samples_split则用来限定一个结点必须要包含至少N个训练样本。那么请你告诉我，当min_samples_leaf和min_samples_split的值设置的太小，可能会发生什么；如果值设的太大，又可能会发生什么？</li>\n</ol><p>提示：请你阅读sklearn中这些超参数和其它超参数的<a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor">说明文档</a>，并尝试去使用它们。</p><ol start="2">\n<li>你在工作中曾经有过哪些避免过拟合的经验，可否在这里分享一下？</li>\n</ol><p>欢迎你在留言区分享你的想法和收获，我在留言区等你。如果这节课帮到了你，也欢迎你把这节课分享给自己的朋友。我们下一讲再见！</p><p><img src="https://static001.geekbang.org/resource/image/16/61/16a837d23777a5e5435b42569b5cea61.jpg?wh=2284x1280" alt=""></p>',
        article_title: "09｜模型优化（中）：防止过拟合，模型也不能太精细",
      },
      {
        title: "10｜模型优化（下）：交叉验证，同时寻找最优的参数",
        id: 419746,
        content:
          '<p>你好，我是黄佳。</p><p>欢迎来到零基础实战机器学习。在前面几节课中，我们已经学习了两种优化机器学习模型的方法，一种是做各种特征工程，让特征集更适合模型；另一种是防止过拟合，让模型不用“那么”精确。</p><p>这两种方式的优化角度不同，特征工程是从数据预处理的角度，给出更高质量的数据，而防止过拟合则是在模型训练的过程中，控制模型的复杂度。其实，除此之外，我们还可以从模型的验证和评估环节入手，来优化模型性能。</p><h2>交叉验证：小数据集的资源复用</h2><p>你知道，在样本充足的情况下，我们会随机将数据分为3个部分：训练集、验证集和测试集。其中，训练集用来训练模型，验证集用来模型调优，测试集用来评估模型性能。</p><p>不过，你还记不记得我们在<a href="https://time.geekbang.org/column/article/413057">第1讲</a>中介绍监督学习的时候，我说过有标签的数据是非常难以获得的。本来就很有限的数据集还要被拆成训练集、测试集和验证集，真是让人特别舍不得。</p><p>而且，我们知道数据集越大，就越不容易出现过拟合的现象。那么，我们如何利用较小的数据集，从而达到较大数据集的效果呢？这就需要交叉验证。</p><p>交叉验证的基本思想是这样的：将训练数据集分为k等份，其中k-1份用作训练集，单独的那一份用作验证集，整个过程重复k次，这也通常称作k折。这样就最大程度重复地使用了训练集中的数据，每一个数据都既做了训练，又做了测试，从而在最大程度上提高模型性能的可信度。</p><!-- [[[read_end]]] --><p>这种交叉验证具体实现起来有4个步骤：</p><ol>\n<li>\n<p>随机清洗数据集，将数据集分为训练集和测试集，将测试集预留出来，放在一边。</p>\n</li>\n<li>\n<p>将训练数据集分成k组（这个k值一般是随机设定，比如3，5，10，实操中以10折居多）。在这一步中，我们挑其中1组作为验证集，剩下的k-1组做训练集（这个过程要重复k次）。我们在这些训练集上拟合模型，可以得到k个不同的模型。然后再在对应的验证集上对这些模型进行评估，就能得到一系列模型的评估分数。最后，我们把这些评估分数进行平均，这个平均分数就是交叉验证的最终结果了。</p>\n</li>\n<li>\n<p>按照步骤1、2，对多个算法进行交叉验证，比如可以针对线性回归、决策树、随机森林等算法。根据每个算法交叉验证的评估结果，从中挑选效果最好的算法模型。</p>\n</li>\n<li>\n<p>使用测试集评估最终模型的分数。</p>\n</li>\n</ol><p>现在，你应该很清楚了：每个数据样本都有1次机会进入验证集中，并用于训练模型k-1次。这样一来，我们就拥有了更多的数据量。整个过程如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/1f/27/1f97a858d3b5f4976a481b0e44210127.png?wh=341x225" alt="" title="K折交叉验证示意图\n"></p><p>在交叉验证中，训练集和验证集的拆分可以通过sklearn.model_selection中的KFold函数实现。在这个函数中，有三个主要参数需要我们了解一下：</p><p><img src="https://static001.geekbang.org/resource/image/06/24/06cc789267e472b55125e52143b26f24.jpg?wh=2000x827" alt=""></p><p>下面，我们来看看交叉验证的实际用法。这里我们仍然借用<a href="https://time.geekbang.org/column/article/417479">第7讲</a>中预测用户LTV的线性回归模型model_lr，我们先用KFold.split对数据集进行拆分，然后对每一折循环，训练出不同的模型，最后给出一个验证分数。完整的代码你可以从<a href="https://github.com/huangjia2019/geektime/tree/main/%E5%8F%98%E7%8E%B0%E5%85%B310">这里</a>下载。</p><pre><code>from sklearn.model_selection import KFold #导入K折工具\nfrom sklearn.metrics import r2_score #导入R2分数评估工具\nkf5 = KFold(n_splits=5, shuffle=False) #5折验证\ni = 1 \nfor train_index, test_index in kf5.split(df_LTV): \n    X_train = df_LTV.iloc[train_index].drop([\'年度LTV\'],axis=1) #训练集X\n    X_test = df_LTV.iloc[test_index].drop([\'年度LTV\'],axis=1) #验证集X\n    y_train = df_LTV.iloc[train_index][\'年度LTV\'] #训练集y\n    y_test = df_LTV.loc[test_index][\'年度LTV\'] #验证集y \n    model_lr.fit(X_train, y_train) #训练模型\n    print(f&quot;第{i}折验证集R2分数：{r2_score(y_test, model_lr.predict(X_test))}&quot;) \n    i += 1\n</code></pre><p>输出如下：</p><pre><code>第1折验证集R2分数：0.5143622747243847\n第2折验证集R2分数：-0.16778272779470416\n第3折验证集R2分数：0.23879516275929713\n第4折验证集R2分数：0.2482389409435588\n第5折验证集R2分数：0.03088299924007265\n</code></pre><p>在实际实战中，sklearn还提供了更简单的方式，我们不用进行上面的KFold拆分过程，只用一个cross_val_score函数就能直接完成模型的K折拆分、训练和验证，一次性得到交叉验证结果。</p><p>接下来，我们再用这种方式来评估一下刚才的线性回归模型model_lr：</p><pre><code>from sklearn.model_selection import cross_val_score # 导入交叉验证工具\n# from sklearn.metrics import mean_squared_error #平均绝对误差\nmodel_lr = LinearRegression() #线性回归模型\nscores = cross_val_score(model_lr, #线性回归\n                  X_train, #特征集\n                  y_train, #标签集\n                  cv=5, # 五折验证\n                  scoring = \'neg_mean_absolute_error\') #平均绝对误差\nfor i, score in enumerate(scores):\n    print(f&quot;第{i+1}折验证集平均绝对误差： {-score}&quot;)\n</code></pre><p>这里我们把线性回归模型作为参数传入cross_val_score函数中，同时采用5折验证，计算出每一次验证的平均绝对误差，也就是预测值和真值之间的平均差异，误差绝对值越大，效果越不好。</p><p>输出的每一折平均绝对误差值如下：</p><pre><code>第1折验证集平均绝对误差：3191.99882739\n第2折验证集平均绝对误差：1402.45679102\n第3折验证集平均绝对误差：1168.49187113\n第4折验证集平均绝对误差：1546.15537555\n第5折验证集平均绝对误差：1138.41271054\n</code></pre><p>看的出来，采用不同的训练集和验证集组合，得到的模型分数不同。最后，我们再把这5折的验证结果进行平均，得到5折交叉验证的平均分数。这个分数，将比仅拆分一次训练集/验证集得到的评估结果更为稳定。</p><p>我这里要强调的是，交叉验证虽然一直在用不同的数据拆分进行模型的拟合，但它实际上并不是在试图训练出新的模型，它只是我们对模型的一种评估方式而已。</p><p>好了，关于交叉验证，我们就说到这里，下面我讲一个对你来说非常实用的问题：如何调参。</p><h2>网格搜索：直到找到最优的参数</h2><p>一个复杂的机器学习模型，有可能有一大堆超参数，模型的性能很大程度上取决于超参数的值。什么超参数是最好的？那可不一定，因为针对不同的问题而言，最佳的参数一直在变化，而且基本上没有太多规律可循。那些机器学习老手很可能会告诉你：调参可是要靠感觉、靠直觉的，你调的多了，感觉也就出来了。</p><p>这样的答案会让咱们这种新手十分头疼，没有什么经验怎么针对当前机器学习任务和当前数据集找到最好的参数？难道要手工的一个一个尝试吗？我们之前学过的线性回归模型，只有两个主要超参数，而且是布尔型变量，组合起来是4种情况，这还可以调一调。但是像随机森林这种比较复杂的模型，参数非常多，怎么可能一个个去试呢？</p><p>你不用担心， <strong>scikit-learn中有一个GridSearchCV工具，中文叫做网格搜索，堪称是辅助我们自动调参的神器</strong>，它可以帮我们自动调参，轻松找到模型的最优参数。</p><p>那么，怎么用这个网格搜索呢？我们需要做的有这几步：</p><ul>\n<li>第一步，明确你选择的模型有哪些可调的参数，这个你可以通过查看算法的说明文档来确定；</li>\n<li>第二步，把模型中各超参数不同取值的排列组合尽可能多地列出来；</li>\n<li>第三步，调用GridSearchCV工具，把可能的排列组合都传进去。</li>\n</ul><p>完成这三步后，GridSearchCV会在后台创建出一大堆的并行进程，挨个执行各种超参数的组合，同时还会使用交叉验证的方法（名称中的CV，意思就是cross validation），来评估每个超参数组合的模型。最后，GridSearchCV会帮你选定哪个组合是给定模型的最佳超参数值。</p><p>好，下面我们就基于<a href="https://time.geekbang.org/column/article/417479">第7讲</a>中的LTV预测项目，看看怎么用GridSearchCV选出随机森林模型比较好的参数组合（在之前的实战中，模型的参数都选择的是默认值）。</p><p>在调用GridSearchCV调参之前，我们首先找出随机森林回归模型中有哪些参数，最好的方法还是去<a href="https://scikit-learn.org/stable/">sklearn官网</a>查看。我们在这儿拿出一些重要的超参数来调一下：</p><p><img src="https://static001.geekbang.org/resource/image/68/fa/687c464e312272c66397ea795fa91efa.jpg?wh=2000x1016" alt=""></p><p>我们知道，随机森林是由多棵决策树集合而成的算法，所以，这些参数都和各决策树的生成和剪枝过程相关。上面这些参数，我们并不一定要完全理解它们的作用，因为我们的重点不是去详细了解每一个超参数，而是让GridSearchCV自动找到对于我们当前任务合适的超参数组合。</p><p>找出随机森林模型的参数后，接下来，我们需要在程序中定义一个字典对象，列出各个超参数，以及我们希望去尝试的值组合，你可以参考下面的代码。在这段代码中，我定义了rfr_param_grid字典对象，并在字典中指定了一系列参数值的组合，举例来说，对于n_estimators这个参数我就选择了100和300，当然你也可以尝试任意其它的值。</p><pre><code>model_rfr = RandomForestClassifier() # 随机森林模型\n# 对随机森林算法进行参数优化\nrf_param_grid = {&quot;max_depth&quot;: [None],\n                  &quot;max_features&quot;: [3, 5, 12],\n                  &quot;min_samples_split&quot;: [2, 5, 10],\n                  &quot;min_samples_leaf&quot;: [3, 5, 10],\n                  &quot;bootstrap&quot;: [False],\n                  &quot;n_estimators&quot; :[100,300],\n                  &quot;criterion&quot;: [&quot;gini&quot;]}\n</code></pre><p>然后，在要调用GridSearchCV 来进行超参数搜索之前，我们先看看GridSearchCV这个函数本身有什么需要设定的参数。</p><p><img src="https://static001.geekbang.org/resource/image/e1/53/e1c2dc9127e70f050e3e2014d3b41853.jpg?wh=2000x1147" alt=""></p><p>下面我们来调用GridSearchCV这个函数：</p><pre><code>from sklearn.model_selection import GridSearchCV # 导入网格搜索工具\nmodel_rfr_gs = GridSearchCV(model_rfr,\n                            param_grid = rfr_param_grid, cv=3,\n                            scoring=&quot;r2&quot;, n_jobs= 10, verbose = 1)\nmodel_rfr_gs.fit(X_train, y_train) # 用优化后的参数拟合训练数据集\n</code></pre><p>在上述代码中，我将GridSearchCV返回的最佳参数组合存储在了rfr_gs这个新的随机森林模型。</p><p>然后，系统就会自动计算每种超参数组合拟合出的模型的准确率/损失：</p><pre><code>Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:   22.0s\n[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:  2.0min\n[Parallel(n_jobs=10)]: Done 430 tasks      | elapsed:  4.8min\n[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  8.7min\n[Parallel(n_jobs=10)]: Done 1230 tasks      | elapsed: 12.6min\n[Parallel(n_jobs=10)]: Done 1296 out of 1296 | elapsed: 13.2min finished\n</code></pre><p>你要注意的是，随机森林模型可能的超参数组合非常多，因此这个训练过程在我们的电脑上可能会持续很久很久，所以我特意没有设定太多的参数组合，而是选取了其中一部分。不过，这个训练过程也是花了十几分钟才搜索完。</p><p>经过GridSearchCV自动地换参、拟合并自动交叉验证评估后，最佳参数组合实际上已经被选出了，它就被存储在model_rfr_gs这个新的随机森林中，我们可以直接用它来做预测。这里，我们调用model_rfr_gs的best_params_属性，来看一下这个最优模型是由哪些超参数组合而成的：</p><pre><code>print(&quot; 最佳参数组合:&quot;, model_rfr_gs.best_params_)\n</code></pre><p>输出如下：</p><pre><code>最佳参数组合: {\'bootstrap\': True, \'max_depth\': 10, \'max_features\': \'sqrt\', \'min_samples_leaf\': 2, \'min_samples_split\': 2, \'n_estimators\': 50}\n</code></pre><p>然后，我们看看 GridSearchCV 模型的准确性。</p><pre><code>from sklearn.metrics import r2_score,   median_absolute_error #导入Sklearn评估模块\nprint(\'训练集上的R平方分数-调参后的随机森林: %0.4f\' % r2_score(y_train, model_rfr_gs.predict(X_train)))\nprint(\'测试集上的R平方分数-调参后的随机森林: %0.4f\' % r2_score(y_valid, model_rfr_gs.predict(X_valid)))\n</code></pre><p>输出如下：</p><pre><code>训练集上的R平方分数-随机森林: 0.7729\n测试集上的R平方分数-随机森林: 0.6523\n</code></pre><p>这个分数需要和以前只调用随机森林模型默认参数的结果进行比较，所以，在这里我对这个模型训练了两次：一次不使用 GridsearchCV（使用默认超参数）；另一次我们使用 GridSearchCV 找到超参数最佳值。下面我们比较一下这两次的分数：</p><p><img src="https://static001.geekbang.org/resource/image/7f/fe/7faff6e1e1ee9fb72d05ce817f467efe.png?wh=384x262" alt=""></p><p>结果显示，经过网格搜索参数调优之后，虽然训练集上的分数降低了，但是在验证集上面的分数提升了，真棒！</p><p>学到这里，你可能会认为上面这一组参数是随机森林回归模型超参数的最佳值。但事实并非如此，上述超参数对于我们正在处理的数据集可能是最佳选择，但是对其他数据集未必适用。</p><p>到这里，我们就学完了两个优化技巧，再加上我们在前面两讲中学的特征工程和正则化技术，你在机器学习性能调优这块，就有了基本的理论和实践知识。恭喜你正式闯过“变现关”！</p><h2>总结一下</h2><p>这一讲中，我们学了两个和模型优化相关的方法，分别是交叉验证和网格搜索。</p><p>交叉验证是机器学习训练和验证模型时有效利用数据的方法，它的基本思路就是重复的使用数据，把样本数据进行切分，然后重组为不同的训练集和验证集。 在这个过程中，某次训练集中的某个样本在下次就可能成为验证集中的样本，所以被称作“交叉”。</p><p>网格搜索是帮我们优化模型超参数的技术，它在本质上是一种穷举法。对于每个超参数，我们都会指定一些可能的值。然后，GridSearchCV会组合这些参数值得到若干组超参数，并使用每组超参数来训练模型，最后挑选出验证集上误差最小的超参数，来作为该模型的最优超参数。</p><p>这两个工具使用起来非常简单，你可以通过代码反复演练它们，尝试各种不同的验证折数和模型超参数。</p><p>那截止到这节课，我们实际上已经学习了四个提升机器学习模型效率的法宝，它们都特别重要。在这里我也帮你复习一下：</p><ol>\n<li><strong>特征工程</strong>：目标是提高数据特征与模型的匹配度，我们总结了特征选择、特征变换和特征构建三个思路来帮助你解决具体问题。</li>\n<li><strong>防过拟合</strong>：过拟合就是模型针对训练集，拟合程度过高，失去了在新数据集上泛化的能力。我们的模型要在拟合和泛化之间保持一个平衡。</li>\n<li><strong>交叉验证</strong>：交叉验证可以帮我们利用好有限的数据集。你要注意，我们说的过拟合也有可能不是模型导致的，而可能是因为数据集划分不合理造成的，这在比较小的数据集上尤为明显。所以，交叉验证所得到的多次评估结果，能够避免某一次数据集划分不合理所带来的偏差。</li>\n<li><strong>参数调优</strong>：每一种算法都有属于自己的一系列可调超参数（外部参数），你要尽力找到对于当前数据集而言最好的一套参数。</li>\n</ol><p>在下一讲中，我们将开启激活关，让我们一起期待吧！</p><h2>思考题</h2><p>这节课就到这里了，最后，我想给你留三个思考题：</p><ol>\n<li>除了最简单的交叉验证函数cross_val_score之外，sklearn还提供另外一个函数cross_validate，这个函数和cross_val_score的区别有两点：一是它允许指定多个评估指标；二是它返回的信息更丰富，除评估分数之外，它还返回一个包含拟合时间、分数时间、以及可选的训练集分数和拟合模型的字典。请你尝试使用这个函数，并分析它返回的信息。</li>\n</ol><p>提示：</p><pre><code>   scores = cross_validate(lasso, X, y, cv=3,\n                             scoring=(\'r2\', \'neg_mean_squared_error\'),\n                            return_train_score=True)\n</code></pre><ol start="2">\n<li>除了普通的KFold方法之外，还有重复K折RepeatedKFold、留一法分折LeaveOneOut、留多法分折LeavePOut等变体，也可以为数据分折，那么。请你尝试用这些方法来拆分数据，做交叉验证。</li>\n</ol><p>提示：</p><pre><code>from sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import LeavePOut\n... \n&gt;&gt;&gt; loo = LeaveOneOut()\n&gt;&gt;&gt; for train, test in loo.split(X):\n...     print(&quot;%s %s&quot; % (train, test))\n</code></pre><ol start="3">\n<li>除GridSearchCV之外，sklearn还提供了另一个网格搜索API，叫RandomizedSearchCV，中文名是随机搜索。对于完全相同的参数空间，随机搜索运行时间比起网格搜索会大大降低，这是因为它采用的是随机参数组合。不过，随机搜索返回的模型性能可能会稍差，这就是时间和最优性能的一种权衡。请你尝试使用RandomizedSearchCV进行搜索，找到比较好的超参数。</li>\n</ol><p>提示：</p><pre><code>from sklearn.model_selection import RandomizedSearchCV\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   n_iter=n_iter_search)\n</code></pre><p>欢迎你在留言区分享你的想法和收获，我在留言区等你。如果这节课帮到了你，也欢迎你把这节课分享给自己的朋友。我们下一讲再见！</p><p><img src="https://static001.geekbang.org/resource/image/f1/23/f16f468d8e15018923564b9e7a784a23.jpg?wh=2284x1280" alt=""></p>',
        article_title: "10｜模型优化（下）：交叉验证，同时寻找最优的参数",
      },
      {
        title: "11｜深度学习（上）：用CNN带你认识深度学习",
        id: 420372,
        content:
          "<p>你好，我是黄佳。</p><p>随着课程的不断深入，我们已经成功闯过两个关卡，获客关和变现关，学到了多种机器学习算法和模型的优化方法。今天这一讲，我们正式进入“激活关”。在这一关中，我们的主要任务是根据易速鲜花App的历史记录，借助深度学习神经网络，来预测它的日激活数趋势。通过“激活关”，相信你会对深度学习神经网络的原理和用法有一个比较深入的理解。</p><p><img src=\"https://static001.geekbang.org/resource/image/a0/38/a02545b94937eaf80701766acf6d7038.jpg?wh=2284x1033\" alt=\"\"></p><p>不过，在正式进入这个项目之前呢，我们需要先打打基础，解决两个问题：1. 深度学习的原理是什么？2. 怎么搭建起一个深层神经网络CNN？这两个问题，我们将通过一个完整的小项目来搞定。</p><h2>问题定义</h2><p>易速鲜花的供应商每天都会发来大量的鲜花图片，不过，这些图片都没有按鲜花的类别进行分门别类，要是由人工来做的话，比较麻烦，成本也比较高。现在，我们需要根据易速鲜花大量已归类的鲜花图片，来建立一个能识别鲜花的模型，给未归类的图片自动贴标签。</p><p><img src=\"https://static001.geekbang.org/resource/image/b9/44/b9b26c72c613730ccdf5a908771a9144.png?wh=585x386\" alt=\"\"></p><p>这是一个典型的分类问题，同时也是一个计算机视觉领域的图片识别问题。那么下面我们先来看看在哪里可以找到这些花的图片。</p><h2>数据收集和预处理</h2><p>不知道你是否还记得，在<a href=\"https://time.geekbang.org/column/article/413648\">第2讲</a>中我曾经说过，在深度学习的实战部分，我会带你去Kaggle网站用一用GPU、TPU等加速器。Kaggle网站是一个在线Jupyter Notebook平台，同时也是数据科学爱好者最好的学校交流场所，里面有数据集、源代码、课程资料等。我们这个项目会在Kaggle网站上完成。</p><!-- [[[read_end]]] --><h3>1.数据的导入及可视化</h3><p>在Kaggle网站上，有一个<a href=\"https://www.kaggle.com/alxmamaev/flowers-recognition\">公开的数据集</a>，里面收纳了各类花朵图片，我们可以将它作为这个项目的数据集。由于这个花朵图片的数据集较大，我们不必把它下载下来，可以直接在网站中创建Notebook，来完成对花朵图片进行归类的工作。</p><p>首先，请你点开这个<a href=\"https://www.kaggle.com/alxmamaev/flowers-recognition\">花朵数据集</a>，单击New Notebook，建立你在Kaggle网站上的第一个深度学习Notebook。如果你没有用过这个网站，那在这个过程中，肯定需要你新注册用户，你跟着网站的说明走就可以了。当然了，你也可以访问我公开发表的<a href=\"https://www.kaggle.com/tohuangjia/cnn-network\">Notebook</a>，并Copy &amp; Edit我的代码。</p><p><img src=\"https://static001.geekbang.org/resource/image/3d/c6/3d32d6aa7131e0bf07e1c64872d36cc6.png?wh=1308x410\" alt=\"\"></p><p>新建Notebook之后，你就会来到像这样的页面，这个页面的基本操作和我们在本机安装的Jupyter Notebook完全一致，它其实就是网页版的Jupyter Notebook：</p><p><img src=\"https://static001.geekbang.org/resource/image/87/00/870784808087a94ca975b97ef3a41900.png?wh=980x744\" alt=\"\"></p><p>这里我要提醒你注意3个地方，就是我在图中用红框标出的：</p><ul>\n<li>\n<p>右上角红框里的Input，代表我们当前正在使用的数据集，基本上每一个Notebook都需要基于相关数据集创建；</p>\n</li>\n<li>\n<p>左下角点击“+ Code”，可以生成一段新的代码单元；</p>\n</li>\n<li>\n<p>右侧第二个红框是GPU加速选项，在跑深度学习项目时，你可以考虑打开它，如果你新注册了Kaggle，就会有30多个小时的GPU使用时间，跑完项目后你要记得关掉。还有就是，在安装新的工具包的时候，要打开Internet选项，不然，无法pip install新的包。</p>\n</li>\n</ul><p>在打开GPU选项的时候，会提示你每周可以使用GPU的小时数，选择“Turn on GPU”就可以了。</p><p><img src=\"https://static001.geekbang.org/resource/image/2a/b6/2a7dab64c717329b73c73aec10092ab6.png?wh=621x255\" alt=\"\"></p><p>下面，我们指定4个花朵目录，并通过Open CV（开源计算机视觉库）工具箱，读入图片的数据。OpenCV是一个跨平台的开源计算机视觉方面的API库，这里我们应用其中的imread和resize功能读入并裁剪图片到150*150像素：</p><pre><code>import numpy as np # 导入Numpy\nimport pandas as pd # 导入Pandas\nimport os # 导入OS\nimport cv2 # 导入Open CV工具箱\n\n\nprint(os.listdir('../input/flowers-recognition/flowers')) #打印目录结构\ndaisy_dir='../input/flowers-recognition/flowers/daisy' #雏菊目录\nrose_dir='../input/flowers-recognition/flowers/rose' #玫瑰目录\nsunflower_dir='../input/flowers-recognition/flowers/sunflower' #向日葵目录\ntulip_dir='../input/flowers-recognition/flowers/tulip' #郁金香目录\n\n\nX = [] #初始化\ny_label = [] #初始化\nimgsize = 150 #图片大小\n# 定义一个函数读入花的图片\ndef training_data(label,data_dir):\n    print (&quot;正在读入：&quot;, data_dir) \n    for img in os.listdir(data_dir): #目录\n        path = os.path.join(data_dir,img) #目录+文件名\n        img = cv2.imread(path,cv2.IMREAD_COLOR) #读入图片\n        img = cv2.resize(img,(imgsize,imgsize)) #设定图片像素维度\n        X.append(np.array(img)) #X特征集\n        y_label.append(str(label)) #y标签，即花的类别\n# 读入目录中的图片\ntraining_data('daisy',daisy_dir) #读入雏菊\ntraining_data('rose',rose_dir) #读入玫瑰\ntraining_data('sunflower',sunflower_dir) #读入向日葵\ntraining_data('tulip',tulip_dir) #读入郁金香\n</code></pre><p>输出如下：</p><pre><code>正在读入： ../input/flowers/daisy\n正在读入： ../input/flowers/sunflower\n正在读入： ../input/flowers/tulip\n正在读入： ../input/flowers/rose\n</code></pre><p>图片数据导入程序之后，我们随机用imshow功能显示几张花朵的图片，来确认一下我们已经成功读入了图片。</p><pre><code>import matplotlib.pyplot as plt # 导入matplotlib\nimport random as rdm # 导入随机数工具\n# 随机显示几张漂亮的花朵图片吧\nfig,ax=plt.subplots(5,2) #画布\nfig.set_size_inches(15,15) #大小\nfor i in range(5):\n    for j in range (2):\n        r=rdm.randint(0,len(X)) #随机选择图片\n        ax[i,j].imshow(X[r]) #显示图片\n        ax[i,j].set_title('Flower: '+y_label[r]) #花的类别\nplt.tight_layout() #绘图\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/f7/3d/f7f9477a11413a070e0c1a45b8290c3d.png?wh=579x386\" alt=\"\"></p><p>好，到这里呢，按照常规的机器学习项目步骤，紧接着下一个数据预处理环节就是数据清洗了，不过我们这个数据集的质量不错，没有需要清洗的内容，所以我们直接来构建特征集X和标签集y。</p><h3>2.构建特征集和标签集</h3><p>在下面的代码中，我们用LabelEncoder给标签y编码，并且把特征集X转换为了张量数组：</p><pre><code>from sklearn.preprocessing import LabelEncoder # 导入标签编码工具\nfrom tensorflow.keras.utils import to_categorical # 导入One-hot编码工具\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y_label) # 标签编码\ny = to_categorical(y,4) # 将标签转换为One-hot编码\nX = np.array(X) # 将X从列表转换为张量数组\n</code></pre><p>请你注意，这时候特征集X的格式不再是DataFrame表结构，而是NumPy数组，在机器学习里，我们把它称为“张量”，它的形状输出如下：</p><pre><code>array([[[[214, 237, 233],\n         [224, 234, 235],\n         [229, 232, 237],\n         ...,\n         [ 67,  93, 124],\n         [ 63,  91, 121],\n         [ 61,  93, 115]]]], dtype=uint8)\n</code></pre><p>如果用NumPy里面shape（就是张量的形状）的属性，我们就会看到当前特征集X的格式是4阶的张量。</p><pre><code>X.shape\n</code></pre><p>输出如下：</p><pre><code>(3265, 150, 150, 3)\n</code></pre><p>这个输出结果表示，在当前这个数据集中，一共有3265张150像素*150像素的图片，且所有图片的颜色通道数为RGB 3。</p><p>相信你已经体会到了，我们这里说的“4阶张量”代表了图片数据集中的4个维度：行（图片数量）、宽（像素宽）、高（像素高）和颜色通道数，缺少其中任何一个，都无法精准描述这个图片数据集。这就是为什么我们当前这个特征集X不再是我们之前常见的二维DataFrame表结构，而是NumPy数组，也就是深度学习中的“张量”。</p><p>如果是视频格式的数据集，则需要5阶张量才放得下，其形状为（样本，帧，高度，宽度，颜色深度）。此外，文本数据集通常是3阶张量，形状为（样本，序号，字编码）</p><p>其实，从严格意义上讲，前几节课中那些的二维DataFrame数据表格，也就是一行一列，也都属于2阶张量。</p><p>那为什么我们这里强调“阶”，而不再叫“维度”呢？这是因为在机器学习中，我们又把每一个阶上的特征的个数称为“特征的维度”。所以，为了避免混淆，当我们提到张量时，就称其为几<strong>阶</strong>张量；而提到特征时，就称特征的<strong>维度</strong>是多少。</p><p>对于这个花朵图片的数据集来说，在图片“宽度”这阶特征上面，就有150个维度，对应着150个值；而“高度”这阶特征呢，也是150个维度；宽、高组合，就形成了150*150，共这么多个像素点。这里面有点绕，你可以停下来体会一下，也可以看看下面的图。</p><p><img src=\"https://static001.geekbang.org/resource/image/b9/3d/b9d7880759d497bb6da4ac604f61153d.jpg?wh=2000x1000\" alt=\"\" title=\"各种张量\"></p><p>现在你应该明白了，为什么像图片、视频、文本这样的数据集，特征的维度和整体特征空间，体量都如此巨大。就这么一张小小的150 * 150像素的RGB图片，特征就有可能达到150 * 150 * 3的天文数字。所以，除了深层神经网络之外，传统机器学习方法根本解决不了。</p><p>而此时，y的格式也转换成了One-hot编码的张量。至于什么是One-hot编码，我们在<a href=\"https://time.geekbang.org/column/article/418354\">第8讲</a>中介绍过，我在这里就不重复了。</p><pre><code>array([[1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       ...,\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.]], dtype=float32)\n</code></pre><p>其中，[1., 0., 0., 0.]代表Daisy（雏菊），[0., 1., 0., 0.]就代表Rose（玫瑰），[0., 0., 1., 0.]就代表Sunflower（向日葵），[0., 0., 0., 1.]就代表Tulip（郁金香）。</p><p>至此，我们的特征集X和标签集y就构建完毕。</p><h3>3. 特征工程和数据集拆分</h3><p>我们在<a href=\"https://time.geekbang.org/column/article/413057\">第1讲</a>中说过，深层神经网络的厉害之处在于，它能对非结构的数据集进行自动的复杂特征提取，完全不需要人工干预。因此，我们并不用做什么复杂的特征工程处理。</p><p>不过，由于神经网络特别喜欢小范围的数值，这里我们只需要做个归一化处理，把0-255的RGB像素值压缩到0-1之间最好。这个步骤非常重要，不然神经网络会跑不起来：</p><pre><code>X = X/255 # 将X张量归一化\n</code></pre><p>接下来，就是数据集的拆分了，拆分的代码相信你已经非常熟悉了：</p><pre><code>from sklearn.model_selection import train_test_split # 导入拆分工具\nX_train, X_test, y_train, y_test = train_test_split(X, y, #拆分数据集\n                             test_size=0.2,random_state=1)\n</code></pre><p>那到这里呢，我们就完成了数据集的所有准备工作，下面进入关键的建立模型环节。</p><h2>选择算法建立模型</h2><p>就我们的鲜花分类来说，选择算法的过程在所有问题中是最简单的。因为对于图像分类识别问题来说，深度学习中的卷积神经网络CNN是不二之选。</p><h3>1.选择算法</h3><p>我们曾经说过，深层神经网络是由大量的人工神经元相互联结而成，这些神经元都具有可以调整的参数，而训练机器、建立模型的过程，也就是确定网络参数的过程。一旦参数确定，模型也就确定下来了。关于深度神经网络的结构，你可以看看下面的图。</p><p><img src=\"https://static001.geekbang.org/resource/image/f5/5a/f58bd8f5074c096279d302101348f45a.jpg?wh=2000x1000\" alt=\"\"></p><p>那么你还记不记得，深度学习和传统机器学习算法相比，它的优势在哪里呢？</p><p>没错，深度学习特别擅长处理非结构化的数据。我们前面的每一个实战所用的数据集，都是具有良好的结构，每一个特征是什么，我们都能说得很清楚。但是，还有另外一大类数据是没有良好结构的，比如说图形图像数据、文本数据等，这些数据集的特征长什么样呢？不太容易说清楚。</p><p>所以，传统的模型需要先做各种各样的特征工程，让数据变得“计算机友好”，再输入模型进行学习。而深度学习模型则可以自动进行特征提取，因此就省略掉了手工做特征工程的环节。你可以看一看图中所示的这个图片识别问题的机器学习流程。</p><p><img src=\"https://static001.geekbang.org/resource/image/df/77/df3931a4c47af6e4b326e73872b0b377.jpg?wh=2000x1000\" alt=\"\"></p><p>那么既然鲜花分类的算法易于确定，下面我们看看怎么选择一个好用的深度学习包（也叫框架）来创建神经网络模型。</p><p>我们知道，在传统机器学习领域中，工具包sklearn是一枝独秀，但深度学习框架就不一样了，在这个领域呈现的是TensorFlow和PyTorch两强争霸的格局。这二者都是开源项目，一个来自Google，一个来自Facebook。PyTorch因为编辑开发环境更具亲和力，支持快速和动态的训练，现在越来越受学术界和研究型开发者的欢迎，而TensorFlow则因为可以直接部署机器学习模型，能快速地开发和构建 AI 相关产品，它仍然在保持着工业界的霸主地位。</p><p>TensorFlow的另一个优势是有简单的内置高级 API，这个API就是非常有名的Keras，这也是初学者进入深度学习领域的最佳入门选择。Keras把TensorFlow的底层深度学习功能进行了良好的封装，是最好用、最适合初学者上手的深度学习工具包了。所以，我们下面就选择Keras来搭建神经网络。</p><h3>2.建立模型</h3><p>在搭建卷积神经网络之前，我们先来简单了解一下卷积网络是怎么一回事，这样我们在用代码构建时，就能清楚地知道每一步在干什么了。这里会出现一些新名词、新概念，不过你也不用害怕，因为我们并不是用编码来实现每一层，而是通过调用API来构建它们，你只要了解基本的结构框架，以及每一层的功能是什么，就足够了。</p><p>一个典型的卷积网络结构如下所示，它实现了一个图像分类功能：输入的是图像，输出的是图像的类别标签。</p><p><img src=\"https://static001.geekbang.org/resource/image/e6/18/e60e42c79c339f2becb63c3b0d6c2a18.png?wh=612x295\" alt=\"\" title=\"卷积网络的结构图\"></p><p>整体来看，卷积神经网络由输入层、一个或多个卷积层和输出层的全连接层组成。下面，我们按照图中从左往右的顺序，逐一来了解。</p><p>网络左边是数据输入部分，也就是输入层。这一层会对数据做初始处理，比如标准化、图片压缩、降维等，让最初的数据集变成形状为（ 样本，图像高度，图像宽度，颜色深度）的数据集。</p><p>接着，到了中间的卷积层。这一层主要负责抽取图片的特征，其中的卷积核（上图中红框部分）也叫滤波器，能够自动进行图像特征的提取。一般卷积层之后会接一个池化层，主要用来降低特征空间的维度，其中，池化层又包括最大池化和平均池化，它们的区别就在于输出时计算图片区域池化窗口内元素的最大值还是平均值。</p><p>通常，卷积+池化的架构会重复几次，形成深度卷积网络。在这个过程中，图片特征张</p><p>量的尺寸通常会逐渐减小，而深度将逐渐加深。就像在图中画的那样，特征图从一张扁扁的纸片形状变成了胖胖的矩形。</p><p>之后是一个展平层，主要负责将网络展平。展平之后通常会接一个普通的全连接层。而最右边的输出层也是全连接层，用Softmax进行激活分类输出层，所有神经网络都是用Softmax做多分类的激活函数。</p><p>卷积网络的核心特点就是“卷积+池化”的架构，而“卷积层”中的参数，其实是远少于全连接层的。这是因为卷积网络中各层的神经元之间，包括输入层的特征和卷积层之间，不是彼此全部连接的，而是以卷积的方式有选择性的局部连接。这种结构除了能大大减少参数的数量之外，还有有利于对图像特征的提取。</p><p>说完了卷积网络的结构和原理，现在我们用Keras来建立卷积神经网络模型。因为Tensorflow和Keras完全集成在Kaggle的Notebook中了，所以你不用pip install它们。我们直接调用其中的API，就能够搭建起网络模型来。下面这段不到20行的代码，就为我们搭建起了一个能够为花朵图片分类的卷积神经网络：</p><pre><code>from tensorflow.keras import layers # 导入所有层 行1\nfrom tensorflow.keras import models # 导入所有模型 行2\ncnn = models.Sequential() # 贯序模型 行3\ncnn.add(layers.Conv2D(32, (3, 3), activation='relu', # 输入卷积层 行4\n                        input_shape=(150, 150, 3))) \ncnn.add(layers.MaxPooling2D((2, 2))) # 最大池化层 行5\ncnn.add(layers.Conv2D(64, (3, 3), activation='relu')) # 卷积层 行6\ncnn.add(layers.MaxPooling2D((2, 2))) # 最大池化层 行7\ncnn.add(layers.Conv2D(128, (3, 3), activation='relu')) # 卷积层 行8\ncnn.add(layers.MaxPooling2D((2, 2))) # 最大池化层 行9\ncnn.add(layers.Conv2D(128, (3, 3), activation='relu')) # 卷积层 行10\ncnn.add(layers.MaxPooling2D((2, 2))) # 最大池化层 行11\ncnn.add(layers.Flatten()) # 展平层 行12\ncnn.add(layers.Dense(512, activation='relu')) # 全连接层 行13\ncnn.add(layers.Dense(4, activation='softmax')) # 分类输出层 行14\ncnn.compile(loss='categorical_crossentropy', # 损失函数 行15\n            optimizer='RMSprop', # 优化器\n            metrics=['acc']) # 评估指标\n</code></pre><p>怎么样，这段代码的结构看起来也不复杂吧。其实神经网络中最主要的结构就是“层”，各种各样不同的层像拼积木一样组合起来，就形成了各种各样的神经网络。而对于我们的卷积神经网络CNN来说，其中最重要的就是Conv2D这个卷积层，它是我们这个神经网络的主要功能层，决定了我们所构建的神经网络是一个卷积神经网络。</p><p>现在我们一起来看看我们刚才搭建的这个CNN网络是个什么样子的结构，你可以用下面的方法来图片化显示这整个的CNN神经网络模型：</p><pre><code>from IPython.display import SVG # 实现神经网络结构的图形化显示\nfrom tensorflow.keras.utils import model_to_dot # 导入model_to_dot工具\nSVG(model_to_dot(cnn).create(prog='dot', format='svg')) # 绘图\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/cc/27/ccde4a8748ea21fe37a16faa22e82427.png?wh=790x1179\" alt=\"\"></p><p>那么这个结构是怎么用刚才的Keras API搭建出来的呢？下面我就给你说一说，以后你自己搭建新的神经网络，也就是这样照猫画虎，搭积木似地往上构建就行了。</p><p>首先，我们通过cnn = models.Sequential()创建一个序贯模型（代码行3）。序贯模型也是最简单的模型，就是像盖楼一样，一层一层往上堆叠着搭新的层。我们所有神经网络都用序贯模型，还有一种模型是函数式模型，是比较高级地技巧，不过比较少见，你如果有兴趣可以查阅资料去了解。</p><p>然后，我们通过cnn.add(layers.Conv2D(32, (3, 3), activation=‘relu’, input_shape=(150, 150, 3)))语句（代码行4）在模型中加入了神经网络的输入层。这里面的激活函数relu我们以后再解释，现在你只要知道relu是一个常用的激活函数就可以了。这里，你需要注意的是，输入层需要通过input_shape=(150, 150, 3)指定输入的特征数据集的形状。这个刚才我们已经说过了，如果形状不对，等会儿拟合时就会报错。</p><p>接着，从代码行5开始直到行13，我们为神经网络添加了各种各样的中间层（也叫隐层），这些层如何添加、配置，我们有足够的自由去尝试。少可以两三层，多可以几万层。</p><p>其中，cnn.add(layers.Conv2D(64, (3, 3), activation=‘relu’))（代码行6）这个语句是用来添加输入层之后的中间卷积层的，这里面的64是输出空间的维度，也就是卷积过程中输出滤波器的数量，而(3, 3)则指定 2D 卷积窗口的高度和宽度。cnn.add(layers.MaxPooling2D((2, 2)))（代码行7）这个语句是用来添加池化层，(2, 2)也是指定 2D 卷积窗口的高度和宽度。</p><p>以此类推，卷积层+池化层的组合会持续出现，然后再输出层之前需要有展品层（代码行12）和全连接层（代码行13）。</p><p>我们接着看代码行14，cnn.add(layers.Dense(10, activation=‘softmax’)) 这一行叫做输出层，其中activation='softmax’这个参数，就用于多分类输出。这个暂时你就记住，先不用去管细节，讲分类问题的时候我们再详谈。</p><p>那么你可能会问，怎么看一个神经网络是普通神经网络DNN，还是CNN或者RNN呢？这其中的关键就是看输入层和中间层主要是什么类型。DNN的输入层和中间层主要是Dense层，CNN的输入层和中间层主要是Conv1D、Conv2D或者Conv3D，RNN的输入层和中间层主要是SimpleRNN或者GRU或者LSTM层。</p><p>怎么样，讲到这里，你明白怎么去搭建神经网络了吗？我知道，如果你是第一次接触深度学习的话，这里的新概念的确有点多，不过，这里其实也并没有什么很难懂的东西，你需要把选择算法建立模型这个环节，反复阅读，咀嚼几遍，就可以掌握搭建神经网络模型的方法。</p><h2>模型的训练和拟合</h2><p>模型构建好之后，我们用非常熟悉的fit语句来进行训练：</p><pre><code># 训练网络并把训练过程信息存入history对象\nhistory = cnn.fit(X_train,y_train, #训练数据\n                  epochs=10, #训练轮次（梯度下降）\n                  validation_split=0.2) #训练的同时进行验证\n</code></pre><p>在训练过程中，我们还指定了validation_split，它可以在训练的同时，自动把训练集部分拆出来，进行验证，在每一个训练轮次中，求出该轮次在训练集和验证集上面的损失和预测准确率。</p><p>训练输出如下：</p><pre><code>Train on 2089 samples, validate on 523 samples\nEpoch 1/5\n2089/2089 [==============================] - 86s 41ms/step - loss: 1.3523 - acc: 0.3978 - val_loss: 1.0567 - val_acc: 0.5411\nEpoch 2/5\n2089/2089 [==============================] - 85s 41ms/step - loss: 1.0167 - acc: 0.5692 - val_loss: 1.0336 - val_acc: 0.5526\nEpoch 3/5\n2089/2089 [==============================] - 85s 41ms/step - loss: 0.8912 - acc: 0.6343 - val_loss: 0.9183 - val_acc: 0.6310\nEpoch 4/5\n2089/2089 [==============================] - 84s 40ms/step - loss: 0.8295 - acc: 0.6596 - val_loss: 0.9289 - val_acc: 0.6138\nEpoch 5/5\n2089/2089 [==============================] - 85s 41ms/step - loss: 0.7228 - acc: 0.7056 - val_loss: 1.0086 - val_acc: 0.5736\n... ...\n</code></pre><p>这个输出的信息包括了训练的轮次（梯度下降的次数）、每轮训练的时长、每轮训练过程中的平均损失，以及分类的准确度。这里的每一个轮次，其实就是神经网络对其中的每一个神经元自动调参、通过梯度下降进行最优化的过程。</p><h2>模型性能的评估</h2><p>刚才的训练过程已经包含了验证的环节。不过，为了更好地体现训练过程中的损失变化情况，我们这里把每轮的损失和准确率做一个可视化，绘制出损失曲线，来展示模型在训练集上评估分数和损失的变化过程。</p><p>这种损失曲线其实你并不陌生，在<a href=\"https://time.geekbang.org/column/article/416824\">第6讲</a>中，我们在讲K-Means算法中的手肘图时候就绘制过类似的曲线：</p><pre><code>def show_history(history): # 显示训练过程中的学习曲线\n    loss = history.history['loss'] #训练损失\n    val_loss = history.history['val_loss'] #验证损失\n    epochs = range(1, len(loss) + 1) #训练轮次\n    plt.figure(figsize=(12,4)) # 图片大小\n    plt.subplot(1, 2, 1) #子图1\n    plt.plot(epochs, loss, 'bo', label='Training loss') #训练损失\n    plt.plot(epochs, val_loss, 'b', label='Validation loss') #验证损失\n    plt.title('Training and validation loss') #图题\n    plt.xlabel('Epochs') #X轴文字\n    plt.ylabel('Loss') #Y轴文字\n    plt.legend() #图例\n    acc = history.history['acc'] #训练准确率\n    val_acc = history.history['val_acc'] #验证准确率\n    plt.subplot(1, 2, 2) #子图2\n    plt.plot(epochs, acc, 'bo', label='Training acc') #训练准确率\n    plt.plot(epochs, val_acc, 'b', label='Validation acc') #验证准确率\n    plt.title('Training and validation accuracy') #图题\n    plt.xlabel('Epochs') #X轴文字\n    plt.ylabel('Accuracy') #Y轴文字\n    plt.legend() #图例\n    plt.show() #绘图\nshow_history(history) # 调用这个函数\n</code></pre><p>损失曲线输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/22/03/22c6de1353412c227c124c6ec999e003.png?wh=721x272\" alt=\"\"></p><p>可以看到，训练集的损失呈现下降趋势，但是测试集上的损失则呈现跳跃，这说明这个神经网络性能不是很稳定，似乎有过拟合的现象。我们将在后面的讲解中探讨如何优化神经网络，让神经网络的损失值更低，准确率更高。</p><p>最后，我们用这个训练好了的模型，在测试集上进行分类结果的评分。</p><pre><code>result = cnn.evaluate(X_test, y_test) #评估测试集上的准确率\nprint('CNN的测试准确率为',&quot;{0:.2f}%&quot;.format(result[1]))\n</code></pre><p>输入如下：</p><pre><code>653/653 [==============================] - 10s 15ms/step\nCNN的测试准确率为 0.69%\n</code></pre><p>输出显示，在653张测试集的图片中测试，模型的分类准确率达到了0.69以上。</p><p>最后呢，我们可以应用模型的predict属性把X特征集传入，进行花朵图片的分类。</p><pre><code>prediction = cnn.predict(X_test) #预测测试集的图片分类\n</code></pre><p>下面的代码我们输出第一个图片（Python是从0开始索引的，所以下标0就是第一张图片）的分类预测值：</p><pre><code>prediction[0] #第一张图片的分类\n</code></pre><p>输出如下：</p><pre><code>array([0.0030566 , 0.13018326, 0.00846946, 0.8582906 ], dtype=float32)\n</code></pre><p>这里请你注意，此时输出的是分类概率。上面的输出结果表示，第一类花Daisy（雏菊）的概率为0.03，第二类花Rose（玫瑰）的概率为0.13，第三类花Sunflower（向日葵）的概率为0.008，第四类花Tulip（郁金香）的概率为0.858。</p><p>下面的代码中，我们选出最大的那个概率，并把它当作CNN的分类结果：</p><pre><code>print('第一张测试图片的分类结果为:', np.argmax(prediction[0]))\n</code></pre><p>输出如下：</p><pre><code>第一张测试图片的分类结果为: 3\n</code></pre><p>结果显示第一个图片被CNN网络模型分类为第4种花（索引从0开始，所以类别3就是第4种花），也就是Tulip（郁金香）。</p><p>好啦，这个神经网络完成了很好的分类功能，准确率达到69%！这比我可强多了，反正我是区分不出来这些花的种类的。那么，神经网络的超参数我们还不知道是怎么一会事，也不知道如何继续优化。不必担心，今天只是讲了个基础。关于性能的超参数的具体用户和性能调优，我们会找一个时间单独讲解的。</p><h2>总结一下</h2><p>怎么样，构建强大的深度学习神经网络，并没有你想象得那么难吧？</p><p>在这一讲中，我带你去了一个非常好用的网站Kaggle，根据这个网站中的花朵图片数据集创建了Jupyter Notebook，下面几节课中，我们都需要利用Kaggle网站中的GPU等资源来加速网络模型的拟合速度。</p><p>我们这节课的重点在于了解卷积神经网络的结构以及它的搭建方法，它由输入层、一个或多个卷积层和全连接输出层组成。要构建出能够实现图像分类的CNN，我们通常需要把“卷积+池化”的组合重复搭建几次，形成深度卷积网络。在这个过程中，图片特征的尺寸通常会逐渐减小，而深度将逐渐加深。然后我们把它展平，之后通常接一个普通的全连接层，再接一个分类输出层。</p><p>这个过程特别像搭积木，全都是通过Keras的add API，添加layers中所提供的各种类型的层来完成的。如果你希望搭建更深层的神经网络，只要加入更多的卷积层，或者加入更多的全连接层就可以，这个过程并无一定之规，你可以随意尝试。</p><p>当然如果你的层与层之间组合有误，或者输入层所指定接收的张量格式和X特征集的形状不匹配，编译的时候就会报错。这也没什么大不了的，就像我们拿起了两个接口不一样乐高玩具，当然无法拼合在一起。你只要根据错误信息，按图索骥，找到合适的组合方案就可以了。</p><h2>思考题</h2><p>这节课到这里就讲完了，我来给你留一道练习题：</p><p>今天我们搭建出了一种形状的神经网络，我刚才也说了，在搭建网络结构的时候，我们就像个建筑师一样，可以自行组合层的数量和各层的样式。那么，现在请你改变一下网络的结构，自己搭建新的网络，比较一下性能。</p><p>提示：你可以在我们今天学的CNN网络的基础上：</p><ol>\n<li>\n<p>增加更多的卷积层、池化层组合；</p>\n</li>\n<li>\n<p>增加或减少中间卷积层滤波器的个数（如64，128，256，512…）；</p>\n</li>\n<li>\n<p>调整中间层中卷积和池化窗口的高度和宽度（如(5, 5)、(7, 7)…）</p>\n<p>cnn.add(layers.Conv2D(256, (5, 5), activation=‘relu’)) # 卷积<br>\ncnn.add(layers.MaxPooling2D((3, 3))) # 最大池化</p>\n</li>\n</ol><p>欢迎你积极留言提交属于你自己的神经网络模型的评估分数，看看谁的模型更棒！我们下一讲见！</p><p><img src=\"https://static001.geekbang.org/resource/image/by/12/byyd94ec14a79486bdf3631e6a47d712.jpg?wh=2284x1280\" alt=\"\"></p>",
        article_title: "11｜深度学习（上）：用CNN带你认识深度学习",
      },
      {
        title: "12｜深度学习（中）：如何用RNN预测激活率走势？",
        id: 421029,
        content:
          '<p>你好，我是黄佳。</p><p>欢迎来到零基础实战机器学习。在上一讲中，我们通过给鲜花图片分类，学习了CNN在图像识别方面的应用。这一讲呢，我们就来学习另外一种深度学习模型——循环神经网络RNN（Recurrent Neural Network）。那么进入正题之前，我先给你讲个段子，让你直观理解一下，循环神经网络和其它神经网络模型有啥不同。</p><p>假如我和你开车去商场，然后我说“嘿！你知道吗，昨天老王的夫人生二胎了！”你说：“是吗？这么大年纪，真不容易。对了，上次你说他的项目没上线，对吧，后来那个项目到底怎么样了呢？”我回答说：“嗨，那个项目啊，别提了，把他整惨了，三次上线都失败了，现在公司在考虑放弃老王负责的这个项目。嗯，到了，你等我一会儿，我去给老王买个_____（此处为填空，选项1：项目；选项2：玩具）慰问一下他。”</p><p>若是这个选择题给循环神经网络之外的其它神经网络做，我们得到的答案可能是“项目”，因为在输入的文本（特征）中，一直是在谈项目，可正确答案显然是“玩具”。也就是说，如果让神经网络实现类似于人脑的语义判别能力，有一个很重要的前提是，<strong>必须从所有过去的句子中保留一些信息，以便能理解整个故事的上下文</strong>。而循环神经网络中的记忆功能恰恰解决了这个“对前文的记忆”功能。</p><!-- [[[read_end]]] --><p>而这种对前文记忆的功能，让循环神经网络特别适合处理自然语言、文本和时间序列这种“新内容依赖于上下文或者历史信息”的数据。</p><p>好，明白了这一点，现在我们来看看这节课要解决的问题吧！</p><h2>定义问题</h2><p>今天这个项目的业务场景非常简单：易速鲜花App从2019年上线以来，App的日激活数稳步增长。运营团队已经拉出两年以来每天的具体激活数字，现在要交给你的任务是，建立起一个能够预测未来激活率走势的机器学习模型。</p><p><img src="https://static001.geekbang.org/resource/image/0e/a8/0ec7c86547ed4a2b4b3280ec09d30ba8.png?wh=196x304" alt=""></p><p>你可以在<a href="https://www.kaggle.com/tohuangjia/flower-app">这里</a>下载这个数据集，或者就基于这个数据集，在Kaggle网站上创建新的Notebook。<a href="https://www.kaggle.com/tohuangjia/rnn-network">代码</a>呢，我把它发布在<a href="https://www.kaggle.com/tohuangjia/rnn-network">Kaggle</a>网站，你也可以Copy &amp; Edit它到自己的Notebook。</p><p>看到这个数据集，你可能会觉得和之前的数据集不太一样了，它的字段特别少，只有两个。现在，我给你几秒钟思考一下，这个数据集的特征和标签分别是什么？你可能会说，标签比较容易找，因为要预测激活数，所以“激活数”就是标签。</p><p>你可能还会进一步分析说，从机器学习的类型来看，这里我们有要预测的标签，所以它是一个监督学习问题，而且，标签是连续性的数值，因此它是一个回归问题。</p><p>这些都没错，不过，特征在哪里呢？难道说，“日期”字段就是这个数据集的特征吗？这些日期看起来像是普普通通的编号，它既不是有意义的值（比如分数），也不是分类的类别编码（比如男生/女生），难道机器学习模型这么厉害，输入这样的编号，就能够预测出未来的App激活数？</p><p>而且，如果你回忆一下我们之前见过的那些数据集，就会发现特征和标签之间会呈现出一种相关性。比如说点赞数、转发数多的时候，往往浏览量也多。但是，在这个数据集中，日期和App的激活数之间是不存在这种相关性的。所以，日期并不是一个合适的特征。</p><p>不过，在这个数据集中，“日期”仍然是非常重要的信息。让我们一起来回顾一下这个问题的目标：本质上，我们是希望从过去的状态中，找到蛛丝马迹，来预测未来的状态。而过去的状态，肯定是和日期相关联的。</p><p>所以，我们可以这样来构建这个数据集的特征：从当前要预测的日期开始，假如是2020年6月30日，我们回推60天，拥有从2020年5月1日到2020年6月29日这60天的激活数历史数值。那么，如果我们把这些历史数值都做为特征，输入机器学习。并且，我们有理由相信，2020年6月30日往后的激活数与前60天的激活数有很强的相关性。根据这样的特征数据，机器就可以学习到一条预测激活数的趋势线了。</p><p>其实，像这样和日期、时间相关的，并且以连续性数值为特征的数据集，就叫做<strong>时间序列数据</strong>，简称<strong>时序数据</strong>。典型的时序数据包括股票价格、点击数、产品销售数量等等。</p><p>那到这里呢，我们就已经把问题定义好了，数据也有了，紧接着下一步就是数据的预处理。</p><h2>收集数据和预处理</h2><h3>1. 数据可视化</h3><p>我们先把数据集导入程序。其中parse_dates=[‘Date’]是把Date字段以日期格式导入。</p><pre><code>import numpy as np #导入NumPy\nimport pandas as pd #导入Pandas \ndf_app = pd.read_csv(\'app.csv\', index_col=\'Date\', parse_dates=[\'Date\']) #导入数据\ndf_app #显示数据\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/9d/e5/9de7486982f923c5567b8e3da706ede5.png?wh=203x377" alt=""></p><p>对于时间序列数据，可视化是相当有必要的。我们用plot这个API，绘制出激活数的历史走势图，其中时间为横坐标，App激活数为纵坐标的图像：</p><pre><code>import matplotlib.pyplot as plt #导入matplotlib.pyplot\nplt.style.use(\'fivethirtyeight\') #设定绘图风格\ndf_app[&quot;Activation&quot;].plot(figsize=(12,4),legend=True) #绘制激活数\nplt.title(\'App Activation Count\') #图题\nplt.show() #绘图\n</code></pre><p>这里需要注意的是，我通过语句plt.style.use(‘fivethirtyeight’)设定了图像的风格。这种风格是从<a href="https://fivethirtyeight.com/">538网站</a>（一个对各种事件和趋势进行预测的网站）的画风中移植过来的，很适合展示时序数据。</p><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/8a/b7/8abaafa09017781766e3d5bc7bda85b7.png?wh=790x314" alt=""></p><p>看得出，自APP上线以来，日激活数整体呈上升的走势。从函数图像上看，也没有任何缺失的数据点。下面我们再来看看这个数据集是否需要做数据清洗工作。</p><h3>2. 数据清洗</h3><p>我们可以用下面这个语句看看有没有NaN值：</p><pre><code>df_app.isna().sum() #有NaN吗？\n</code></pre><p>结果显示数据集中没有NaN值。</p><p>对于App激活数，我只想保证数据集里全部都是正值，所以，可以用下面的语句查看有没有负值和零值：</p><pre><code>(df_app.Activation &lt; 0).values.any() #有负值吗？\n</code></pre><p>其中，values.any()这个API表示，只要Orders字段中有任意一个值是负值或者零值，就会返回True。</p><p>输出如下：</p><pre><code>False\n</code></pre><p>说明整个数据集没有一个0值或者负值。那我们也不需要做任何的清洗工作了。接下来，我们进入直接训练集和测试集的拆分。</p><h3>3. 拆分训练集和测试集</h3><p>我们假设以2020年10月1日为界，只给模型读入2020年10月1日之前的数据，之后的数据留作对模型的测试。那么，我们就以2020年10月1日为界，来拆分训练集和测试集：</p><pre><code># 按照2020年10月1日为界拆分数据集\nTrain = df_app[:\'2020-09-30\'].iloc[:,0:1].values #训练集\nTest = df_app[\'2020-10-01\':].iloc[:,0:1].values #测试集\n</code></pre><p>对于这段代码，我解释几点：</p><ul>\n<li>df_app[:‘2020-09-30’]代表10月1日之前的数据，用于训练模型；df_app[‘2020-10-01’:]代表10月1日之后，用于对训练好的模型进行测试。</li>\n<li>代码中的iloc属性，是Pandas中对DataFrame对象以行和列位置为索引抽取数据。其中的第一个参数代表行索引，指定“:”就表示抽取所有行；而第二个参数中的“0:1”代表要抽取的列索引的位置为第2列，也就是“激活数”这一列。</li>\n<li>最后，.values这个属性就把Pandas对象转换成了Numpy数组，我们在<a href="https://time.geekbang.org/column/article/420372">上一讲</a>中说过，机器学习中也把Numpy数组称为张量，神经网络模型需要Numpy张量类型作为输入。</li>\n</ul><p>这时候，如果我们显示这个数据对象Train，就会发现它已经成为了一个NumPy数组：</p><pre><code>Train #显示训练集对象\n</code></pre><p>输出如下：</p><pre><code>array([[419],\n       [432],\n       ...\n       [872],\n       [875]])\n</code></pre><p>对于神经网络来说，输入的张量形状非常重要，我们要显式指明输入维度，绝不能错，所以我们要经常用NumPy中的.shape属性查看当前数据对象的形状。</p><pre><code>print(\'训练集的形状是：\', Train.shape)\nprint(\'测试集的形状是：\', Test.shape)\n</code></pre><p>输出如下：</p><pre><code>训练集的形状是： (639, 1)\n测试集的形状是： (117, 1)\n</code></pre><p>目前，训练集是639行的一维数组，测试集是117行的一维数组，但它们都是二阶张量。</p><p>为了直观地显示拆分，我们可以把原始数据集按照拆分日期用不同颜色进行显示。</p><pre><code># 以不同颜色为训练集和测试集绘图\ndf_app[&quot;Activation&quot;][:\'2020-09-30\'].plot(figsize=(12,4),legend=True) #训练集\ndf_app[&quot;Activation&quot;][\'2020-10-01\':].plot(figsize=(12,4),legend=True) #测试集\nplt.legend([\'Training set (Before October 2020)\',\'Test set (2020 October and beyond)\']) #图例\nplt.title(\'App Activation Count\') #图题\nplt.show() #绘图\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/fb/fb/fb1fd22eb1a4ac3c8445231c729346fb.png?wh=715x292" alt=""></p><p>好啦，拆分出训练集和测试集后，下面我们做特征工程。</p><h3>4. 特征工程</h3><p>对于神经网络来说，这是一个特别重要的步骤。我们上节课说过，神经网络非常不喜欢数值跨度大的数据，所以，我们对训练特征数据集进行归一化缩放。</p><pre><code>from sklearn.preprocessing import MinMaxScaler #导入归一化缩放器\nScaler = MinMaxScaler(feature_range=(0,1)) #创建缩放器\nTrain = Scaler.fit_transform(Train) #拟合缩放器并对训练集进行归一化\n</code></pre><p>归一化完成之后，我们来做最后一个数据预处理的步骤：构建特征集和标签集。</p><h3>5. 构建特征集和标签集</h3><p>我们前面说过，这个数据集的标签就是App激活数，特征是时序数据。如果我们要预测今天的App下载数量，那时序数据特征的典型构造方法就是，把之前30天或者60天的App下载数量作为特征信息被输入机器学习模型。</p><p>所以，在下面的这段代码中，我们创建了一个具有 60 个时间步长（所谓步长，就是时间相关的历史特征数据点）和 1 个输出的数据结构。对于训练集的每一行，我们有 60 个之前的App下载数作为特征，1个当前训练集元素作为标签。</p><pre><code># 创建具有 60 个时间步长和 1 个输出的数据结构 - 训练集\nX_train = [] #初始化\ny_train = [] #初始化\nfor i in range(60,Train.size): \n    X_train.append(Train[i-60:i,0]) #构建特征\n    y_train.append(Train[i,0]) #构建标签\nX_train, y_train = np.array(X_train), np.array(y_train) #转换为NumPy数组\nX_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1)) #转换成神经网络所需的张量形状\n</code></pre><p>在这个过程中的最后一步，我是用NumPy中的reshape方法，把特征数据集转换成神经网络所需要的形状的。我之所以强调这一点，是因为如果形状不对，程序在训练时就会报错，这个错误对初学者来说很常见。</p><p>这里我显示一下目前X_train集的形状：</p><pre><code>X_train.shape #X_train的形状\n</code></pre><p>输出如下：</p><pre><code>X_train的形状是： (579, 60, 1)\n</code></pre><p>输出显示它是一个三阶张量，每一阶上面的维度是579（579行数据）、60（每一个当日激活数往前60天的激活数）和1（每一个时间点只有激活数一个特征）。</p><p>我们用同样的方法构建测试集：</p><pre><code>TrainTest = df_app[&quot;Activation&quot;][:] #整体数据\ninputs = TrainTest[len(TrainTest)-len(Test) - 60:].values #Test加上前60个时间步\ninputs = inputs.reshape(-1,1) #转换形状\ninputs  = Scaler.transform(inputs) #归一化\n# 创建具有 60 个时间步长和 1 个输出的数据结构 - 测试集\nX_test = [] #初始化\ny_test = [] #初始化\nfor i in range(60,inputs.size): \n    X_test.append(inputs[i-60:i,0]) #构建特征\n    y_test.append(inputs[i,0]) #构建标签\nX_test = np.array(X_test) #转换为NumPy数组\nX_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1)) #转换成神经网络所需的张量形状\n</code></pre><p>如果我们显示测试集的形状，会发现它是形状为(117, 60, 1)的张量。</p><p>好啦，至此，数据预处理的工作就终于完成了。我们进入到下一个环节：选择算法，创建模型。</p><h2>选择算法并建立模型</h2><p>前面我已经给你剧透了，对于预测App未来激活数的问题，我们会选择神经网络中的RNN。其实，我们可选的算法还是挺多的，比如说普通的回归算法，还有像AR（自回归）、MA（滑动平均）、ARMA（自回归滑动平均）、ARIMA（差分自回归滑动平均）等专门处理时序数据的模型，还有Facebook发布的Prophect算法，也可以用于时间序列的预测。</p><p>当然，最为高效、最常用的时序预测模型，仍然莫过于深度学习中的RNN了。这也是我们这次实战中会选用RNN模型。那么，为什么RNN擅长时序数据的处理呢？这里我们就要多说两句它的原理了。</p><p>我们知道，循环神经网络是神经网络的一种，不过，和其它类型的神经网络相比，最大的不同是它建立了自身的记忆机制，增加了时间相关的状态信息在各层神经元间的循环传递机制。即一个序列的当前输出与前面各个神经元的输出也是有关的，即隐藏层之间不再是不相连的，而是有连接的。这就让循环神经网络能够更加自由和动态地获取输入的信息，而不受到定长输入空间的限制。</p><p><img src="https://static001.geekbang.org/resource/image/78/y7/78189a848e83facdf0dc641ec66c0yy7.png?wh=500x275" alt="" title="循环神经网络建立了与时间状态相关的记忆机制"></p><p>因此，向循环神经网络中输入的数据都有这样的特点， 数据集中不仅要包含当前的特征值，还要包括前一刻或者前几刻的状态。</p><p>循环神经网络的应用场景正是这种有“上下文历史”的数据，比如说我们文章开头说的那个给老王买礼物的例子，那其实是在对自然语言处理。再比如视频处理，当前帧中发生的事情在很大程度上取决于之前数帧中的内容。另外，还有一个典型的应用场景就是我们这里的实战案例，循环神经网络要处理的是一个典型的时间序列分析数据集，根据前面60天的激活数，对未来的数字进行预测。</p><p>你会发现，这些场景和数据集，都有一个显著的特点，就是要预测的内容，和之前一段时间的特征值密切相关，就适合选择循环神经网络来建立模型进行处理。</p><p>到这里，你应该理解了为什么我们的时序数据预测要用RNN来处理，现在，我们就来考虑怎么搭建RNN。</p><p>在Kares中，主要有三种循环神经网络层可以搭建循环神经网络，分别是Simple RNN、LSTM和GRU，那我们该选哪一个呢？我们来做个甄别。</p><p>Simple RNN，顾名思义，就是最简单的循环神经网络结构，它的结构如下图所示。这个结构比较简单，只是在输入特征X的基础之上加入了$h_{t}$这个时间状态信息，也就是“记忆”功能。</p><p><img src="https://static001.geekbang.org/resource/image/7b/f0/7b5b14a9f68dfe7ce36d8d981269aef0.jpg?wh=2000x950" alt="" title="Simple RNN神经元结构"></p><p>不过这种结构有一个缺陷，就是会出现“短期记忆的问题”。我们知道，神经网络在训练的过程中，参数是从后面的层向前面的层反向传播的，同时还会在每一步计算梯度，做梯度下降，用于更新神经网络中的权重。</p><p>如果前一层对当前层的影响很小，那么梯度值就会很小，反之亦然。如果前一层的梯度很小，那么当前层的梯度会更小。这就使得梯度在我们反向传播时呈指数缩小。而较小的梯度意味着它不会影响权重更新。所以，对于Simple RNN来说，较早期输入的信息对预测判断的影响会比较小，这就是我们前面说的“短期记忆问题”。</p><p>对于我们这个项目的数据集来说，时间跨度比较大，Simple RNN是很难捕捉到这种长期的时间关联的。</p><p>不过，LSTM（Long Short-Term Memory，长短期记忆网络）可以很好地解决这个问题。LSTM的神经元由一个遗忘门、一个输入门、一个输出门和一个记忆细胞组成，来记录额外的信息。记忆细胞负责记住时间相关的信息，而三个门负责调节进出神经元的信息流。这个过程中的数学原理我们这里不详述，你只需了解在这个过程中，每个记忆单元可获得连续的梯度流，能学习数百个时间步长的序列而误差保持原值，从而解决梯度消失问题。</p><p><img src="https://static001.geekbang.org/resource/image/0e/a9/0e776737ea64ede63bd0e8ecd7f6e1a9.png?wh=2000x950" alt="" title="LSTM神经元结构"></p><p>所以说，LSTM网络可以弥补Simple RNN对较长时期前历史信息相对不敏感的缺陷，它也被称为“穿越时空的旅程”，它的这种结构非常适合处理时间序列数据。</p><p>那么GRU呢？它适不适合我们这个项目呢？其实，GRU也是为了解决Simple RNN的短期记忆问题，它的复杂性介于Simple RNN和LSTM之间，在结构上要比LSTM简单一些。这里你只需要理解GRU是速度和性能的折衷选择就行。</p><p><img src="https://static001.geekbang.org/resource/image/cf/9b/cfb58909c2379995800968e4f0084c9b.jpg?wh=2000x950" alt="" title="GRU神经元结构"></p><p>对于预测App激活数走势这个项目来说，如果仅从性能角度考虑，那LSTM是最理想的，所以，我们就构建一个以LSTM为主要层结构的循环神经网络：</p><pre><code>from tensorflow.keras.models import Sequential #导入序贯模型\nfrom tensorflow.keras.layers import Dense, LSTM #导入全连接层和LSTM层\n# LSTM网络架构\nRNN_LSTM = Sequential() #序贯模型\nRNN_LSTM.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1))) #输入层LSTM,return_sequences返回输出序列\nRNN_LSTM.add(LSTM(units=50, return_sequences=True)) #中间1层LSTM，return_sequences返回输出序列\nRNN_LSTM.add(LSTM(units=50, return_sequences=True)) #中间2层LSTM，return_sequences返回输出序列\nRNN_LSTM.add(LSTM(units=50)) #中间3层LSTM\nRNN_LSTM.add(Dense(units=1)) #输出层Dense\n# 编译网络\nRNN_LSTM.compile(loss=\'mean_squared_error\', #损失函数\n                 optimizer=\'rmsprop\', #优化器\n                 metrics=[\'mae\']) #评估指标\nRNN_LSTM.summary() #输出神经网络结构信息\n</code></pre><p>这个神经网络的结构如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/69/d0/69384c44a82fyy05b5abf72a2e6829d0.png?wh=478x397" alt=""></p><p>到这里呢，我们就搭建好了合适的循环神经网络模型，搭建的过程和上一讲中搭建CNN的方法非常相似。接下来，我们开始做模型训练。</p><h2>训练并评估模型</h2><p>这里我们训练50次，并在训练的同时进行80/20比例的数据验证：</p><pre><code>history = regressor.fit(X_train, y_train, epochs=50, validation_split=0.2) # 训练并保存训练历史信息\n</code></pre><p>输出如下：</p><pre><code>Epoch 1/50\n15/15 [==============================] - 11s 258ms/step - loss: 0.0813 - val_loss: 0.0640\nEpoch 2/50\n15/15 [==============================] - 2s 122ms/step - loss: 0.0101 - val_loss: 0.0511\nEpoch 3/50\n15/15 [==============================] - 2s 117ms/step - loss: 0.0125 - val_loss: 0.0064\n... ....\nEpoch 48/50\n15/15 [==============================] - 2s 118ms/step - loss: 0.0056 - val_loss: 0.0084\nEpoch 49/50\n15/15 [==============================] - 2s 121ms/step - loss: 0.0033 - val_loss: 0.0367\nEpoch 50/50\n15/15 [==============================] - 2s 119ms/step - loss: 0.0036 - val_loss: 0.0151\n</code></pre><p>由于这是一个回归值的预测问题，没有分类准确率指标，仅有损失值这个指标，我们就用<a href="https://time.geekbang.org/column/article/420372">上一讲</a>中说过的损失曲线来显示训练过程中损失值的变化：</p><p><img src="https://static001.geekbang.org/resource/image/47/e1/479a41e5a4bba451f1c7a65a246bb5e1.png?wh=376x283" alt=""></p><p>可以看到，训练50轮之后，训练集上的损失已经很小了。不过，测试集上面的损失存在着振荡上升现象，这也是过拟合的一个标志。</p><h2>利用模型进行预测</h2><p>好啦，现在模型有了，评估也结束了，现在我们就来到了最令人激动的环节，让我们用模型预测一下测试集（也就是代表着未来）的激活数吧。不过，在预测结束之后，需要用inverse_transform对预测值做反归一化。否则，激活数将是一个0-1之间的值。</p><pre><code>predicted_stock_price = regressor.predict(X_test) #预测\npredicted_stock_price = sc.inverse_transform(predicted_stock_price) #反归一化\nplot_predictions(test_set,predicted_stock_price) #绘图\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/a9/fa/a9b4e7e27d297fd33fa85f5caea67afa.png?wh=437x300" alt=""></p><p>看得出来，我们的回归曲线和实际曲线的走势是相当接近的。不过，这个模型是否有进一步优化的可能，这个问题我们下一讲继续探讨。那到这里，我们的App激活率走势预测的任务就顺利结束了。</p><h2>总结一下</h2><p>现在，我们来回顾一下重点内容。</p><p>在这一讲中， 我们介绍了时间序列数据，以及善于处理时间序列数据集的循环神经网络算法RNN。RNN和其它类型的神经网络相比，它的特点是建立了自身的记忆机制，善于根据历史信息预测后续走势。通过Keras中的API，我们轻松搭建起了一个循环神经网络对数据集，进行App激活数的预测，而这个神经网络中最主要的层是LSTM层。</p><p>还有一点请你注意，要通过历史数据预测未来是非常有难度的事情，所以，我们并不能够单纯的依赖于模型给出的结果。在实际情况中，影响未来的因子有很多，就拿股票的价格为例，一个预测系统不仅仅要有历史股价，还要结合当前市场的整体行情、新闻信息、公司财报等多方面的数据来建立模型才较为完善。但即使如此，我们仍然是很难预测走势的。像我们的App激活数的预测，会比股票价格预测简单一些，那也需要更多的产品、运营等更多信息，把这些信息和历史数据同时传递给模型，才能够训练出更有意义的模型。</p><h2>思考题</h2><p>这节课就到这里了，我给你留一个思考题：</p><p>我们说，在Keras中，一共有三种主要的循环神经网络层，分别是SimpleRNN、GRU和LSTM，我们今天使用了其中最强大的LSTM层，请你试着用其它两种解决今天的问题。</p><p>欢迎你在留言区和我分享你的观点，如果你认为这节课的内容有收获，也欢迎把它分享给你的朋友，我们下一讲再见！</p><p><img src="https://static001.geekbang.org/resource/image/d5/40/d5f7c3591c99e0fd04997ef76738e940.jpg?wh=2284x1280" alt=""></p>',
        article_title: "12｜深度学习（中）：如何用RNN预测激活率走势？",
      },
      {
        title: "13｜深度学习（下）：3招提升神经网络预测准确率",
        id: 422439,
        content:
          '<p>你好，我是黄佳。欢迎来到零基础实战机器学习。</p><p>在前面两讲中，我们创建了CNN和RNN两个常用的深度神经网络模型。今天，我们来专门谈一谈神经网络的优化，让前两讲中所创建的网络模型拥有更好的性能。</p><p>关于性能优化，我们不是第一次讲了，在<a href="https://time.geekbang.org/column/article/418354">第8讲</a>到<a href="https://time.geekbang.org/column/article/419746">第10讲</a>中，我们对应普通的机器学习模型，讲了三个阶段的性能调优：</p><ul>\n<li>在数据处理阶段，我们介绍了特征工程对于模型性能的重要性；</li>\n<li>在模型训练阶段，我们介绍了过拟合的危害以及如何避免过拟合；</li>\n<li>在测试评估阶段，我们介绍了交叉验证以及参数调优的方法。</li>\n</ul><p>其实，深度学习的性能优化和其它普通的机器学习模型类似，我们也可以从数据处理、网络模型本身，以及如何对神经网络进行参数调优这三个主要方面去考虑。</p><h2>数据方面的考量：图像数据增广</h2><p>我们已经知道，要提高模型的性能，数据方面的考量就是提升数据的“质”和“量”。那么，提升“质”，就是做好特征工程。不过你知道，神经网络对特征工程的要求是比较低的，因为神经网络本身就可以完成特征提取。</p><p>那怎么增加数据的“量”呢？我们都清楚，有标签的数据搜集起来真是太困难了，尤其是带标签的图片数据。那有没有什么方法能够在只有这么多图片的情况下，来提升我们<a href="https://time.geekbang.org/column/article/420372">第11讲</a>中那个鲜花图片分类网络的性能呢？</p><!-- [[[read_end]]] --><p>还真有。在机器学习领域，有一个专门解决这类问题的“大杀器”，叫数据增广(data augmentation)，也叫数据增强，它能通过已有的图片，增加数据量，从而提高卷积网络图像处理问题的性能，增强模型的泛化能力。</p><p>对原始的图像进行数据增广的方式有很多，比如水平翻转，还有一定程度的位移、颠倒、倾斜、裁剪、颜色抖动（color jittering）、平移、虚化或者增加噪声等，这些都比较常用。此外，我们还可以尝试多种操作的组合， 比如同时做旋转和随机尺度变换，还可以提升所有像素在HSV颜色空间中的饱和度和明度，以及在色调通道对每张图片的所有像素增加一个-0.1~0.1之间的值等。</p><p>像这样一张图片，我们通过上述“变换”，可以当成七、八张，甚至十张、百张来用。</p><p><img src="https://static001.geekbang.org/resource/image/b4/0e/b413a803e5ea9abc87d2f356c791200e.png?wh=342x342" alt="" title="针对图片的数据增广：一张变多张"></p><p>这样一来，无论是图片的数目，还是多样性，模型在训练时都能够观察到数据的更多内容，从而拥有更好的准确率和泛化能力。</p><p>在Keras中，我们可以用Image Data Generator工具来定义一个数据增广器，完整代码参见<a href="https://www.kaggle.com/tohuangjia/cnn-tuning">这里</a>。</p><pre><code># 定义一个数据增强器，并设定各种增强选项\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator #数据增强器\naugs_gen = ImageDataGenerator( #各种增强参数，具体说明可参见Keras文档\n           featurewise_center=False,\n           samplewise_center=False,         \n           featurewise_std_normalization=False,          \n           samplewise_std_normalization=False,  \n           zca_whitening=False, \n           rotation_range=10,  \n           zoom_range = 0.1, \n           width_shift_range=0.2,  \n           height_shift_range=0.2,\n           horizontal_flip=True,  \n           vertical_flip=False) \naugs_gen.fit(X_train) # 针对训练集拟合数据增强器\n</code></pre><p>现在，我们就用这个数据增广器对<a href="https://time.geekbang.org/column/article/420372">第11讲</a>中创建的卷积神经网络模型CNN进行优化，看看数据增广能给这个模型带来什么变化。CNN网络模型的结构和编译参数都和原来的模型一样，唯一的区别是在训练时，需要通过augs_gen.flow动态生成被增强后的训练集。</p><pre><code>history = cnn.fit( # 拟合   \n    augs_gen.flow(X_train,y_train,batch_size=16), # 增强后的训练集\n    epochs = 30,  # 指定轮次\n    verbose = 1) # 指定是否显示训练过程中的信息\n# show_history(history) # 调用这个函数\n</code></pre><p>由于数据增广需要时间，这次模型训练相比<a href="https://time.geekbang.org/column/article/420372">第11讲</a>中没有数据增广的训练，速度慢了很多。训练过程结束之后，我们对比一下数据增广前后的分类准确率。</p><p>增广之前：</p><pre><code>result = cnn.evaluate(X_test, y_test) #评估测试集上的准确率\nprint(\'数据增强之前CNN的测试准确率为\',&quot;{0:.2f}%&quot;.format(result[1]*100))\n</code></pre><p>输出如下：</p><pre><code>21/21 [==============================] - 0s 7ms/step - loss: 4.4355 - acc: 0.6003\n数据增强之前CNN的测试准确率为 60.03%\n</code></pre><p>增广之后：</p><pre><code>result = cnn.evaluate(X_test, y_test) #评估测试集上的准确率\nprint(\'数据增强之后CNN的测试准确率为\',&quot;{0:.2f}%&quot;.format(result[1]*100))\n</code></pre><p>输出如下：</p><pre><code>21/21 [==============================] - 0s 8ms/step - loss: 0.6121 - acc: 0.7856\n数据增强之后CNN的测试准确率为 78.56%\n</code></pre><p>可以看到，数据增广之后的测试结果更令人满意，测试误差大幅减小，准确率也从60%上升至78%以上。对于这个花朵的分类问题来说，这已经是相当不错的成绩了。</p><p>那么，数据方面的考量我们就介绍到这里，下一步，我们来看看神经网络在模型训练阶段要注意哪些问题。</p><h2>训练时的考量：防止网络过拟合</h2><p>在<a href="https://time.geekbang.org/column/article/419218">第9讲</a>的模型调优部分，我们反复强调过模型过拟合的危害，过拟合的模型会严重降低模型在其它数据集上的泛化能力。那么，神经网络会不会也出现类似的问题呢？我们仔细观察一下我在训练<a href="https://time.geekbang.org/column/article/421029">上一讲</a>中的神经网络模型时得到的这张损失曲线图片：</p><p><img src="https://static001.geekbang.org/resource/image/c8/d7/c8580a0bef726dae3728ee51bc8b78d7.png?wh=405x300" alt=""></p><p>不知道你有没有发现，神经网络在训练数据集上的损失在一直变小，直到趋近于0，达到非常高的拟合准确度。然而，验证集上的损失并不是在稳定地减小，一开始是跟着训练集的损失的减小而逐渐变小，到后来，又呈现出爬升以及大小弹跳的状态。这就意味着过拟合已经出现了，这个过程就像下面这张图显示一样：</p><p><img src="https://static001.geekbang.org/resource/image/d1/8c/d17236e233b72a04abec86942d130f8c.jpg?wh=2000x1224" alt=""></p><p>我们知道，神经网络算是一种复杂的模型，它的深度架构层次深、神经元的数量很多，参数也很多，所以神经网络才能够模拟出现实世界中各种各样的关系。</p><p>你可能会问我这样一个问题：佳哥，你说模型过于复杂，就会在训练数据集上发生过拟合，可你又说，如果神经网络模型不复杂，就解决不了复杂的问题，这不是自相矛盾了吗？</p><p>这的确是神经网络模型所面临的一个两难问题。对于小数据而言，深度神经网络由于参数数量太多，容易出现过拟合的风险。而对于神经网络这么复杂的模型来说，要避免过拟合还挺难做到的。</p><p>不过，深度学习三巨头之一的杰弗里·欣顿（Geoffrey Hinton）和他的学生发现了一种解决方式：他们把一种叫做 Dropout 的层添加到了神经网络中，将该层的一部分神经元的输出特征随机丢掉（设为0），相当于随机消灭一部分神经元：</p><p><img src="https://static001.geekbang.org/resource/image/9e/0e/9e9878cd94f7381712b2f3b2c5bc420e.jpg?wh=2000x847" alt=""></p><p>那么，为什么杀死一部分神经元就能达到避免过拟合的效果呢？其实，这就是神经网络中最常用的正则化机制，你还记得正则化的原理吧，它的目的是为了让模型粗糙一点儿，不要过分追求完美。</p><p>Hinton的这个灵感来自银行的防欺诈机制。他去银行办理业务时，发现柜员不停地换人。他就猜想，银行工作人员要想成功欺诈银行，他们之间要互相合作才行，因此一个柜员不能在同一个岗位待得过久。这让他意识到，在某些神经网络层中的各个神经元之间的参数可能也是针对训练数据集形成了某种“固定套路”，那么，随机删除一部分神经元，就有可能打破这些套路，阻止它们的“阴谋”，从而降低过拟合。</p><p>对于大型的深层神经网络来说，添加Dropout层已经是一个标准配置了。下面，我们就给<a href="https://time.geekbang.org/column/article/421029">上一讲</a>中的RNN模型增加Drop层。完整代码请见<a href="https://www.kaggle.com/tohuangjia/rnn-tuning">这里</a>。</p><pre><code>from tensorflow.keras.models import Sequential #导入序贯模型\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout #导入全连接层,LSTM层和Dropout层\nfrom tensorflow.keras.optimizers import Adam\n# LSTM网络架构\nRNN_LSTM = Sequential() #序贯模型\nRNN_LSTM.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1))) #输入层LSTM,return_sequences返回输出序列\nRNN_LSTM.add(Dropout(0.2)) #Dropout层减少过拟合\nRNN_LSTM.add(LSTM(units=50, return_sequences=True)) #中间层LSTM，return_sequences返回输出序列\nRNN_LSTM.add(Dropout(0.2)) #Dropout层减少过拟合\nRNN_LSTM.add(LSTM(units=50, return_sequences=True)) #中间层LSTM，return_sequences返回输出序列\nRNN_LSTM.add(Dropout(0.2)) #Dropout层减少过拟合\nRNN_LSTM.add(LSTM(units=50)) #中间层LSTM\nRNN_LSTM.add(Dropout(0.2)) #Dropout层减少过拟合\nRNN_LSTM.add(Dense(units=1)) #输出层Dense\n# 编译网络\nRNN_LSTM.compile(loss=\'mse\', #损失函数\n             optimizer=\'rmsprop\', #优化器\n             metrics=[\'mae\']) #评估指标\nRNN_LSTM.summary() #输出神经网络结构信息\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/a2/96/a21f1f6ac27430a4b8fea1d4fa028496.png?wh=435x367" alt=""></p><p>此处，Dropout(0.2)层中的0.2，也就是在Dropout层中会被随机丢弃掉的神经元的比例，通常设为0.2 ～ 0.5。注意，Dropout 只是对训练集起作用，在测试时没有神经元被丢掉。</p><p>接着，我们来看一下添加了Dropout层之后，RNN网络训练过程的损失曲线：</p><p><img src="https://static001.geekbang.org/resource/image/d7/bf/d76ba1e3fa08b3c09b86c2e02f60d7bf.png?wh=446x305" alt=""></p><p>添加了Dropout层之后，损失曲线显得更平滑了，尽管仍然有震荡的现象，但是震荡的幅度呈现逐渐减小的趋势，而且验证集损失也随着轮次的增多而减小。</p><p>对测试集预测后显示预测曲线如下：</p><p><img src="https://static001.geekbang.org/resource/image/e1/d0/e17bdee021e6a410b43dac9c811bc4d0.png?wh=453x310" alt=""></p><p>曲线中真值和预测值的走势非常接近。</p><p>我们可以计算一下调优前后的均方误差，并进行比较：</p><pre><code>print(&quot;调优前MSE损失值 {}.&quot;.format(mean_squared_error(y_test, y_pred)))\nprint(&quot;添加Dropout层之后的MSE损失值 {}.&quot;.format(mean_squared_error(test, y_pred_dropout)))\n</code></pre><p>此时显示模型的均方误差为：</p><pre><code>调优前MSE损失值 0.06139160691187054.\n添加Dropout层后得MSE损失值 0.034998731473164235.\n</code></pre><p>比起没有添加Dropout层的均方误差，加入Dropout层后的模型均方误差从0.06减小到了0.03。可以说，我们的性能优化是成功的。</p><p>当然，增加Dropout层并不是我们唯一可以在模型创建过程中可以调节的内容，我们还可以通过增加或者减少模型的层数、通过改变层的类型（比如用GRU或者SimpleRNN来替换LSTM）来找到对于当前数据集最为适配的网络结构，从而减小误差，优化模型的性能。不过，这个过程并没有固定原则，只能反复去尝试。</p><p>介绍完模型创建过程中的正则化技巧，下面我们接着看看，神经网络在调参时有什么需要注意的地方。</p><h2>调参时的考量：更新优化器并设置学习速率</h2><p>其实，在神经网络的训练过程中，除了存在过拟合的问题之外，还有一个很严重的问题需要解决，就是卡在“局部最低点”的问题。</p><p>和线性回归函数一样，神经网络也是通过梯度下降来实现参数的最优化，不过，有一点不同的是：线性回归函数仅存在一个全局最低点，而神经网络因为函数十分复杂，会出现很多的局部最低点，在每一个局部最低点，导数的值都为0。没有求导后的正负，梯度下降也就没有任何方向感，所以这时候，神经网络的参数也不知道应该往哪里“走”，因此，神经网络模型并不能够总是得到最佳的训练结果。</p><p><img src="https://static001.geekbang.org/resource/image/ee/9c/ee62296a4035522a76c3e1d55fc00e9c.jpg?wh=2000x847" alt="" title="全局最低点和局部最低点"></p><p><img src="https://static001.geekbang.org/resource/image/d1/6c/d174aa305e883edc5cae302b875cea6c.jpg?wh=2000x847" alt="" title="神经网络中的局部最低点"></p><p>基于这个问题，人们发明了一系列的神经网络优化器，这些优化器可以在编译神经网络时作为参数传入神经网络，解决局部最低点的问题。</p><p>具体是怎么解决的呢？我们就以SGD优化器为例。SGD优化器利用了动量的原理，也就是在梯度下降时，借助惯性越过当前的局部最低点，来寻找网络中损失更小的地方。</p><p>这个思路不难理解，你可以想象一个小球从山坡上滑下来，在下坡的过程中，遇到一个局部最低点，如果小球此时的速度很慢，就会卡在这个局部最低点上。这时候，小球无论是向左移动还是向右移动，都很难越过去，对于模型来说，无论是向左还是向右，都会导致损失值增大。</p><p><img src="https://static001.geekbang.org/resource/image/85/f8/855a4fc351127bd83a50f2821092f5f8.jpg?wh=2000x847" alt=""></p><p>这时候，我们给这个SGD优化器传入一个叫“学习速率”的参数，来调节小球的快慢（也就是梯度下降的快慢），这里你可以把“学习速率”直观理解为“加速度”。如果SGD优化器的学习速率很小，那小球就冲不出这个局部最低点，参数就无法继续优化；如果学习速率很大，它就可能帮小球成功越过局部最低点，进入下一个下坡轨道，去寻找更优的权重参数。</p><p>除了SGD优化器之外，还有许多其他优化器，比如Adagrad，它也是一种基于梯度的优化方法，叫作自适应梯度（adaptive gradient），也就是不同的参数可以拥有不同的学习速率。</p><p>Adagrad能根据前几轮迭代时的历史梯度值来调整学习速率。对于数据集中的稀疏特征来说，Adagrad会使用较大的学习速率，此时梯度下降步幅将较大。这里的稀疏特征，意思就是指类别非常多，但是每个类别的特征数量很少，一个很典型的例子就是对文本数据的词向量编码。</p><p>对于非稀疏特征，Adagrad则使用较小的值更新学习速率。因此，这个优化算法适合处理含稀疏特征的数据集，比如，在文本处理的词向量（word embedding）训练过程中，对频繁出现的单词赋予较小的更新，对不经常出现的单词则赋予较大的更新。</p><p>还有我们之前见过的RMSprop优化器，它解决的是Adagrad 中学习速率有时会急剧下降的问题。RMSProp抑制学习速率下降的方法不同于普通的动量，它是采用窗口滑动加权平均值，来计算二阶动量。同时，它还可以保存Adagrad 中每个参数自适应不同的学习速率。</p><p>另一种常见的优化器叫Adam，它是一种基于一阶和二阶矩的自适应估计的随机梯度下降方法。这种优化器计算效率高，内存需求小，是前面各种优化器的集大成者，并且非常适合数据和参数都较大的问题。</p><p>在实际的机器学习项目中，这几种优化器都可用，目前较常用的是RMSprop和Adam，但是你也可以尝试其他的，哪个效果好就用哪个。</p><p>另外，在刚才的优化器介绍中，出现了一些新名词和数学概念，你可能不太理解，这并没有关系，我们只需要从应用的角度出发，把优化器当作是调解神经网络模型的“手柄”就可以了。当然，除了我上面介绍的优化器之外，还有很多其它类型的优化器，你可以查阅<a href="https://keras.io/api/optimizers/">Keras文档</a>。</p><p>那么，问题来了，在我们构建神经网络的过程中，怎么设定这些优化器呢？答案是compile方法。当我们搭建好神经网络的架构后，需要通过这个方法进行编译。具体的代码，我们在之前CNN和RNN的实战中其实已经见过了：</p><pre><code>cnn.compile(loss=\'categorical_crossentropy\', # 损失函数\n            optimizer=\'RMSprop\'), # 更新优化器并设定学习速率\n            metrics=[\'acc\']) # 评估指标\n</code></pre><pre><code>RNN_LSTM.compile(loss=\'mse\', #损失函数\n                 optimizer=\'rmsprop\', #优化器\n                 metrics=[\'mae\']) #评估指标\n</code></pre><p>可以看到，这里有3个可以调节的参数，分别是loss、optimizer和metrics。其中，optimizer就是设置优化器，loss是用来指定损失函数的。对于多元分类问题，我们一般用categorical_crossentropy指定损失函数；对于二元分类问题，一般是用binary_crossentropy；如果是回归问题，你可以选择mse，也就是均方误差。</p><p>至于评估指标metrics，我们当然也不陌生。对于分类问题，我们可以使用acc，也就是分类准确率，作为评估指标；对于回归问题，则可以选择使用mae，即绝对平均绝对误差。</p><p>在这三个参数里，optimizer自然是和性能优化最相关的参数。很多时候，神经网络完全没有训练起来，就是优化器以及优化器的学习速率设定的不好。我们构建<a href="https://time.geekbang.org/column/article/421029">上一讲</a>中的循环神经网络模型的时候，采用了RMSprop优化器。现在，我们就在compile方法中更换一下优化器，把RMSProp更换成Adam，并同时设定一个学习速率，来看一看网络的性能是否有所提升。</p><pre><code>from tensorflow.keras.optimizers import Adam #导入Adam优化器\n……\nRNN_LSTM.compile(loss=\'mse\', # 损失函数 \n             optimizer=Adam(lr=1e-4), # 更新优化器并设定学习速率 \n             metrics=[\'mae\']) # 评估指标\nhistory = RNN_LSTM.fit(X_train,y_train, # 指定训练集 \n                   epochs=10, # 指定轮次 \n                   batch_size=256, # 指定批量大小 \n                   validation_data=(X_test,y_test)) # 指定验证集\n</code></pre><p>接着，我们来看一下更新了优化器之后，RNN网络训练过程的损失曲线：</p><p><img src="https://static001.geekbang.org/resource/image/3c/b9/3c0f5e1421ff9f7e2e8288160793f1b9.png?wh=378x281" alt=""></p><p>可以看到，训练集和验证集的损失曲线展现出了更多的一致性。</p><p>此时，我们显示模型两次调优前后的均方误差：</p><pre><code>print(&quot;调优前MSE损失值 {}.&quot;.format(mean_squared_error(y_test, y_pred)))\nprint(&quot;添加Dropout层之后的MSE损失值 {}.&quot;.format(mean_squared_error(test, y_pred_dropout)))\nprint(&quot;设置优化器后的MSE损失值 {}.&quot;.format(mean_squared_error(y_test, y_pred_optimizer)))\n</code></pre><p>输出如下：</p><pre><code>调优前MSE损失值 0.06139160691187054.\n添加Dropout层后的MSE损失值 0.034998731473164235.\n设置优化器后的MSE损失值 0.006510688077187477.\n</code></pre><p>结果显示，模型在测试集上的均方误差进一步减小了，这说明我们的优化方向是正确的。</p><p>你可能会想知道为什么Adam优化器在这里比RMSProp更优？这其实并不是很容易解释，深度学习比较强大，但是它又近似于“玄学”，可解释性相对较差。因此，对于每一个数据集，我们都需要反复尝试、调试各种参数，哪个好就用哪个。这是深度学习本身的特点。因此，优化器的选择和学习速率的设定也是如此，需要我们凭借经验和感觉来确定。总体上说，我们比较常用的优化器就是RMSprop和Adam这两种。</p><h2>总结一下</h2><p>好，今天这一讲到这里就结束了，我们来回顾一下你在这节课中学到了什么。</p><p>在这一讲中，我们介绍了三个深度学习神经网络性能优化的方法。第一个方法是针对图形数据做增广，也就是把有限的数据个数变多。其实不只是CNN网络，对于其他神经网络模型，乃至神经网络模型之外的模型来说，数据的数量都是多多益善。数据的量越大，模型出现过拟合的风险就越小。</p><p>第二个方法是在网络模型中增加一些Dropout层，这种类型的神经网络层，通过阻止各个神经元之间的共同作用，阻止它们形成固定的特征提取模式来提高神经网络的泛化能力。</p><p>第三个方法是尝试使用不同类型的神经网络优化器，来克服网络训练时落入局部最低点的问题。其中，最常用的神经网络优化器是Adam，但具体到每一个数据集来说，可能还有更适合自己的优化器，因此，我建议你在实际项目中最好还是多试试不同的优化器。就目前而言，RMSprop和Adam都是常用的优化器，而Adam 更是多种优化思路的集大成者，一般情况下是优化器的首选项。</p><p>如何优化深层神经网络的性能是一个很大的课题。希望这一讲的内容能带给大家一个基本的思路。</p><h2>思考题</h2><p>这节课就到这里了，最后，我给你留3个思考题：</p><ol>\n<li>请你试着在我们创建的LSTM网络中加入更多或者更少的Dropout层，并显示损失曲线，看一看神经网络性能的变化。</li>\n<li>请你试着在我们创建的神经网络中加入更多或者很少的CNN层或者LSTM层，并显示损失曲线，看一看神经网络性能的变化。</li>\n<li>请你试着使用其它的神经网络优化器编译我们创建的神经网络，并显示损失曲线，看一看神经网络性能的变化。</li>\n</ol><p>欢迎你在留言区分享你的想法和收获，我在留言区等你。如果这节课帮到了你，也欢迎你把这节课分享给自己的朋友。我们下一讲再见！</p><p><img src="https://static001.geekbang.org/resource/image/2a/33/2a1e83754fd18c8fedbfae24819dyy33.jpg?wh=2284x1280" alt=""></p>',
        article_title: "13｜深度学习（下）：3招提升神经网络预测准确率",
      },
      {
        title: "14｜留存分析：哪些因素会影响用户的留存率？",
        id: 423109,
        content:
          "<p>你好，我是黄佳。欢迎你来到零基础实战机器学习，今天我们正式开启留存关。</p><p>在开启这一关之前，我想给你讲一个小时候的故事。这是70年代人的回忆了，那时候，小学的校门口有两家租“小人书”摊，一毛钱看一本。小人书，就是一大堆的64开小画本，里面是西游记、水浒传、封神榜、射雕英雄传啥的。</p><p>两家的小人书其实都差不多的，那个年代也没有玄幻，也没有网络写手。可是呢，A家的生意总比B家好，我一开始不知道怎么回事，后来我想起  A家的老板记忆力很好，他总能记住我们这些孩子谁是谁，而且他每次都和我们说：“看五本送一本哈！”如果我们看了三本，他就会说：“下回再看两本就多送一本免费的。”</p><p>多年以后，我恍然大悟，原来老板在30年前就建立起了买5送一和老客户会员卡的制度。只不过他是凭借自己的记忆力来完成的。正是由于这个原因，A老板的用户留存率比较高。</p><p>什么是用户留存？留存就是让老用户一直使用你的产品。其实，谈留存，就不能不提另外一个关键概念“流失”。用户的留存和流失，是阴和阳的两极，此消彼长。收集和分析用户行为数据，有助于我们摸清用户是“留”还是“失”之间的微妙门道。</p><p><img src=\"https://static001.geekbang.org/resource/image/95/8b/955848c6394c66812c9a29d87d2d828b.jpg?wh=2284x1033\" alt=\"\"></p><p>那么，今天这节课我们就通过机器学习模型来显示易速鲜花的会员留存曲线，并且分析“易速鲜花”会员卡中设置的哪些机制，是对会员留存影响比较大的因子，哪些又容易造成用户的流失。学习了这些，你就掌握了一组非常实用的运营分析工具，这对于如何留客有很大的指导意义。</p><!-- [[[read_end]]] --><h2>定义问题</h2><p>现在，让我们回到易速鲜花公司的运营部，一起为提升“易速鲜花”的用户留存率而奋斗。易速鲜花其实已经建立起了一套完善的会员机制，来增加用户的黏性。不过，运营部门的老总不确定的是，在一系列的促销套餐和会员卡类型中，哪些因素和用户的留存关系更为密切。</p><p>你从运营那边拿到了这样的<a href=\"https://github.com/huangjia2019/geektime/tree/main/%E7%95%99%E5%AD%98%E5%85%B314\">数据集</a>：</p><p><img src=\"https://static001.geekbang.org/resource/image/c4/8a/c44ac391fb773b147bd5e80f2c3d678a.png?wh=1064x359\" alt=\"\"></p><p>在这个数据集中，有下面一系列的字段：</p><ul>\n<li>用户码；</li>\n<li>性别；</li>\n<li>一系列的会员专享套餐优惠，包括玫瑰套餐、紫罗兰套餐、郁金香套餐、百合套餐、康乃馨套餐、胡姬花套餐、生日套餐、情人节套餐等，这个是注册会员时，随机赠送给会员的；</li>\n<li>会员卡的类型；</li>\n<li>入会月数 ；</li>\n<li>会费支付方式；</li>\n<li>平均月消费；</li>\n<li>总消费；</li>\n<li>是否已停付会费。</li>\n</ul><p>我们今天的目标呢，就是从这个数据中找出，哪些因素和用户的留存关系更为密切，哪些因素可能会导致老用户的流失。而这个问题，既不是一个回归问题，也不是一个分类问题，它是一个数据探索类型的问题。</p><p>对于这个问题，首先，我们要做的仍然是数据集的导入，数据清洗，以及简单的可视化工作。</p><h2>数据的预处理</h2><h3>1. 数据导入</h3><p>数据集我们已经有了，你在<a href=\"https://github.com/huangjia2019/geektime/tree/main/%E7%95%99%E5%AD%98%E5%85%B314\">这里</a>可以下载到。现在，第一步是导入必须的库，并载入数据集。</p><pre><code>import numpy as np #导入NumPy\nimport pandas as pd #导入Pandas\ndf_member = pd.read_csv('易速鲜花会员留存.csv') #载入数据集\ndf_member.head() #显示数据头几行\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/dc/b7/dca4a5f6c3b2739ea33b7c4daa5da3b7.png?wh=1090x422\" alt=\"\"></p><p>从输出中，我们看到这个数据集一共有7043条易速鲜花会员的数据。下面我们就开始做数据清洗的工作。</p><h3>2.  数据清洗</h3><p>我们先用describe方法看一看这个数据集。</p><pre><code>df_member.describe() # 显示数据集数值字段概率\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/c2/42/c2yy5b3eb82b3a4cd38f59e6cf6bd342.png?wh=326x254\" alt=\"\"></p><p>不知道你有没有观察到一个有意思的事儿，就是这个describe方法把用户码当成了数值性数据（这也难免，因为这里的用户码是数字编号）。此外，这个表格中没有“总消费”这个字段。我推测“总消费”这个字段有点问题，它有可能看起来是数值，但是实际的格式是字符串。</p><p>我用is_numeric_dtype方法验证一下我的推测，这个方法会显示一个字段是否为数值类型。</p><pre><code>from pandas.api.types import is_numeric_dtype #导入is_numeric_dtype工具\nis_numeric_dtype(df_member['总消费']) #是否为数值字段？\n</code></pre><p>输出显示：</p><pre><code>False\n</code></pre><p>果然，“总消费”这个字段的格式有问题，目前它并不是一个数值字段，只是看起来像是数值的字符串。这里我们用to_numeric这个API对它做个转换：</p><pre><code>df_member['总消费'] = pd.to_numeric(df_member['总消费'], errors='coerce') #把总消费字段转换成数值字段\ndf_member['总消费'].fillna(0, inplace=True) #补充0值\n</code></pre><p>然后，我们再用is_numeric_dtype验证，就会得到输出结果True，这说明“总消费”字段的格式成功转换成了“数值型”。</p><h3>3.  数据可视化</h3><p>在数据可视化这个部分，我们通过饼图来看一看性别、会费支付方式、会员卡类型、已停付会费这四个字段的分布比例。这些分布情况，尤其是留存与流失的会员占比，对于我们确定当前运营的重点关注点很有指导意义。</p><pre><code>import matplotlib.pyplot as plt #导入绘图工具\nplt.figure(figsize=(10,8)) #图片大小\nplt.subplot(2, 2, 1) #子图1\nax = df_member.groupby('性别').count()['用户码'].plot.pie(autopct='%1.0f%%') #饼图1\nplt.subplot(2, 2, 2) #子图2\nax = df_member.groupby('会费支付方式').count()['用户码'].plot.pie(autopct='%1.0f%%') #饼图2\nplt.subplot(2, 2, 3) #子图3\nax = df_member.groupby('会员卡类型').count()['用户码'].plot.pie(autopct='%1.0f%%') #饼图3\nplt.subplot(2, 2, 4) #子图4\nax = df_member.groupby('已停付会费').count()['用户码'].plot.pie(autopct='%1.0f%%') #饼图4\nplt.show() #显示\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/a1/a2/a169982d2bc27669498423a3a92cb0a2.png?wh=538x449\" alt=\"\"></p><p>当然，这些信息，其实你也可以通过柱状图来显示。在上面这张图中，我希望你注意一下右下角“已停付会费”这个分布图，因为它能直接体现出我们的用户是否流失，而用户是否流失就是我们的标签。</p><p>下面，我们开始做一些特征工程，以确保所有字段都能被机器学习模型处理。</p><h3>4. 特征工程</h3><p>我们再来看看这个数据集：</p><p><img src=\"https://static001.geekbang.org/resource/image/dc/b7/dca4a5f6c3b2739ea33b7c4daa5da3b7.png?wh=1090x422\" alt=\"\"></p><p>在这个数据集中，最后一列“已停付会费”字段显示有一些用户已流失。目前这个字段的值的类型是汉字，我们需要转换为0、1值才能被模型读入。同理，我们对“性别”字段也做类似处理。</p><pre><code># 把汉字转换成0、1值\ndf_member['已停付会费'].replace(to_replace='是', value=1, inplace=True)\ndf_member['已停付会费'].replace(to_replace='否',  value=0, inplace=True)\ndf_member['性别'].replace(to_replace='女', value=0, inplace=True)\ndf_member['性别'].replace(to_replace='男', value=1, inplace=True)\n</code></pre><p>而对于各种会员套餐类型，我们把汉字“是”、“否”转换成易于机器读取的布尔类型变量。</p><pre><code># 其它的是、否字段转换成布尔型数据\nbinary_features = ['玫瑰套餐', '紫罗兰套餐', '郁金香套餐', \n                   '百合套餐', '康乃馨套餐', '胡姬花套餐', \n                   '生日套餐','情人节套餐']\nfor field in binary_features:\n    df_member[field] = df_member[field] == '是'\n</code></pre><p>这时候，数据集情况如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/03/7c/0346762e6ae2079b006159acf6d2ee7c.png?wh=1090x493\" alt=\"\"></p><p>下一步，我们要做另外一个可视化工作，就是显示会员的留存曲线，这个曲线在留存分析中非常重要，可以让我们发现用户留存相关的很多秘密，所以，我专门拉出来好好给你讲一讲。</p><h2>Kaplan-Meier生存模型：显示整体留存曲线</h2><p>绘制留存曲线，通常的做法是用Python手工绘图，这里我要向你推荐一个非常好用的工具包：lifelines（生命线库），它的优点是简单快捷，还能显示整个数据集中用户随着时间推移的留存情况。</p><p>lifelines最早是由精算师和医疗专业人员开发的，通过病人的病情和其他特征，来分析病人的生存可能性。后来，数据分析师们发现，这个模型既然能够判断哪些因子会影响病人的存活率（死亡率），那么，要是把因子换成用户行为特征和产品特征，不就也能分析出用户的留存率（流失率）吗？于是，这个工具被运营人员和数据分析师利用了起来，为分析用户留存提供了很多有用的方法。</p><p>在lifelines工具包中有一个Kaplan-Meier生存模型，它能显示整体的用户留存曲线，通过绘制不同群体的留存曲线，我们就可以观察到数据集中的各个特征对于留存的影响大小。</p><p>现在，我们回到这个项目的数据集，观察一下哪些指标与“留存时间”的关系最密切：</p><p><img src=\"https://static001.geekbang.org/resource/image/03/7c/0346762e6ae2079b006159acf6d2ee7c.png?wh=1090x493\" alt=\"\"></p><p>你看出来了吗？它们分别是最后一个字段‘已停付会费’，以及第三个字段‘入会月数’”。有了这两个指标，我们就可以计算出这些用户的生命周期长度，也就是他们大概在入会多长时间后会流失了。</p><p>首先我们要安装lifelines工具包：</p><pre><code>pip install lifelines\n</code></pre><p>下面，我们导入lifelines，用其中的Kaplan-Meier生存模型来查看普通用户随时间而变化的留存率。我们把“入会月数” 和“已停付会费”两个字段输入该模型，这个模型可以拟合用户数据，并以绘制出含置信区间的用户留存曲线。</p><pre><code>import lifelines #导入生存分析工具包\nkmf = lifelines.KaplanMeierFitter() #创建KMF模型\nkmf.fit(df_member['入会月数'], #拟合易速鲜花会员流失数据\n        event_observed=df_member['已停付会费'], \n        label='会员预期留存线')\nfig, ax = plt.subplots(figsize=(10,6)) #画布\nkmf.plot(ax=ax) #绘图\nax.set_title('Kaplan-Meier留存曲线-易速鲜花会员们') #图题\nax.set_xlabel('入会月数') #X轴标签\nax.set_ylabel('留存率(%)') #Y轴标签\nplt.show() #显示图片\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/0d/ef/0d7e570660dc58a1a4961f70c56067ef.png?wh=607x385\" alt=\"\"></p><p>你看，图中的曲线有蓝色的阴影，这个阴影就是置信区间。置信区间是一个范围，比如说有一批学生的考试成绩是从0到100分，如果我们给出一个分数的区间[55分-95分]，然后说  “该区间的置信水平为95%”，这就代表学生的考试成绩落在这个范围的概率是95%。这里的[55分-95分] 就被称做置信水平为95%的学生考试成绩的置信区间。一般来说，我们使用95%的置信水平进行区间估计。</p><p>从我们这个项目来看，考虑到图中留存率的蓝色阴影所覆盖的范围，我们可以说入会月数为20个月之后，有95%的可能，用户留存的概率在78%-82%之间。而在第70个月之后，也就是注册为会员5年之后，留存率有95%的可能在58％-64%这个概率区间。</p><p>不过，上面的留存曲线并不能给我们提供更多的用户特定分组的留存情况的对比信息，我们还需要从用户特征入手，分组绘制留存曲线，然后在同一张图中显示，这样可以帮我们挖掘出更多影响用户留存率的因素。</p><p>下面，我们就根据用户特征来绘制不同的细分留存曲线。我们先创建一个函数，让这个函数根据用户具体特征，也就是生存分析中的因子，来专门绘制留存曲线：</p><pre><code>def life_by_cat(feature, t='入会月数', event='已停付会费', df=df_member, ax=None): #定义分类留存曲线函数\n    for cat in df[feature].unique(): #遍历类别\n        idx = df[feature] == cat #当前类别\n        kmf = lifelines.KaplanMeierFitter() #创建KaplanMeier模型\n        kmf.fit(df[idx][t], event_observed=df[idx][event], label=cat) #拟合模型\n        kmf.plot(ax=ax, label=cat) #绘图\n</code></pre><p>有了这个函数，我们就可以显示用户细分类别的留存曲线，并进行对比。</p><p>下面，我们先看看会员费缴纳方式对留存的影响，操作很简单，我们只需要把这个字段传输进刚才定义的life_by_cat函数即可：</p><pre><code>fig_pmt, ax_pmt = plt.subplots(figsize=(10,6)) #画布\nlife_by_cat(feature='会费支付方式', ax=ax_pmt) #调用函数\nax_pmt.set_title('会费支付方式对留存的影响') #图题\nax_pmt.set_xlabel('入会月数') #X轴标签\nax_pmt.set_ylabel('留存率(%)') #Y轴标签\nplt.show() #显示图片\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/f7/cc/f750afe8b5224ee0fe04ea3bac47a0cc.png?wh=607x384\" alt=\"\"></p><p>可以看到，如果选择就餐时付会员费，那么流失率将大幅上升。那我们就有理由怀疑，如果要客户下次就餐时支付会员费，那会使很多用户干脆不再前来就餐。另一个对留存有负面影响的支付方式是手工转账支付会员费。而最佳的会员费支付方案是说服用户采用微信自动扣款，或通过花呗付款。</p><p>接着，我们再借助这个life_by_cat函数，来看看男生与女生的留存周期有没有显著差异。你先猜一猜，性别对于易速鲜花的会员留存率的影响，会不会像付款方式那么大？</p><pre><code>fig_gender, ax_gender = plt.subplots(figsize=(10,6)) #画布\nlife_by_cat(feature='性别', ax=ax_gender) #调用函数\nax_gender.set_title('性别对留存的影响') #图题\nax_gender.set_xlabel('入会月数') #X轴标签\nax_gender.set_ylabel('留存率(%)') #Y轴标签\nplt.show() #显示图片\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/a5/bd/a52c251520ee4f07cab8f5b5e046dfbd.png?wh=607x384\" alt=\"\"></p><p>你看，图中两条留存曲线几乎重合，这表明性别差异对易速鲜花用户留存影响并不是十分明显。想想也是，无论男生还是女生，对订购鲜花这件事情的喜好程度好像确实没有什么分别。</p><p>到这里呢，我们就通过按特征类别分组的留存曲线，显示出了哪些特征是对用户留存率是有影响的，哪些特征是没有影响的。不过，你可能还是会问：这个留存曲线展示的是全部的用户，或者至少是某一类用户的留存情况，可我的老板就是让我具体关注某几个高价值用户，那么有没有什么方法能展示出某一个人在未来10年或者20年后的留存情况呢？答案当然是有的。不过，这就要使用lifelines库中的另外一个工具，Cox危害系数模型。</p><h2>Cox危害系数模型：预测用户留存概率</h2><p>Cox危害系数模型（proportional hazards model）会分析每一个字段对于生存的影响，然后预测出某一个人大概在多久之后会“死亡”，并给出随时间延长，生存的概率值大小，也就是可能性。我们这个场景中的“死亡”，也就是指会员的流失。</p><p>所谓“危害”，你可以理解为风险因子，它用来表示分析该特征（或称为因子）是否会增加或减少生存机会，在这里，当然就是指对应特征对用户流失的影响之大小了。然后对用户个体就可以根据这些字段作为特征进行预测分析，判断未来用户留存或者流失的概率。</p><p>由于Cox危害系数模型可读取的格式是虚拟变量编码，而现在各字段的数据格式是分类编码，在调用该方法之前，我们还需要进一步做点数据的整理工作，把每个字段都规范成为Cox模型可以读取的格式。</p><h3>1. 数据预处理</h3><p>先通过Pandas中的pd.get_dummies方法，可以把多类别字段分解为多个二元类别字段，也就是之前在<a href=\"https://time.geekbang.org/column/article/418354\">第8讲</a>说过的虚拟变量。比如把“会员卡类型”字段，拆分为“会员卡类型-年卡”、“会员卡类型-月卡”、“会员卡类型-双年卡”等。这样一来，输入Cox危害系数模型的所有字段都是二元类别字段，都只有1，0（是，否）两个类别。</p><p>你可能觉得奇怪，为什么这个步骤没在之前的特征工程部分完成呢？这是因为我们希望保留“会费支付方式”、“会员卡类型”等这些分类字段来展示不同用户分组的留存曲线，而一旦划分为虚拟变量，原始字段就会消失，所以，我们现在才做这个步骤。</p><p>具体的拆解代码如下：</p><pre><code>#把分类字段转换为哑编码\ncategory_features = ['会员卡类型', '会费支付方式'] #要转换的分类的字段\ndf_member = pd.get_dummies(df_member, #哑变量的个数会比总类别数少1 \n                           drop_first=True, #这是因为drop_first=True\n                           columns=category_features) #这能够避免回归中的多重共线性问题 \ndf_member #显示数据\n</code></pre><p>在这段代码中，关于drop_first = True的含义，我们在<a href=\"https://time.geekbang.org/column/article/418354\">第8讲</a>中已经探讨过了。如果我们为分类变量的 k个级别保留 k个虚拟变量，就会存在一个级别的冗余。对于多元线性回归模型来说，这会导致数据集中的多重共线性问题，从而影响回归曲线的建模。</p><p>把多类别字段分解为多个二元类别字段后，新的数据集如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/e8/b9/e8a96f17cd70a66a3b60c2c4b2d5aeb9.png?wh=1093x404\" alt=\"\"></p><p>整理好数据后，我们开始创建模型。</p><h3>2. 创建并拟合模型</h3><p>下面的语句就创建出Cox模型。</p><pre><code>cph = lifelines.CoxPHFitter() #创建CoxPH模型\n</code></pre><p>然后，我们用数据集拟合模型：</p><pre><code>cph.fit(df_member, duration_col='入会月数', #拟合模型\n        event_col='已停付会费', show_progress=False)\n</code></pre><p>模型拟合结束后，就可以显示某一个用户的生存周期了。</p><h3>3. 显示留存概率曲线</h3><p>我们通过模型中的predict_survival_function函数，来显示对其中某一个用户的生存概率的预测：</p><pre><code>#显示会员生存曲线\ncph.predict_survival_function(df_member.loc[3]).plot()\n</code></pre><p>留存概率曲线输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/af/87/afbdfeee5bfc721d64958a66c5d61a87.png?wh=371x246\" alt=\"\"></p><p>对这个用户来说，初始的留存概率当然是1.0；在注册会员10个月之后，他留存下来的可能性降低到50%左右；在注册会员20个月之后，他留存的概率减少到了20%以下；30个月之后，这个用户就极有可能会流失，此时的留存概率几乎为0。</p><p>到这里，我们已经能够预测出用户的留存时间长短，但是怎么知道每一个指标（特征字段），对用户流失的影响有多大呢？其实，Cox危害系数模型仍然能帮到我们。</p><h2>Cox危害系数模型：分析影响留存的因子</h2><p>Cox危害系数模型的用途不仅限于预测用户留存的概率，它还可以用来挖掘各个特征字段和用户留存（也就是生命周期）的关联程度。</p><p>下面这段代码就通过可视化的方式，来显示每一个特征的危害系数及其置信区间。</p><pre><code>fig, ax = plt.subplots(figsize=(12,7)) #画布\nax.set_title('各个特征的留存相关系数') #图题\ncph.plot(ax=ax) #绘图\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/a6/2f/a62056dac085b2f6d8b7e5b9f07baf2f.png?wh=815x438\" alt=\"\"></p><p>在这个图中，大于0的系数值预示着对留存有负面影响，值越大，影响越大；而小于0的系数值则对留存有正面影响，值越小，影响越大。</p><p>可以看出，像月卡、手工转账支付会员费这些，都是潜在的流失原因。这说明，“易速鲜花”应该努力让用户办长期卡，用自动付费取代手工转账。而各种优惠套餐的危害系数都小于0，说明参与这些活动的会员留存率会有显著提高，所以，应该让会员积极参与这些活动，有利于留存。</p><p>好啦，到这里，我们就成功找到了与用户留存高度相关的因子。那么接下来，运营团队会怎么设计新的会员配套、怎么说服会员尽量签订长期合同、怎么优化付款方式……就都是他们的事儿啦。</p><h2>总结一下</h2><p>现在，我们来做个总结。今天我们主要学习了lifelines这个生存分析库，这个包中有很多用来进行留存分析的实用工具。</p><p>其中，Kaplan-Meier生存模型能帮助我们显示整体的用户留存曲线，通过绘制不同特征类型的人群的留存曲线，我们可以观察到不同特征对于留存的影响。经过观察，我们发现“易速鲜花”会员入会5年之后，整体留存率还能够在60%左右。而性别对于留存率的影响并不明显，但是会员缴纳会费的方式，则能够对留存率产生影响，这就指导我们在营销时，应该尽量说服会员在办卡时选择自动付费。</p><p>此外，我们还介绍了lifelines中的Cox危害系数模型，这个模型能帮我们预测特定用户的留存概率，并显示出数据集各个特征与留存相关的系数。这个相关系数和留存曲线一样，让我们能够发现对用户留存率产生正面和负面影响的因子。比如说给会员的各种优惠套餐配套，都对老会员的留存起到了正面作用，而有些套餐，比如郁金香套餐，胡姬花套餐等，正面作用尤为明显。</p><p>总之，这两个工具为我们做会员留存分析，提供了非常有用的指导性信息，希望你能用好它。</p><h2>思考题</h2><p>这节课就到这里了，我给你留两个思考题：</p><ol>\n<li>显示整体留存曲线时，我选择了会费支付方式和性别这两个字段，查看特征字段对留存是否有影响。那么，在这个数据集中，你还能找到哪个字段，会对留存与否产生比较大的影响呢？请你绘制这个字段的留存曲线。</li>\n<li>除了我们这里介绍的Lifelines生命线工具包，你还能找到，或者用过哪些工具，能够辅助我们找到影响用户留存的因子呢？</li>\n</ol><p>欢迎你在留言区和我分享你的观点，如果你认为这节课的内容有收获，也欢迎把它分享给你的朋友，我们下一讲再见！</p><p><img src=\"https://static001.geekbang.org/resource/image/4f/b5/4f13d2cdd2397b58184e7234365df3b5.jpg?wh=2284x1280\" alt=\"\"></p>",
        article_title: "14｜留存分析：哪些因素会影响用户的留存率？",
      },
      {
        title: "15｜二元分类：怎么预测用户是否流失？从逻辑回归到深度学习",
        id: 423893,
        content:
          '<p>你好，我是黄佳。欢迎来到零基础实战机器学习。</p><p>在上一讲中，我们用lifelines包中的工具，在“易速鲜花”的会员信息中，挖掘出了与用户流失相关比性较大的几个因素。今天，运营部门又来了新需求，我们通过这个需求，一起来看看怎么解决二元分类问题。</p><p>之前，我们接触的绝大多数业务场景都是回归场景，但是，后面更多的场景实战中，也需要分类算法大显身手，而今天这一讲将为我们解决后续诸多分类问题打下基础，它的重要性不言而喻。</p><p>好，让我们直接开始吧！</p><h2>定义问题</h2><p>这回运营部门又提出了什么需求呢？运营部门的同事是这样向你诉苦的：“要留住会员，真的是难上加难。老板要求我们啊，对于每一个流失的客户，都要打电话，给优惠来挽留，还要发一个调查问卷，收集为什么他不再续费的原因，你说这不是事后诸葛亮吗？人都走了，挽留还有什么意思呢？你们数据这块能不能给建立一个模型，预测一下哪些客户流失风险比较高，然后我们可以及时触发留客机制，你看行吗？”</p><p>以你现在对机器学习的理解，你觉得这个需求可以做到吗？当然可以。现在，让我们先来回顾一下运营部门给我们的<a href="https://github.com/huangjia2019/geektime/tree/main/%E7%95%99%E5%AD%98%E5%85%B315">这个数据集</a>。</p><p><img src="https://static001.geekbang.org/resource/image/c4/8a/c44ac391fb773b147bd5e80f2c3d678a.png?wh=1064x359" alt=""></p><p>根据运营同事的描述，我们要预测的标签就是“已停付会费”这个字段。从已有的数据中训练出模型，我们自然就可以推知其它具有类似特征的会员，“停付会费”的可能性大小。</p><!-- [[[read_end]]] --><p>既然有标签，这肯定是一个监督学习问题。再进一步，那它是回归问题，还是分类问题呢？这就要看标签是连续值还是离散值了。“是否已停付会费”这个字段的值，要么为“是”，要么为“否”，也就是非1即0，自然是离散的。所以，这是一个分类问题，而且它还是一个典型的二元分类问题。</p><p>因此，机器学习中的分类模型可以告诉我们每一个用户具体的流失风险。如果风险高，那这个用户很有可能会流失，他就需要被运营团队关注了。请你注意，这个“高风险值”是多个特征相组合显现出的结果，并不是单纯取决于某个特征。</p><p>分析到这里，我们就开始着手处理了。</p><h2>数据预处理</h2><p>对于这个问题来说，数据的读入、清洗、可视化和特征工程等工作，我们在上一讲中已经做好了：</p><p><img src="https://static001.geekbang.org/resource/image/03/7c/0346762e6ae2079b006159acf6d2ee7c.png?wh=1090x493" alt=""></p><p>所以，我们就直接来构建特征集和标签集就可以了：</p><pre><code>X = df_churn.drop([\'Churn\'], axis = 1)   # 基于df_churn构建特征集\ny = df_churn.Churn.values # 基于df_churn构建标签集\n</code></pre><p>然后，我们再来拆分一下训练集和测试集。</p><pre><code>from sklearn.model_selection import train_test_split #导入train_test_split模块\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2) #拆分数据集\n</code></pre><p>准备好训练集和测试集之后，我们直接进入算法选择的环节。</p><h2>算法选择</h2><p>我们刚才说，“预测哪些客户流失风险比较高”是一个二元分类问题。那么，能够解决这种分类问题的算法又有哪些呢？在<a href="https://time.geekbang.org/column/article/417479">第7讲</a>中，我们在做回归分析时，介绍了很多回归算法。其实，和回归类似，机器学习中能够用来解决分类问题的算法也非常多，我把比较常用的分类算法整理在下面这个表中：</p><p><img src="https://static001.geekbang.org/resource/image/33/23/3340aa5b75faa25dd717f288f1614f23.jpg?wh=2000x1141" alt=""></p><p>我们说过，在解决具体问题的时候，我们通常会选择多种算法进行建模，相互比较之后，再确定比较适合的模型。不过，在这一讲中，我们不太可能把上面所有的模型都挨个讲解和尝试一遍，所以，我就挑两个没讲过的算法：逻辑回归和神经网络，来带你解决这个问题。如果你对其他模型的效果很感兴趣，可以自行做个尝试。</p><p>我们先来看逻辑回归算法。</p><h2>用逻辑回归解决二元分类问题</h2><p>逻辑回归是最为基础的分类算法，它在分类算法中的地位和和线性回归在回归算法中的地位一样，也常常作为基准算法，其它算法的结果可以与逻辑回归算法进行比较。</p><p>其实，逻辑回归的本质仍然是线性回归，这也是为什么它的名字中仍然保留了“回归”二字。只不过，在线性回归算法的基础之上，它又增加了一个Sigmoid函数。</p><p><img src="https://static001.geekbang.org/resource/image/0a/df/0a4d5bdb93yy537e32f8f99067c199df.png?wh=768x209" alt=""></p><p>这个函数的作用是什么呢？它其实是在线性回归算法的预测值基础上，把预测值做一个非线性的转换，也就是转换成0~1区间的一个值。而这个值，就是逻辑回归算法预测出的分类概率。这个过程你可以参考下面的Sigmoid函数图像以及它的公式。</p><p><img src="https://static001.geekbang.org/resource/image/58/1b/58f1c7800c6d30678b9779ea8e3d761b.jpg?wh=2000x965" alt="" title="Sigmiod函数的图像和公式"></p><p>我们以会员是否流失为例，如果Sigmoid函数转化之后的预测结果值为0.7，就说明流失的概率大于0.5，可以认为该会员可能会流失。如果Sigmoid函数转化之后的预测结果值为0.4，就说明该会员留存的可能性比较高。对于其它的二元分类判断，比如病患是否患病、客户是否存在欺诈行为等等，都是同样的道理。</p><p>明白了这一点后，我们就用逻辑回归算法来预测一下“易速鲜花”的哪些客户流失风险比较高。首先，我们导入逻辑回归算法，并创建逻辑回归模型，我把模型命名为logreg（即LogisticRegression的缩写）：</p><pre><code>from sklearn.linear_model import LogisticRegression #导入逻辑回归模型\nlogreg = LogisticRegression() # lr,就代表是逻辑回归模型\n</code></pre><p>然后，我们通过fit方法，开展对机器的训练：</p><pre><code>logreg.fit(X_train,y_train) #拟合模型\n</code></pre><p>模型拟合好之后，我们就可以对模型的分数进行评估了。</p><pre><code>print(&quot;SK-learn逻辑回归测试准确率{:.2f}%&quot;.format(logreg.score(X_test,y_test)*100))   #模型分数\n</code></pre><p>注意，这里的score方法给出的是预测准确率的均值。</p><p>输入如下：</p><pre><code>SK-learn逻辑回归测试准确率78.70%\n</code></pre><p>结果显示，这个逻辑回归模型在测试集上的准确率为78.70%。</p><p>最后，我们用这个模型来预测具体的用户是否会流失，我们选择测试集的第一个用户查看结果。</p><pre><code>y_pred = logreg.predict(X_test) #对测试集进行预测\nprint(&quot;测试集第一个用户预测结果&quot;, y_pred[0]) #第一个用户预测结果\n</code></pre><p>输出如下：</p><pre><code>测试集第一个用户预测结果 0\n</code></pre><p>我们得到的结果是0，说明这个逻辑回归模型判断第一个用户并不会流失，这个预测结果与真值一致。</p><p>这个模型看起来好像还不错，我们是不是可以把它应用于会员流失风险的评估呢？先别急，我们再来看看神经网络模型的表现如何。</p><h2>用神经网络解决二元分类问题</h2><p>你可能还在奇怪，我们已经用神经网络模型解决过问题了，为什么这里还要选择它呢？没错，在<a href="https://time.geekbang.org/column/article/420372">第11讲</a>和<a href="https://time.geekbang.org/column/article/421029">第12讲</a>中，我们用CNN完成了图像识别，用RNN完成了点击量的预测。不过，我们还没有使用普通的神经网络模型来解决分类问题。</p><p>相对于逻辑回归以及其它的分类算法，神经网络适合解决特征数量比较多、数据集样本数量庞大的分类问题，因为神经网络结构复杂，它的拟合能力当然也就比较强。所以，神经网络是我们解决分类问题时一个不错的选择。</p><p>怎么理解呢？这要从最早的神经网络说起。1958年，Rosenblatt提出了一种一元的“感知器”（Perceptron）模型，这是一种单个神经元的神经网络，也是最基本、最简单的神经网络，它的结构如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/1f/f7/1ff7808c6e6cde196e7e0f9ce4f9a0f7.jpg?wh=2000x884" alt="" title="一元的感知器和逻辑回归模型完全相同"></p><p>你看，这种一元的感知器从整体上是先接收特征的输入，然后做线性回归，进而通过Sigmoid函数求出分类概率，最后转化为分类值。 它实际上就是一个逻辑回归模型。所以，你可以认为，<strong>最基本、最简单的神经网络就是逻辑回归模型</strong><strong>。</strong></p><p>当然，现代神经网络已经演化成了具有很多层的深度学习网络，每一层也有非常多的类型，它们能够解决的问题也就更复杂了。但是，无论是浅层神经网络，还是深层神经网络，它们的网络结构都可以细分并简化为多个“一元感知器”，所以，现代的神经网络也都能够很好地解决二元分类问题。</p><p>现在回到我们的项目，请你思考一个问题，既然我们要预测一下“易速鲜花”的哪些客户流失风险比较高，那什么样的神经网络模型比较合适呢？我们之前学过CNN和RNN，它们合适吗？我们说，CNN网络主要用来处理图形图像等计算机视觉问题，RNN网络主要是处理自然语言、文字和时序问题。而我们当前这个数据具有很好的特征结构，它不是图片、文本，也不是时序数据，因此，它不需要CNN，也不需要RNN。</p><p>在这里呢，我们其实用普通的Dense层，也就是密集连接层，来搭建神经网络就可以了。Dense层是最普通的全连接网络层，因为它其中既没有卷积，也没有循环。而这样的神经网络我们叫它DNN。DNN网络非常适合解决分类问题，尤其是特征比较多的情况。</p><p>那DNN的网络结构是什么样的呢？别着急，我们先把这个模型构建起来，再一探究竟。由于数据集不大，这次实战并不需要GPU的出场，所以，我们直接在Jupyter Notebook上跑神经网络模型就可以了。</p><p>首先，我们安装Keras和Tensorflow这两个神经网络框架：</p><pre><code>!pip install keras\n!pip install tensorflow\n</code></pre><p>安装过程输出如下：</p><pre><code>Collecting keras\n......\nSuccessfully installed keras 2.6.0\n......\nCollecting tensorflow\nDownloading tensorflow-2.6.0-cp38-cp38-win_amd64.whl (423.2 MB)\n......\nSuccessfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.\n......\n</code></pre><p>下面就开始搭建DNN神经网络模型：</p><pre><code>import keras # 导入Keras库\nfrom keras.models import Sequential # 导入Keras序贯模型\nfrom keras.layers import Dense # 导入Keras密集连接层\ndnn = Sequential() # 创建一个序贯DNN模型\ndnn.add(Dense(units=12, input_dim=17, activation = \'relu\')) # 添加输入层\ndnn.add(Dense(units=24, activation = \'relu\')) # 添加隐层\ndnn.add(Dense(units=1, activation = \'sigmoid\')) # 添加输出层\ndnn.summary() # 显示网络模型（这个语句不是必须的）\n# 编译神经网络，指定优化器，损失函数，以及评估标准\ndnn.compile(optimizer = \'RMSProp\', #优化器\n            loss = \'binary_crossentropy\', #损失函数\n            metrics = [\'acc\']) #评估标准\n</code></pre><p>DNN神经网络的结构输出如下：</p><pre><code>Model: &quot;sequential&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 12)                228       \n_________________________________________________________________\ndense_1 (Dense)              (None, 24)                312       \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 25        \n=================================================================\nTotal params: 565\nTrainable params: 565\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre><p>可以看到，这个DNN神经网络的结构和我们之前见过的CNN和RNN大同小异，唯一的区别就在于，我们这里使用的是Dense层。</p><p>另外，在编译时，损失函数、优化器和评估指标的选择也不太一样。其中，我们把损失函数指定为binary_crossentropy，这是专门用来二分类问题的损失函数；而对于优化器，我们选择的是RMSProp；评估指标则是Accuracy（分类准确率）。</p><p>如果你仔细看上面搭建DNN网络模型的代码，可能会留意到：前两个Dense层的activation值为relu，而最后一层是sigmoid。这是什么意思呢？</p><p>其实，这里的activation就是神经网络的激活函数。激活函数在神经网络中是用来引入非线性因素的，目的是提升模型的表达能力。否则的话，各层神经元之间只存在线性关系，这种模型的表达能力就不够强，不能覆盖更复杂的特征空间，因此神经网络的每一个神经元，在向下一层网络输出特征时都需要用激活函数进行激活。</p><p>在早期的神经网络中，神经元全都是使用sigmoid函数作为激活函数的，后来又出现了和sigmoid函数类似的tanh函数。不过，人们发现，当输入的特征值较大时，tanh函数的梯度（导数）接近于零，这时参数几乎不再更新，梯度的反向传播过程将被中断，可能会出现梯度消失的现象，而这会影响神经网络的性能。</p><p>这里所说的“反向传播”，就是指在神经网络的梯度下降中，从后面的层向前面层传播的过程（神经网络中，既有从前面层向后面层的传播，也有从后面的层向先前面层的传播）。</p><p><img src="https://static001.geekbang.org/resource/image/59/bd/597a475218f23a9aea9a4350d65c55bd.jpg?wh=2323x1071" alt="" title="常用的神经网络激活函数的函数图像"></p><p>再后来，人们发现了一个更好的神经元激活函数：ReLU函数。它实现起来非常简单，不仅加速了梯度下降的收敛过程，而且还没有饱和问题，这大大缓解了梯度消失的现象。</p><p>不过，ReLU函数也不是完全没有缺点，在某些情况下，如果参数的权重都处于负值区间，ReLU函数对损失函数的导数可能永远为零。这个神经元将永远不参与整个模型的学习过程，等同于“死掉”。</p><p>为了解决这个问题，人们又发明了Leaky ReLU、eLu、PreLu、Parametric ReLU、Randomized ReLU等变体，为ReLU函数在负区间赋予一定的斜率，从而让它的导数不为零。不过，对于我们初学者来说，不需要深入掌握这些变体的区别和用法，我们一般使用ReLU函数激活就可以了，它也是目前普通神经网络中最常用的激活函数。</p><p>上面所说的激活过程，只是针对于神经网络内部的神经元而言的。而对于神经网络的输出层来说，激活函数的作用就只是确定分类概率了。</p><p>我们知道，概率必须是一个0~1之间的值，这时候，ReLU等函数就无法发挥作用了。所以，如果是二元分类问题，我们在神经网络的输出层中会使用sigmoid函数来进行分类激活；如果是多元分类问题，我们则使用softmax函数进行分类激活。</p><p>搞清楚了DNN网络模型的结构后，现在我们开始进行神经网络的训练。不过，在开始训练之前，我们要做一下格式的转换，把Dataframe格式的对象转换为NumPy张量。关于张量，我们在<a href="https://time.geekbang.org/column/article/420372">第11讲</a>中介绍过，这里就不重复了。</p><pre><code>X_train = np.asarray(X_train).astype(np.float32) # 转换为NumPy张量\nX_test = np.asarray(X_test).astype(np.float32) # 转换为NumPy张量\nhistory = dnn.fit(X_train, y_train, # 指定训练集\n              epochs=30,        # 指定训练的轮次\n              batch_size=64,    # 指定数据批量\n              validation_split=0.2) #这里直接从训练集数据中拆分验证集，更方便\n</code></pre><p>训练过程输出如下：</p><pre><code>Epoch 1/30\n71/71 [==============================] - 1s 6ms/step - loss: 4.1202 - acc: 0.6761 - val_loss: 2.9610 - val_acc: 0.4632\nEpoch 2/30\n71/71 [==============================] - 0s 3ms/step - loss: 1.1665 - acc: 0.7182 - val_loss: 0.8802 - val_acc: 0.6016\nEpoch 3/30\n71/71 [==============================] - 0s 2ms/step - loss: 1.1551 - acc: 0.7087 - val_loss: 2.1645 - val_acc: 0.7773\n......\nEpoch 29/30\n71/71 [==============================] - 0s 3ms/step - loss: 0.8423 - acc: 0.7495 - val_loss: 1.3655 - val_acc: 0.7862\nEpoch 30/30\n71/71 [==============================] - 0s 3ms/step - loss: 0.8477 - acc: 0.7404 - val_loss: 1.1125 - val_acc: 0.7977\n</code></pre><p>这个训练信息不够直观，我们要做个处理。不知道你记不记得在<a href="https://time.geekbang.org/column/article/420372">第11讲</a>中，我们介绍过显示损失曲线和准确率曲线的方法，现在我们就用这个方法看一看在上述训练过程中，随着梯度的下降和模型的拟合，损失和准确率在训练集和验证集上的变化情况：</p><pre><code>def show_history(history): # 显示训练过程中的学习曲线\n    loss = history.history[\'loss\'] #训练损失\n    val_loss = history.history[\'val_loss\'] #验证损失\n    epochs = range(1, len(loss) + 1) #训练轮次\n    plt.figure(figsize=(12,4)) # 图片大小\n    plt.subplot(1, 2, 1) #子图1\n    plt.plot(epochs, loss, \'bo\', label=\'Training loss\') #训练损失\n    plt.plot(epochs, val_loss, \'b\', label=\'Validation loss\') #验证损失\n    plt.title(\'Training and validation loss\') #图题\n    plt.xlabel(\'Epochs\') #X轴文字\n    plt.ylabel(\'Loss\') #Y轴文字\n    plt.legend() #图例\n    acc = history.history[\'acc\'] #训练准确率\n    val_acc = history.history[\'val_acc\'] #验证准确率\n    plt.subplot(1, 2, 2) #子图2\n    plt.plot(epochs, acc, \'bo\', label=\'Training acc\') #训练准确率\n    plt.plot(epochs, val_acc, \'b\', label=\'Validation acc\') #验证准确率\n    plt.title(\'Training and validation accuracy\') #图题\n    plt.xlabel(\'Epochs\') #X轴文字\n    plt.ylabel(\'Accuracy\') #Y轴文字\n    plt.legend() #图例\n    plt.show() #绘图\nshow_history(history) # 调用这个函数\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/60/2a/6081c7da04yy83e55e3dddd7ea13ee2a.png?wh=718x273" alt="" title="训练集曲线平滑，但验证集损失曲线和准确率曲线有振荡的情况"></p><p>从图中可见，训练集的损失逐渐下降，准确率逐渐提升。但是，验证集的曲线不那么漂亮，有振荡的情况，这种情况意味着网络没有训练起来。接下来，我们还是看一看它在测试集上的准确率是否理想吧：</p><pre><code>result = dnn.evaluate(X_test, y_test) #评估测试集上的准确率\nprint(\'DNN的测试准确率为\',&quot;{0:.2f}%&quot;.format(result[1])*100)\n</code></pre><p>输出如下：</p><pre><code>45/45 [==============================] - 0s 1ms/step - loss: 1.0171 - acc: 0.7658\nDNN的测试准确率为 77%\n</code></pre><p>再看看第一个测试集用户的预测结果：</p><pre><code>prediction = dnn.predict(X_test) #预测测试集的图片分类\nprint(\'第一个用户分类结果为:\', np.argmax(prediction[0]))\n</code></pre><p>输出如下：</p><pre><code>第一个用户分类结果为: 0\n</code></pre><p>0值代表客户未流失，说明这个神经网络模型预测该用户并不会流失，与真值相符。</p><p>那这个模型到底满不满足我们的需求呢？其实，我们还是不能确定。这个问题我们暂且放在一边，后续再做探讨。</p><p>现在，我想请你思考一下，DNN神经网络模型在测试集上的预测准确率达到了77%，表面上还可以。但是，损失曲线和准确率曲线图却显示，这个模型的损失和准确率都出现了很大的振荡波动，时好时坏。这又是什么原因呢？</p><h2>归一化之后重新训练神经网络</h2><p>其实，这种振荡现象的出现是数据所造成的。我们之前说过，神经网络非常不喜欢未经归一化的数据，因此，对于神经网络来说，我们前面对这个数据集做预处理时，可能缺少了一个环节，就是归一化。</p><p>下面，我们就把应该做的对X特征集的归一化工作给补上：</p><pre><code>from sklearn.preprocessing import MinMaxScaler #导入归一化缩放器\nscaler = MinMaxScaler() #创建归一化缩放器\nX_train = scaler.fit_transform(X_train) #拟合并转换训练集数据\nX_test = scaler.transform(X_test) #转换测试集数据\n</code></pre><p>然后，我们仍然用同样DNN神经网络训练数据，并绘制损失曲线和准确率曲线：</p><pre><code>history = dnn.fit(X_train, y_train, # 指定训练集\n                  epochs=30,        # 指定训练的轮次\n                  batch_size=64,    # 指定数据批量\n                  validation_split=0.2) #指定验证集,这里为了简化模型，直接用训练集数据\nshow_history(history) # 调用这个函数\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/1a/7c/1ab4e30d68148151abba8209db2cfd7c.png?wh=723x273" alt="" title="归一化之后，损失曲线和准确率曲线变得平滑"></p><p>结果显示，振荡现象消失了，曲线的变得平滑了很多，这是神经网络能够正常训练起来的一种表现：</p><p>最后，我们看一下新的神经网络模型的测试准确率。</p><pre><code>result = dnn.evaluate(X_test, y_test) #评估测试集上的准确率\nprint(\'DNN（归一化之后）的测试准确率为\',&quot;{0:.2f}%&quot;.format(result[1])*100)\n</code></pre><p>输出如下：</p><pre><code>45/45 [==============================] - 0s 2ms/step - loss: 78.6179 - acc: 0.7800\nDNN（归一化之后）的测试准确率为 78%\n</code></pre><p>可以看到，归一化后的测试准确率为78%，比起刚才的77%，基本没什么差别。</p><p>到这里，我们针对“预测哪些客户流失风险比较高”这个任务，产出了三个模型：逻辑回归模型、未做归一化的神经网络模型DNN，以及归一化之后的神经网络模型DNN。那这三种模型是不是都符合我们的需求呢？哪一种模型更好呢？你可以思考一下，在下一讲中，我会为你揭晓答案。</p><h2>总结一下</h2><p>这节课，我们用逻辑回归和深度学习神经网络预测了“易速鲜花”的会员是否会流失，这是一个典型的二元分类问题。二元分类是很常见的一类监督学习问题，能够用于解决二元分类问题的算法也非常多，包括逻辑回归、朴素贝叶斯、KNN等等。</p><p>逻辑回归是解决二元分类问题最简单的方法，它的实现也比较简单，就是从sklearn中导入、创建并拟合逻辑回归模型，方法与我们在前面几关中的步骤完全相同。</p><p>值得一提的是，在解决二元分类问题时，我们可以通过logreg.predict()函数来预测分类的值，也可以通过logreg.predict_proba()函数来输出分类的概率，概率越高，模型就认为归为该类可能性越大。举例来说，0.51和0.99两个概率值，模型都预测用户会流失，但是有多大的信心？明显概率为0.99的高得多。</p><p>在用神经网络解决二元分类问题时，我们选择了最普通的神经网络模型DNN，它的创建和训练过程与我们在<a href="https://time.geekbang.org/column/article/420372">第11讲</a>到<a href="https://time.geekbang.org/column/article/422439">第13讲</a>中的步骤并无区别。由于神经网络模型的predict()函数输出的是概率，如果你需要手工进行分类转化，可以用np.argmax函数来完成，这个步骤我们在<a href="https://time.geekbang.org/column/article/420372">第11讲</a>也介绍过，你如果不太清楚，可以再复习一下。</p><p>最后，我还想强调一点，通过这一讲，我们已经看到，对于神经网络的输入张量，如果不做归一化，就会影响神经网络的训练效果。因此，我希望你能明白为神经网络输入张量做归一化的重要性。</p><h2>思考题</h2><p>好，这节课就到这里，我给你留两道思考题：</p><ol>\n<li>除了逻辑回归和神经网络之外，我们还列出了不少分类算法，你能否尝试使用其它分类算法来解决这个问题？</li>\n<li>请你试着调整DNN神经网络的结构（增加减少层和神经元的个数）、调整编译时的各参数，或者增加减少训练的轮次等，看一看有什么发现。</li>\n</ol><p>欢迎你在留言区和我分享你的观点，如果你认为这节课的内容有收获，也欢迎把它分享给你的朋友，我们下一讲再见！</p><p><img src="https://static001.geekbang.org/resource/image/55/42/55cbce594de6e275f6dce9ddafce6b42.jpg?wh=2284x1280" alt=""></p>',
        article_title:
          "15｜二元分类：怎么预测用户是否流失？从逻辑回归到深度学习",
      },
      {
        title: "16｜性能评估：不平衡数据集应该使用何种评估指标？",
        id: 424124,
        content:
          '<p>你好，我是黄佳。欢迎来到零基础实战机器学习。</p><p>上一讲中，我们通过逻辑回归和深度学习神经网络两种模型，判断了会员流失的可能性，准确率大概在78%左右。我想考一考你，这个准确率是否能够反映出模型的分类性能？</p><p>也许你会回答，看起来没什么问题啊。但是，如果我告诉你，对于这个数据集来说，即使不用任何机器学习模型，我闭着眼睛也能够达到70%以上的预测准确率。你会不会吓一跳，说，这怎么可能呢？</p><p>其实，如果你仔细观察一下这个数据集已经流失和留存下来的会员比例，就会发现，在这个数据集中，留下的会员是73%，而已经离开的会员占27%。</p><p><img src="https://static001.geekbang.org/resource/image/1c/72/1ca4d96d0bc5bf49a2bdyy6883028672.jpg?wh=2000x1193" alt="" title="流失与否？"></p><p>这也就是说，如果我直接提出一个模型，<strong>判断所有的会员都会留存，<strong><strong>那</strong></strong>我这个模型的预测准确率就是73%</strong>。所以说，要达到70%以上的预测准确率，真的是没有什么难度。</p><p>我再举一个极端一点的例子，在银行客户欺诈行为的检测系统中，存在欺诈行为的客户可能不到万分之一。那么，一个模型只要预测所有的客户都没有欺诈行为，这个模型的准确率就能达99.999%。</p><p>然而，这样的模型没有任何意义。因为<strong>我们的目标不是判断出这9999个正常客户，而是要想法设法找出那万分之一的异常客户。</strong>所以，对于我们这个问题来说，如何精确定位那23%的可能流失的客户，才是关键所在。</p><!-- [[[read_end]]] --><p>因此，<strong>评估分类模型要比评估回归模型的难度大很多，评估的方法也更为多样化，尤其是对于各类别中样本数量并不平衡的数据集来说，<strong><strong>我们</strong></strong>绝对不能单用准确率这一个方面作为考量的标准</strong>。</p><p>那么，什么指标才更合适呢？这里，我给你介绍四个重要的分类评估方法。这四个分类评估方法和分类准确率一样，可以广泛适用于几乎所有的分类问题，尤其是对于样本类别不平衡的问题来说，这些方法比准确率更为客观。</p><h1>混淆矩阵</h1><p>在认识第一种评估方法“混淆矩阵”（Confusion Matrix）之前，我们先来看一个例子。假设“易速鲜花”一共有100个会员（举个例子而已），其中73个留存了下来，27个流失了。那么，我们就可以这样表示这个数据集的真值：</p><p><img src="https://static001.geekbang.org/resource/image/d3/9f/d3b2cc3d6f2db186e44a3af9e40ef69f.jpg?wh=2000x455" alt=""></p><p>如果这时候有一个模型A，它的预测结果是77个留存，23个流失。那么，上面这张表就会变成这样：</p><p><img src="https://static001.geekbang.org/resource/image/57/yy/578731a274e6f5a609105f76da9705yy.jpg?wh=2000x562" alt=""></p><p>要想知道这个模型A预测得准不准，我们就要看一下在这77个留存用户和23个流失用户中，有多少是预测正确的，多少是错误的。但是，在现有的表格中，我们并不能了解到。所以现在，我们不妨引入这样一个矩阵：</p><p><img src="https://static001.geekbang.org/resource/image/ff/ee/ff82061b7f6b762d38668577df6123ee.jpg?wh=2000x972" alt=""></p><p>这是一个由预测值和真值共同组成的矩阵，四个象限从上到下，从左到右分别为：</p><ul>\n<li><strong>真负</strong>：真值为负、预测为负，即True Negative，缩写为TN；</li>\n<li><strong>假正</strong>：真值为负，预测为正，即False Positive，缩写为FP；</li>\n<li><strong>假负</strong>：真值为正，预测为负，即False Negative，缩写为FN；</li>\n<li><strong>真正</strong>：真值为正，预测为正，即True Positive，缩写为TP。</li>\n</ul><p>这个矩阵就是混淆矩阵，这里的“真”“假”就代表实际值（真值）和预测值一致与否；而“正”“负”就代表预测出来的值是1还是0。所以，对于“预测用户是留存还是流失”这个问题来说：</p><ul>\n<li>真负(TN)，代表被模型判断为留存的留存用户数：70；</li>\n<li>假正(FP)，代表被模型判断为流失的留存用户数：3；</li>\n<li>假负(FN)，代表被模型判断为留存的流失用户数： 7；</li>\n<li>真正(TP)，代表被模型判断为流失的流失用户数： 20。</li>\n</ul><p>这样矩阵就能反应出模型预测的真实情况了。对于模型A来说，在73个留存用户中，它预测对了70个；在27个流失用户中，它预测对了20个。所以，它整体的准确率就是：</p><p>$$模型A的整体准确率=\\frac{70+20}{100}=90\\%$$</p><p>当然，对于我们<a href="https://time.geekbang.org/column/article/423893">上一讲</a>的项目来说，<strong>在这个矩阵中，我们最看重的应该是“真正”这个象限的数字，因为它代表了模型找出了多少个即将流失的用户。只有这个数字，才能对“易速鲜花”的运营状况产生正面促进。</strong></p><p>这时候，如果还有另一个模型B，它的预测结果和模型A的一样，也是77个留存用户，23个流失用户，那我想，你应该不会轻易认为模型B的准确率和模型A的一样了，因为它的混淆矩阵很可能是这样的：</p><p><img src="https://static001.geekbang.org/resource/image/7a/52/7a0bc49f9ee3a6da6c79ea17a7c81052.jpg?wh=2000x972" alt=""></p><p>在75个留存用户中，模型B预测准了55个，还算勉强可以。但是对于23个流失客户来说，模型B只预测出5个。所以，这个模型B预测的整体准确率为：</p><p>$$模型B的整体准确率=\\frac{55+5}{100}=60\\%$$</p><p>由于我们真正的目的是找到流失客户，从这个目的来讲，模型B的准确率还不到20%呢。</p><p>到这里，我想你应该感受到混淆矩阵的威力了。那对于<a href="https://time.geekbang.org/column/article/423893">上一讲</a>里“预测哪些客户流失风险比较高”这个项目，我们就可以借助混淆矩阵，来评估三个模型（逻辑回归模型、未做归一化的DNN神经网络，以及归一化之后的DNN神经网络）的优劣了。</p><p>要计算这三个模型的混淆矩阵，我们可以用sklearn中的confusion_matrix工具。不过，我们还需要定义一个用来显示混淆矩阵的函数，我把它命名为show_matrix（这里我们不重复数据导入和预处理以及模型训练的代码），完整代码请在<a href="https://github.com/huangjia2019/geektime/tree/main/%E7%95%99%E5%AD%98%E5%85%B316">这里</a>下载：</p><pre><code>from sklearn.metrics import confusion_matrix # 导入混淆矩阵\nimport seaborn as sns #导入seaborn画图工具箱\ndef show_matrix(y_test, y_pred): # 定义一个函数显示混淆矩阵\n    cm = confusion_matrix(y_test,y_pred) # 调用混淆矩阵\n    plt.title(&quot;混淆矩阵&quot;) # 标题\n    sns.heatmap(cm,annot=True,cmap=&quot;Blues&quot;,fmt=&quot;d&quot;,cbar=False) # 热力图\n    plt.show() # 显示混淆矩阵\n</code></pre><p>然后，我们调用show_matrix函数，先来显示逻辑回归模型的混淆矩阵：</p><pre><code>show_matrix(y_test, y_pred) # 逻辑回归\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/3a/d4/3ab52cd01b3c1db9966cae2ca823a6d4.jpg?wh=2000x1000" alt=""></p><p>接着，我们再显示DNN神经网络做归一化之前和归一化之后的混淆矩阵。注意，<strong>我这样做目的，是为了向你展示，两个分类准确率看起来近似的模型，它们的混淆矩阵内容可能会很不一样哦！</strong></p><p>此外，我还要说明一点，我们在这一讲中跑出来的预测结果，很可能和上一讲中的预测结果不完全一致。因为神经网络每次训练时是随机拆分数据集的，而且梯度下降的随机性和局部最低点也将导致每次的模型结果会有所不同。</p><pre><code>y_pred = ann.predict(X_test,batch_size=10) # 预测测试集的标签\ny_pred = np.round(y_pred) # 将分类概率值转换成0/1整数值\nshow_matrix(y_test, y_pred) #神经网络\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/72/4e/72c50c784861024bf5ff2418e32f6f4e.jpg?wh=2000x1025" alt=""><br>\n<img src="https://static001.geekbang.org/resource/image/65/53/65af36cf08a5eb7a82527d242b505a53.jpg?wh=2000x1025" alt=""></p><p>有了这几个混淆矩阵之后，我们才能够真正看出每个模型的优劣。请你注意，我们第一眼要看的是右下角的“真正”值，也就是模型到底抓出来了多少“真的可能会流失”的付费会员。</p><p>结果显示，在361个流失会员中，逻辑回归模型抓出了171人，DNN神经网络归一化之后的模型抓出了199人（这其实不错了），而未做归一化的DNN神经网络仅仅抓到了50人。哪个模型更靠谱？我想你心里已经有答案了。</p><p>到这里，你应该更加理解为什么整体准确率不足以反映出模型的真正分类情况。不过，你可能会问，混淆矩阵虽然直观，但它不是一个分数性的指标，如果我老板就是喜欢像“准确率”这样的分数型指标，怎么办？</p><p>很简单，我们可以在混淆矩阵的基础之上，引入“精确率”（也叫查准率）和“召回率”（也叫查全率）两个指标。</p><h2>精确率和召回率</h2><p>我们先说精确率。精确率也叫查准率，它的意义是：<strong>在被分为正例的示例中，实际为正例的比例</strong>。我们可以这样计算精确率：</p><p>$$精确率（Precision） = \\frac{被模型预测为正的正样本}{(被模型预测为正的正样本 + 被模型预测为正的负样本)}$$</p><p>其中，“被模型预测为正的正样本”就是混淆矩阵中的“真正”，也就是TP；“被模型预测为正的负样本”则是混淆矩阵中的“假正”，也就是FP  。所以，这个公式就是：</p><p>$$精确率（Precision） = \\frac{TP}{(TP + FP)}$$</p><p>现在，我们拿刚才这个DNN神经网络归一化后的混淆矩阵为例：</p><p><img src="https://static001.geekbang.org/resource/image/65/53/65af36cf08a5eb7a82527d242b505a53.jpg?wh=2000x1025" alt=""></p><p>就流失的用户而言：</p><ul>\n<li>真负(TN)：被模型判断为留存的留存用户数：899;</li>\n<li>假正(FP)：被模型判断为流失的留存用户数：149;</li>\n<li>假负(FN)：被模型判断为留存的流失用户数： 162;</li>\n<li>真正(TP)：被模型判断为流失的流失用户数： 199.</li>\n</ul><p>所以，流失会员的<strong>精确率为：</strong></p><p>$$流失会员的精确率（Precision） = \\frac{199}{(199 + 149)}=57\\%$$</p><p>可以看到，虽然这个归一化后的DNN神经网络模型整体上的准确率为78%，但是它基于流失用户的精确率只有57%，说明它仍有提升的空间。</p><p>当然，我们也可以基于留存的用户来判断这个模型的精确率：</p><ul>\n<li>真负(TN)：被模型判断为流失的流失用户数：199;</li>\n<li>假正(FP)：被模型判断为留存的流失用户数： 162;</li>\n<li>假负(FN)：被模型判断为流失的留存用户数： 149;</li>\n<li>真正(TP)：被模型判断为留存的留存用户数： 899.</li>\n</ul><p>这个模型基于留存用户的精确率为：</p><p>$$留存会员的精确率（Precision） = \\frac{899}{(899 + 162)}=85\\%$$</p><p>这是评判模型的另外一个视角。当然，从解决运营人员问题的角度出发，我们还是要基于流失用户来看模型的精确率，这样才能帮助运营人员进行有针对性的留客活动。</p><p>除了精确率，还有另一个标准是<strong>召回率</strong>，也叫<strong>查全率</strong>。你应该听说过“召回”这个词吧，就是劣质品蒙混过了质检这关，跑出厂了，被发现后得给召回来，销毁掉。召回率是覆盖面的度量，它度量的是被正确判为正例的正例比例，它和精确率是成对出现的概念，公式如下：</p><p>$$召回率（Recall） = \\frac{TP}{(TP + FN)}$$</p><p>召回率考量了“假负（FN）”的存在，也就是需要考虑被误判为“合格品”的“劣质品”，在我们这个问题中的假负就是被误判为“留存”的“要流失”的会员。</p><p>对于刚才这个例子：</p><p><img src="https://static001.geekbang.org/resource/image/65/53/65af36cf08a5eb7a82527d242b505a53.jpg?wh=2000x1025" alt=""></p><p>就流失会员而言，归一化后的DNN神经网络模型的<strong>召回率为：</strong></p><p>$$召回率（Recall） = \\frac{199}{(199 + 162)}=55\\%$$</p><p>我们看到，这个算法的召回率比精确率低一些。那么，我们应该以上面57%的精确率为准呢，还是应该以55%的召回率为准？答案是：当我们需要更多考量被模型预测为正的负样本时（本来是忠诚会员，误判为流失），我们看精确率；当我们需要更多考量被模型预测为负的正样本时（本来要流失了，误判为忠诚会员），我们看召回率。</p><p>对于我们这个问题，如果你问我，我会更关注召回率，因为我们就是害怕会员流失嘛。但是，如果精确率不够，运营人员会做很多无用功，把力气、时间和经费白白花在本来不会流失的人身上。</p><p>那有没有一个指标可以直接解决问题，不用这么绕来绕去？接下来，我就给你介绍一个单一指标，它可以基本搞定对不平衡数据集的分类评估。</p><h2>F1分数</h2><p>这个指标就是F1分数。它结合了精确率和召回率的共同特点。F1分数的公式如下：</p><p>$$F1 = 2 \\times \\frac{精准率\\times召回率}{(精准率 + 召回率)}$$</p><p>这个指标可以同时体现“精确率”和“召回率”的评估效果，在数学上定义为“精确率和召回率的调和均值”。只有当召回率和精确率都很高时，分类模型才能得到较高的F1分数。</p><p>F1指标特别适合于评估各类别样本分布不平衡的问题。因此，当你想用一个简单的方法来比较多种分类模型的优劣时，你选F1分数就对了。</p><p>下面，我们用代码求出<a href="https://time.geekbang.org/column/article/423893">上一讲</a>中三个模型的F1分数。在sklearn中，F1分数可以通过分类报告classification_report工具进行显示。此时，classification_report也会把精确率、召回率和准确率一起显示出来，一举多得了。这三个指标的计算，也都会在classification_report内部完成，所以，我们只需要把预测值和真值传进这个函数就好了。</p><p>我们先定义一个显示分类报告的函数show_report：</p><pre><code>from sklearn.metrics import classification_report # 导入分类报告\ndef show_report(X_test, y_test, y_pred): # 定义一个函数显示分类报告\n    print(classification_report(y_test,y_pred,labels=[0, 1])) #打印分类报告\n</code></pre><p>再调用这个函数打印出分类报告：</p><pre><code>show_report(X_test, y_test, y_pred)\n</code></pre><p>下面，我们直接给出逻辑回归、归一化前后DNN神经网络模型的分类报告。</p><p>逻辑回归：</p><pre><code>          precision    recall  f1-score   support\n       0       0.83      0.89      0.86      1048\n       1       0.60      0.47      0.53       361\naccuracy                           0.78      1409\n</code></pre><p>DNN神经网络（归一化前）：</p><pre><code>          precision    recall  f1-score   support\n       0       0.77      0.99      0.87      1048\n       1       0.81      0.14      0.24       361\naccuracy                           0.77      1409\n</code></pre><p>DNN神经网络（归一化后）：</p><pre><code>          precision    recall  f1-score   support\n       0       0.85      0.86      0.85      1048\n       1       0.57      0.55      0.56       361\naccuracy                           0.78      1409\n</code></pre><p>在这个Report中，我们更关注1值，也就是相对于流失客户的精确率、召回率和F1分数，尤其是F1分数。结果显示，虽然逻辑回归和归一化后的神经网络准确率都是78%，但是，F1分数最高的模型是归一化后的DNN神经网络。所以，对这个问题来说，归一化后的DNN神经网络性能最棒。</p><p>现在，有了F1分数这个比较优秀的分类评估指标，我们对模型的评估就靠谱得多了。不过，你可能会想：有没有什么图形化的显示方式，能更为直观地比较出多个模型的性能优劣呢？的确有。</p><h2>ROC曲线和AUC</h2><p>除了精确率、召回率、F1分数之外，还有两个经常与二元分类器一起使用的工具：一个是“受试者工作特征曲线”（receiver operating characteristic curve，简称ROC）；另一个是“曲线下面积”（Area under the Curve of ROC，简称AUC）。</p><p>ROC曲线绘制的是“真正类率”和“假正类率”的信息。其中，真正类率也叫真阳性率（TPR），表示在所有实际为阳性的样本中，被正确地判断为阳性的比率。它其实就是召回率的另一个名称。</p><p>$$真阳性率（TPR） = \\frac{TP}{(TP + FN)}$$</p><p>假正类率也叫伪阳性率（FPR），表示在所有实际为阴性的样本中，被错误地判断为阳性的比率：</p><p>$$伪阳性率（FPR） = \\frac{FP}{(FP + TN)}$$</p><p>要绘制ROC曲线，我们需要先构建一个ROC空间。ROC空间其实就是把伪阳性率（FPR）定义为 X 轴，真阳性率（TPR）定义为 Y 轴，所形成的坐标系空间。构建好ROC空间后，如果给定一个二元分类模型和它所预测的分类概率，我们就能根据样本的真实值和预测值，在ROC空间中画出一个个坐标点 (X=FPR, Y=TPR) 了。</p><p>那怎么判断这些坐标点的好坏呢？我们从坐标点 (0, 0) 到 (1,1) 画一个对角线，将ROC空间划分为左上／右下两个区域。在这条线以上的点代表了一个好的分类结果（真阳性率比伪阳性率高），而在这条线以下的点代表了差的分类结果（真阳性率比伪阳性率低）。由此，你可能也猜出了，在ROC空间里，越靠近左上的点越好，越靠近右下越劣，而对角线上的点，真阳性率和伪阳性率值相同，这就相当于随机猜测的结果。</p><p><img src="https://static001.geekbang.org/resource/image/92/56/92b794722d77378ba7fdb18ef1f64756.png?wh=467x480" alt=""></p><p>如果我们将同一模型所有样本的 (FPR, TPR) 坐标都画在ROC空间里，连成一条线，就能得到该模型的ROC曲线。这条曲线与ROC空间右边缘线和下边缘线围成的面积，就是这个模型的曲线下面积（AUC）。</p><p>我们前面说，在ROC空间里，越靠近左上的点越好，越靠近右下越劣。因此，对于AUC来讲，AUC越大，说明这个模型越趋向左上角越好。从具体的数值来看，AUC的取值范围在0.5和1之间。 AUC越接近1.0，说明模型性能越高；等于0.5时，只相当于随机一猜，说明模型无应用价值。</p><p>如果你想要比较不同的分类模型，就可以在同一个ROC空间中，把每个模型的ROC曲线都画出来，通过比较AUC的大小，就能判断出各个模型的优劣。</p><p>下面，我们就比较一下逻辑回归和DNN神经网络（归一化之后）的ROC曲线和AUC值。因为没有归一化的DNN神经网络性能很差，所以，我们在这里就不把它放在一起比较了。</p><p>首先，我们导入sklearn中绘制ROC曲线和计算AUC的工具roc_curve和auc：</p><pre><code>from sklearn.metrics import roc_curve #导入roc_curve工具\nfrom sklearn.metrics import auc #导入auc工具\n</code></pre><p>然后，我们在测试集上对DNN神经网络模型做一个预测，根据预测结果计算出FPR,、TPR和AUC的值。这里dnn代表神经网络。</p><pre><code>y_pred_dnn = dnn.predict(X_test).ravel() #神经网络预测概率值\nfpr_dnn, tpr_dnn, thresholds_dnn = roc_curve(y_test, y_pred_dnn) #神经网络 TPR FPR和ROC曲线\nauc_dnn = auc(fpr_dnn, tpr_dnn) #神经网络 AUC值\n</code></pre><p>同样地，我们也计算出逻辑回归模型的FPR、TPR和AUC值，这里lr代表逻辑回归。</p><pre><code>y_pred_lr = lr.predict_proba(X_test)[:, 1]　＃逻辑回归预测概率值\nfpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_pred_lr)　#逻辑回归 TPR FPR和ROC曲线\nauc_lr = auc(fpr_lr, tpr_lr)　#逻辑回归 AUC值\n</code></pre><p>最后，我们在一张图中显示这两个模型的ROC曲线和AUC值：</p><pre><code>plt.plot([0, 1], [0, 1], \'k--\')　＃设定对角线\nplt.plot(fpr_dnn, tpr_dnn, label=\'神经网络 (area = {:.3f})\'.format(auc_dnn))　#绘制神经网络ROC曲线\nplt.plot(fpr_lr, tpr_lr, label=\'逻辑回归 (area = {:.3f})\'.format(auc_lr))　#绘制逻辑回归ROC曲线\nplt.xlabel(\'False positive rate\')　＃X轴FPR\nplt.ylabel(\'True positive rate\')　＃Y轴TPR\nplt.title(\'ROC曲线\')　＃图题\nplt.legend(loc=\'best\')　＃图例\nplt.show()　＃绘图\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/69/4f/690f4b41fd664bd7d5a03758e8a3c94f.png?wh=383x274" alt=""></p><p>在这个曲线中，我们可以很明显地看出，DNN神经网络的ROC曲线在整体上要高于逻辑回归模型（更逼近左上角），AUC值（曲线下面积）是0.811，比逻辑回归的AUC值0.756要高。这说明，DNN神经网络模型的性能要好过逻辑回归。</p><h2>总结一下</h2><p>好，这一讲的内容就结束了。学完了这一讲，你需要铭记有一个重要的点，那就是<strong>模型的好与不好，是基于用什么标准衡量</strong>。对于正样本和负样本比例极度不平衡的样本集，我们需要选择正确的评估标准。</p><p>那么对于分类问题，尤其是各个类别的值很不平衡的数据集来说，哪些评估标准是更重要的呢？首先应该看混淆矩阵，混淆矩阵能够给出我们真正、假正、真负、假负的值，这个矩阵的信息量比准确率大很多。</p><p>通过混淆矩阵中的真正、假正、真负、假负的值，我们进而能求出精确率、召回率以及二者的综合指标F1分数。其中的F1指标，就是评估样本类别数量并不平衡的二分类问题的最简单方法。而要计算精确率、召回率以及F1分数，也很简单，我们用sklearn的classification_report就能轻松实现。</p><p>此外，我们还可以通过ROC曲线和AUC值，用图表的形式来直观地比较各个模型的分类性能优劣。请你注意，ROC曲线越靠近左上角，模型越优，而AUC的值越接近1，模型越优。</p><h2>思考题</h2><p>好，这节课就到这里了，最后，我给你留两道思考题：</p><ol>\n<li>请你从多个维度思考，如何对神经网络模型进行优化，以实现更好的分类效果，得到更优的F1分数。</li>\n</ol><p>提示思路：改变训练的轮次、调整激活函数，优化器，神经网络结构。</p><ol start="2">\n<li>在<a href="https://time.geekbang.org/column/article/419746">第10讲</a>中，我们介绍了怎么用KFold工具拆分数据集，做K折验证。其实，对于非平衡数据，我们也可以用StratifiedKFold、StratifiedShuffleSplit等拆分数据集，来做分层采样。所谓分层采样，就是在每一份子集中都保持和原始数据集相同的类别比例。若数据集有2个类别，比例是8:2，则划分后的样本比例仍约是8:2。请你尝试通过这个方式，对逻辑回归模型进行K折验证。此外，train_test_split也有stratify参数，你也可以尝试用它来保持数据分割时的类别比例。</li>\n</ol><p>欢迎你在留言区和我分享你的观点，如果你认为这节课的内容有收获，也欢迎把它分享给你的朋友，我们下一讲再见！</p><p><img src="https://static001.geekbang.org/resource/image/90/ec/90ba66f0ca40dbf8215567b8668f63ec.jpg?wh=2284x1280" alt=""></p>',
        article_title: "16｜性能评估：不平衡数据集应该使用何种评估指标？",
      },
      {
        title: "17｜集成学习：机器学习模型如何“博采众长”?",
        id: 424305,
        content:
          '<p>你好，我是黄佳。恭喜你连闯４关，成功来到最后一关“裂变关”。</p><p>回忆一下这一路的旅程，在获客关中，我们给用户分组画像；在变现关中，我们关注用户的生命周期价值；在激活关中，我们预测了App的激活数字；在留存关中，我们分析了与用户流失相关的因素。</p><p><img src="https://static001.geekbang.org/resource/image/8c/95/8ce98a4187cc752f3ab9d7ebb988da95.jpg?wh=2000x1000" alt=""></p><p>那么在裂变关中，我们将从数据中寻找蛛丝马迹，发现“易速鲜花”运营中最佳的“裂变方案”。不过，除了介绍运营中的裂变方案外，今天，我们还要好好讲一讲集成学习。</p><p>为什么要专门拿出一讲来谈集成学习呢？我们在<a href="https://time.geekbang.org/column/article/419218">第9讲</a>说过，我们用机器学习建模的过程，就是和过拟合现象持续作斗争的过程。而集成学习在机器学习中是很特别的一类方法，能够处理回归和分类问题，而且它对于避免模型中的过拟合问题，具有天然的优势。那么，集成学习的优势是怎么形成的？学习了今天的课程后你就会找到答案。</p><h2>定义问题</h2><p>老规矩，我们先来定义今天要解决的问题。</p><p>说起裂变，你可能并不会感到陌生。裂变是让产品自循环、自传播的重要工具。像邀请新人得红包、分享App领优惠券、友情助力拿赠品、朋友圈打卡退学费等等，都是裂变的玩法。</p><p>最近，“易速鲜花”运营部门提出了两个裂变思路。方案一是选择一批热销商品，让老用户邀请朋友扫码下载App并成功注册，朋友越多，折扣越大。我们把这个方案命名为“疯狂打折”，它走的是友情牌。方案二是找到一个朋友一起购买，第二件商品就可以免费赠送，这叫“买一送一”。</p><!-- [[[read_end]]] --><p>提出两个裂变方案之后，运营部门收集了<a href="https://github.com/huangjia2019/geektime/tree/main/%E8%A3%82%E5%8F%98%E5%85%B317">转化数据</a>。那么，我们今天的目标就是<strong>根据这个数据集，来判断一个特定用户在特定的裂变促销之下，是否会转化。</strong></p><p><img src="https://static001.geekbang.org/resource/image/b9/5a/b9fa22305a417d65f9704360ecc5825a.png?wh=797x182" alt=""></p><p>这个问题和我们之前预测用户是否会流失非常相似，也是一个二元分类问题。</p><p>下面我们就导入相关的包，并读入数据：</p><pre><code>import pandas as pd #导入Pandas\nimport numpy as np #导入NumPy\ndf_fission = pd.read_csv(\'易速鲜花裂变转化.csv\') #载入数据\ndf_fission.head() #显示数据\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/db/f7/db26f6b958f0493277d021e598324df7.png?wh=789x409" alt=""></p><p>在这个数据集中，共有10000个数据样本，也就是10000个用户的信息。其他字段都很好理解，我们重点来看看“裂变方案”字段。这个字段表示的是该用户所导流到的裂变类型，一个用户看到的是“疯狂打折”优惠页，还是“买一送一”优惠页，是随机分配的结果。对于一个用户，二者只能属于其一。而与之对应的“是否转化”字段就<strong>是我们用来预测转化率的标签<strong><strong>了</strong></strong>。</strong></p><p>下面我们进入数据可视化和预处理的环节。</p><h2>数据可视化和预处理</h2><p>在数据可视化部分，我只想看一看转化和未转化的比例，也就是购买产品和未购买产品的比例。看这个比例是为了看这个数据集中的各类别样本数是否平衡。</p><pre><code>import matplotlib.pyplot as plt #导入pyplot模块\nimport seaborn as sns #导入Seaborn\nfig = sns.countplot(\'是否转化\', data=df_fission) #创建柱状计数图\nfig.set_ylabel(&quot;数目&quot;) #Y轴标题\nplt.show() #显示图像\n</code></pre><p>输入如下：</p><p><img src="https://static001.geekbang.org/resource/image/67/a1/67b604b7a681b0d8fe1e65b0c96961a1.png?wh=389x260" alt=""></p><p>结果显示，在10000个用户中，大概有2000个用户购买了产品，转化率大概在20%。说明运营人员给我们的是一个并不平衡的分类数据集。这个结论将为我们后续的算法选择做出指导。</p><p>下面，我们把数据集中的类别变量，转变为机器学习模型能够读取的虚拟变量（也叫哑编码或哑变量）：</p><pre><code># 把二元类别文本数字化\ndf_fission[\'性别\'].replace(&quot;女&quot;,0,inplace = True)\ndf_fission[\'性别\'].replace(&quot;男&quot;,1,inplace=True)\n# 显示数字类别\nprint(&quot;Gender unique values&quot;,df_fission[\'性别\'].unique())\n# 把多元类别转换成多个二元哑变量，然后贴回原始数据集\ndf_fission = pd.get_dummies(df_fission, drop_first = True)\ndf_fission # 显示数据集\n</code></pre><p>输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/45/fe/454f8b349de840356a56e455e0ed0afe.png?wh=944x176" alt=""></p><p>然后，我们再来构建标签和特征数据集，并拆分出训练集和测试集，最后对特征进行归一化缩放。我们对这些步骤已经非常熟悉了，我就不需再过多解释了：</p><pre><code>X = df_fission.drop([\'用户码\',\'是否转化\'], axis = 1) # 构建特征集\ny = df_fission.是否转化.values # 构建标签集\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)\nfrom sklearn.preprocessing import MinMaxScaler #导入归一化缩放器\nscaler = MinMaxScaler() #创建归一化缩放器\nX_train = scaler.fit_transform(X_train) #拟合并转换训练集数据\nX_test = scaler.transform(X_test) #转换测试集数据\n</code></pre><p>这样，数据集就准备好啦，现在，我们进入算法选择环节。</p><h2>算法选择：集成学习</h2><p>我们前面说，判断一个特定用户在特定的裂变促销之下是否会转化，属于二元分类问题。对于二分类问题，我们在<a href="https://time.geekbang.org/column/article/423893">第15讲</a>中也提到过，它不仅可以用逻辑回归解决，还可以通过SVM、集成学习、神经网络等多种模型完成。现在，我们就借助这一讲的项目，着重来了解一下集成学习方法的原理和应用。</p><p><strong>集成学习(ensemble learning)，是通过构建出多个模型（这些模型可以是比较弱的模型），然后将它们组合起来完成任务的机器学习算法</strong>。所以，它实际上并不是一种机器学习算法，而是一个机器学习算法的家族。通常情况下，集成学习将几个表现一般的模型集成在一起，就能大幅提升模型的性能，这就是集成学习的天然优势。</p><p>在这个算法家族中，很多算法都是“网红”算法，比如随机森林、梯度提升机（英文叫GB或GBDT）和极限梯度提升（eXtreme Gradient Boosting，即XGBoost，有时候简称XGB）等，这些都是非常流行的机器学习算法，在很多许多领域都取得了成功，并且还是很多人赢得各种机器学习竞赛的主要方法。</p><p>那么，为什么几个表现一般的模型集成在一起，性能会大幅提升？下面，我们来探究一下。首先，请你回忆一下，我们在<a href="https://time.geekbang.org/column/article/419218">第9讲</a>中讲过，机器学习在训练时有一个从欠拟合到过拟合的过程：</p><p><img src="https://static001.geekbang.org/resource/image/55/37/557e4ff908038d3d43e5653903e1ee37.png?wh=865x273" alt="" title="从欠拟合到过拟合"></p><p>如果结合我们已经比较熟悉的损失曲线，就可以用下面这张图来描述这一过程：</p><p><img src="https://static001.geekbang.org/resource/image/1d/e1/1d4032ed253404d751383fa3df6a9ee1.png?wh=516x400" alt="" title="损失、偏差、方差与模型复杂度之间的关系"></p><p>我们说，当给定一个学习任务，在训练初期，模型对训练集的拟合还未完善，训练集和测试集上面的损失也都比较大，这时候的模型处于欠拟合状态。由于模型的拟合能力还不强，数据集的改变是无法使模型的效率产生显著变化的，所以，如果我们把此时的模型应用于训练集和测试集的数据，都会出现“<strong>高偏差</strong>”。</p><p>随着训练次数增多，模型的拟合能力在调整优化的过程中会变得越来越强，训练集上和测试集上的损失也都会不断下降。</p><p>当充分训练之后，模型已经完全拟合了训练集数据，训练集上的损失也变得非常小。但是，这时候的模型很容易受数据的影响，数据的轻微扰动都会导致模型发生显著变化。这时候，如果我们把模型应用于不同的数据集（包括测试集），会出现<strong>很高的方差</strong>，也就是过拟合的状态。</p><p>总的来说就是，模型在欠拟合状态会出现“高偏差”，在过拟合状态会出现“高方差”，这都不符合我们的预期。只有在拟合刚刚好的时候，模型才是相对成功的。这时候，模型的偏差和方差处于平衡态，均不会太高。</p><p>你可能注意到，在上面的讲述中，我引入了“方差”和“偏差”这两个新概念。方差，是从统计学中引用过来的概念，它表示的是一组数据距离其均值的离散程度。而“偏差”是机器学习里概念，用来衡量模型的准确程度。</p><p>在机器学习中，“低偏差”和“低方差”是我们希望达到的效果。可是，一般来说, 低偏差与低方差是鱼与熊掌不可兼得的，这被称作偏差-方差窘境 (bias-variance dilemma)。下面这张打靶图，就形象地说明了这一点：<br>\n<img src="https://static001.geekbang.org/resource/image/1b/65/1b4846d4f28a0954b7e8eecc38b0f265.png?wh=497x439" alt="" title="方差和偏差对预测结果所造成的影响"></p><p>其实，机器学习性能优化领域的最核心问题，就是不断地在“欠拟合-过拟合”之间，也就是“偏差-方差”之间，探求最佳平衡点，换句话说，就是训练集优化和测试集泛化的平衡点。而机器学习的性能优化是有顺序的，我们一般是先减小偏差，再聚焦于降低方差。</p><p>那说了这么多，你也许有点不耐烦了：这些内容和集成学习到底有什么关系呢？</p><p>其实啊，集成学习的优点就在于，它可以通过组合一些比较简单的算法，来保留这些算法训练出的模型“方差低”的优势；在此基础之上，集成学习又能引入复杂的模型，来扩展简单算法的预测空间。所以，集成学习是同时减小方差和偏差的大招。</p><p>集成学习的核心思想是训练出多个模型并将这些模型进行组合。根据分类器的训练方式和组合预测的方法，集成学习中两种最重要的方法就是：降低偏差的Boosting和降低方差的Bagging。</p><p>下面，我们就一边讲解这两种方法，一边用它们来解决“特定用户在特定的裂变促销之下是否会转化成功”的预测问题。</p><p>我们先来看Boosting。</p><h2>降低偏差：Boosting方法</h2><p>Boosting方法是把梯度下降的思想应用在了机器学习算法的优化上，让弱模型对数据的拟合逐渐增强。它的基本思路就是：持续地通过新模型来优化同一个基模型（基模型，也就是Boosting开始时的初始模型），当一个新的弱模型加入进来的时候，Boosting就在原有模型的基础上整合这个新模型，然后形成新的基模型。而对这个新的基模型的训练，则会一直聚集于之前模型的误差点（也就是原模型预测出错的样本）上，这样做的目标是不断减小模型的预测误差。</p><p>在Boosting方法中，有三种很受欢迎的算法，分别是AdaBoost、GBDT 和XGBoost。其中，AdaBoost会对样本进行加权；GBDT在AdaBoost的基础上，还会定义一个损失函数，通过梯度下降来优化模型；而XGBoost则在GBDT的基础上，进一步优化了梯度下降的方式。</p><p>这三种算法都可以用来解决我们这一讲的分类问题（其实也可以用于回归问题）。下面，我们来逐一做个讲解。</p><h3>1. AdaBoost算法</h3><p>我们先来使用AdaBoost算法。在处理分类问题时，AdaBoost 会先给不同的样本分配不同的权重，被分错的样本的权重在Boosting 过程中会增大，新模型会因此更加关注这些被分错的样本；反之，被分正确的样本的权重会减小。接着，AdaBoost会将修改过权重的新数据集输入到新模型进行训练，产生新的基模型。最后，AdaBoost会把每次得到的基模型组合起来，并根据其分类错误率对模型赋予权重，集成为最终的模型。</p><p>下面，我们用AdaBoost算法来预测一下用户在特定的裂变促销之下是否会转化，并给出评估分数：</p><pre><code>from sklearn.ensemble import AdaBoostClassifier # 导入AdaBoost 模型\ndt = DecisionTreeClassifier() # 选择决策树分类器作为AdaBoost 的基准算法\nada = AdaBoostClassifier(dt) # AdaBoost 模型\nada.fit(X_train, y_train) # 拟合模型\ny_pred = ada.predict(X_test) # 进行预测\nprint(&quot;AdaBoost 测试准确率: {:.2f}%&quot;.format(ada.score(X_test, y_test)*100))\nprint(&quot;AdaBoost 测试F1分数: {:.2f}%&quot;.format(f1_score(y_test, y_pred)*100))\n</code></pre><p>输出如下：</p><pre><code>AdaBoost 测试准确率: 78.75%\nAdaBoost 测试F1分数: 50.18%\n</code></pre><p>结果显示，AdaBoost算法测试准确率是78.75%，F1分数为50.18%。这个F1分数并不是很理想，我们需要考虑一下有没有更优的算法。而下面要介绍的GBDT算法就对AdaBoost算法做出了进一步的改进。</p><h3>2. GBDT算法</h3><p>GBDT算法也叫梯度提升（Granding Boosting）算法，它是梯度下降和Boosting方法结合的产物。因为常见的梯度提升都是基于决策树模型（机器学习中就把决策树模型简称为树）的，所以我们这里会把它称作是GBDT，即梯度提升决策树（Granding Boosting Decision Tree）。</p><p>我们知道，前面的AdaBoost算法只是对样本进行加权，但GBDT 算法与之不同，它还会定义一个损失函数，并对损失和机器学习模型所形成的函数进行求导，每次生成的模型都是沿着前面模型的负梯度方向（一阶导数）进行优化，直到发现全局最优解。也就是说，在GBDT的每一次迭代中，当前的树所学习的内容是之前所有树的结论和损失，在学习中，GBDT会拟合得到一棵新的树，而这棵新的树就相当于是之前每一棵树的效果累加。</p><p>下面，我们用GBDT算法来预测用户是否转化，并给出评估分数：</p><pre><code>from sklearn.ensemble import GradientBoostingClassifier # 导入梯度提升模型\ngb = GradientBoostingClassifier() # 梯度提升模型\ngb.fit(X_train, y_train) # 拟合模型\ny_pred = gb.predict(X_test) # 进行预测\nprint(&quot; 梯度提升测试准确率: {:.2f}%&quot;.format(gb.score(X_test, y_test)*100))\nprint(&quot; 梯度提升测试F1分数: {:.2f}%&quot;.format(f1_score(y_test, y_pred)*100))\n</code></pre><p>输出如下：</p><pre><code>梯度提升测试准确率: 87.00%\n梯度提升测试F1 分数: 61.19%\n</code></pre><p>结果显示，GBDT算法的测试准确率是87.00%，F1分数为61.19%。F1分数果然大幅提升，看来GBDT算法还不错。其实，还有比GBDT更厉害的集成学习算法，它就是算法XGBoost算法。</p><h3>3. XGBoost算法</h3><p>XGBoost算法也叫极端梯度提升（eXtreme Gradient Boosting），有时候也直接叫作XGB。它和GBDT 类似，也会定义一个损失函数。不过，不同的是，GBDT 只用到一阶导数信息，而XGBoost会利用泰勒展开式把损失函数展开到二阶后求导。由于利用了二阶导数信息，XGBoost在训练集上的收敛会更快。</p><p>在使用XGBoost之前，我们需要通过pip语句安装XGBoost包，当然，你也可以在Anaconda的Environments（环境）界面中，直接搜索并安装XGBoost包：</p><pre><code>pip install xgboost\n</code></pre><p>下面，我们用XGBoost算法来预测用户是否转化，并给出评估分数：</p><pre><code>from xgboost import XGBClassifier # 导入XGB 模型\nxgb = XGBClassifier() # XGB 模型\nxgb.fit(X_train, y_train) # 拟合模型\ny_pred = xgb.predict(X_test) # 进行预测\nprint(&quot;XGB 测试准确率: {:.2f}%&quot;.format(xgb.score(X_test, y_test)*100))\nprint(&quot;XGB 测试F1分数: {:.2f}%&quot;.format(f1_score(y_test, y_pred)*100))\n</code></pre><p>输出如下：</p><pre><code>XGB 测试准确率: 87.00%\nXGB 测试F1分数: 63.17%\n</code></pre><p>结果显示，XGBoost算法的测试准确率是87.00%，F1分数为63.17%。</p><p>可以看出，在上述三种算法中，XGBoost算法的性能是最佳的。虽然它的预测准确率和GBDT相同，但是它的F1分数高，而F1分数是我们更重视的指标。</p><p>下面，我们再来介绍并使用能降低方差的Bagging方法。</p><h2>降低方差：Bagging方法</h2><p>Bagging 是Bootstrap Aggregating 的缩写，有人把它翻译为套袋法、装袋法，或者自助聚合，到现在，还没有一个统一的叫法。所以，我们就直接用它的英文名称Bagging。</p><p>Bagging算法的基本思想是从原始的数据集中抽取数据，形成K个随机的新训练集，然后训练出K个不同的模型。</p><p>具体来讲，Bagging算法首先会在原始样本集中随机抽取K轮，每轮抽取n个训练样本作为一个训练集。这时候，有些样本可能被多次抽取，而有些样本可能一次都没有被抽取，这叫做有放回的抽取）。在抽取K轮之后，就会形成K个训练集，注意，这K个训练集是彼此独立的。这个过程也叫作bootstrap（可译为“自举”或“自助采样”）。</p><p>接着，Bagging算法会每次使用一个训练集，并通过相同的机器学习算法（如决策树、神经网络等）得到一个模型。因为有K个训练集，所以一共可以得到K个模型。我们把这些模型称为“基模型”（base estimator）或者“基学习器”。</p><p>最后，对于这K个模型，Bagging算法会用不同的方式得到基模型的集成结果：如果是分类问题，Bagging算法会对K个模型投票，进而得到分类结果；如果是回归问题，Bagging算法会计算K个模型的均值，将其作为最后的结果。</p><p>因为随机抽取数据的方法能减少可能的数据干扰，所以经过Bagging 的模型将会具有“低方差””。一般来说，Bagging有三种常见的算法：决策树的Bagging、随机森林算法和极端随机森林。</p><h3>1. 决策树的Bagging</h3><p>多数情况下的Bagging，都是基于决策树的。构造随机森林的第一步，其实就是对多棵决策树进行Bagging，我们把它称为树的聚合（Bagging of Tree）。</p><p>决策树这种模型，具有显著的低偏差、高方差的特点。也就是说，决策树模型受数据的影响特别大，一不小心，训练集准确率就接近100% 了，而这种效果又不能移植到其他的数据集上，所以，这是很明显的过拟合现象。集成学习的Bagging算法就是从决策树模型开始，着手解决它太过于精准，又不易泛化的问题。当然，Bagging的原理并不仅限于决策树，它还可以扩展到其他机器学习算法。</p><p>下面，我们用决策树的Bagging算法来预测用户是否转化，并给出评估分数：</p><pre><code>from sklearn.ensemble import BaggingClassifier # 导入Bagging 分类器\nfrom sklearn.tree import DecisionTreeClassifier # 导入决策树分类器\nfrom sklearn.metrics import (f1_score, confusion_matrix) # 导入评估指标\ndt = BaggingClassifier(DecisionTreeClassifier()) # 只使用一棵决策树\ndt.fit(X_train, y_train) # 拟合模型\ny_pred = dt.predict(X_test) # 进行预测\nprint(&quot; 决策树测试准确率: {:.2f}%&quot;.format(dt.score(X_test, y_test)*100))\nprint(&quot; 决策树测试F1 分数: {:.2f}%&quot;.format(f1_score(y_test, y_pred)*100))\nbdt = BaggingClassifier(DecisionTreeClassifier()) # 树的Bagging\nbdt.fit(X_train, y_train) # 拟合模型\ny_pred = bdt.predict(X_test) # 进行预测\nprint(&quot; 决策树Bagging 测试准确率: {:.2f}%&quot;.format(bdt.score(X_test, y_test)*100))\nprint(&quot; 决策树Bagging 测试F1分数: {:.2f}%&quot;.format(f1_score(y_test, y_pred)*100))\n</code></pre><p>输出如下：</p><pre><code>决策树测试准确率: 85.10%\n决策树测试F1分数: 58.17%\n决策树Bagging 测试准确率: 85.55%\n决策树Bagging 测试F1 分数: 58.66%\n</code></pre><p>结果显示，对于这个数据集来说，决策树Bagging的测试集F1分数有提升，但是优势不明显。不过没关系，我们可以试一下更常见的随机森林算法，它其实是经过改进的“决策树Bagging算法”。</p><h3>2. 随机森林算法</h3><p>当我们说到集成学习，最关键的一点是：各个基模型的相关度要小，差异性要大。模型间差异越大（也就是异质性越强），集成的效果越好。两个准确率为99% 的模型，如果其预测结果都一致，也就没有提高的余地了。因此，对决策树做集成，关键就在于各棵树的差异性是否够大。</p><p>为了实现基模型差异化的目标，随机森林在树分叉时，增加了对特征选择的随机性，而并不总是考量全部的特征。这个小小的改进，就进一步提高了各棵树的差异。</p><p>下面，我们用随机森林算法来预测用户是否转化，并给出评估分数：</p><pre><code>from sklearn.model_selectifrom sklearn.ensemble import RandomForestClassifier # 导入随机森林模型\nrf = RandomForestClassifier() # 随机森林模型\nrf.fit(X_train, y_train) # 拟合模型\ny_pred = rf.predict(X_test) # 进行预测\nprint(&quot; 随机森林测试准确率: {:.2f}%&quot;.format(rf.score(X_test, y_test)*100))\nprint(&quot; 随机森林测试F1分数: {:.2f}%&quot;.format(f1_score(y_test, y_pred)*100))\n</code></pre><p>输出如下：</p><pre><code> 随机森林测试准确率: 87.20%\n 随机森林测试F1分数: 61.79%\n</code></pre><p>可以看到，随机森林测试F1分数很明显地高于决策树模型和决策树的Bagging。</p><h3>3. 极端随机森林算法</h3><p>最后，我们再来看极端随机森林算法。极端森林算法在随机森林的基础上，又增加了进一步的随机性。</p><p>随机森林算法在树分叉时会随机选取m个特征作为考量，对于每一次分叉，它还会遍历所有的分支，然后选择基于这些特征的最优分支。这在本质上仍属于贪心算法（greedy algorithm），即在每一步选择中都采取在当前状态下最优的选择。而极端随机森林算法则不同，它一点也不“贪心”，它甚至不回去考量所有的分支，而是随机选择一些分支，从中拿到一个最优解。</p><p>下面，我们用极端随机森林算法来预测用户是否转化，并给出评估分数：</p><pre><code>from sklearn.ensemble import ExtraTreesClassifier # 导入极端随机森林模型\next = ExtraTreesClassifier() # 极端随机森林模型\next.fit(X_train, y_train) # 拟合模型\ny_pred = ext.predict(X_test) # 进行预测\nprint(&quot; 极端随机森林测试准确率: {:.2f}%&quot;.format(ext.score(X_test, y_test)*100))\nprint(&quot; 极端随机森林测试F1分数: {:.2f}%&quot;.format(f1_score(y_test, y_pred)*100))\n</code></pre><p>输出如下：</p><pre><code> 极端随机森林测试准确率: 87.05%\n 极端随机森林测试F1分数: 60.58%\n</code></pre><p>结果显示，对于“预测一个特定用户在特定的裂变促销下是否会转化”这个问题来说，极端随机森林测试F1分数，要高于决策树和树的Bagging，但是低于随机森林算法。</p><p>最后，我们可以把上述各个集成学习算法的得分，都显示在一张图中，来比较一下它们在这个问题上的优劣。</p><p><img src="https://static001.geekbang.org/resource/image/4d/84/4d2bf3a6b9f18e1164824e485b972684.png?wh=937x316" alt=""></p><p>图中可以看出，对于这个问题，XGBoost、随机森林和极端随机森林，都是比较好的选择。</p><h2>总结一下</h2><p>好，到这里，我们关于集成学习的介绍就结束了，我们来做个总结。</p><p>集成学习模型是将多种同质或者异质的模型集成组合在一起，来形成更优的模型。而在这个过程的目标就是，减少机器学习模型的方差和偏差，找到机器学习模型在欠拟合和过拟合之间的最佳平衡点。</p><p>那么，怎么降低偏差呢？我们可以用Boosting方法，把梯度下降的思想应用在机器学习算法的优化上，使弱模型对数据的拟合逐渐增强。Boosting也常应用于决策树算法上，其中的GBDT 和XGBoost算法都非常受欢迎。</p><p>那怎么降低方差呢？这就要借助Bagging方法了。Bagging方法通常基于决策树，在随机生成数据集上训练出各种各样不同的树。其中的随机森林算法是在树分叉时，增加了对特征选择的随机性。随机森林在很多问题上都是一个很强的算法。我们可以把它作为一个基准，如果找到了比随机森林还优秀的模型，你就可以很欣慰了。</p><p>那么最后，如果让我在集成学习家族的算法里，给你推荐两种常用且效果好的算法，基于我个人的经验，我会觉得XGBoost、GBDT和随机森林是优于其它几种的。</p><h2>思考题</h2><p>好，在这一讲的最后，我给你留3个思考题：</p><ol>\n<li>我在使用这些集成学习算法时，都只使用了默认参数，请你用GridSearchCV进行参数的网格搜索，找出针对这个问题更优的参数，训练出更好的模型；</li>\n<li>请你绘出上述各个算法的混淆矩阵；</li>\n<li>请你使用深度神经网络解决这个问题，看看是深层网络的性能好，还是集成学习的性能好？</li>\n</ol><p>欢迎你在留言区和我分享你的观点，如果你认为这节课的内容有收获，也欢迎把它分享给你的朋友，我们下一讲再见！</p><p><img src="https://static001.geekbang.org/resource/image/fe/83/fe84199aa2a33ef737b51a0809b3a183.jpg?wh=2284x1280" alt=""></p>',
        article_title: "17｜集成学习：机器学习模型如何“博采众长”?",
      },
      {
        title: "18 | 增长模型：用XGBoost评估裂变海报的最佳受众群体",
        id: 424481,
        content:
          "<p>你好，我是黄佳。欢迎来到零基础实战机器学习。</p><p>上一讲中，我们通过多种集成学习方法，预测出了用户接收到某个裂变后是否会购买产品，这是二分类问题。这一讲中呢，我们继续研究裂变，不过，我们不再对用户是否转化进行预测，而是要评估裂变方案的最佳受众是谁。通过这一讲，我希望你能掌握一种解决运营问题的重要思路：增长模型。这个模型，常用于评估营销活动、推广海报对用户的订单增量的影响，国外的一些大厂也在使用它提升促销转化率。</p><p>这一讲内容有点小复杂，我们就开门见山，直接介绍运营部门面临的具体问题吧。</p><h2>问题的定义和分析</h2><p>运营团队为易速鲜花的用户设计了两张H5海报页面，对应着两种裂变方案，分别是“情侣花享”和“拼团盛放”。前者是类似于买一送一的促销，后者是用户生成专属海报拉朋友拼团，团越大折扣越大。</p><p>经过一段时间的试运营，运营团队收集到了下面这些数据给到我们，这个数据集你可以在<a href=\"https://github.com/huangjia2019/geektime/tree/main/%E8%A3%82%E5%8F%98%E5%85%B318\">这里</a>下载。</p><p><img src=\"https://static001.geekbang.org/resource/image/d5/b0/d56cbea1f1232e1d3a2a2acb9df740b0.png?wh=665x180\" alt=\"\"></p><p>可以看到，这个数据集不仅包含了两种裂变方案的用户转化数据（也就是购买与否），还包含了在没有任何促销活动情况下，用户的转化情况。</p><p>这其实是运营人员常用的A/B测试，就是在其它条件完全相同的情况下，系统随机给三分之一的用户显示“情侣花享”页，给另外三分之一的用户显示“拼团盛放”页，然后给剩下的三分之一用户显示没有促销的页面，以此来观察两种促销活动（裂变方案）的转化率增量。所以，这里的“裂变类型”字段就是指该用户接收到的促销页面，是系统随机分配的结果。</p><!-- [[[read_end]]] --><p>现在，运营团队想知道，这两种裂变方案的转化率增量是怎样的？也就是说，这两种裂变方案对一个用户“购买可能性”（购买概率）的增量分别是多少？</p><p>这可能不太好理解，我给你举个例子。就拿“情侣花享”这个方案来说，如果一个50多岁的女性用户平时没有买花的需求，也不是“情侣花享”的潜在用户。那她在收到这个促销活动页面后，就不会产生购买的念头，也就是说“购买可能性”不会发生变化，增量为0。那这时候我们也不用花精力对她做这样的促销了，因为这个方案在她身上不起作用。</p><p>同样的道理，如果一个用户正在犹豫：老婆即将过生日，要不要给买束鲜花呢？就在这时，啪！“情侣花享”的促销活动来了，那他很可能就当场下单了。这种情况，就是“情侣花享”方案增加了该用户的“购买可能性”，具体的增量是多少，需要机器学习模型来预测。</p><p>当然，也会有这样一种情况：假设有一个20岁左右、已经是3年会员的某男性用户，是我们的忠粉，他每个月都会给女友买花。对于这样的用户，即便我们不做任何促销，他也会购买产品。所以，我们做不做裂变推广、采用哪一种裂变方案，都对他的最终购买决策没有任何影响。因此，这时候，我们就希望机器学习模型可以判断出，该用户购买可能性的增量为0。那运营团队就不用花费力气把促销海报发给他，也不需要过多考虑他更喜欢哪种折扣方案。对吧？</p><p>到这里，你应该清楚这个问题的含义了，我们实际上就是要预测：在一个特定裂变促销的前后，一个用户“购买可能性”的增量是多少？并且，你可能已经发现了，我们其实就是要从中找到那些正在犹豫“要不要买”的摇摆者，只有这些人，才<strong>最值得</strong>把特定的裂变方案发给他。</p><p>那我们怎么解决这个问题呢？这就需要增长模型大显身手了。</p><h2>增长模型</h2><p>增长模型，英文是Uplift modelling或者incremental modelling，所以也翻译为提升建模、增量建模等，它多用于评估营销活动对用户的订单增量的影响。国外的互联网公司<a href=\"https://www.youtube.com/watch?v=2J9j7peWQgI\">优步（Uber）</a>等，也在使用该模型进行数据分析，以增加用户的下单次数。 请你注意哈，这里的增长模型是解决我们今天的问题的一种思路，它并不是一种具体的机器学习算法。</p><p>在介绍具体的思路之前，我们首先来了解一下，针对某个特定的裂变推广方案，用户可能有多少种不同的反应。前面几个例子就展示了三种反应，其实，总体来看，一共会有4种结果，也就是说，一个用户在接到促销之后，做出的反应必然属于下面这四种情况之一：</p><p><img src=\"https://static001.geekbang.org/resource/image/15/8f/15f4a5cfa5036249e968809ccbf36d8f.png?wh=675x467\" alt=\"\"></p><p>根据不同的反应，我们可以把用户分为四个类型：</p><p><strong>第一类人是“完全不睬者”</strong>：这类人很容易理解，英文叫Lost Causes，他们对当前促销产品没有兴趣，不管你怎么打广告，给什么样的优惠，裂变玩出花来，他们也不会买这个产品。可以说是一毛不拔，所以，在做促销时我们可以把他们忘掉。上面第一个例子的用户就属于这个群体。</p><p><strong>第二类人是“促销响应者”</strong>：这类人是我们关注的重点，英文叫Persuadable。他们只有在收到我们的促销活动后，才会购买。这类人对价格和折扣非常敏感，要等有合适的活动才肯花钱。就比如去京东买书，一看，只有9折？折扣力度不大，那就等“618”或者“双11”吧，并且在离开网站之前，他们还可能在想买的商品中标注一个降价提醒。上面第二个例子中的用户，就是一个促销响应者。</p><p><strong>第三类人是“肯定会买者”</strong>：这类人代表我们的忠实用户，英文叫Sure Things。他们很满意当前产品，即便没有特别的促销活动也会购买。上面第三个例子中的用户，就属于这一类。</p><p><strong>第四类人是“别来烦我者”</strong>：这一类人比较奇怪，英文叫Do-Not-Disturb或Sleeping Dogs。这是一群收到促销反而不会购买的用户。他们内心的想法估计是：我最讨厌广告，你越想让我买，我就越不买！又或者是：你什么时候不来烦我，我再买。所以，你会惊讶地看到，这第四类人在有促销的时候不会购买，在无促销的时候反而会买。</p><p><strong>而增长模型的原理就是要尽可能地定位出“促销响应者”和“别来烦我者”，来进行促销活动。</strong>从刚才的分析，我们可以知道，促销活动是A好还是B好，不是第一和第三类人说了算，而是第二和第四类人决定的。因为，促销活动会影响他们的决定。对于第二类人促销响应者，不打折他们就不会购买，所以促销活动就应该针对这个群体开展。与此同时，促销活动应该尽量避免发送到第四类人那里，因为促销会令他们反感。</p><p>不过，在实际项目中，我们是无法直接找出这四类人的。但我们可以通过“有没有收到裂变”和“是否转化”这两个字段，把整个用户做这样一个划分：裂变购买者、裂变未购买者、无裂变购买者和无裂变未购买者。请你注意，虽然我们在这里的划分，并不直接对应于上面讲的那“4类人”，但是它们二者之间还是有关联的：</p><ul>\n<li>裂变购买者：包括“肯定会买者”和“促销响应者”；</li>\n<li>裂变未购买者：包括“完全不睬者”和“别来烦我者”；</li>\n<li>无裂变购买者：包括“肯定会买者”和“别来烦我者”；</li>\n<li>无裂变未购买者：包括“完全不睬者”和“促销响应者”。</li>\n</ul><p>然后，我们可以通过机器学习模型来定位，每一个用户落入上面这4种人（也就是4种可能情况）的概率。再通过下面这个增量公式（这个公式的说明我们一会儿再细讲）求出“促销响应”的概率增加和“别来烦我”的概率减少，就是裂变所带来的转化增量了，即：</p><p>$增量分数 =（裂变购买者+无裂变未购买者）-（裂变未购买者+无裂变购买者）$</p><p>因此，从整体上看，用增长模型解决我们这个项目问题的思路，可以分为两部分：首先是判断一个用户落入这4种分类的概率分别是多少；然后根据概率，通过增量公式来计算出一个用户在收到裂变页面前后的购买概率的增量。</p><p>那现在，我们就带着这个思路，进入这个项目的具体实战。</p><h2>读入数据、预处理</h2><p>还是一样，按照机器学习的步骤，我们先导入相关的包，并读入数据：</p><pre><code>import pandas as pd #导入Pandas\nimport numpy as np #导入NumPy\ndf_fission = pd.read_csv('易速鲜花增长模型.csv') #载入数据\nprint('用户数:', df_fission.count()['用户码']) #查看数据条目数\ndf_fission.head() #显示头几行数据\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/8f/85/8ff081504cea22dcd95eaa5b06cb1c85.png?wh=571x193\" alt=\"\"></p><p>这个数据集中，共64000个数据样本，其中曾助力、曾拼团、曾推荐是该用户曾经参与过的活动的记录。而裂变类型字段就是随机分配给该用户的促销裂变页面。</p><p>下面，我们做一个简单的数据可视化，观察一下各用户群组对促销的转化率情况。</p><p>我们先来显示随机分配“情侣花享”、“拼团盛放\"和“没有促销”的用户人数，看看收到这三种页面的用户数量是不是和我们最开始说的一样，大致相等。因为我们要用同等数量的数据样本来比较转化效果的差异。</p><pre><code>import matplotlib.pyplot as plt #导入pyplot模块\n   import seaborn as sns #导入Seaborn\n   fig = sns.countplot('裂变类型', data=df_fission) #创建柱状计数图\nfig.set_xticklabels(fig.get_xticklabels(),rotation=25)   #X轴标签倾斜\nfig.set_ylabel(&quot;数目&quot;) #Y轴标题\nplt.show() #显示图像\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/bb/2b/bbc8b7a41331f875f7d71dac64f9382b.png?wh=394x278\" alt=\"\"></p><p>看来收到助力砍价、拼团狂买和无裂变页面的用户数是基本相同的，这和我们所期望的相符。</p><p>因为每一个用户是否成功转化都已记录在案，下面我们就来计算两种裂变方案带来的转化率，与无裂变情况下的转化相比，有没有提升，并显示出每一种情况下购买产品的用户所占的比例，也就是转化率的均值。</p><pre><code>df_plot = df_fission.groupby('裂变类型').是否转化.mean().reset_index() #促销分组的转化率均值\nplt.bar(df_plot['裂变类型'],df_plot['是否转化']) #不同促销转化均值柱状图\n</code></pre><p>输出如下：</p><p><img src=\"https://static001.geekbang.org/resource/image/37/dc/37c5ab475eef6ed112cf6d49567ac9dc.png?wh=381x248\" alt=\"\"></p><p>可以看出，“情侣花享”和“拼团盛放”这两种裂变模式，都能带来转化率的显著提升，但提升的效果是有差异的。</p><p>分析完数据后，我们接下来做特征工程，把原来的数据变为机器模型可读入的格式。这段代码是为机器学习模型创建哑变量，我们以前做过很多次，相信你已经很熟悉了。</p><pre><code>df_dummies = df_fission.drop(['裂变类型'],axis=1) #在拆分哑变量前，先拿掉裂变类型\ndf_dummies = pd.get_dummies(df_dummies) #为分类变量拆分哑变量\ndf_dummies['裂变类型'] = df_fission['裂变类型'] #把裂变类型放回去\ndf_fission = df_dummies.copy() #把哑变量数据集复制给元数据集\ndf_fission.head() #显示数据\n</code></pre><p>输出如下：<br>\n<img src=\"https://static001.geekbang.org/resource/image/54/e4/54a6c47dcdbf893e04fe777d3990c5e4.png?wh=810x185\" alt=\"\"></p><p>经过前面的分析我们知道，要想知道在一个特定的裂变方案，对一个用户“购买可能性”的影响有多大，就需要把该裂变方案与无裂变的情况做独立比较，才能得到该裂变方案带来的购买可能性的增量（转化增量）。所以，我们先聚焦于“情侣花享”这种裂变模式，通过下面的代码，找出\"情侣花享\"和“没有促销”这种两组用户的所有数据：</p><pre><code>df_discount = df_fission.query(&quot;裂变类型 == '情侣花享' | 裂变类型 == '没有促销'&quot;) # 只看“情侣花享组”的裂变效果，先忽略“拼团盛放”组\n</code></pre><p>得到了这两组用户数据后，我们就可以开始构建这个数据集的标签了。</p><h2>构建增长标签</h2><p>我们前面分析到，要先把用户划分为四类：裂变购买者、裂变未购买者、无裂变购买者和无裂变未购买者。根据用户的一系列特征，判断一个用户落入这4种用户群组的概率分别是多少。所以，这4个用户群组就是我们的增长标签。我们用0，1，2，3来给每一类群组的标签编码：</p><ul>\n<li>0代表裂变购买者；</li>\n<li>1代表裂变未购买者；</li>\n<li>2代表无裂变购买者；</li>\n<li>3代表无裂变未购买者。</li>\n</ul><p>然后，我们通过下面的代码，构建增长模型的标签：</p><pre><code>df_discount.loc[(df_discount.裂变类型 == '情侣花享') &amp; (df_discount.是否转化 == 1), '标签'] = 0 #有应答裂变组,裂变购买者\ndf_discount.loc[(df_discount.裂变类型 == '情侣花享') &amp; (df_discount.是否转化 == 0), '标签'] = 1 #无应答裂变组,裂变未购买者\ndf_discount.loc[(df_discount.裂变类型 == '没有促销') &amp;  (df_discount.是否转化 == 1), '标签'] = 2 #有应答控制组,无裂变购买者\ndf_discount.loc[(df_discount.裂变类型 == '没有促销') &amp;  (df_discount.是否转化 == 0), '标签'] = 3 #无应答控制组,无裂变未购买者\ndf_discount.head()\n</code></pre><p>输出如下：<br>\n<img src=\"https://static001.geekbang.org/resource/image/a9/81/a94533e5e17c77e6a0c2e80a9a63be81.png?wh=850x192\" alt=\"\"></p><p>有了分类标签之后，我们接下来就构建出特征集和标签集，并进行训练集和测试集地拆分：</p><pre><code>X = df_discount.drop(['标签','是否转化'],axis=1) #特征集，Drop掉便签相关字段\ny = df_discount.标签 #标签集\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=16)\n</code></pre><p>接着，我们进入算法选择环节。</p><h2>选择算法训练模型</h2><p>判断一个用户落入4种分类的概率分别是多少，这其实是一个典型的多类别分类问题。解决多类别分类问题的算法和二分类问题的算法完全相同，可以通过一系列传统机器学习算法或者深度学习神经网络算法来完成。这里我们就选择XGBoost算法。</p><p>我之所以会选择XGBoost算法求分类概率，是因为在<a href=\"https://time.geekbang.org/column/article/424305\">上一讲</a>中，我们已经看到XGBoost算法作为一种集成学习方法，在分类问题中具有超强的性能。 XGBoost算法，可以说是目前最有名且最实用的集成学习算法了。</p><p>下面，我们就导入、创建并拟合xgboost模型：</p><pre><code>import xgboost as xgb #导入xgboost模型\nxgb_model = xgb.XGBClassifier() #创建xgboost模型\nxgb_model.fit(X_train.drop(['用户码','裂变类型'], axis=1), y_train) #拟合xgboost模型\n</code></pre><p>模型训练成功之后，我们就可以计算出每一个用户落入4种分类标签的概率了。</p><pre><code>uplift_probs = xgb_model.predict_proba(X_test.drop(['用户码','裂变类型'], axis=1)) #预测测试集用户的分类概率\nuplift_probs #显示4种概率\n</code></pre><p>输出如下：</p><pre><code>array([[0.00787458, 0.51820630, 0.04026253, 0.43365657],\n       [0.04618792, 0.46886584, 0.01700269, 0.46794358],\n       [0.06478575, 0.5077343 , 0.06103172, 0.36644822],\n       ...,\n       [0.11078195, 0.40745038, 0.06231715, 0.4194505 ],\n       [0.17259875, 0.38025224, 0.11393689, 0.33321217],\n       [0.06996422, 0.33474946, 0.03079985, 0.5644865 ]], dtype=float32)\n</code></pre><p>注意，这里我们并不需要用predict函数直接求得分类标签的值（0，1，2，3），因为这些值计算不出裂变增量，而是通过predict_proba，得到了测试集中每一个用户落入4种分类标签的概率数组。其中，哪个标签的概率大，就说明用户落入其中可能性大。</p><p>比如说，第三行数据[0.00787458，0.5182063 , 0.04026253，0.43365657]，这个4个概率分别代表裂变购买者、裂变未购买者、无裂变购买者和无裂变未购买者。也就是说，XGBoost模型预测这个用户有0.5182063的可能性成为裂变购买者，有0.00787458的可能性成为裂变未购买者，有0.04026253成为无裂变购买者，0.43365657成为无裂变未购买者。</p><p>下面，我们就进入了关键环节：计算特定裂变所带来的具体增量。</p><h2>计算裂变增量</h2><p>上面的机器学习模型已根据数据对每一个用户的购物特点作出预测。那么，对于每一种裂变方案，我们应该如何算出增量的大小呢？我们前面说可以用增量公式来计算：</p><p>$增量分数=（裂变购买者+无裂变未购买者）-（裂变未购买者+无裂变购买者）$</p><p>那么上面这个增量分数的计算公式是怎么推出来的呢？其实，原始的增量公式（Generalized Weighed Uplift）是这样的：</p><p>$增量分数 = \\frac{P(TR)}{P(T)} + \\frac{P(CN) }{ P( C )} - \\frac{P(TN)}{ P(T) }- \\frac{P(CR) }{P( C )}$</p><p>注意，增量分数就是指增量的大小，在这个公式中：</p><ul>\n<li>TR，即treatment responsive，表示“有促销应答”，对应于“裂变购买者”；</li>\n<li>TN，即treatment non-responsive，表示“无促销应答”，对应于“裂变未购买者”；</li>\n<li>CR，即control responsive，表示“有控制应答”，对应于“无裂变购买者”；</li>\n<li>CN，即control non-responsive，表示“无控制应答”，对应于“无裂变未购买者”。</li>\n</ul><p>而公式里的 P 表示概率值，T 表示促销人群 (TR + TN)，C 表示无促销人群 (CR + CN)。关于这个原始的增量公式，如果你有兴趣想深入了解，可以看看<a href=\"http://arno.uvt.nl/show.cgi?fid=149632\">这篇论文</a>。</p><p>在这次实战的数据分配中，因为控制组和裂变组的人数是相等的，公式中的分母P(T) 和 P( C )都是一个定值，即50%。所以，这个公式也可以简化为：</p><p>$增量分数 = (P(TR) + P(CN)) - (P(TN) + P(CR))$</p><p>由于我们的目标是要定位“<strong>促销响应者</strong>”和“<strong>别来烦我者</strong>”这两种人，<strong>其实也就是想求出有裂变较之无裂变所带来的促销响应可能性的提升和别来烦我可能性的减少</strong>，所以，你可以这样理解上面的增量分数公式：</p><p>$增量分数 =（裂变购买者+无裂变未购买者）-（裂变未购买者+无裂变购买者）$</p><p>或者也可以写成：</p><p>$增量分数 = (促销响应可能性的提升) + (别来烦我可能性的减少)$</p><p>公式的前半部分(P(TR) + P(CN))就包含促销响应者，虽然也有肯定会买者和完全不睬者，但无需关注。后半部分P(TN) + P(CR)则代表着别来烦我者，所以要取负值，这类人越少越好，其中也有肯定会买者和完全不睬者，但也无需关注。</p><p>因此，根据上面的增量公式，就求出了每个用户裂变促销前后的“购买可能性”的增量。</p><p>下面的代码就求出了这种裂变为每一个用户带来的增量分数。</p><pre><code>discount_uplift = X_test.copy() #构建增量分数数据集   \ndiscount_uplift['P_TR'] = uplift_probs[:,0] #添加有应答裂变概率\ndiscount_uplift['P_TN'] = uplift_probs[:,1] #添加无应答裂变概率\ndiscount_uplift['P_CR'] = uplift_probs[:,2] #添加有应答控制概率\ndiscount_uplift['P_CN'] = uplift_probs[:,3] #添加无应答控制概率\n#计算增量分数\ndiscount_uplift['增量分数'] = discount_uplift.eval('P_TR + P_CN - (P_TN + P_CR)') \ndiscount_uplift #显示增量分数\n</code></pre><p>输出如下：<br>\n<img src=\"https://static001.geekbang.org/resource/image/4a/5c/4aef95531a55b4f25b0eaca3ef3de45c.png?wh=1037x247\" alt=\"\"></p><p>这个分数，也就是用户从没有促销，到收到这个裂变方案的概率提升值。那么，这时我们如果按照相同的方法求出另外一中裂变方案的概率提升值，就能够比较出，对于一个特定的用户，哪种裂变形式，会更奏效！</p><p>而且，有了两种裂变方案针对于每一个用户的增量，我们还可以求出其均值。这个值就能用来比较两种裂变促销的效果优劣。也就是，整体上对于该App的用户来说，哪种裂变能够带来更多的增量。这样，我们今天的问题就得到了很好的解决。</p><h2>总结一下</h2><p>在这次实战中，我们解决了运营团队提出的，评估裂变方案适合哪类受众群体这个问题。这个解决方案的思路是通过比较两种裂变带来的转化率增长，同时看看各用户分组对裂变转化率造成的影响。</p><p>这个过程的核心，是根据机器学习模型来辅助判断每个用户是否适合推送当前裂变方案。首先调整数据集，只保留一种裂变类型（情侣花享）的用户组和无裂变用户组，并根据用户组别以及是否购买商品这两个字段，为数据集中的每一个用户构建出标签，分别是裂变购买者TR、裂变未购买者TN、无裂变购买者CR和无裂变未购买者CN。</p><p>然后，我们构建出机器学习的训练集和预测集，听过XGBoost算法创建机器学习模型，并根据训练集的特征和标签拟合模型。  拟合好的模型就可以在测试集上对其他用户进行预测，得出新用户落于上述四种情况的可能性，即P(TR)、P(TN)、P(CR)、P(CN)四个概率，并根据概率，计算裂变带来的增量。</p><p>这个过程，我们灵活的应用了机器学习模型预测多分类问题的概率的功能，完成了对裂变方案的评估。希望这个Case，能够启发你将机器学习模型应用到更多的业务运营场景中去。</p><h2>思考题</h2><p>我在这里给你留一个思考题。</p><p>我们在实战过程中，选择了“情侣花享”这种裂变形式，计算出引入该裂变之后的增量。请你继续选择“拼团盛放”这个裂变形式，完成类似的增量预测，然后比较一下两种裂变的增量均值，那个整体效果更好。</p><p>欢迎你在留言区和我分享你的观点，如果你认为这节课的内容有收获，也欢迎把它分享给你的朋友，我们下一讲再见！</p><p><img src=\"https://static001.geekbang.org/resource/image/e3/b4/e384871f4e7af473yy3eda96c6ccc2b4.jpg?wh=2284x1280\" alt=\"\"></p>",
        article_title: "18 | 增长模型：用XGBoost评估裂变海报的最佳受众群体",
      },
    ],
  },
  {
    chapterTitle: "持续赋能篇",
    children: [
      {
        title: "19 | 胸有成竹：如何快速定位合适的机器学习算法？",
        id: 425277,
        content:
          '<p>你好，我是黄佳。欢迎来到零基础实战机器学习。</p><p>首先，恭喜你闯过了所有的业务关卡，我们即将开启这个课程的第三个模块：持续赋能篇，这也是我们的最后一个模块。在前面的动手实战过程中，相信你已经对机器学习中的多种算法胸有成竹了！</p><p>那么这儿先回顾一下我们一起学习了哪些算法。</p><p>在获客关，我们用RFM值给电商用户做了分组画像。其中，我们学习了无监督学习中的<strong>聚类算法</strong>，这也是我们这个课程中唯一一个监督学习之外的算法。在变现关，我们预测了用户生命周期价值LTV，并讲解了各种模型优化方式。在此过程中，我们学习了<strong>线性回归</strong>、<strong>决策树</strong>和<strong>随机森林算法</strong>。</p><p>在激活关的深度学习部分，我们学习了如何用<strong>CNN网络</strong>对图片分类；并用<strong>RNN网络</strong>处理了时序数据，预测App的激活数。在留存关预测用户是否会流失的部分，我们通过<strong>逻辑回归算法</strong>和深度学习中的<strong>DNN网络</strong>解决了二元分类问题。在裂变关，我们评估了裂变海报的最佳受众群体。在这个实战中，我们用<strong>XGBoost</strong>这种集成学习方法完成了多元分类。</p><p><img src="https://static001.geekbang.org/resource/image/19/e8/19fd60c66f07eb2ae5910dab575802e8.jpg?wh=2000x1000" alt=""></p><p>当然，面对机器学习这个深度全然不可测的海洋，我们在这个课程中使用过的算法只是沧海一粟。不过，我们所选择的这9种算法，是在机器学习入门阶段中最为常用、也最为实用的算法，它们不仅能有效地帮助我们解决诸多实际问题，也能为我们在机器学习领域进一步的钻研打下坚实的基础。只要用好了这9种算法，面对需要分析和挖掘的数据，你已经就拥有了9种非常强大的武器，可以开始战斗了。</p><!-- [[[read_end]]] --><p>那么，你可能会疑惑：面对新的问题以及需要分析、挖掘的数据，如何快速的定位最合适的算法呢？这是一个很好的问题，同时也是一个很大的课题，而且这个问题也没有直接且确定的答案。不过，今天这一讲，我会试着根据我的经验和知识，给你总结一些简单的原则。</p><p>在接下来的介绍中，我们的算法覆盖范围并不仅限于课程中的9个算法，也会涉及其它一些算法，如果有些算法让你觉得陌生，你可以自己进行一些搜索。</p><p>在这里我先给出一个guideline：<strong>我们可以从问题本身、标签的类型、数据集的大小、特征的数量、计算时间的要求、以及模型的可解释性这些方面，来比较和选择算法。</strong></p><h2>从问题的类型开始</h2><p>要选择合适的算法，我们首先还是要回到问题本身。把问题定义好，并明确你要解决的目标，是选择算法的前提。</p><p>如果你的目标是对有标签的数据进行预测或分类，那么你就需要从监督学习算法家族中进行选择；如果你的目标是从没有标签的数据中挖掘出一些信息，或者是对数据做特征工程，比如降低数据的维度，那么你就需要在无监督学习算法家族中进行选择。</p><p>如果你的目标是生成没有见过的新数据，例如生成完全不存在的人脸，或者让机器续写个哈利波特啥的，那么生成式机器学习算法值得你研究一下。近年来非常吸引眼球的生成式对抗网络GAN（Generative Adversarial Networks）、Google推出的DeepDream算法，以及变分自编码器VAE等，都是解决这类问题的优秀算法。</p><p>现在，我们回到最为主流的监督学习问题，来继续算法的选择。在监督学习下面，我们又有两大类问题，就是回归问题和分类问题，我们可以根据标签是连续的数值，还是离散的数值，来确定问题的类型，相信你已经非常熟悉了。这两大类问题，均拥有超多的应用场景，也各自拥有一大批的算法。我们首先看一下针对回归问题的算法选择指南。</p><h2>回归问题算法选择指南</h2><p>当明确了问题本身属于回归问题后，我们就需要在线性回归、决策树、随机森林、XGBoost、朴素贝叶斯以及神经网络这些常见的回归算法中进行选择。这时候，我们就需要考虑数据集大小、特征的维度（也就是特征的多少）、训练所需的时间等因素。</p><p>当你的回归问题并不复杂，并且希望从易于解释的模型开始时，我建议你选择线性回归模型作为可靠的首选基准算法，线性回归对大、小数据集都适用，而且也可以处理高维特征数据。</p><p>如果你的数据集特征和数据样本数量都不是很多，而且特征之间有关联关系，那么SVM（支持向量机）可以作为一个选择，这个模型对于特征数量有限、意义接近的中等大小的数据集来说比较强大。在使用SVM之前，需要进行特征缩放。</p><p>如果你手头上的数据集样本数量和特征数量都十分巨大，那么可以考虑朴素贝叶斯模型，它比线性模型的速度要快得多，适用于非常大的数据集和高维数据，不过，它的性能通常要低于线性模型。</p><p>当你对模型运行速度有要求的时候，比如你的模型是在线训练、在线预测，那么决策树可以作为你的一个选择。因为它解决回归问题的速度很快，而且也不需要数据缩放。决策树的另一个优点是可以进行可视化，而且非常容易解释。</p><p>不过你要注意，如果参数设置不当，决策树就会出现过拟合的现象；如果限制了树的深度的话，又比较容易出现精度不够的问题。因此，决策树通常是作为集成学习方法的基模型而存在的，很少独立使用。</p><p>如果你追求的是模型的性能和表现，那么，随机森林几乎总是比单棵决策树的表现要好，它的性能非常强大，也不需要数据缩放。但随机森林并不合适处理高维稀疏数据集。</p><p>当然，XGBoost的性能通常比随机森林还略高一点。与随机森林相比，XGBoost的训练速度虽然更慢，但预测速度更快，需要的内存也更少。不过，在参数方面，XGBoost可调的外部参数比随机森林更多，所以调参时候会更繁琐一些。</p><p>另外，如果你的问题是时间序列问题，或者特征数量极为巨大，又没有良好的结构，而且特征之间可能存在非线性依赖关系的时候，你应该考虑使用深度神经网络。因为深度神经网络可以构建非常复杂的模型来解决这类问题，特别是对于大型数据集而言，很有优势。</p><p>但是，请你注意，深度神经网络难于解释，如果你需要优化过程的具体推导细节，那深度学习模型很难给出。此外，它对数据缩放和参数选取也比较敏感，而且，大型神经网络需要很长的训练时间。这些都是你需要根据实际情况考量的因素。</p><p>总之，面对新数据集，我建议你通常最好先从简单模型开始，比如线性模型、朴素贝叶斯或最近邻KNN，看能得到什么样的结果。对数据有了进一步了解之后，你可以考虑构建更复杂模型的算法，比如随机森林、梯度提升决策树、SVM或神经网络。</p><p>下面我们再来看分类问题的算法选择技巧。</p><h2>分类问题算法选择指南</h2><p>分类问题的算法选择，在思路上和回归问题略有不同。对于分类问题，我更多是基于问题的性质来寻找合适的算法，而不是从数据集大小、特征数目的多少以及训练时间的快慢来考量。</p><p>当我们要解决用户是否会流失、欺诈检测、促销和裂变效果等问题时，可以采用逻辑回归算法作为这些普通分类问题的基准算法，因为它简单且易于解释。当然，其它分类算法也可以用于解决这类问题。</p><p>而对于投资方面的决策、银行贷款的评估决策等，我们经常使用决策树算法。因为决策树算法最直观，最接近人类的思维方式，算法也非常易于用图表展示。当然，决策树在分类问题中也容易出现过拟合问题，因此，决策树很少单独作为一种算法来解决问题，一般都是作为集成学习方法的基模型出现。</p><p>对于类别比例不平衡的问题，比如检测生产系统中的坏件，预测高风险的病患等，我推荐你采用随机森林算法，它解决了决策树算法的过拟合问题。同时，XGBoost算法和随机森林类似，也是解决各种分类问题的好选择，性能比随机森林略高。</p><p>如果你要解决文本的情感属性判断、文本分类、推荐系统或者是人脸识别问题，那么朴素贝叶斯算法是深度学习出现之前的首选算法。和深度学习相比，朴素贝叶斯算法的优势在于可解释性较好。因此，在要求算法具有良好的解释性时，你可以通过朴素贝叶斯算法解决这类问题。</p><p>而对于疾病诊断和判断、手写数字识别等多种分类问题，SVM是一种比较好的解决方案。当然深度学习也可以解决这类问题，和深度学习相比，SVM的优势也在于可解释性较好。因此，在要求算法具有良好的解释性时，我们用SVM算法来解决多种分类问题比较好。</p><p>最后，我不得不提的是，深度学习神经网络实为解决大数据时代的分类问题的大杀器，我们前面所说的各种分类问题，都可以用深度学习来解决。尤其是非结构化的分类问题，比如语音识别、计算机视觉、自然语言处理、文本分类、自动驾驶等，深度学习都是首选项。</p><p>当然，非要“鸡蛋里挑骨头”的话，那就是在数据量小或者特征结构很好的情况下，深度学习的优势体现得不是很明显，我们还是要尽量选择简单的方案；另外，深度学习的可解释性不好，就像一个黑箱，因此，调参的过程也比较费力。</p><p>好了，这就是分类问题算法选择的一些原则。下面，我再给你总结一下选择算法时的考量因素。</p><h2>选择算法时的其它考量因素</h2><p>我们前面说，<strong>选择算法时有两个最主要的出发点，第一是问题的类型</strong>，我们要回到问题本身，看它是监督学习还是无监督学习；<strong>第二是预测的标签类型</strong>，一般来说，监督学习问题最为主流，这时候我们需要根据标签来进一步确定，问题属于回归问题还是分类问题。</p><p>除此之外，我们在选择算法时还有一些其他的考虑因素，我们一起看看。</p><h3>训练数据的大小</h3><p>通常，我们都希望能收集到大量数据，获得更可靠的预测。但很多时候，数据的量是一个制约因素，我们也没办法。因此，如果你的训练数据集较小，也就是数据样本较少，但是特征数较多（如文本数据），这时应该选择具有高偏差、低方差的算法，比如如线性回归、朴素贝叶斯或线性SVM。</p><p>如果你的训练数据足够大，并且特征数量也比较少，那你就可以使用低偏差、高方差的算法，如 KNN、决策树或核SVM。当然，对于数据很多的情况，神经网络也是很好的选择。</p><h3>特征的数量</h3><p>除数据样本量之外，特征的数量也影响模型的选择。数据集可能有大量的特征，有些特征可能并不是和问题十分相关，因此也并不重要。某种数据集，例如遗传学问题或文本数据，与数据样本的数量相比，特征的数量可能非常大。</p><p>过多的特征会使多数算法的训练时间过长，不过，SVM模型就不会受特征数量的约束，因此，对于大特征空间和较少数据量的情况，SVM模型是我们的最佳选择。当然，如果你要用其他的机器学习模型，可以使用PCA和特征选择技术来降低特征的维度，选择重要特征进行学习。</p><p>除此之外，深度学习也为我们处理巨大特征量的问题提供了非常好的解决方案。当特征数量巨大且特征空间结构复杂时，深度学习神经网络是绝佳选择。</p><h3>性能和可解释性的权衡</h3><p>有时我们还必须在模型的性能和可解释性之间进行权衡，比如有些合规性文件会要求公司在应用AI时，在给出判断结论的同时给出算法推导的原理和过程，这时候，我们采用相对简单的模型就能降低此部分合规性文档的解释难度。</p><p>高度可解释的算法（如线性回归模型）意味着我们可以轻松理解任何单个预测变量如何与要预测的目标相互关联，因为模型简单，所以很容易解释。但是，线性回归产生的映射函数范围小，其形状只能覆盖很小的特征空间，因此这类算法也被称为限制性算法。</p><p>也有些算法非常灵活，因为它们可以生成更广泛的映射函数形状。比如深层神经网络模型，它可以拟合任意形状的函数，但灵活的模型是以低可解释性为代价，来提供更高的准确性的。<strong>一般来说，随着算法灵活性和准确性的增加，其可解释性会降低。</strong>下图显示了各种算法在准确性和可解释性之间的权衡。</p><p><img src="https://static001.geekbang.org/resource/image/9c/b5/9c350f6b007824d0a9a24af1e817aab5.png?wh=380x267" alt="" title="准确性和可解释性算法之间的权衡"></p><p>至于我们要使用哪种算法，就要取决于业务问题的目标了。如果你的目标是进行严密的推理，比如疾病的确诊过程，那么限制性模型会更好，因为它们更具可解释性。如果你以更高的精度为目标，那么灵活的模型会更好。</p><h3>速度或训练时间</h3><p>更高的准确度通常也意味着更长的训练时间。此外，更大规模的数据集样本也需要更多时间来训练。在实际应用中，这个因素也会驱动算法的选择。</p><p>朴素贝叶斯、线性回归和逻辑回归这样的算法易于实现且运行迅速。像 SVM 这样的算法涉及参数调整，需要更多时间，而神经网络和随机森林，也都需要大量时间来训练数据。因此，如果你的机器学习模型是在线的应用，比如在网页上，用户选择了一个商品，你需要在一两秒钟之内呈现出推荐列表的清单，这时候你就不应该选择最精准但会耗时很长的算法了，而应该考虑朴素贝叶斯等更快的算法。</p><h3>数据的线性程度</h3><p>最后，我们还可以通过评估数据特征和标签之间的线性程度，来选择具体的算法。</p><p>许多算法都假设不同类别可以由直线（或更高维的平面或者空间）来分隔，比如逻辑回归和支持向量机。线性回归算法则假设特征和标签数据的分布趋势遵循着一条直线。如果数据是线性的，那么这些算法的表现相当不错。</p><p>然而，数据并不总是线性的，因此对于更为复杂的数据集，我们需要可以处理高维和复杂数据结构的算法，比如核 SVM、随机森林、XGBoost和神经网络。</p><p>那么，如何找出监督学习数据集的线性程度呢？对于回归问题，我们可以像在<a href="https://time.geekbang.org/column/article/414504?cid=100085501">第3讲</a>中做的那样，显示特征和标签之间的散点图，看看是否能够拟合出线性回归线；对于分类问题，我们先尝试逻辑回归或线性SVM 并检查误差值和分类性能。如果误差值很高、分类性能差，则意味着数据不是线性的，需要复杂的算法来拟合。</p><h2>总结一下</h2><p>好了，到这里我们这一讲就结束了。关于快速定位合适的机器学习算法，这其中的要点是定义好问题，明确是监督学习问题还是无监督学习问题，是分类问题还是回归问题，这能让我们排除掉不相干的算法。</p><p>在选择具体的算法时，我建议你从训练数据的大小、特征的数量、是着重考量模型的性能还是考量模型的可解释性、是否要求模型有很快的训练速度，以及数据的线性程度这几个方面，来选择最适宜的算法。</p><p>另外，我建议你从探索数据开始，熟悉你的数据。因为“更好的数据往往胜过更好的算法”，你对数据集的特征之间的性质了解得越清楚，越有可能得到高性能的模型。当然，对模型背后的原理和假设越了解，以及模型外部参数设定的含义越熟悉，也越有可能得到高性能的模型。</p><p>在定位合适的机器学习算法时，还有一个最基本的原则是，从简单的模型开始构建基准模型，然后尝试更复杂的方法。</p><p>最后，请你始终记住，尽可能地尝试多种算法，多种参数组合，然后比较它们的性能，以选择最适合特定任务的算法。但也不要忘记尝试集成方法，因为它通过博彩众长，能为你提供更好的准确性。</p><h2>思考题</h2><p>这一路学习下来，面对不同的问题，你在算法选择方面有哪些心得？你发现哪些算法对于哪些问题有较好的性能呢？欢迎你在留言区和我分享你的观点，如果你认为这节课的内容有收获，也欢迎把它分享给你的朋友，我们下一讲再见！</p><p><img src="https://static001.geekbang.org/resource/image/c9/69/c9736533a2508cc501774069c7beyy69.jpg?wh=2284x1280" alt=""></p>',
        article_title: "19 | 胸有成竹：如何快速定位合适的机器学习算法？",
      },
      {
        title: "20 | 模型部署：怎么发布训练好的机器学习模型？",
        id: 426022,
        content:
          '<p>你好，我是黄佳。欢迎来到零基础实战机器学习。</p><p>截止到现在，我们这个课程一直都是在本机上跑模型、分析和预测数据。但实际上，机器学习技术并不局限于离线应用，它也可以被部署到生产系统中发挥作用。</p><p>在具体部署时，出于对灵活性或者运行速度的需要，开发人员有可能会用Java、C或者C++等语言重写模型。当然，重写模型是非常耗时耗力的，也不一定总是要这样做。像Tensorflow、PyTorch、Sklearn这些机器学习框架都会提供相应的工具，把数据科学家调试成功的机器学习模型直接发布为Web服务，或是部署到移动设备中。</p><p>在这一讲中，我们会通过一个非常简单的示例，来看看怎么把我们之前做过的<a href="https://time.geekbang.org/column/article/414504">预测微信文章浏览量</a>的机器学习模型嵌入到Web应用中，让模型从数据中学习，同时实时接收用户的输入，并返回预测结果。由于移动设备上的部署，还需要一些额外的步骤和工具，如Tensorflow Lite等，我这里不做展示，你有兴趣的话可以阅读<a href="https://www.tensorflow.org/lite/guide#3_run_inference_with_the_model">相关文档</a>。</p><p>我先给你看看搞定后的页面：</p><p><img src="https://static001.geekbang.org/resource/image/3f/a2/3f301e335d50431a2d1581d0dafe60a2.png?wh=467x211" alt=""></p><p>在进入具体的部署之前，我带你先整体了解一下，要搭建这个基于Web的机器学习应用，我们需要构建哪些模块。</p><h2>整个项目需要构建哪些模块？</h2><p>在这个项目中，我们实际上要构建的模块有三个：</p><!-- [[[read_end]]] --><ul>\n<li><strong>机器学习模型</strong>：我们会把机器学习模型序列化，生成二进制文件，通过载入这个文件，Web应用程序中就生成了可以调用的机器学习模型对象，我们可以根据Web端传入的点赞数、转发数，用模型来预测对应的浏览量。关于序列化，我们一会儿会详细讲到。</li>\n<li><strong>用Flask开发的Web应用</strong>：它通过REST API，接收用户输入的点赞数和转发数信息，根据我们的模型计算预测值并返回它。Flask是一个使用Python编写的轻量级Web应用框架，简单易用，这里我们将用它来开发Web应用。</li>\n<li><strong>HTML页面模板</strong>：允许用户输入点赞数和转发数信息并提交给Web服务器，当Web应用收到信息之后，就会调用机器学习模型，并在HTML页面中显示浏览量的预测值。</li>\n</ul><p>那么，这三个模块在整个Web应用中是如何协作的呢？我们来看看这张流程图：</p><p><img src="https://static001.geekbang.org/resource/image/31/2c/314a503b141c96418ff287f44f112e2c.jpg?wh=2000x911" alt=""></p><p>首先，用户通过HTML页面输入点赞数和转发数，并发送预测的REST API请求。代理服务器Nginx接收到请求后，将请求转发给后端服务器uWSGI。接着，后端服务器uWSGI会把请求继续转发给Web应用程序，这时候，Web应用程序会载入机器学习模型，并根据接收到的点赞数和转发数进行预测。预测出的结果再经由Web应用程序、后端服务器uWSGI、代理服务器Nginx，一路返回给HTML页面，呈现给用户。</p><p>你可能已经注意到，在图的上半部分，还有一个“数据导入机器学习模型做训练”的过程。这个过程其实只需要完成一次，就是在我们首次创建好模型的时候，用提前准备好的数据集训练模型。后续Web应用程序每次载入模型都不需要再调用数据集进行训练。</p><p>基于上面这个流程，我们再来看下这个项目的目录结构（你可以在<a href="https://github.com/huangjia2019/geektime/tree/main/%E8%BF%9B%E9%98%B6%E7%AF%8720">这里</a>下载这些文件）：</p><p><img src="https://static001.geekbang.org/resource/image/dd/b5/dd509707e46e0db991815efdf2fb70b5.png?wh=479x182" alt=""></p><p>可以看到，在这个项目中，包含了5个文件（文件夹），分别是：</p><ul>\n<li><strong>templates</strong>：这个文件夹中存放着HTML文件<strong>index.html</strong>，这是给用户看的网页模板，也就是对应着用户能够浏览并输入信息的HTML页面；</li>\n<li><strong>app.py</strong>：它就是Web API的开发源文件，负责部署机器学习模型；</li>\n<li><strong>model.py</strong>：这是一个Python脚本文件，存放着我们创建、拟合机器学习模型的代码。Python脚本文件不是像Jupyter Notebook那样一个单元格，一个单元格交互地执行，而是在命令行整体执行。这个文件执行后会生成一个存储模型的二进制文件model.pkl，提供给Web API导入。</li>\n<li><strong>model.pkl</strong>：就是刚才讲的model.py执行时生成的文件，用来存储模型。</li>\n<li><strong>易速鲜花微信软文.csv</strong>：这就是我们用来训练模型的数据集。</li>\n</ul><p>下面我们就从最简单、最容易理解的HTML页面开始，一步步搭建来这个项目。</p><h2>创建HTML模板</h2><p>因为我们现在不再是从本机上跑机器学习模型，那么就需要有一个HTML页面，提供给用户通过网络来输入要预测的东西（也就是特征）。这也就是我们通常说的前端，这个不难理解。</p><p>HTML的语法也很简单，具体代码如下：</p><pre><code>&lt;!DOCTYPE html&gt;\n&lt;html &gt;\n&lt;head&gt;\n  &lt;meta charset=&quot;UTF-8&quot;&gt;\n  &lt;title&gt;机器学习部署应用&lt;/title&gt;\n\n\n&lt;/head&gt;\n\n\n &lt;div class=&quot;login&quot;&gt;\n\t&lt;h1&gt;浏览量预测&lt;/h1&gt;\n\n\n     &lt;!-- Main Input For Receiving Query to our ML --&gt;\n    &lt;form action=&quot;{{ url_for(\'predict\')}}&quot;method=&quot;post&quot;&gt;\n        &lt;input type=&quot;text&quot; name=&quot;点赞数&quot; placeholder=&quot;点赞数&quot; required=&quot;required&quot; /&gt;\n\t\t&lt;input type=&quot;text&quot; name=&quot;转发数&quot; placeholder=&quot;转发数&quot; required=&quot;required&quot; /&gt;\n        &lt;button type=&quot;submit&quot; class=&quot;btn btn-primary btn-block btn-large&quot;&gt;预测浏览量&lt;/button&gt;\n    &lt;/form&gt;\n\n\n   &lt;br&gt;\n   &lt;br&gt;\n   {{ prediction_text }}\n\n\n &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre><p>这段代码其实构建了这样一个页面：</p><p><img src="https://static001.geekbang.org/resource/image/98/fd/984b75dc7321503a462c618c47464afd.png?wh=481x224" alt=""></p><p>在这个页面中，用户需要填写两个字段：一个是点赞数，另一个是转发数。这二者都是机器学习模型的特征字段。在这两个字段的旁边还有一个Button“预测浏览量”，用户输入好特征字段后，就可以直接点击这个Button，此时页面会发送Request给Web应用，让它返回模型所预测的标签。</p><p>现在，我们把上面的HTML代码Copy下来，另存为HTML格式的文档，并放在项目结构的templates目录之下就好了。注意，它目前只是一个模板，其中的具体内容还要通过Web应用进行动态的渲染，这个我们等会再细说。</p><p><img src="https://static001.geekbang.org/resource/image/d9/7c/d9732c9449d37b74ba81224e96a4a67c.png?wh=560x154" alt=""></p><p>网页模板创建好了之后，我们就要创建能够预测微信软文浏览量的机器学习模型。这个机器学习的创建流程非常简单，你可以回忆一下<a href="https://time.geekbang.org/column/article/414504">第3讲</a>和<a href="https://time.geekbang.org/column/article/415149">第4讲</a>的内容，同时，你也可以在<a href="https://github.com/huangjia2019/geektime/tree/main/%E8%BF%9B%E9%98%B6%E7%AF%8720">这里</a>找到“易速鲜花微信软文”数据集。下面，我会带你重新创建一个极简的模型，你可以把它视为我们对之前内容的一次复习。</p><h2>创建机器学习模型</h2><p>整个创建过程说起来很简单。首先，我们使用 Pandas 的fillna函数，处理一下数据集中点赞数字段的缺失值（你也可以考虑用均值填充）。然后，我们再创建线性回归模型作为机器学习算法，并进行拟合和预测。最后，我们把这个模型保存为model.py文档。</p><p>这里你需要注意的是，“.py”格式的Python脚本文件可以整体运行，它不是像Jupyter Notebook中那样按每个单元格来一步步运行，这其实只是为了简化流程。而编辑器，我使用的是Annaconda中自带的Spyder（如下图所示），当然你也可以使用其他Python开发环境来编辑，这并没有特别的限制。</p><p><img src="https://static001.geekbang.org/resource/image/0c/1f/0c02b1yy0b2a17yy4c199ec7e53cf51f.png?wh=1262x772" alt="" title="Spyder中的机器学习模型代码"></p><p>我们看一下整个创建机器学习模型的Python代码：</p><pre><code>import pandas as pd # 导入Pandas\nimport pickle # 导入序列化工具Pickle\n\n\ndf_ads = pd.read_csv(\'易速鲜花微信软文.csv\') #导入数据集\n\n\ndf_ads[\'转发数\'].fillna(df_ads[\'点赞数\'], inplace=True) #一种补值方法\n\n\nX = df_ads.drop([\'浏览量\'],axis=1) # 特征集，Drop掉标签相关字段\ny = df_ads.浏览量 # 标签集\n\n\nfrom sklearn.linear_model import LinearRegression #导入线性回归模型\nregressor = LinearRegression() #创建线性回归模型\n\n\nregressor.fit(X, y) #拟合模型\n\n\npickle.dump(regressor, open(\'model.pkl\',\'wb\')) #序列化模型（就是存盘）\n\n\nmodel = pickle.load(open(\'model.pkl\',\'rb\')) #反序列化模型（就是再导入模型）\n\n\nprint(model.predict([[300, 800]])) #进行一个预测\n</code></pre><p>这段Python代码就是我们反复操练过的机器学习实战5步，相信你并不陌生。代码中唯一的新东西是Pickle。Pickle 是Python 中的序列化和反序列化工具，可以将Python对象以文件的形式存放在磁盘上，以供后面的Web应用调用它。</p><p>具体来讲，Pickle会把内存中的模型对象（也就是Python对象），转换为0和1的字节流，形成字节流文件model.pkl存储到磁盘，这个过程叫存盘，也叫“序列化”。</p><p>这个字节流文件model.pkl可以被保存到磁盘的任意地方，当我们需要调用模型的时候，Pickle 会把磁盘中的model.pkl再转换为Python对象，存在内存中。这个时候模型就被还原了，可以使用了。这个过程就叫做“反序列化”。</p><p>在上面的代码中，pickle.dump()做的就是序列化，pickle.load()做的就是反序列化。对于这两个过程，你可以通过下面这张图做进一步的理解：</p><p><img src="https://static001.geekbang.org/resource/image/d9/36/d96fa0b749552704f742ed700bae3036.jpg?wh=2000x945" alt="" title="序列化和反序列化"></p><p>当我们把上面那段Python代码存为model.py文档，并在Spyder中运行model.py后，模型就完成了创建、训练和拟合的过程，同时也经历了序列化。这时候，model.py所在目录下就出现了一个字节流文件model.pkl文件：</p><p><img src="https://static001.geekbang.org/resource/image/dd/b5/dd509707e46e0db991815efdf2fb70b5.png?wh=479x182" alt=""></p><p>我们前面提到，在Web应用调用模型的过程中，需要把模型对象做一个转换，然后模型才能被使用。虽然我们用<a href="https://docs.python.org/3/library/pickle.html">Pickle</a>实现这个过程比较方便，但是它要求应用程序也是Python语言开发的，否则应用程序无法读取<a href="https://docs.python.org/3/library/pickle.html">Pickle</a>所生成的序列化文件。</p><p>显然，这并不适用于我们所有的应用开发需求。因此，这里我再给你介绍几种模型序列化和导出的格式，如果你在工程实践中需要使用，参考相关文档即可，它们的原理和Pickle是类似的。</p><ul>\n<li><a href="https://joblib.readthedocs.io/en/latest/"><strong>Joblib</strong></a>：Joblib是一组在 Python 中提供轻量级流水线（pipelining）的工具，它的功能和Pickle相似。Joblib通过缓存机制将磁盘上的文件隐式链接到原始 Python 对象的执行上下文，并支持简单的并行计算。和Pickle相比，它的优势是并行化、良好的日志、追踪功能以及大数据支持。</li>\n<li><a href="https://github.com/onnx"><strong>ONNX</strong></a>：ONNX是一种开放神经网络交换格式，支持跨语言存储和移植预测模型。大多数深度学习库都支持它，其中，sklearn有专门的扩展库，负责把模型转换为ONNX的格式。</li>\n<li><a href="https://en.wikipedia.org/wiki/Predictive_Model_Markup_Language"><strong>PMML</strong></a>：PMML是预测模型的另一种交换格式。与ONNX一样，sklearn也有一个专门的扩展库用于将模型转换为PMML格式。不过，它的缺点是仅支持某些类型的预测模型。 PMML历史悠久，出现于1997年，因此，有大量的应用程序使用这个格式，例如PEGA和SAP等。</li>\n<li><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html#about-pojos-and-mojos"><strong>POJO 和 MOJO</strong></a>：POJO和MOJO是<a href="https://www.h2o.ai/">H2O.ai</a>的导出格式，旨在为 Java 应用程序提供一个易于嵌入的模型。然而，它们仅适用于使用 H2O 的平台。</li>\n</ul><p>好了，现在我们有了HTML网页模板和机器学习模型，下一步，我们开始创建Web应用，用这个应用来调用机器学习模型。</p><h2>创建Web应用，并在其中调用模型</h2><p>创建Web应用并不复杂，我们用Flask就可以轻松完成。Flask是一个使用Python编写的轻量级Web应用框架，它内置开发服务器和调试器，也支持RESTful 请求，不需要我们做复杂的搭建。</p><p>我们前面说，这个Web应用的主要作用是，在接收到HTML页面传来的信息后，调用模型计算出预测结果，然后把预测结果返回给HTML页面。</p><p>当然，这只是一个大致的过程，详细来讲，这里面有两个关键步骤需要你注意。第一，在调用模型的时候，我们需要把模型从字节流文件反序列化为Python对象，然后这个模型才能正式被使用，这一点我们在前面提到过。</p><p>第二，Web应用在接收到用户输入的信息后，实际上会调用index.html模板生成一个/predict页面，这个页面是最终呈现给用户的，其中会包含预测结果。在模型预测出结果后，Web应用会把这个结果提交到这个/predict页面，返回给用户。</p><p>理解了这两点后，我们现在来看具体的代码：</p><pre><code>import numpy as np #导入NumPy\nfrom flask import Flask, request, render_template #导入Flask相关包\nimport pickle #导入模块序列化包\n\n\napp = Flask(__name__) \nmodel = pickle.load(open(\'model.pkl\', \'rb\')) #反序列化模型\n\n\n@app.route(\'/\')\ndef home(): # 默认启动页面\n    return render_template(\'index.html\') # 启动index.html\n\n\n@app.route(\'/predict\',methods=[\'POST\'])\ndef predict(): # 启动预测页面\n\n\n    features = [int(x) for x in request.form.values()] # 输入特征\n    label = [np.array(features)] # 标签\n    prediction = model.predict(label) # 预测结果\n\n\n    output = round(prediction[0], 2) #输出预测结果\n\n\n    return render_template(\'index.html\', #预测浏览量\n                           prediction_text=\'浏览量 $ {}\'.format(output)\n\n\nif __name__ == &quot;__main__&quot;: # 启动程序\n    app.run(debug=True)\n</code></pre><p>那么这个Web应用，就把我们之前训练好的模型和HTML页面串联起来了，并能接收HTML请求，返回结果。</p><p>考虑到你可能不太理解这些代码的具体含义，现在我给你简单讲解一下。前几行是导入相关包，这没什么好解释的，我们直接从第6行代码开始看。第6行model = pickle.load(open(‘model.pkl’, ‘rb’))的意思就是导入序列化后的模型，并进行反序列化。</p><p>紧接着下一行代码@app.route (‘/index’)是一个装饰器。简单来说，它只是将它下面定义的方法映射到装饰器中提到的 URL，也就是每当用户访问该 URL ‘/’ （完整地址也会有一个 ip 地址和一个端口号，比如 <a href="http://127.0.0.1:5000/">http://127.0.0.1:5000/</a>），index()方法将被自动调用， index() 方法返回index.html 。注意，我们在index.html 为用户提供了两个输入框，用户可以在其中输入任意的点赞数和转发数的值。</p><p>而在home()函数中，flask.render_template()会查找index.html模板，并渲染出（也就是在运行时动态生成）一个最终给到用户的HTML页面。</p><p>第12行代码@app.route (‘/predict’)也是一个装饰器，这个装饰器将predict()方法映射到以“/predict”结尾的URL ，它接受用户提供的输入，完成所有预处理，生成最终特征集，然后调用模型并获得最终预测值。</p><p>而其中render_template()函数所做的工作就是动态渲染HTML页面部分，将预测结果放入 HTML 页面中。只有当用户输入特征，并点击index.html页面中的提交按钮时，预测结果才会被显示。在index.html模板中，我们已经为这些变量值创建了占位符，而render_template 负责获取模板文件夹中的index.html，把预测值放进HTML模板，渲染出包含最终预测结果的HTML页面。</p><p>通过上述讲解，我想你应该可以理解这段创建Web应用的代码了。那到这里呢，我们所有的编码工作也就完成了！下面，我们就来启动这个Web应用。</p><h2>使用已经发布的App</h2><p>我们单击Spyder的Run按钮就能启动Web应用。当然，你还可以在命令行界面中使用python app.py来运行它。</p><p><img src="https://static001.geekbang.org/resource/image/6e/d8/6eedb2yy314c75709ee7d7324e3cabd8.png?wh=489x223" alt=""></p><p>App启动运行后，IPython Console中的输出如下：</p><pre><code>runfile(\'C:/Users/jacky.huang/20/app.py\', wdir=\'C:/Users/jacky.huang/20\')\n * Serving Flask app &quot;app&quot; (lazy loading)\n * Environment: production\n   WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: on\n * Restarting with windowsapi reloader\n</code></pre><p>这表明，此时Web应用已经开始在WSGI服务器上运行，等候客户端页面随时发送请求。</p><p>现在，我们打开一个本机浏览器，访问<a href="http://127.0.0.1:5000/">http://127.0.0.1:5000/</a>页面。服务器返回如下HTML页面：</p><p><img src="https://static001.geekbang.org/resource/image/cc/42/cc412225070cfd4ebc03bda9503ab142.png?wh=486x191" alt=""></p><p>然后，我们充当用户角色，输入点赞数和转发数，希望基于这些数值来预测浏览量：</p><p><img src="https://static001.geekbang.org/resource/image/98/fd/984b75dc7321503a462c618c47464afd.png?wh=481x224" alt=""></p><p>点击“预测浏览量”提交数据后，我们得到了浏览量预测结果：</p><p><img src="https://static001.geekbang.org/resource/image/3f/a2/3f301e335d50431a2d1581d0dafe60a2.png?wh=467x211" alt=""></p><p>这说明服务器成功接到HTML页面发送过来的POST API的请求，并根据模型的预测，通过<a href="http://127.0.0.1:5000/predict">http://127.0.0.1:5000/predict</a>页面返回了预测结果：当点赞数为500，转发数为300时，机器学习模型预测的文章浏览量是46292。</p><p>这样，我们就把一个简单的机器学习模型打包成了一个Web应用，成功的部署到网页中啦！</p><h2>总结一下</h2><p>到这里，我们这一讲就结束了。现在，我们整体做个回顾。</p><p>通常，我们会把机器学习模型部署到生产系统（Web端或移动端）中投入使用。部署机器学习模型有多种方式，比如开发人员重新编写模型、通过Web应用把模型部署到Web服务器、通过Tensorflow Lite等工具部署到移动端等等。</p><p>这一讲我们重点介绍了怎么把机器学习模型发布为Web应用。总体上，有这么几个开发环节：</p><ol>\n<li>开发前端网页HTML呈现模板，用于传递用户输入的特征，同时显示服务器返回的预测结果；</li>\n<li>创建机器学习模型，对于模型的训练数据集，我这里仍然使用了csv文件；</li>\n<li>通过Pickle工具进行序列化，保存机器学习模型；</li>\n<li>用Flask框架开发一个轻量级的Web应用。其中，我们是先反序列化了机器学习模型，然后再把机器学习应用部署到Web服务器上。</li>\n</ol><p>实际上，这几个环节都分别有多种实现方式，比如在模型训练数据时，你可以通过访问数据库的方式来获取实时的数据文件；在模型序列化时，你也可以使用Joblib来代替Pickle；在开发Web应用时，你可以选择除Flask之外的其他框架……具体方式，你可以根据实际需要灵活选择。</p><h2>思考题</h2><p>下面，我给你留两道思考题。</p><ol>\n<li>这里我是把预测微信软文浏览量的模型集成在了Web应用中，你是否可以选择其它的实战模型（比如预测用户的LTV价值），也把它集成在Web应用中，通过HTML访问并预测。</li>\n<li>你还有哪些机器学习模型上线部署的经验，可否跟我分享一下？</li>\n</ol><p>欢迎你在留言区和我分享你的观点，如果你认为这节课的内容有收获，也欢迎把它分享给你的朋友，我们下一讲再见！</p><p><img src="https://static001.geekbang.org/resource/image/3c/6c/3c5bff0cd573294980d2696d57d6196c.jpg?wh=2284x1280" alt=""></p>',
        article_title: "20 | 模型部署：怎么发布训练好的机器学习模型？",
      },
      {
        title: "21｜持续精进：如何在机器学习领域中找准前进的方向？",
        id: 426876,
        content:
          '<p>你好，我是黄佳。欢迎来到零基础实战机器学习。</p><p>我常常说，一旦你踏进了机器学习领域，就等同于踏进了“终身学习”之旅。因为机器学习领域庞大而繁杂，我们这20多讲的内容仅仅是一个开始。除了我们讲过的监督学习、无监督学习、半监督学习、强化学习、深度学习和集成学习之外，还有迁移学习、自动化机器学习、联邦学习、图神经网络等多种学习类型和研究方向。</p><p>那么，作为机器学习使用者的我们，面对机器学习领域里众多的“新热点”和种类繁多的研究方向，应该怎么去找准关注点和发力点，更高效地赋能自己的个人发展呢？在这一讲中，我将结合机器学习最新发展现状和技术演进趋势，来带你探讨这个问题。</p><h2>迁移学习</h2><p>我这里给你的第一个建议就是<strong>不要重复造轮子</strong>。要知道，人类的知识大厦都是经年累月，逐层搭建起来的，机器学习发展到今天，不仅有非常多的包供我们调用，甚至也已经有越来越多的模型，我们可以直接使用。在上一讲中，我们就预训练了一个模型，然后在Web应用中直接把这个训练好的模型拿过来使用。其实，对于别人已经预训练成功的大型模型，我们也可以做个微调，然后应用到自己的业务场景中。这个思路，就是“迁移学习”。</p><p>迁移学习（Transfer Learning）是把为任务 A 开发的模型作为初始点，重新使用在为任务 B 开发模型的过程中。在深度学习的计算机视觉任务和自然语言处理任务里，迁移学习特别常见。因为大型神经网络模型的训练、调试、优化的过程会消耗巨大的时间资源和计算资源，为了避免资源浪费，很多公司会找到开源发布出来的模型，通过迁移学习将这些模型已习得的强大技能迁移到相关的的问题上。</p><!-- [[[read_end]]] --><p>举例来说，我们在<a href="https://time.geekbang.org/column/article/420372">第11讲</a>中，创建了一个简单的CNN神经网络的鲜花分类器，准确率达到了80%左右。而这种深度学习图像分类神经网络，许多知名机构其实也在不断地发布，它们更复杂，精度也更高，比如<a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/">牛津 VGG 模型</a>、<a href="https://cloud.google.com/tpu/docs/inception-v3-advanced">谷歌 Inception 模型</a>、<a href="https://docs.microsoft.com/zh-cn/azure/machine-learning/algorithm-module-reference/resnet">微软 ResNet 模型</a>等。下面两张图，展示的就是Google Inception V3模型和VGG19模型的网络结构。</p><p><img src="https://static001.geekbang.org/resource/image/fc/93/fc0dc1997b0e068a743302d882af4e93.jpg?wh=2000x794" alt="" title="Google Inception V3 结构\n（来源：Pulmonary Image Classification Based on Inception-v3 Transfer Learning Model ）"></p><p><img src="https://static001.geekbang.org/resource/image/54/f0/54a41149d7e71a9736198b6bfcc0c7f0.jpg?wh=2000x1000" alt="" title="牛津VGG 19的网络结构\n（来源：Melanoma Thickness Prediction Based on Convolutional Neural Network with VGG-19 Model Transfer Learning）"></p><p>作为机器学习的应用者，我们就可以利用这些训练好的大型卷积网络，来实现自己的机器学习任务，而且这个迁移过程并没有我们想象得那么困难。我们现在就试着对牛津VGG19的网络模型基础进行微调。</p><pre><code>import tensorflow as tf # 导入tensorflow\nfrom tf.keras.applications import VGG19 # 基网络是VGG19\nfrom tf.keras import models # 导入模块\nfrom tf.keras import layers # 导入层\nfrom tf.keras import optimizers # 导入优化器\n# 预训练的卷积基\nconv_base = VGG19(weights=\'imagenet\', include_top=False,\ninput_shape=(150, 150, 3))\nconv_base.trainable = True # 解冻卷积基\n# 冻结其他卷积层, 仅设置block5_conv1 可训练\nset_trainable = False\nfor layer in conv_base.layers:\n    if layer.name == \'block5_conv1\': #仅设置block5_conv1 可微调\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True #仅设置block5_conv1 可微调\n    else:\n        layer.trainable = False #其它4个块冻结\nmodel = models.Sequential() # 序贯模型\nmodel.add(conv_base) # 基网络的迁移\nmodel.add(layers.Flatten()) # 展平层\nmodel.add(layers.Dense(128, activation=\'relu\')) # 微调全连接层\nmodel.add(layers.Dense(10, activation=\'softmax\')) # 微调分类输出层\nmodel.compile(loss=\'binary_crossentropy\', # 交叉熵损失函数\n# 为优化器设置小的学习速率, 就是在微调第5 卷积层的权重\noptimizer=optimizers.adam(lr=1e-4),\nmetrics=[\'acc\']) # 评估指标为准确率\nmodel.fit(X_train, y_train, epochs=2, validation_split=0.2) # 训练网络\n</code></pre><p>上面的代码并不难理解，其中最主要的逻辑就是，我们要先解冻VGG19所包含的5个卷积块的基模型，然后再通过set_trainable对象冻结其中4个卷积块，这就是我们从VGG19这个模型中迁移过来的知识，最后我们把VGG19的第5个模块block5_conv1设为可调、可训练的模块即可。</p><p>这样，我们就实现了在预训练的卷积网络VGG19 基础之上的微调。微调之后，我们得到了一个新模型：</p><p><img src="https://static001.geekbang.org/resource/image/f1/98/f1ca19bfb547eb94fef660fc91aa8698.png?wh=1169x449" alt="" title="基于VGG19 网络的迁移学习模型"></p><p>有了这个新模型后，我们就可以训练新模型，并拿这个新模型对任意图像数据集做分类了，当然也包括我们的易速鲜花图片数据。</p><p>通过上面的迁移学习示例，我希望你能具有“多利用他人成果，避免从零开始”的思维，当大型的模型有所发展，我们就可以在新模型的基础上“与时俱进”，把最新的前沿成果利用起来。</p><p>接下来，我要给你第二个建议，<strong>多关注“自动化机器学习”的可能性</strong>。</p><h2>自动化机器学习</h2><p>你肯定已经体会到了特征工程的麻烦和参数调优的苦恼。在典型的机器学习应用中，为了让数据适合机器学习，机器学习工程师必须应用适当的数据预处理、特征工程、特征提取和特征选择方法。在这些步骤之后，还必须进行算法选择和超参数优化，以最大限度地提高模型的预测性能。</p><p>这里面的每一步都非常有挑战。不仅如此，在尝试部署新开发的AI系统和机器学习模型时，企业和组织也经常在系统可维护性、可扩展性和治理方面苦苦挣扎，而AI项目往往又无法产生预期的回报。</p><p>那如何让AI项目更加顺畅地落地呢？自动化机器学习（AutoML）非常值得一提。自动化机器学习是把机器学习应用于现实世界问题，并将任务自动化的过程。它力图简化机器学习项目中的步骤，形成从数据预处理、特征工程，到算法选择，参数调优直至部署上线的完整解决方案，使整个机器学习项目变得更加高效。AutoML 的目标就是让非专家也能使用机器学习模型和技术。</p><p>当调参和部署的负担减轻之后，机器学习工程师们就可以花更多的时间来关注机器学习模型本身，尝试不同的架构，设计并优化新的算法。</p><p>自动化机器学习包括以下三个方面。这里我在介绍每一种自动化机器学习的同时，也给出了相应的工具的链接，如果你有兴趣，可以尝试尝试。</p><ol>\n<li><strong>自动化特征工程</strong></li>\n</ol><p>我们知道，机器学习算法的性能很大程度上取决于数据特征的质量，所以特征工程涉及了大量的试验和深厚的领域知识，对于新手来说，要进行有效的特征工程，难度就比较大。而自动化特征工程的目的就是通过不断迭代、调整，自动创建出新的特征集，直到模型达到令人满意的准确性为止。</p><p>目前已存在的一些自动化特征工程框架包括<a href="https://automl.github.io/auto-sklearn/master/">DataRobot</a>、H2O的<a href="https://www.h2o.ai/products/h2o-driverless-ai/">Driverless AI</a>和<a href="https://tsfresh.readthedocs.io/en/latest/">tsfresh库</a>，这些框架可以辅助我们探索特征之间的关系，完成一部分特征工程的任务。</p><ol start="2">\n<li><strong>自动化模型选择和超参数调整</strong></li>\n</ol><p>机器学习算法的目的是根据数据特征来训练机器模型，以预测新数据的标签值。但每种算法都适合于特定类型的问题，没有一种机器学习算法在所有数据集上都能有最佳表现，而且超参数调整过程也必不可少。在模型选择过程中，我们会尝试使用不同的变量，不同的系数或不同的超参数。这个过程极大地考验着工程师的经验、技巧和耐心。</p><p>这时候，我们可以就借助自动化模型选择工具，针对手头的特定任务，遍历所有合适的模型，并选择产生最高准确性或最低误差的模型。目前市面上已有的自动化模型选择工具有<a href="https://github.com/automl/auto-sklearn">Auto-sklearn</a>、H2O的<a href="https://www.h2o.ai/products/h2o-driverless-ai/">Driverless AI</a>、微软Azure的<a href="https://azure.microsoft.com/zh-cn/services/machine-learning/automatedml/">Automated ML</a>工具包、Google Cloud的<a href="https://cloud.google.com/automl">AutoML</a>等，这些工具只要有了贴好标签的数据后，就可以直接训练模型，帮我们自动尝试不同的模型，并完成参数调试和算法优化。</p><ol start="3">\n<li><strong>自动部署机器学习模型</strong></li>\n</ol><p>虽然机器学习工程师会把项目焦点放在开发算法和调试参数上，模型的部署和在生产系统中的运行维护很少被提及，但是，这里面也存在很多挑战。有时候，部署上线花费的时间甚至会超过训练、调试模型所用的时间。因此，自动部署的工具，对于并不了解DevOps的机器学习工程师们来说，其实大有用途。</p><p>我们常用的自动部署工具有：</p><ul>\n<li><a href="https://docs.microsoft.com/en-gb/azure/machine-learning/tutorial-deploy-models-with-aml">Microsoft Machine Learning Service</a>：可以将机器学习模型部署为可扩展的Kubernetes集群的Web服务；</li>\n<li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html">Amazon Sage Maker</a>和<a href="https://cloud.google.com/ai-platform/prediction/docs/online-predict">Google Cloud ML</a>：能快捷实现Web部署功能。通过对托管机器学习模型的Web服务进行HTTP调用，这二者都可以实现推断和预测数据；</li>\n<li><a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a>：可将Tensorflow机器学习模型部署到生产系统中。只需要几行代码，我们就可以通过Tensorflow模型生成API。</li>\n</ul><p>从上述三个方面可以看到，从特征工程到模型选择、参数调试，再到生产部署，自动化机器学习几乎覆盖了机器学习流程中的每一个环节。当然，这些工具是不是好用，能不能真正解决机器学习项目中的痛点，就要仁者见仁智者见智了。也许这些自动化工具还处于它们生命周期的初级阶段，不是我们所希望的那么“易学易用”。但是，我们有理由认为，优秀的AutoML工具一定会让机器学习变得越来越简单易用。</p><p>接下来，我要给你的第三个，也是最后一个建议：<strong>关注并思考机器学习的最新进展将如何进一步拓展AI落地应用的覆盖面</strong>。因为这样能够帮你占得先机，一旦实用的工具包出现，技术开始实际落地时，你就可以先人一步把它们用起来，做潮流的引领者。而目前，最新机器学习热门研究方向是图神经网络。</p><h2>图神经网络</h2><p>图神经网络（GNN）是2021年机器学习学术界的“红星”，也是各大深度学习顶会研究的焦点技术，到现在依旧方兴未艾。</p><p>我们说，迁移学习解决的主要问题是如何在机器学习模型之间进行知识的传递和迁移，而自动化机器学习的愿景是为了让机器学习的应用落地过程变得越来越简单，那么，深度强化学习和图神经网络等学习类型的出现，则意味着<strong>人工智能的应用领域变得越来越广阔，而且多种AI源流正在融合，让机器学习领域呈现出百川归海的趋势</strong>。</p><p>从机器学习的源流演变历程来看，有三个主要学派起着关键作用，它们分别是符号主义（Symbolicism）、连接主义（Connectionism）和行为主义（Actionism）。</p><p><strong>符号主义</strong>注重研究知识表达和逻辑推理。这一学派的主要成果有两个，一个是贝叶斯因果网络，另一个是知识图谱。贝叶斯因果网络又称信度网络，是Bayes方法的扩展，也是目前知识表达和推理领域最有效的理论模型之一。它的提出者是2011年图灵奖获得者Judea Pearl。而知识图谱主要由谷歌、微软、百度等搜索引擎公司推动，目标是把搜索引擎从关键词匹配，推进到语义匹配。</p><p><strong>连接主义</strong>的起源是仿生学，用数学模型来模仿神经元，形成深层神经网络模型。神经网络在1950年代诞生，期间也有过发展的低迷期，但目前深度学习风头正劲，领军人物有Yann LeCun、Geoff Hinton和Yoshua Bengio，人称三巨头，他们在2018年同获图灵奖。不过，我们说过，深度学习模型最遭人诟病的缺陷是解释性差。Judea Pearl就曾经在《为什么》一书中诟病深度学习既不易解释，又无法进行因果推理，你如果感兴趣可以了解一下。</p><p>而<strong>行为主义</strong>则是把控制论引入到机器学习中，这一学派认为，机器的自适应、自组织、自学习功能是由系统的输入输出反馈行为决定的。其中，最著名的成果是强化学习。强化学习，强调如何基于环境而行动，以取得最大化的预期利益，其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。</p><p>从几年前开始，Google的DeepMind研究员们把传统强化学习与深度学习融合，形成了<strong>深度强化学习</strong>（deep reinforcement learning）。2013年，DeepMind发布的论文<a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">《用深度强化学习打Atari游戏》</a>（<a href="http://playing%20atari%20with%20deep%20reinforcement%20learning/">Playing Atari with Deep Reinforcement Learning</a>）就是深度强化学习的开山之作，它介绍深度强化网络DQN（deep Q network）集成了强化学习的决策能力，也集成了深度学习在复杂问题问题上强大的拟合能力。</p><p>深度强化学习的出现让强化学习技术真正走向实用。到目前为止，深度强化学习领域出现了大量的算法和论文，并且能够解决人机博弈、自动驾驶等现实场景中的复杂问题。深度强化学习的代表作AlphaGo，已经能够轻松战胜当今世界所有的人类围棋高手了。</p><p>相对于深度学习和强化学习，符号主义的AI应用表现虽然没有那么抢眼，但是仍有很大发展。近年来，大量知识图谱不断涌现，这些蕴含人类大量先验知识的宝库却尚未被深度学习有效利用。因此，在AI最新的发展趋势中，因果网络、知识图谱、深度学习、强化学习也呈现出相互融合的趋势。</p><p>在这一背景之下，来自Google的DeepMind、谷歌大脑、MIT等机构的27位作者在2018年发表了又一篇重磅论文<a href="https://research.google/pubs/pub47094/">《关系归纳偏置、深度学习和图网络》</a>（<a href="https://arxiv.org/abs/1806.01261">Relational inductive biases, deep learning, and graph networks</a>）。这篇论文指出人工智能的未来是把传统的贝叶斯因果网络和知识图谱，与深度强化学习融合。而把端到端学习与归纳推理相结合，将有望解决Judea Pearl所说的深度学习无法进行关系推理的问题，能进一步提升深度学习模型效果。</p><p>这样，以知识图谱为代表的符号主义、以深度学习为代表的连接主义，就逐渐脱离了原先各自独立发展的轨道，走上了协同并进的新道路。</p><p>具体来讲，怎么融合呢？DeepMind认为要从图神经网络（Graph Neural Networks，GNN）入手。<a href="https://research.google/pubs/pub47094/">《关系归纳偏置、深度学习和图网络》</a>这篇论文中也探讨了如何在深度学习结构（比如全连接层、卷积层和递归层）中，使用图神经网络（也叫图深度学习）的关系归纳偏置，为操纵结构化知识和生成结构化行为提供可能性。</p><p>那么，图神经网络和普通神经网络又有何不同？这就要从数据集的类型说起。</p><p>我们知道，神经网络已经在对象检测、图像识别、机器翻译和语音识别等领域大显身手，这是因为深度学习擅长捕捉欧几里得数据的隐藏模式，解决了计算机视觉和自然语言处理中的很多问题。像图像、文本、视频等尺寸维度确定并且有序排列的数据就是我们说的“欧几里得数据”。</p><p>对应于“欧几里得数据”，还有另一种类型的数据被称为“非欧几里德数据”，这类数据的对象关系非常复杂，并且具有相互依赖的图形，比如社交网络、知识图谱、复杂的文件系统等，它们的表现形式更加非结构化，难以用一个序列或者一个网络来表示，所以我们把非欧几里德数据也统称为图。下面就是一个复杂的社交网络图的抽象展示。</p><p><img src="https://static001.geekbang.org/resource/image/90/d8/906555e6c46915dae4e7a91017cd10d8.jpg?wh=2000x1000" alt="" title="社交网络图"></p><p>很明显，和简单的文本和图像相比，图更加复杂，我们处理它的难点有很多：</p><ul>\n<li>图的大小是任意的；</li>\n<li>图的拓扑结构复杂，没有像图像一样的空间局部性；</li>\n<li>图没有固定的节点顺序，没有固定的形式，而且无序节点可变大变小，其中节点可以有不同数量的邻居。换句话说，图没有一个参考节点，还常常是动态的；</li>\n<li>图包含多模态的特征；</li>\n<li>……</li>\n</ul><p>而现有的机器学习算法有一个核心假设，即实例彼此独立，这对于图数据来说是错误的，因为图数据的每个节点都通过各种类型的链接与其他节点相关。那么，对于这样的图数据，如何利用深度学习进行建模呢？</p><p>答案就是融合了因果推理、知识图谱、强化学习和深度学习等诸多元素的图神经网络。图神经网络的目标就是对图所描述的数据进行推理，建立起可以直接应用于图的神经网络，并提供方法来执行节点级、边级和图级预测任务，以完成卷积神经网络、循环神经网络等深度学习网络无法完成的任务。</p><p>因此，你不难看出，当神经网络发展到图神经网络，机器学习的应用范围就更可以进一步扩展了。在社交网络数据分析、推荐系统、物理建模、自然语言处理和图上的组合优化问题等，AI应用肯定能有新的突破。</p><p>对于图神经网络，我们这里只是做拓宽视野性质的说明，如果你对它有兴趣，更多的内容可以参考<a href="https://github.com/thunlp/GNNPapers/blob/master/README.md">清华大学整理的GNN论文集</a>。相信在不久的将来，除了论文之外，还会出现更多好用、易用的GNN工具包供我们直接调用，解决更加具体的问题。</p><h2>总结一下</h2><p>好，我们这一讲到这里就结束了，我们现在总结一下。</p><p>在这一讲中，我从三个角度带你探讨了作为“机器学习的应用者”，如何利用好机器学习领域的最新进展，来赋能个人发展。</p><p>首先，通过迁移学习，我们可以利用各大公司开源、发布的最新、最稳定的模型，在其基础上进行微调，以适应自己的项目需要。</p><p>另外，我们还可以借助自动化机器学习，比如自动化特征工程、自动化模型选择和超参数调整、自动部署机器学习模型，来简化机器学习项目的开发部署流程，降低开发难度，提高我们的项目开发效率。</p><p>最后，作为机器学习的应用者，我们也应当了解人工智能从何处起源，要发展到哪里去，这将有助于我们洞悉未来，占得先机。而深度强化学习和图神经网络的出现，代表着符号主义、连接主义和行为主义这三大AI源流的汇聚，这种汇聚让最新的机器学习模型比以前更加强大，而且能应用到更多实际的场景中。</p><p>总之，机器学习这个领域异常庞大，发展也非常迅速，我们还有许许多多的热点问题没有探讨，比如AI和物联网与5G的结合、AI和网络安全的结合，再比如对AI和伦理问题、AI和数据安全、以及如何通过联邦学习在数据共享的同时保护数据隐私等等。如果你的工作和上述某些方面相关，或者对其中某个方向感兴趣，不妨去深入探索。</p><p>虽然我们的课程没有涉及每一个方向，但有一点是永恒不变的：<strong>任何技术都需要“人”去实现它</strong>。如果我们把AI看作是一个工具，那它只会让我们的工作变得更轻松、更愉悦。机器学习为我们提供了大量的机会，同时也提出了很多具体的问题和挑战。当然，问题和挑战本身都是机会，这取决于我们如何去应对它们。我希望你能多学习、多思考。</p><h2>思考题</h2><p>在这一讲的最后，我希望你能够分享一下你对机器学习的理解和感悟，以及你所了解的机器学习和整个人工智能领域的最新进展。最后，祝愿你在机器学习之路上走的更远，用好机器学习这个工具，并做出自己的探索和贡献！</p><p><img src="https://static001.geekbang.org/resource/image/b4/54/b47216f33caf675988e0bd421af8b154.jpg?wh=2284x1280" alt=""></p>',
        article_title: "21｜持续精进：如何在机器学习领域中找准前进的方向？",
      },
    ],
  },
  {
    chapterTitle: "结束语",
    children: [
      {
        title: "一套习题，测出你对机器学习的掌握程度",
        id: 427352,
        content:
          '<p>你好，我是黄佳。</p><p>咱们课程的核心内容都已经更新完了，在临近告别前，我还给你准备了一份期末测试题，这套测试题共有 3 道单选题和 17 道多选题，满分 100，核心考点都出自前面讲到的所有重要知识，希望可以帮助你进行一场自测！快点击下面的按钮开始测试吧！</p><p><a href="http://time.geekbang.org/quiz/intro?act_id=1000&exam_id=2868"><img src="https://static001.geekbang.org/resource/image/28/a4/28d1be62669b4f3cc01c36466bf811a4.png?wh=1142*201" alt=""></a></p><p>最后，我还给你准备了一个<a href="https://jinshuju.net/f/s5a2mw">毕业问卷</a>。问题不多，大概两分钟就可以填完，主要是想听一下你对这门课的看法和建议。期待你的反馈！</p><!-- [[[read_end]]] -->',
        article_title: "一套习题，测出你对机器学习的掌握程度",
      },
      {
        title: "结束语 | 可以不完美，但重要的是马上开始",
        id: 428089,
        content:
          '<p>你好，我是黄佳。</p><p>终于开始提笔写结束语了，这也意味着这门二十多讲的《零基础实战机器学习》要完结了。首先，这里非常感谢每一位可爱的用户，一直在坚持学习、留言，给我提出一些很好的建议和反馈。</p><p>其实，从开始打磨到专栏完结，这中间只有3个多月的时间。但这3个多月，对我来说一直是在全力和极客时间的编辑打磨这门课。从最开始的课程设计讨论选择什么样的案例、选择什么样的组织逻辑，到后面每一篇文章、每一个步骤怎么去讲，我们都进行了反复的推敲和打磨。这样做的目的只有一个，那就是为你呈现一个轻松而快乐的机器学习旅程，让你在不具备扎实的机器学习理论基础的前提下，就能够学得会、学得爽。</p><p>课程完结了，我可以说一句：不辱使命！</p><p>回顾整个过程，远没有我想象中的轻松。但我非常庆幸，自己选择了开始这门课，并在遇到困难的时候没有退缩，而是凭着自己的热爱和坚持，把它做成。</p><p>因此，当编辑问我，在课程最后一讲有没有什么技术体系之外的话要和你分享的，我不假思索地回答：“<strong>可以不完美，但重要的是马上开始。</strong>”</p><h2>从SAP新手到第一本SAP中文书作者</h2><p>其实，道理大家都会说，也都懂。但是，理解后去真正践行就是另外一回事了。我之所以如此“信奉”这句话，和我的经历有莫大的关系。</p><!-- [[[read_end]]] --><p>时间倒回至2003年，那时我刚工作不到一年，还是一个SAP ABAP新手程序员。那时候，我虽在国外，但英文也一般，当时我入门ABAP语言可以说是非常艰难。这其中的原因主要是，那时候网上的学习资料可远远没有今天这么丰富，没有好的中文文档可以学习，而英文文档读起来又非常不舒服。不过，工作还是要做，该学的东西还是要学，这也是我们必须要克服的。</p><p>有一天，一个想法突然闯进我的脑袋：像SAP这样已经日益“火起来”的软件，怎么能没有中文的编程书呢？为什么我就不能写一本出来？这个想法出现之后，就再也挥之不去。当然，与之同时出现的想法是：“我只是一个不到一年的ABAP新手，有什么资格写一本书呢？”“我一点知名度都没有，没有出版社会有兴趣和我合作的……”</p><p>客观来讲，这些想法都没错。一个新手要想写一本够水准的书，困难可想而知；而作为没有知名度的人，出版一本书的可能性微乎其微，尤其是在2003年的时候出书要比现在困难得多。但我仍然决定抛开这些顾虑，尝试一番。于是，我几乎给我知道的国内所有技术书的出版社发送了邮件，后来终于有一个编辑回复了我的想法，同意出版这本书。从开始到定稿，我花了整整一年多的时间。</p><p>其实现在回想起来，那个编辑的回信只有一句话，我现在仍然记忆犹新：“选题很好，我社愿意出版。”这句话让我非常兴奋，马上开始投入写作。我大量阅读SAP英文文档，并且在新加坡图书馆借了能够找到的所有SAP英文出版物，逐字逐句研习，同时思索着如何把自己从不懂到学懂的心路历程完整地复现下来，分享给我脑中的潜在读者。</p><p>此时，奇怪的事情发生了，当我以“出书”为目标重新开始学习之后，我的学习过程变快乐了、不再枯燥了，我的学习速度也比以前快了很多。当我仔细回顾这一过程，发现有三个学习方法对我日后的成长非常重要。</p><p><strong>第一，就是阅读英文原版资料很重要</strong>。其实，不仅SAP早期的官方文档都出自英文，现在机器学习的主流框架，比如Scikit-learn、Keras、TensorFlow、PyTorch等，它们的官方文档和最新论文也几乎都是英文的。如果没有比较强的英文阅读能力，就不大可能获取第一手的资料。这一关我在写书时通过大量阅读逐渐突破了，如果你想走在技术的前沿，我建议你也尽早养成读英文资料的习惯。</p><p><strong>第二，我发现如实记录并仔细体会自己从不懂到懂的过程很有好处</strong>。一个已经对一项技能非常熟练的专家，往往会忘记自己入门过程中的艰难困苦，会误以为这个技能本来就“十分简单”，其实并非如此。每一个技能都有一个先难后易的学习曲线，把自己克服困难的过程仔细记录下来，认真复盘，不仅可以分享给后来者，对你以后学习其它技能，也会大有裨益，能让你少走很多弯路。</p><p><strong>第三，是很多技术作者都非常认同的一个观点，就是“输出倒逼输入”</strong>，这是费曼学习法的精髓，通过教别人来让自己学得更深、学得更透。当然，也许你并不会以“写一本书”为目标，但是，在你的学习过程中也会有非常多“输出”的机会，比如说把自己的知识总结成博客、给公众号投稿、在公司里面做一次技术分享，等等。你会发现你的收获要远远多过你的付出。</p><p>就这样，我用了一年的时间完成了这本书，在定稿之后，我把书稿寄给了那个同意出版这本书的编辑，结果发现邮件被退回，原来他已经离职了。我再次联系出版社说明情况，也不再有人回复我。</p><p>到这里，你是不是觉得我之前所做的努力都白费了？我自然极度失落，头脑中好像有很多虚拟的小人对着我说：“看吧，早就知道你这书出不出来嘛。”可是，尽管沮丧，我却没有停下来，毕竟这么难做的一件事，如果一切都十分顺利才是怪事，而且最为重要的是，此时我已经感受到了自己的成长，而这种成长超越了“出书”本身。</p><p>当然，目标还是要完成的。我从头开始寻找新的出版社，在持续的寻找和等待中，我又花了一年的时间，把第一稿完全重写了一遍。当给第二个出版社交稿的时候，从技术到文字，书稿的水平和质量已经大幅提升。最终，我成功出版了第一本SAP中文技术书《SAP程序设计》。</p><p>其实，<strong>我想不管结果如何，我的收获都要比“不开始”所获得的多很多。因为你一旦开始，成长也就开始了。如果始终迈不开那一步，就等于放弃了所有成功，包括成长的可能。</strong></p><p><strong>我们每个人的一生中，都会出现几个非常棒的想法，这些是属于你自己的机遇。当机遇来临时，抓住它最好的方式就是马上开始。如果你瞻前顾后，无限放大那些阻碍因素，那你永远也不可能达成你的目标。</strong></p><p>所以，我真心希望已经领略了“机器学习”风采的你，能够把握住未来那个“专属于你”的机遇，勇敢前行，勿求事事完美，只需尽情享受成长所带来的愉悦。</p><p>最后，我还为你准备了一个<a href="https://jinshuju.net/f/s5a2mw">毕业问卷</a>，希望你能用2分钟填写一下，和我分享你对这门课建议以及你对机器学习的想法，非常期待你的反馈！</p><p>感谢你的一路陪伴，日后我们有机会江湖再见！</p><p><a href="https://jinshuju.net/f/s5a2mw"><img src="https://static001.geekbang.org/resource/image/89/3e/89e622230564da1381eef0b97a3fb03e.jpg?wh=1142x801" alt=""></a></p>',
        article_title: "结束语 | 可以不完美，但重要的是马上开始",
      },
    ],
  },
];
