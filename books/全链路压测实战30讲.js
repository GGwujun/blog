exports.category = "test";
exports.title = "全链路压测实战30讲";
exports.data = [
  {
    chapterTitle: "开篇词",
    children: [
      {
        title: "开篇词 | 打破认知神话，做接地气的全链路压测",
        id: 427602,
        content:
          '<p>你好，我是高楼。</p><p>可能很多人已经认识我了，因为这已经是我在极客时间开设的第三门课程了。这次，我想和你聊聊全链路压测。</p><p>回顾从业十几年的经历，我一直在做性能测试、性能分析、性能优化的工作。早年间我在各大测试论坛分享自己的工作经验，并形成了关于性能测试完整的知识链。后来，我开始自己带团队做项目，完整做过40多个项目，团队也从最初的四五个人到300 余人。与我合作过的人都了解，我做性能项目的宗旨就是上线不死，死了不收钱。</p><p>2019年，我的第一个课程<a href="https://time.geekbang.org/column/intro/100042501">《性能测试实战30讲》</a>上线了，2020年，我又开了第二门课程<a href="https://time.geekbang.org/column/intro/100074001">《高楼的性能工程实战课》</a>。在这两个课程中，我描述了我认为在测试过程中重要的部分，比如整体概念梳理、性能分析思路等。</p><p>我希望通过这两个课程，可以抛出一个价值观：<strong>让性能变得有价值。</strong>我希望更多的人知道，性能如果只是停留在“测试”的层面，根本不能发挥它本该具有的价值，“调优”也是非常重要又不可忽视的部分。</p><p>对于一个企业级性能项目来说，性能的价值是给出结论数据，判断出系统的最大容量、稳定性运行时长。但是，大部分企业的性能项目做得如同儿戏一般，只有最简单的验证，系统资源大量闲置，而调优的动作几乎没有。当然了，也有可能是因为他们并不知道怎么做。但无论如何，这样的系统匆忙上线，一定会浪费大量不必要的资源。至于一个性能人员能把性能做得多深入，就取决于个人的努力了。</p><!-- [[[read_end]]] --><p>我还提出过一个重要的概念，就是“RESAR性能工程”。“RESAR 性能工程”这个名字是我自己起的，你不用去网上搜索，因为现在还搜不到。它对性能项目过程中的各个具体的动作做了更详细的描述，使之成为可以落地的具体实践。</p><p><img src="https://static001.geekbang.org/resource/image/yy/c1/yy23853dd67da65539483294077dfdc1.jpg?wh=1920x1010" alt=""></p><p>在前两个专栏，我一直强调<strong>把性能从“测试”思维转换到“工程”思维</strong>。这个思维的转变，涵盖了性能项目的全流程，不仅有技术的细节，还有需求、模型、监控、分析的完整逻辑。只有这个逻辑完整了，性能才能做得有价值。</p><p>在我看来，全链路压测只是性能容量场景中的一个具体案例。它并没有跑出“RESAR性能工程”的范围。RESAR性能工程中的所有逻辑思维，都将在这个全链路压测的专栏中落地实践。</p><p>不过如果你没有看过上一个专栏也没有关系，你可以先看看我在上面给出的思维导图，了解一下它具体分为哪些板块。在项目具体落地的过程中，我也会把每个模块略作介绍，保证不让你有蒙圈的感觉。</p><p><img src="https://static001.geekbang.org/resource/image/c1/94/c1f2f43415a39498b3a2a573df6cdc94.jpg?wh=1920x1015" alt=""></p><p>你可能会问，既然全链路压测的逻辑已经包含在RESAR性能工程里了，为什么还要写一个全链路压测的专栏呢？它和之前的专栏有什么区别和联系呢？让我们先来看一下全链路压测行业的现状。</p><p>在网络上可以找得到的全链路文章中，大概可以看到阿里、有赞、饿了么、美团、滴滴、京东、字节、陌陌、达达等企业的技术文章，里面都提到了全链路压测在企业内部的落地。但他们是如何落地的，在细节上我们只能猜测一二。</p><h2>全链路压测行业现状</h2><p>在性能领域中，很多人看到全链路压测的目标和效果之后，都会觉得这是一套可以上天遁地的完整逻辑。事实是不是这样呢？看看网上能搜索到的信息，它们基本上有这么几个特点：</p><ol>\n<li>目标和方案只有摘要级的说明。</li>\n<li>全链路压测平台的细节描述宽泛，没有完整的落地细节。</li>\n<li>只有大厂的一些笼统资料，但并没有投入成本（人员成本、资金成本、时间成本）的数据。</li>\n<li>无架构级全链路改造的细节。</li>\n<li>无架构级监控平台范围的细节。</li>\n<li>无性能分析逻辑的细节。</li>\n</ol><p>因此，在网上看了各种资料之后，你可能还是有很多困惑：首先，不知道自己的企业能不能支撑；其次，不知道应该如何具体实施；再者，不知道投入成本有多大。</p><p>在这些问题还没有搞清楚之前，偏偏一些企业的管理者只看效果，觉得这件事情是需要做的，于是安排了相关的任务给具体的实施层。实施层由于没有管理权限、没有成本计算能力、没有技术细节的实现能力，于是越做越觉得坑太大，填不住。只能想各种招儿来应对领导这一看似合理的安排。</p><p>举个例子，全链路压测的出发点是对线上的真实系统做改造后直接在线上压测，而一些企业根本不具备这样的条件，所以为了跟上潮流，他们只会做一些小的动作，在线上减少覆盖范围并分段进行压测。但是这个改变，其实完全失去了全链路压测的价值。</p><p>实际上，全链路的落地是要经过综合考虑后，公司上下共同努力的结果。从整个全链路项目来看，企业里从老板到最底层的工程师，都会需要参与进来。也就是说，全链路涉及老板、产品经理、架构师、开发工程师、测试工程师、运维工程师等各个角色。</p><p>不过，不同岗位的关注点是不同的。老板关注的是，全链路压测带来的业务容量提升和企业利润增长；产品经理要关注的是，业务容量的增长和后续功能的设计；架构师要关注的是，全局架构设计对全链路压测的宏观支撑；开发工程师要关注的是，业务细节和技术细节的改造；测试工程师要关注的是，覆盖住这些改造；运维工程师要关注的是，全链路改造和线上的压测过程对系统整体状态的影响。</p><p>如果你的公司打算实施全链路压测，而你刚好处在这些岗位或者对这些岗位有意向，那么我的这门课很可能对你有帮助。因为我对全链路压测是认真的，我要做的就是实战，就是<strong>“把全链路压测拉到地面上”</strong>，让你真正地了解它。</p><p>不过，在正式进入课程之前，我还想跟你聊聊几个你可能会关心，但是和技术无关的问题。</p><h3>全链路压测最难的是什么？</h3><p>全链路压测最难的，到底是什么呢？</p><p>是应用改造吗？是压力工具改造吗？是逻辑思维的转变吗？</p><p>在我看来，都不是。最难的应该是：<strong>管理协调</strong>。</p><p>由于全链路压测涉及到的团队多、系统多、链路长，这就导致了管理成本的迅速增加。</p><p>相比较而言，技术的实现就是细节的落地过程，消耗的只是人员和时间成本。</p><p>虽然我们这个技术专栏不会涉及管理协调（因为每个企业都有自己的管理逻辑，而且管理主要是和人相关，所以我无法给你一个放到不同企业都可用的管理思路），但你要记住，人员的协调非常重要，千万不可以小看它的影响。</p><h3>全链路压测一定会涉及压力工具的改变吗？</h3><p>这一点是不一定的。因为全链路压测的出现，是为了响应互联网企业大流量的需求。但是，不是所有的互联网企业都有那么大的容量需求，如果容量需求不大，是不需要改造压力工具的。</p><p>那有没有某种工具只是为了全链路压测而存在呢？答案是：没有！</p><p>因为压力工具是为了对服务端发送请求而存在的，所以不管你用传统压力工具，还是用现在市面上的所谓全链路压测工具，只要能够按需求对服务端发出请求，那就足够了。</p><p>有人说，我的全链路压测是可以基于多个公网上的压力服务器调用的，这一点和传统压力工具有很大的区别。我只能呵呵一笑，你觉得传统压力工具做不到吗？</p><h3>全链路线下压测有没有价值？</h3><p>因为全链路压测是为了满足线上环境的容量需求，所以所有的全链路相关的动作也都是基于线上环境来讨论的。但是，如果一些企业出于对制度或风险的考虑，不能或不想在线上环境做全链路压测，可不可以把全链路压测的逻辑应用于线下环境的压测呢？</p><p>答案当然是：完全可以，但是！是不是就怕看到但是？哈哈，但我还是得说：但是全链路压测如果不在线上做，那就摆脱了环境的限制，摆脱了影响生产系统的风险。</p><p>第一，“全链路”这个词强调的改造部分就不需要做了。因为改造的目的是区别压力流量和生产流量，而线下没有生产流量，所以不需要做改造。</p><p>第二，如果线下环境的业务流量需求没有线上环境要求的大，那么全链路压测强调的压测工具的分布式改造也就没必要了。</p><p>第三，如果线下环境的业务流量需求和线上环境要求的一样大，企业也愿意把线下环境搭建成和线上完全一致，那就完全可以复制全链路线上压测的逻辑。</p><p>第四，有人说，换到线下用流量复制的工具来实现线上流量的放大回放，是否也有意义呢？其实这个动作也不是非常有必要，因为如果已经在线下做了，那压力工具完全可以配置出线上的请求比例，用不用流量录制回放的压力工具无所谓，只要能按业务比例实现压力就好。</p><p>所以，你看，<strong>全链路线下压测不是没有价值，而是付出的成本似乎不会小</strong>。虽然应用的改造不用做了，但环境就是最大的问题。注意，这里说的环境可不仅是硬件环境堆在那里，架构部署和数据的部分也不简单。如果去除硬件和部署架构，那显然就和以前我们做的性能项目无差了。</p><h2>这个全链路压测专栏会如何组织？</h2><p>在这个全链路压测专栏中，基于“把全链路压测拉到地面上”的定位，我会这样来组织。</p><p><img src="https://static001.geekbang.org/resource/image/6d/b0/6daf70b87fc6c8089236e0f09d09d8b0.jpg?wh=1920x807" alt=""></p><p>我把这个专栏分成了六个部分，分别是核心理论、实践需求、环境准备、场景执行、性能分析和结果报告。在这六个部分中，我会把全链路压测实际落地的过程，真实、详尽地记录下来。目的就是，希望让你看到全链路压测的实现细节，不再把全链路当成神话一样来崇拜。</p><ul>\n<li>在核心理论模块，我会给你概括一下全链路压测过程中需要的重要逻辑。如：改造部分的逻辑、模拟场景的逻辑等。</li>\n<li>在实践需求模块，我会对性能项目中的几个重要环节进行详细说明。比如，压测方案设计、梳理核心链路、明确压测范围、数据构造、系统构造方案、性能监控等。</li>\n<li>在实践环境准备模块中，我会介绍全链路压测实践环境准备工作，对全链路压测项目中，前面的环境初始化环节的实操进行说明。</li>\n<li>在场景执行模块，我会带着你通过压测平台来实现全链路压测的场景。这里，我们会使用到各种不同的压力工具，比如炒得火热的流量回放工具等。</li>\n<li>在性能分析模块，我会根据此项目场景执行过程中实际遇到的问题，进行具体的一步步分析，对有价值的性能问题，我会一一记录。</li>\n<li>在结果报告阶段，我会写一个侧重于全链路压测视角的报告。教你怎么把压测结果以最清晰和高效的方式呈现出来。</li>\n</ul><p>在全链路压测项目中，这里面的每个环节都直接决定项目的成败。所以希望你能紧跟我的步伐，把握好每个细节，让项目顺利落地。</p><p>最后，我想说，如果成为一名优秀的性能工程师是你的心之所向，如果你不甘于在性能项目中因为知识的匮乏受到白眼，如果你需要可以和其他的任何一个技术岗位平起平坐的机会，如果你希望可以在下面的技术冲突中得到尊重。如果你希望在性能这条路上走得更远，那就与我一起踏上这段旅程，我会把我从业十几年来的经验毫无保留地分享给你。</p><p>好了，今天就说到这里。在课程正式开始之前，让我也认识一下你吧！关于全链路压测，你有什么心得，又有哪些积存已久的困惑？欢迎你在留言区和我交流讨论。</p><p>希望这个专栏，可以让你找到一些共鸣，助力你的技术和思维能力再攀高峰，我们出发吧！</p>',
        article_title: "开篇词 | 打破认知神话，做接地气的全链路压测",
      },
    ],
  },
  {
    chapterTitle: "核心理论",
    children: [
      {
        title: "01 | 全链路压测：为什么很多测试人员迷信它？",
        id: 427711,
        content:
          '<p>你好，我是高楼。</p><p>时光如梭，梭梭催人进步。在技术行业中，更是如此。</p><p>在性能行业中，全链路压测的概念产生和落地实践已经有很多年了。很多企业也是不遗余力地投入资源来做全链路压测，但其中苦楚只有经历过的人才懂。尽管如此，还是有更多的企业紧跟其上，趋之若鹜。为什么全链路压测会有这么高的热度？</p><p>要理解这一点，你必须得先看看全链路压测的这些目标：</p><ol>\n<li>全链路压测被称为系统整体容量保障的核武器。</li>\n<li>全链路压测可以实现生产环境的压测服务，模拟真实的生产峰值场景，以发现真实的线上瓶颈并实现监控分析。</li>\n<li>全链路压测可以实现精准的容量规划，确保线上系统的正常运行。</li>\n<li>全链路压测可以实现海量的并发请求，以模拟真实的用户峰值场景。</li>\n<li>全链路压测可以实现压测流量和生产流量的隔离，避免对生产流量产生影响。</li>\n<li>全链路压测可以自动化压测，减少人工成本，并提高压测频率，快速发现问题。</li>\n</ol><p>看完这些目标，你是不是有一种热血上涌的感觉？内心不由得感慨：“这真是一把神器，得全链路压测即可高枕无忧！”</p><p>这还没完，你可能还在网上看到过很多有关全链路压测的文章，文章里常常会涉及阿里、有赞、饿了么、美团、滴滴、京东、字节、陌陌、达达等一些企业。这些文章经常会给人一种感觉：为了增加系统运行安全性，全链路压测是企业必须要做的一件事情。</p><!-- [[[read_end]]] --><p>那我们是不是要紧跟大厂的步伐，在自己的企业中来做全链路压测呢？</p><p>盲目跟随当然不行！每个企业的阶段和项目状态都不相同，必须根据实际情况来判断。怎么判断？这还要从全链路压测的概念说起。</p><h2>拆解全链路压测</h2><p>我们先来拆解一下全链路压测，啥是“链路”？简单点说就是几个点连成的线。这个词在IT行业中非常常用，但是在性能行业中，却是近几年才被广泛使用的“新词”。要讲链路，就得说到微服务分布式架构的发展。</p><p>一开始，在微服务分布式架构还没有流行起来的时候，人们大多使用SOA架构，它大概的技术架构是这样的：</p><p><img src="https://static001.geekbang.org/resource/image/b1/7b/b1941366b44cf2000ee57e873464367b.jpg?wh=1920x1516" alt=""></p><p>当然，系统之间也有调用，但都会通过ESB，调用链路也比较短。</p><p>进入到微服务分布式架构之后，由于微服务被拆得越来越细，大概的技术架构就变成了下图所示的样子：</p><p><img src="https://static001.geekbang.org/resource/image/45/b8/45209d910f126f37908e995a9a8fyyb8.jpg?wh=1920x649" alt=""></p><p>当然，也有其他的表现形式，我们这里只是举个例子方便你理解。从上图看，调用链路明显变长了，这是大流量带来的系统拆分导致的。在这种情况下，识别问题也就更加困难了。</p><p>之前一个系统的功能点多，而现在一个系统的功能点少；之前测试一个系统就有一堆的业务逻辑，现在测试一个系统只有很简单的业务逻辑；之前一个系统就可以完成的业务操作，现在得跳好几个系统才能实现。</p><p>在互联网的初期，压测主要关注单系统接口，而这完全不能说明系统处理业务的容量能力。随着业务的大规模发展，性能也必须做到覆盖“全部”系统，“全链路”这个概念就显得格外重要了，它指的是系统的全链路。</p><p>说到这里，“全链路”的内涵就解释得差不多了，那说到压测，就得有工具、有平台。</p><p>由于系统架构的改变是容量改变引起的，因而容量出现大爆发的时候，要想实现压测，工具就必须支持这么大的容量。而之前常用的压力工具像LoadRunner、JMeter等，它们本身在分布执行能力、参数化拆分管理能力等方面都有着天生的弊端，所以我们必须另外打造具有大容量并发能力的工具。</p><p>这也是现在各企业想尽办法来实现自己的大并发压测平台的原因。</p><p>理清了概念，我们再来通过对比加深一下理解。因为全链路压测是为了满足线上环境的容量需求，所以这里我们主要将全链路线上压测和传统线下压测进行对比：</p><p><img src="https://static001.geekbang.org/resource/image/f3/b5/f333c161f50604e39b467bcc1df85fb5.jpg?wh=1920x1044" alt=""></p><p>从表格中，你可以很清晰地看到二者的区别，但是这些对比点是不是固定的呢？不是的。比如说，在传统线下压测环境中，我们也能使用生产的脱敏数据。所以这个表格只是给你一个粗略的感觉。</p><p>我们抽象一下全链路线上压测和传统线下压测的区别，其实就一个关键点：在线上测还是在线下测。其他的区别也可以不算是区别，因为那些区别点都是可以平衡掉的，比如说压力场景、铺底数据、监控手段等，关键在于投入多少。投入的内容包括了：系统的改造投入、人员的投入、环境的投入、时间的投入。</p><p>有人说，既然投资这么大，不搞全链路不行吗？</p><h2>你需要全链路压测吗？</h2><p>当然可以。在我看来，现在很多企业都把全链路压测给整偏了。不管企业是大是小、系统是大是小，也不管成本高低，很多人都只是想实现全链路压测，摸到技术的风向标，这在我看来大可不必。</p><p>实际上，要不要使用全链路压测需要充分考虑企业的实际条件，你可以先来考虑这么几个问题。</p><p>第一，你的企业有没有那么大的流量需求？<br>\n按我前面所列的量级，如果不到几十万、上百万、上千万/每秒的交易量，其实完全不用考虑全链路压测平台的实现逻辑。如果你只是觉得这逻辑听起来更高大上而去实现它，那投入的成本等于说是打了水漂。</p><p>第二，你的系统要不要做全链路线上压测？<br>\n如果你不是在线上做全链路压测，那很多业务流量的改造工作就可以完全忽略。甚至，你都不用考虑改造什么压力工具，这纯属是在给自己找麻烦。</p><p>第三，你的系统能不能做全链路线上压测？<br>\n大家可以看到，网上有关全链路压测的文章几乎都来源于互联网企业，而其他行业则是在后面跟风。为什么偏偏是那些互联网大厂特别需要全链路压测呢？首先是因为请求量大。其实，互联网大厂需要的环境太大，致使再弄一套测试环境的成本过高。再者，做全链路线上压测的系统，即使出了问题，也不会对企业利润产生太大的影响，这一点至关重要。但是，对于一个刚起步的小互联网企业来说，如果连正常的业务功能都不能保证按时按质地实现，还要加这个改造的工作量，我觉得也是不理智的。</p><p>我看到有些银行、证券企业也在做全链路线上压测，在我的经验里，银行、证券这些和钱有关的大企业，做全链路线上压测都是经过了多轮的验证和压测范围的缩减的。你看，这些大企业尚且如此，普通小企业就更应该根据实际情况量力而行了。</p><p>有人说，我们系统很多，一个系统一个系统地在线上做全链路压测行不行？如果你这么说，就意味着已经不是“全链路”了呀。</p><p>第四，你的组织支持不支持你做真正的全链路线上压测？<br>\n这是一个很严肃的话题。很多大领导是看行业新闻来判断方向的，技术层的领导是看行业风向来判断具体动作的，而苦逼的底层干活的人只能执行任务。</p><p>在全链路线上压测这件事情上，必然不是底层干活的人（部门经理及以下）可以决定的。全链路线上压测是需要企业上下层协调一致才可以做得到的。如果领导只是给你安排了一个做全链路线上压测的任务，但没有给你具体的权限支撑，是不可能做得下去的。而恰好有很多上层领导就是这种光安排任务，不给具体支持的风格。</p><p>这时，你得跟领导详细沟通一下，把成本利弊都分析清楚。如果从企业角度思考后，你们一致认为全链路压测是必须要做的，那就需要领导更具体的支持才行，不然可能很难推进。</p><p>基于以上四点，你可以在企业中考量一下还需不需要做全链路压测，我估计，问完自己这四个问题，很多人都会发现自己的公司不太适合做全链路压测。不过对于另一些企业来说，全链路压测却不可或缺。因为它能解决以下三个问题：</p><ol>\n<li>直接使用生产环境，避免了环境的差异性。</li>\n<li>实现高并发广域网压测平台，模拟了真实的用户场景。</li>\n<li>不用进行线下线上容量的推算。</li>\n</ol><p>传统的线下压测显然做不到这三点。如果以上三个问题对企业的影响较大，那么全链路压测就很有必要了。问题来了，如果想做到这几点，要做哪些具体的改造呢？</p><h2>如果企业要做全链路压测，系统要做哪些改造？</h2><h3><strong>压力工具改造</strong></h3><p>这涉及到流量问题。如果你的压力场景只需要万级每秒以下的请求量级，其实不需要做工具改造，传统工具就能做得到。但是如果需要更大流量，就得对压力工具进行改造了。压力工具改造的内容有哪些呢？</p><ol>\n<li>压力发起部分：主要是多节点分布式的改造。</li>\n<li>参数化部分：主要是数据量大引起的改造需求。在传统的压测工具中，基本上都是使用Master-Slave的方式，master负责拆分参数化数据，但当数据量过大的时候，显然这是个问题点。</li>\n</ol><p>改造的部分只有这两点，在其他的功能点上传统的压力工具是可以覆盖大流量的压测需求的。</p><h3><strong>业务流量的改造和隔离</strong></h3><p>在业务流量的改造和隔离上通常有两种做法。第一，实现全链路的压测流量识别，从而实现隔离。第二，只在入口做压测流量的识别，直接旁路到另一套独立的链路中去（这里的硬件可以共用，但是会增加部署的复杂度）。</p><p>下面我们来说说，如果用第一种方法进行业务改造，有哪些技术点需要考虑。</p><p>业务标记改造的目的是实现业务流量的隔离，业务流量隔离的目的是不让压测流量影响真实的线上用户。<strong>贯穿业务改造的关键是整个业务流的识别跟踪，最后还可以方便地进行数据的清理。</strong>具体业务改造需要包含哪些方面呢？</p><ul>\n<li>\n<p>脚本改造<br>\n也就是加上流量标记，以实现后续的流量隔离。这一点任何一个压力工具都只需要加个参数即可，没有复杂度。</p>\n</li>\n<li>\n<p>应用服务改造<br>\n这将是改造工作量最大的部分。改造要实现的是对流量标记的识别，再把请求旁路到相应的存储组件中去。<br>\n有人说，这里能不能直接在网关上做改造，后续所有的技术组件都单独搭建一套呢？显然这也是可以的，只是单独搭建还是涉及到成本。如果愿意投入，直接搭建一套生产环境更为霸气。<br>\n应用的改造主要是对现有的业务代码进行入侵式的改造，增加业务开发的工作量。 实现的手段就是跨线程透传，将压测流量写入ThreadLocal对象中。</p>\n</li>\n<li>\n<p>中间件的改造<br>\n对于一些跨服务调用使用的中间件，由于需要对压测流量进行标记，所以也是需要改造的。</p>\n</li>\n<li>\n<p>存储逻辑的改造<br>\n不管是缓存还是数据库、队列，都要对压测流量进行识别，以便隔离。目的就是不影响生产数据。<br>\n对缓存（比如Redis)，我们可以实现不同的key值；对于数据库，我们可以实现影子表或影子库。</p>\n</li>\n<li>\n<p>日志改造<br>\n压测流量的日志最好不要和生产日志在一起。有人说，既然有了标记，那按标记删日志就可以了嘛。但是，有过性能经验的人都知道，日志做得不好，对性能的影响还是非常大的。所以既然已经做了应用服务的改造，那把压测流量的日志也单独写才是更理智的。</p>\n</li>\n</ul><p>第二种业务流量改造方法比较简单，只要在网关做压测流量的识别即可，后面就全都是部署的活了。</p><h3><strong>全链路压测对监控系统的影响</strong></h3><p>对于监控系统，不应该用改造这个词了。由于线上生产系统是必须有监控的，因而全链路压测不涉及太多对监控系统的改造，它更多的是工作量的增加，例如增加影子库这样的监控点。</p><p>对于一些改造后并没有增加节点的技术组件和模块来说，在监控上则没有影响。</p><h2>总结</h2><p>好，到这里，关于做全链路压测的必要性和改造点我就讲完了，我们来对这节课做个小结。</p><p>首先，我们梳理了全链路压测的概念，它强调了线上覆盖全业务链的最大容量场景。</p><p>然后，我们探讨了什么样的公司适合全链路压测。我请你思考了四个问题：</p><ul>\n<li>你的企业有没有那么大的流量需求？</li>\n<li>你的系统要不要做全链路线上压测？</li>\n<li>你的系统能不能做全链路线上压测？</li>\n<li>你的组织支持不支持你做真正的全链路线上压测？</li>\n</ul><p>考虑好这几个问题，才能确定是否适合做全链路压测。</p><p>最后，我带你梳理了进行全链路压测前具体的改造路径，它包括压力工具改造和业务流量的改造与隔离。另外，全链路压测还会对监控系统产生一定影响。</p><p>面对全链路线上压测，希望你理性分析它的实施成本和上层的支持力度。切忌在没有必要的航线上不断试错。如果你的企业确实需要做全链路压测，那就要把改造的细节列清楚，并计算出成本。不盲从，不夸大，不缩小，做真正有价值的事情。</p><h2>思考题</h2><p>在结束今天的学习之前，我还想请你思考这两个问题：</p><ol>\n<li>什么样的系统才需要全链路线上压测？</li>\n<li>如何计算全链路线上压测的成本？</li>\n</ol><p>欢迎你在留言区与我交流讨论。当然了，你也可以把这节课分享给你身边的朋友，他们的一些想法或许会让你有更大的收获。我们下节课见！</p>',
        article_title: "01 | 全链路压测：为什么很多测试人员迷信它？",
      },
      {
        title: "02 | RESAR 全链路流程：如何搞定所有容量场景？",
        id: 428085,
        content:
          '<p>你好，我是高楼。</p><p>在开篇词中，我已经提出过，全链路压测只是性能容量场景中的一个具体案例，它并没有跑出“RESAR性能工程”的范围。你可能会问，既然这样，这个全链路压测专栏还有什么新鲜的内容呢？这个专栏讲的是另一套性能工程级的逻辑吗？下面我们就来仔细地说一下。</p><p><img src="https://static001.geekbang.org/resource/image/yy/c1/yy23853dd67da65539483294077dfdc1.jpg?wh=1920x1010" alt=""></p><p>这张RESAR性能工程的全景图已经在开篇词中展示过，性能需求指标、性能环境、性能场景、性能分析、性能报告是其中重要的五个部分。全链路压测同样也可以按这五个大的部分来讨论。</p><p>请注意，这五个部分看似是做事情的先后顺序，但顺序不是我想表达的。我想表达的是，对完整的性能项目来说，应该从哪几个方面来保证性能项目的成功。而这些事情是跟用什么开发模型来实施关系不大的。所以，这五个部分，你放到瀑布模型中也好，放到敏捷也好，放到DevOps也好，都可以。重点是你必须要做到，不然性能项目的结果就是不可控的。</p><p>那既然全链路压测只是一个特定的性能场景，对应到RESAR性能工程中，也就是性能场景了。</p><p>既然这样，我们为什么还要用这么多篇幅来讨论全链路压测呢？那是因为，全链路压测场景需要做的工作实在是有点多，“需求-环境-场景-分析-报告”几乎都有要考虑的内容，所以将它拆开仔细研究很有价值。</p><!-- [[[read_end]]] --><p>下面我们就从流程的角度来说明一下，全链路压测场景，在整个RESAR性能工程的执行流程中，有哪些是需要特别关注的。</p><h2>RESAR性能工程的通用流程在全链路压测中的具体变化</h2><p>在RESAR性能工程的通用流程中，我画过这样的一个工作流程图来说明各个环节要做什么事情。</p><p><img src="https://static001.geekbang.org/resource/image/aa/39/aa8d495c44ab02yye87ed0ebbf6b6339.jpg?wh=1920x716" alt=""></p><p>针对这个通用流程，如果要满足全链路压测，我们来做一下标记，看一下，有哪些变化。</p><p><img src="https://static001.geekbang.org/resource/image/46/52/46946de33f0485d9f2f9a6fde8319652.jpg?wh=1920x1272" alt=""></p><p>图中，蓝色字体标示的是变化的关键点，这些内容和线下环境中性能项目要做的事有一些区别。我们一一对应看一下。</p><ul>\n<li>需求指标部分</li>\n</ul><p>显然，由于要考虑“全链路”，那就要梳理要压测的链路有哪些。如果是业务不复杂的系统，这项工作还是比较好做的。但如果是复杂的系统，就要分清主次了。其实全链路要考虑的范围，通常不是一个企业的所有业务链路，而是要把最核心的业务场景覆盖的链路梳理出来。</p><ul>\n<li>性能模型部分</li>\n</ul><p>这部分其实和传统的性能模型没有太大区别，动作都是要做的。即便你说我的容量模型就是和生产一致，那也要有一个判断。在具体实施的时候，不管你是生产引流、录制流量回放还是用压力工具，目标都是为了让性能模型和生产一致。</p><ul>\n<li>性能方案部分</li>\n</ul><p>这部分将有非常大的变化，因为会涉及到大量的改造动作。这些改造的动作不仅是开发去做个旁路的逻辑，还会涉及到部署的变化，所以这个方案是最最重要的内容。当然你可以把方案中的内容拆到其他阶段去完成，但是内容上不可少。</p><ul>\n<li>环境准备部分</li>\n</ul><p>由于全链路压测是在线上直接进行的，所以环境的准备也要格外地小心。在这一环节中，就可以看出不同企业的不同策略了。举例来说，互联网企业之所以可以做线上压测，主要是因为风险可控；而对银行业来说，在线上做全链路压测的风险极有可能是不能承担的，最多做做业务缩减版（比如说做查询业务）的“全链路”压测。如果你做了其他业务的压测，万一出现问题，银监会会立马找上门来。而互联网企业最多就是承受自己企业的风险，相比较而言风险就可控多了。</p><ul>\n<li>性能监控部分</li>\n</ul><p>性能监控部分应该说区别不太大，最多是把改造多出来的部分加到已有的监控体系中去。这里要多啰嗦一句，一定要保证监控计数器的完整性，因为在我的经验中，很多企业说是做了全方位360度无死角、透明度100%的监控平台，但是在我看来，也只是达到了模块级的覆盖，具体分析问题时，仍然是缺东少西的。</p><ul>\n<li>性能场景执行部分</li>\n</ul><p>这算是性能行业中最活跃的部分，但也是投入多见效少的部分。我们看到市场上也有一些标识为“全链路压测工具”的产品，也有一些为了实现全链路压测而特意开发的功能。而实际上，真正要做到让全链路压测工具覆盖不同用户特性，根本不是压测工具的功劳，而是网络本身的功劳。所以你用传统压力工具也好，用新的压力工具也好，只要压力机分布在不同的网络中，就可以实现模拟真实用户的特性。 至于工具执行上的策略，和RESAR性能工程理论中的场景执行策略一致就行了。</p><ul>\n<li>性能结果/报告部分</li>\n</ul><p>在这一部分中，除了需要完成RESAR性能工程之前规定的动作之外，还要做数据清理工作。毕竟，这些压力数据都是生产上的垃圾数据。</p><p>通过以上的详细说明，你大概可以发现，RESAR性能工程的思维逻辑是完全可以覆盖全链路压测的。因为RESAR性能工程是一套方法论，在具体的落地中，你把它用在什么样的性能场景都可以，而全链路压测场景就是一个具体的落地场景。</p><p>有了这个认识之后，我们再来看看，RESAR性能工程中规定的其他内容，在全链路压测中有必要改动吗？下面我们找几个关键部分来分析。</p><h2>RESAR性能分析决策树</h2><p>性能分析决策树是我在RESAR性能工程中提到的一个非常重要的概念，因为我强调性能项目要有分析调优的动作，而性能分析决策树就决定了能否做到这一点。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/ab/y1/ab02b74fa8c5cf3a2a8059b9f71a9yy1.jpg?wh=1920x1156" alt=""></p><p>不管是在什么样的性能项目中，项目级的性能分析决策树都是必须要有的。因为它展示的是一个架构级的、全技术栈的、全组件模块的计数器全集。有了这个全集，在出现性能问题时，才会有要分析的数据。</p><p>在线下环境做性能场景时，如果出现了问题，反正也不影响谁，我们直接再来一遍就可以了。但全链路压测就不一样了。因为压测是在生产环境中执行的，如果出现问题，为了不影响真实用户，你不能把问题就这么放着不管，你要做的第一步就是恢复。但是恢复之后怎么找问题呢？那就得靠监控数据了。</p><p>所以性能分析决策树对全链路压测来说，更为重要了。在每个全链路压测的项目中，都要先把性能分析决策树创建好，再根据RESAR性能工程的理论逻辑，把监控平台能覆盖的计数器和性能分析决策树中的计数器一一比对，有缺失时，一定要补全。</p><p>有了性能分析决策树和性能监控平台给出的具体数据之后，就来到了我们的“RESAR性能分析七步法”了。</p><h2>RESAR性能分析七步法</h2><p>我把性能分析的逻辑归纳为如下七步，这个归纳总结，在我的每个性能项目中都有具体的落地实践，这个思维引导着我解决了一个又一个未知的性能问题。</p><p><img src="https://static001.geekbang.org/resource/image/98/96/9888b314e4a6cc9e739ac8611f30d896.jpg?wh=1920x1254" alt=""></p><p>在全链路压测的过程中，如果出现了性能问题，其分析逻辑依旧可以按这七步来走。记住，这里要分析的可能就是历史监控数据了，因为我们上面说到了，全链路压测在线上出现问题时，首先是恢复，其次才是分析问题。</p><p>那这里又有一个致命的问题了：做定向监控分析时，数据不全，或需要做下一步动作才能得到有效的数据，怎么办？</p><p>我的建议是：先到测试环境中去模拟这个问题，能复现最好，但是如果测试环境中复现不了，就只能祈祷性能分析决策树的完整性了。如果还是不行，你只能到线上再执行一遍，让问题再次出现。与此同时，也要做好相关工作，使得即使是在测试时，用户的体验也不受太大影响。</p><p>总之，不管是什么样的性能问题，都是RESAR性能分析七步法可以覆盖到的，全链路压测的问题也不可能例外。</p><h2>性能瓶颈证据链</h2><p>有了RESAR性能分析七步法，就必然会有性能瓶颈的证据链。没有证据链的性能瓶颈分析就是耍流氓。而这个证据链的查找逻辑可以对应七步法中的“定向监控分析”部分。我在图片中给出了示例，你可以看一下：</p><p><img src="https://static001.geekbang.org/resource/image/50/31/50c39f7e88e1691ab54c3f5a68018131.jpg?wh=1920x1141" alt=""></p><p>对每个计数器，它的分析过程就是产生证据链的过程。也就是说，每个计数器出现问题（当然，大部分时候是需要多个计数器关联分析的）时所做的动作、产生的结果，你都严格地记录下来，然后通过自己的背景知识找到其间的关联性，这就成了证据链。</p><p>这个分析逻辑，在全链路压测的场景中是更为重要的。因为在线上环境中，我们没有多少试错、重复执行的机会，所以重视分析过程，才能让我们花更少的时间来精确地判断瓶颈产生的根本原因。</p><p>通过上面的描述，我相信你已经能理解RESAR性能工程和全链路压测之间的关系了。当然，在RESAR性能工程中还有大量的细节。比如说：业务模型的具体落地、场景实现的具体要求、性能指标的细化程度等等。它们在全链路压测落地的过程中，都是和RESAR性能工程中的要求完全一致的。</p><h2>总结</h2><p>好了，今天的课程就讲到这里，我们再来回顾一下今天的内容。</p><p>RESAR性能工程从前到后、从上到下地描述了一个性能项目的完整过程，它可以应用在所有的性能项目中，因而也可以覆盖全链路压测这个话题。</p><p>尽管在全链路压测中，一些具体动作（比如说要改造很多内容）和传统线下压测的内容不同，但这些都是为了准备环境。 而RESAR性能分析决策树、性能分析七步法、性能瓶颈证据链的思路，在全链路压测项目中仍然是快速找到问题的关键。倘若没有这些思维逻辑，我们还是只是实现了“压测”，而不“分析调优”。</p><p>有人会问：“在全链路压测中，分析调优是性能测试工程师的职责吗？如果是的话，那么大的系统，怎么可能看得过来呢？”在我看来，问出这个问题的人一定是从“性能测试工程师”的角度出发的，而我一直在说的分析调优，是一个项目组的事情，甚至是整个企业中所有相关方的事情。所以，分析调优的人员范围不限于性能测试工程师，毕竟，全链路压测也不是性能测试工程师能组织得起来的，而是需要高层的支持、多团队的协作才能办得到。</p><h2>思考题</h2><p>在课程结束前，我也想听一听你的思考。</p><ol>\n<li>在RESAR性能工程中，你能想到哪些细节是在全链路压测中要重点关注的？</li>\n<li>全链路压测中的性能问题分析有什么特点？和传统的线下性能项目中的性能问题分析有什么具体的区别？</li>\n</ol><p>欢迎你在留言区与我交流讨论。当然，你也可以把这节课分享给你身边的朋友，分享学习所得，共同进步。我们下节课见！</p>',
        article_title: "02 | RESAR 全链路流程：如何搞定所有容量场景？",
      },
    ],
  },
  {
    chapterTitle: "实践需求",
    children: [
      {
        title: "03 | 压测方案：你是否忽略了一个重量级文档？",
        id: 428497,
        content:
          '<p>你好，我是高楼。</p><p>从这里开始，我们就进入全链路压测的第二部分了。我会基于一个完整的项目给你讲一讲全链路压测的核心需求。这一讲这是第二部分的开篇，非常重要，它是这个项目整体的全链路压测方案。这个方案是我基于上一个专栏<a href="https://time.geekbang.org/column/intro/100074001">《高楼的性能工程实战课》</a>中给出的性能方案撰写的。</p><p><img src="https://static001.geekbang.org/resource/image/4f/f6/4fc2226cb959209fce7cd9b1c6bbcbf6.jpg?wh=1920x1079" alt=""></p><p>它包括性能指标、系统架构图、场景设计、监控设计、系统改造等部分，为后面的环境准备、具体执行奠定了基础。压测方案虽然常常被人忽视，但实际上起着非常关键的指导性作用。下面是我认为真正完整并且有意义的全链路压测方案的样子，希望能给你一些启发。</p><h1>背景</h1><p>接下来的课程，我要呈现的是一个完整项目从 0 到 1 落地全链路压测的过程。在项目的选择上，我将沿用我上一个专栏中的<a href="https://github.com/xncssj/7d-mall-microservice">开源微服务电商项目</a>。我们这个系统采用的技术栈，是当前技术市场中流行的主流微服务技术栈，这样的环境具有很高的借鉴价值。而且，从我的 RESAR 全链路压测方案和逻辑来看，也足以支撑当前主流的微服务技术栈。</p><h1>压测目标</h1><ol>\n<li>根据经典的电商下单流程，测试当前系统的单接口最大容量。</li>\n<li>录制真实的线上流量，回放压测流量，充分利用当前服务器资源，找到当前系统的性能瓶颈并优化，最终达到最佳容量状态。</li>\n<li>结合稳定性场景，做到精准的容量规划，给服务做限流降级提供数据上的参考。</li>\n<li>结合异常场景，实践并判断当前系统中的异常情况对线上产生的影响。</li>\n</ol><!-- [[[read_end]]] --><h1>压测范围</h1><p>通过梳理核心压测链路，即电商下单主流程，可确定压测范围，如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/1d/8d/1dec39c3055566d68d7e9d196332838d.jpg?wh=1920x918" alt=""></p><h1>准则</h1><h2>启动准则</h2><ol>\n<li>\n<p>确定旁路基础数据和生产一致，或按模型缩放。</p>\n</li>\n<li>\n<p>确定预压测（小流量）可以生成真实的业务。</p>\n</li>\n<li>\n<p>环境准备完毕，包括：<br>\n3.1. 基础静态数据压测验证通过。<br>\n3.2. 各组件基础参数梳理并配置正确。<br>\n3.3. 压力机到位并部署完毕。<br>\n3.4. 网络配置正确，连接通畅，可以满足压测需求。</p>\n</li>\n</ol><p>4.压测计划、方案评审完毕。</p><p>5.架构组、运维组、开发组、测试组及相关保障人员到位。</p><h2>结束准则</h2><ol>\n<li>达到项目的容量水位要求。</li>\n<li>关键性能瓶颈已解决。</li>\n<li>确定精准的容量规划数据。</li>\n<li>完成压测报告和性能调优报告。</li>\n</ol><h2>暂停 / 再启动准则</h2><ol>\n<li><strong>暂停准则</strong></li>\n</ol><ul>\n<li>系统环境变化，如系统主机硬件损坏、网络传输时间超长、压力发生器出现损坏、生产库出现脏数据、系统主机因别的原因需升级暂停等。</li>\n<li>需要调整线上环境资源，如操作系统升级、数据库参数调整、服务器伸缩扩容等。</li>\n<li>出现压测风险中列出的问题。</li>\n</ul><ol start="2">\n<li><strong>再启动准则</strong></li>\n</ol><ul>\n<li>压测中发现问题得以解决。</li>\n<li>线上环境恢复正常。</li>\n<li>压测风险中出现的问题已解决。</li>\n<li>线上环境调整完毕。</li>\n</ul><h1>业务指标/性能指标</h1><p><img src="https://static001.geekbang.org/resource/image/62/a8/62883034ffc5e7aeeb244ee085b81ca8.jpg?wh=1920x1144" alt=""></p><p>我们可以基于大促历史数据，分析流量洪峰前后的流量模型，结合当前业务量的增长趋势乘以最大容量水位系数，预估一个目标 TPS。比如这里我们暂定下单目标 TPS 为 1000，水位系数为 1（集群负荷/集群最大容量），不考虑冗余。</p><h1>系统架构图</h1><h2>系统技术栈</h2><p>我在下面整理了整个架构中使用的技术组件。它方便我们在后续的压测工作中整理出相应的关键性能参数配置。可以看到，我们这个项目已经覆盖了当前技术市场中的主流技术组件。</p><ol>\n<li>微服务框架：<a href="https://spring.io/projects/spring-cloud">Spring Cloud</a> 、<a href="https://github.com/alibaba/spring-cloud-alibaba">Spring Cloud Alibaba</a></li>\n<li>容器+MVC 框架：<a href="https://spring.io/projects/spring-boot">Spring Boot</a></li>\n<li>认证和授权框架：<a href="https://spring.io/projects/spring-security-oauth">Spring Security OAuth2</a></li>\n<li>ORM 框架：<a href="http://www.mybatis.org/mybatis-3/zh/index.html">MyBatis</a></li>\n<li>数据层代码生成：<a href="http://www.mybatis.org/generator/index.html">MyBatisGenerator</a></li>\n<li>MyBatis 物理分页插件：<a href="http://git.oschina.net/free/Mybatis_PageHelper">PageHelper</a></li>\n<li>文档生产工具：<a href="https://github.com/xiaoymin/swagger-bootstrap-ui">Knife4j</a></li>\n<li>搜索引擎：<a href="https://github.com/elastic/elasticsearch">Elasticsearch</a></li>\n<li>消息队列：<a href="https://www.rabbitmq.com/">RabbitMQ</a></li>\n<li>分布式缓存：<a href="https://redis.io/">Redis</a></li>\n<li>NoSQL 数据库：<a href="https://www.mongodb.com/">MongoDB</a></li>\n<li>应用容器引擎：<a href="https://www.docker.com/">Docker</a></li>\n<li>数据库连接池：<a href="https://github.com/alibaba/druid">Druid</a></li>\n<li>对象存储：<a href="https://github.com/aliyun/aliyun-oss-java-sdk">OSS</a>、<a href="https://github.com/minio/minio">MinIO</a></li>\n<li>JWT 登录支持：<a href="https://github.com/jwtk/jjwt">JWT</a></li>\n<li>日志收集、处理、转发：<a href="https://github.com/logstash/logstash-logback-encoder">LogStash</a></li>\n<li>日志队列和缓冲：<a href="http://kafka.apache.org/">Kafka\t</a></li>\n<li>日志采集：<a href="https://www.elastic.co/cn/beats/filebeat">Filebeat</a></li>\n<li>可视化分析与展示：<a href="https://www.elastic.co/cn/kibana/">Kibana</a></li>\n<li>简化对象封装工具：<a href="https://github.com/rzwitserloot/lombok">Lombok</a></li>\n<li>全局事务管理框架：<a href="https://github.com/seata/seata">Seata</a></li>\n<li>应用容器管理平台：<a href="https://kubernetes.io/">Kubernetes</a></li>\n<li>服务保护：<a href="https://sentinelguard.io/zh-cn/">Sentinel</a></li>\n<li>分布式链路追踪系统：<a href="https://zipkin.io/">Zipkin</a></li>\n<li>基础资源监控： <a href="https://prometheus.io/">Promethues</a></li>\n<li>容器级链路监控：<a href="https://github.com/weaveworks/scope"> Weave Scope </a></li>\n<li>可视化看板：<a href="https://grafana.com/">Grafana </a></li>\n</ol><h2>系统逻辑架构图</h2><p>画系统的逻辑架构图是为了后续性能分析的时候，脑子里能有一个业务链路径。我们在做性能分析时，要做响应时间的拆分，而只有了解了逻辑架构图才可以知道从哪里拆到哪里。</p><p><img src="https://static001.geekbang.org/resource/image/46/32/46ae02d0f31af5831382216700a75632.jpg?wh=1920x1453" alt=""></p><h2>系统部署架构图</h2><p>画部署架构图是为了让我们快速知道线上环境有多少节点、多少机器。在执行全链路压测时，可以根据经验对系统容量有一个预估。</p><p><img src="https://static001.geekbang.org/resource/image/c5/aa/c5yydb42e28e109e5ff4917c254afeaa.png?wh=1673x926" alt=""></p><h1></h1><h1>压测实施前提条件</h1><h2>硬件环境</h2><p>通过对整体线上环境硬件资源的整理，我们可以根据经验预估大概能支持多大的业务量级，而不至于拍脑袋定指标。<br>\n<img src="https://static001.geekbang.org/resource/image/8e/71/8eab0431aa569c593ccc3792b2914271.jpg?wh=1920x3462" alt=""></p><p>我们可以看到，当前云主机总共使用在应用中的资源是：38 C的 CPU 资源，88 G的内存资源。NFS 网络存储服务器不用在应用中。我们这里选择的云主机类型主要为计算密集型（通常比例1:2）。</p><p>在成本上，所有云主机加在一起一年大概六万左右的费用，这其中还包括云硬盘、云网络等杂七杂八的费用。</p><h2>工具准备</h2><h3>流量平台</h3><p><img src="https://static001.geekbang.org/resource/image/82/a0/821d420088byya2f2fced81fdc66baa0.jpg?wh=1920x1052" alt=""></p><p>为了使得压测的请求跟真实的线上业务请求接近，在压测请求的来源方式上，我们将对线上的业务流量进行录制和回放，采用流量回放的方式来进行压测。</p><p>流量回放工具选择了 GoReplay，使用 Minio 对象存储对文件管理，可以做到对流量文件和 Gor 执行器 HTTP 上传下载。</p><p>同时在压测过程中，我们考虑将 GoReplay 数据做实时采集，然后再由 Grafana 来可视化展现。</p><p>最后，我们考虑结合 Shell 脚本，使用分布式任务调度中心对命令执行做统一任务调度。</p><h3>监控工具</h3><p><img src="https://static001.geekbang.org/resource/image/db/13/dbecdbe408292270281ed0a6f8ac7a13.jpg?wh=1920x1115" alt=""></p><p>根据 RESAR 全链路压测中的“全局-定向”的全栈监控思路，我们在选择第一层监控工具时，要采集全量的全局计数器，采集的计数器包括各个层级，这里请参考前面的架构图。</p><p>但是，请你注意，在全局监控中，我们要尽量避免使用定向的监控手段，比如说 Java 应用中的方法级监控。因为在项目开始之初，我们不能确定到底在哪个层面会出现问题，所以不适合使用定向监控思路。</p><p>那全局监控怎么来做才最合理呢？这里我们可以结合线上运维现有的监控手段。注意，在性能监控过程中，我们尽量不要自己臆想，随意搭建监控工具。</p><h2>基准数据</h2><h3>铺底数据</h3><p><img src="https://static001.geekbang.org/resource/image/f6/78/f615d8d95ee94cd5dda48391950ac978.jpg?wh=1920x885" alt=""></p><h3>流量数据</h3><p>在流量录制的做请求重写，加上压测标记参数，比如 Goreplay 使用–http-set-param 参数针对 URL 中存在的 key 进行改写，对不存在的参数进行追加。</p><p>这里需要强调的是，在 RESAR 全链路压测中，基础数据要满足几个特性：</p><ol>\n<li><strong>生产环境的真实数据分布：</strong>要想做到这一点，最合理的方式是脱敏生产数据。在我们这个系统中，生产流量录制后，脱敏去噪后共计 243 万条用户数据和 250 万条地址数据。</li>\n<li><strong>流量数据一定要使用真实数据来覆盖真实用户：</strong>在全链路压测中一定要用真实流量数据来做参数化，而用多少数据取决于压测场景的设计。涉及真实的用户账号，需要 session 状态的，这里我们将考虑系统进行识别并单独处理。</li>\n<li><strong>流量数据一定要满足压测的可用性：</strong>通过分析大促流量洪峰历史数据的增长趋势，做到精准预估当前压测需要的数据量级。</li>\n</ol><h2>系统改造</h2><h3>压测标记</h3><p>在全链路压测中生成压测流量后，线上业务中需要区分流量(正常流量 &amp; 压测流量)，我们称之为<strong>链路打标</strong>，也有的叫做<strong>流量标记</strong>，由于我们这个项目为微服务架构，几乎都是 RESTful 风格的 HTTP 接口，而 HTTP 是一个比较通用的协议，我们在压测流量的 Header 中增加一个标记项。一个 flag 的 key 。值为 7DGroup 的代表压测流量，key 不存在或者 key 值不等于 7DGroup，代表非压测流量。</p><h3>压测透传</h3><p>线上压测要保证压测安全且可控，不会影响真实用户的使用，不会对线上环境造成任何的数据污染。所以首先解决的是压测流量的识别与透传问题。</p><p>目前微服务架构中分布式跟踪系统作为基础设施，不会限制使用线程池等会池化复用线程的组件，并期望业务逻辑尽可能地透明。从技术能力上讲，全链路压测流量治理与分布式链路追踪系统是一样的，即流量标记。</p><p>目前市面上，几乎所有分布式链路追踪的实现，理论基础都是基于 Google Dapper 的<a href="http://bigbully.github.io/Dapper-translation/">那篇论文</a>，其中最重要的核心概念就是 TraceId 和 SpanId。而对于分布式系统的压测流量透传主要涉及两大方面：</p><p><strong>跨线程间的透传：</strong></p><p>对于涉及多线程调用的服务来说，要保证压测标记在跨线程的情况下不丢失。主线程根据压测请求，将测试标记写入当前线程的 ThreadLocal 对象中，利用 <a href="https://github.com/alibaba/transmittable-thread-local">InheritableThreadLocal </a>的特性，将父线程 ThreadLocal 中的变量传递给子线程，这样就保证了压测标记的传递。而对于采用线程池的情况，同样对线程池进行封装，在往线程池中添加线程任务时，额外保存了ThreadLocal 中的变量，执行任务时再替换 ThreadLocal 中的变量。</p><p><strong>跨服务间的透传：</strong></p><p>对于跨服务的调用，我们对所有涉及到的服务进行了一一改造。利用 <a href="https://spring.io/projects/spring-cloud-sleuth">Spring Cloud Sleuth</a>的服务间原生传递上下文特性，在原有传输上下文的基础上，添加压测标记的属性，以保证跨服务传输中始终带着压测标记。这样也就免去了代码的硬耦合。</p><h3>压测隔离</h3><p>线上压测最棘手的问题是写请求的压测，因为它会向生产库中写入大量的脏数据。我们借鉴了目前主流大厂的“影子库和数据偏移”的隔离方案。</p><ul>\n<li>MySQL 使用影子库，将压测流量对 MySQL 的读写打到影子库上。即正常的业务库名为 mall，则影子库名 mall_shadow，这样压测之后数据的清理工作就变得方便多了。</li>\n<li>MongoDB 使用影子库，即正常的业务库名为 mall，则影子库名 mall_shadow。</li>\n<li>Redis 使用影子库，即正常的业务库名为 db1，则影子库名为 db5。</li>\n<li>RabbitMQ 使用数据偏移，发送消息后自动识别并加上压测标记。</li>\n</ul><h3>日志隔离</h3><p>微服务请求调用链追踪始终都是体现在分布式服务之间，我们通过网关入口透传，改造下日志框架，使接下来的日志都拦截并记录这个压测标记，那么最后存下来的日志有 traceID 和 tag 组合，最后搜索 ES 日志的时候，就能做到区分请求(正常流量 &amp; 压测流量)了。</p><p>考虑到日志打印一般是无序的，在一个用户的使用过程中，可能有多个不同的线程来进行处理。典型的例子是当用户访问某个页面时，应用服务可能会创建一个新的线程来处理该请求，也可能从线程池中复用已有的线程。在一个用户 session 存续期间，可能有多个线程处理过该用户的请求。当需要追踪某个用户在系统中的相关日志记录时，就会变得很麻烦。</p><p>解决办法就是引入 MDC（Mapped Diagnostic Context，映射调试上下文）。而 <a href="http://www.slf4j.org/">SLF4J </a>提供了<a href="http://www.slf4j.org/api/org/slf4j/MDC.html"> MDC</a> ( Mapped Diagnostic Contexts )功能，它的实现也是利用了 ThreadLocal 机制。改造代码只需要将指定的值 put 到线程上下文的 Map 中，然后在对应的地方使用 get 方法获取对应的值，从而达到自定义和修改日志输出格式内容的目的。</p><p>考虑到压测流量比较大，产生的日志也非常多，压测流量的日志很容易覆盖正常流量的日志。我们选择改写 Logback&nbsp;的 Appender，根据流量标记将日志写入对应影子目录中，并且配置默认的删除策略，日志只保存三天。</p><h3>Mock 设计</h3><p>对于第三方接口，例如下单支付，给用户发消息等，我们不能将压测流量的请求打过去，所以需要改写请求的逻辑，将 Mock 的逻辑保存在 Mock Server 上面，考虑使用 AOP 技术进行统一封装。</p><h1>压测设计</h1><h2>场景执行策略</h2><h3>场景递增策略</h3><p>对于性能场景，我们认为它必须满足两个条件：<strong>连续</strong>和<strong>递增</strong>，所以在这次的全链路压测执行过程中，我们会把这两点应用到下面的业务场景中。</p><p>这里面的详细介绍可以参考性能工程专栏的<a href="https://time.geekbang.org/column/article/357539">《05 | 性能方案：你的方案是否还停留在形式上？》</a></p><h3>业务场景</h3><p>在 RESAR 全链路压测中，性能场景只需要四类，执行顺序从上到下依次为：</p><ul>\n<li>基准场景</li>\n<li>容量场景</li>\n<li>稳定性场景</li>\n<li>异常场景</li>\n</ul><p>请你注意，除了这四类性能场景外，再没有其他类型的场景了。在每一个场景分类中，我们都可以设计多个具体的场景来覆盖完整的业务。具体可以参考<a href="https://time.geekbang.org/column/intro/402">《高楼的性能工程实战课》</a>，这里我就不逐一解释了。</p><h2>监控设计</h2><h3>全局监控</h3><p>有了前面的监控工具部分，监控设计就反而简单了。对于我们这个系统，全局监控如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/71/b5/71e39cd615bb0ac4b322fe638e1918b5.jpg?wh=1960x826" alt=""></p><p>从上图来看，我们使用 Prometheus/Grafana/Spring Boot Admin/Zipkin/Weave Scope/ELFK/Sentinel 实现具有全局视角的第一层监控。对工具中没有覆盖的第一层计数器，我们只能在执行场景时再执行命令来补充了。</p><p>注意，为了应对线上的风险，我们还需要提供压测熔断机制来应对突发事件，降低压测对线上服务的影响。这里我们主要考虑两个方面：</p><ul>\n<li>基于核心业务指标异常告警熔断</li>\n<li>基于基础资源 Metric 告警熔断</li>\n</ul><h3>定向监控</h3><p>那后面的定向监控怎么办呢？在这里我也大体列一下常用的工具。这些工具是在有性能问题的时候才会去使用的。</p><p><img src="https://static001.geekbang.org/resource/image/9d/06/9d58295fb9731952341267ac7df50406.jpg?wh=1920x791" alt=""></p><p>在性能分析的过程中，还会使用很多其他工具查找证据链，这里只能给出最常用的工具，无法全部罗列出来。大家要根据系统使用到的技术组件灵活使用工具，在课程后续的操作中，我们也有可能使用一些没有列出的工具。</p><h1>项目组织架构</h1><p>在全链路压测方案中，一定要画出项目的组织架构图，并且请你一定要在这部分写明各组织人员的工作范围和职责，避免出现扯皮的情况。我大体画了一下常见的组织架构图：</p><p><img src="https://static001.geekbang.org/resource/image/c3/9f/c35f09184f8bc09f6c32ede59c4eb29f.jpg?wh=1920x1684" alt=""></p><p>这是我按照工作内容而不是职场中的工作职位来设计的组织架构，我觉得这是一个合理的组织架构。在这张图中，性能工程师所负责的事情是现在大部分性能从业人员都在做的。此外，我们还需要性能分析工程师、架构师、开发工程师、运维工程师的支持和保障。</p><p>业务方作为性能的业务需求来源也必不可少。如果业务方提不出来什么合理的性能需求，那这个全链路压测项目很有可能一片混乱。</p><p>至于老板这个角色，在性能项目中，我们经常看到的老板都不懂什么叫性能，只会叫着要支持 XXX 并发用户数，支持 XXX 在线用户数。其实，这样的老板沟通起来也很简单，就是拿结果给他看就好了。不过，在全链路压测项目的执行过程中，当线上资源不足时，请你一定要让老板知道，同时降低老板的预期，要不然在后续的沟通中会很费劲。</p><h1>成果输出</h1><h2>过程性输出</h2><ol>\n<li>压测方案</li>\n<li>流量数据</li>\n<li>场景执行结果</li>\n<li>监控结果</li>\n<li>问题记录</li>\n</ol><p>在全链路压测项目中，过程性输出有这些内容就够了。我们经常看到很多项目在执行完之后，除了有一份压测报告之外，什么过程性输出都没有。实在不理解这样的企业是怎么积累性能经验的。</p><h2>结果输出</h2><p>通常情况下，我们做的性能项目都会输出两个最终交付的文档：一个是性能场景执行结果记录的报告（就是现在我们常写的压测报告），另一个是性能调优报告。</p><ul>\n<li><strong>全链路压测报告</strong></li>\n</ul><p>压测报告想必大家见得多了，这里只强调几点：</p><ol>\n<li>一定要有结论，而不是给出一堆类似“资源使用率是多少”“TPS 是多少”“响应时间是多少”这种描述类的总结语。我们要给出“当前系统可支持 XXX 并发用户数”这样的结论。</li>\n<li>一定不要用“可能”这种模棱两可的词。</li>\n<li>压测报告中要有对运维工作的建议，也就是要给出关键性能参数的配置建议，比如线程池、队列、超时、服务限流降级等。</li>\n<li>性能结果报告中要有对后续压测工作的建议。</li>\n</ol><ul>\n<li><strong>性能调优报告</strong></li>\n</ul><p><strong>调优报告是整个性能项目的精华，调优报告中一定要记录下每一个性能问题的问题现象、分析过程、解决方案和解决效果。可以说，调优报告完全是一个性能团队技术能力的体现。</strong></p><h1>项目风险分析</h1><p>对于全链路压测项目的风险，我把比较常见的风险列在这里：</p><ol>\n<li>业务侧的性能需求不明确。</li>\n<li>线上环境问题。</li>\n<li>脏数据问题。</li>\n<li>梳理链路不准确。</li>\n<li>多个团队间协调沟通困难，无法协同。</li>\n<li>瓶颈分析不到位，影响进度。</li>\n<li>…</li>\n</ol><p>在我们这个课程所用的项目中，比较大的风险就是：</p><ol>\n<li>硬件资源有限。</li>\n<li>项目时间不可控，因为出了问题，并没有人支持，只能自己搞。</li>\n</ol><p>不过请你放心，我们会努力克服困难，把这个全链路项目的执行过程都记录下来。</p><p>到这里，整个全链路压测方案就结束了。</p><h1>总结</h1><p>在这节课里，我把一个全链路压测方案该有的内容以及要写到什么程度，都给你梳理了一遍。希望能给你一些借鉴。</p><p>压测方案是一个性能项目的重要输出。对于一个完整的项目来说，压测方案就显得极为重要。因为它指导了整个项目的实施过程。在压测方案中，我强调了几个重点：核心链路、性能指标、系统架构图、系统改造、场景设计、监控设计等，它们都会对整个全链路压测项目的质量起到关键作用。</p><p>看完我的方案，希望你可以在后续的项目中，自己尝试去写一个完整的全链路压测方案。如果在撰写的过程中遇到什么问题，也可以在留言区向我提问。</p><h1>课后题</h1><p>这节课的内容到这里就要结束了，最后，请你思考两个问题：</p><ol>\n<li>如何精确预测线上压测基础数据量？</li>\n<li>为什么要强调系统架构图的重要性？</li>\n</ol><p>欢迎你在留言区与我交流讨论。当然了，你也可以把这节课分享给你身边的朋友，他们的一些想法或许会让你有更大的收获。我们下节课见！</p>',
        article_title: "03 | 压测方案：你是否忽略了一个重量级文档？",
      },
      {
        title: "04 | 核心链路：如何梳理符合真实业务场景的核心链路？",
        id: 430223,
        content:
          '<p>你好，我是高楼。</p><p>上一讲，我给你展示了我认为一个完整的全链路压测方案应该具有的样子。在项目的选择上，我们使用的是开源微服务电商项目。这个项目采用的技术栈，是当前技术市场中流行的主流微服务技术栈，所以很可能对你也有借鉴意义。</p><p>这节课，我们还是用这个项目来讲讲，怎么梳理出符合真实业务场景的核心链路。</p><p>我们知道，在完整的企业级系统中，业务逻辑是非常复杂的。为了清晰地把业务实现到技术层面，就需要一层层地分析。</p><p>可是对一线做技术的人来说，这种做法常常会让人陷到细节里无法自拔，从而丧失全局观。所以，在做技术细节的同时，我们也很有必要再转换一下思维，从更全面的视角来观察系统。</p><p>再具体到全链路压测来看，因为我们面对的是一个完整而又复杂的线上系统，所以我们就要从架构的视角出发来分析业务链路。</p><p>不过提到架构，很多人对它并没有明确的概念，另一些人又觉得架构是个宏观的东西，无法把控。</p><h2>从架构聊起</h2><p>既然如此，我们就先来聊一聊，架构是个什么东西？</p><p>简单来说，架构就是一个系统的抽象描述，它包括系统中的组成元素以及元素与元素之间的关系。</p><p>既然是抽象，那不同的视角抽象出来的东西就不一样。像RUP 4+1视图，它就包括逻辑视图、开发视图、进程视图和物理视图，而场景则是用来描述他们之间的关系的。</p><!-- [[[read_end]]] --><p>在逻辑层面，我们经常聊到的业务架构、技术架构、部署架构等，这些都是架构的不同视角。</p><p>在实现层面，我们经常聊到的单体架构、SOA架构、微服务分布式架构、分层架构、微核架构、云架构等，这些都是从不同的技术层面来分类的。</p><p>上面说的都还只是架构的一些具体的分类。更系统地来看，技术发展到现在，已经把一个系统、完整的架构分为了四个大类，它们分别是：应用架构、技术架构、数据架构、安全架构。这是从不同的实现视角来承接业务架构的。但是这样的视角还必须落到系统中，还需要再细化到逻辑架构和物理层面，才可以最终落地。</p><p>讲了这么多架构的分类，你可能要问了，它和我们梳理业务核心链路有什么关系呢？其实讲这些是因为，链路要在架构上来画才能描述得更清楚。</p><p>那从哪种架构开始梳理业务链路更清楚呢。先来看一下，我们这个系统应用架构的逻辑图。</p><p><img src="https://static001.geekbang.org/resource/image/46/32/46ae02d0f31af5831382216700a75632.jpg?wh=1920x1453" alt=""></p><p>像这种框框画出来的图，是不是让人看一眼大概就能知道里面有什么东西了？但这张图还没有表示出具体的部署情况。如果你觉得这个图不好看，我再放一个这样的图。</p><p><img src="https://static001.geekbang.org/resource/image/2f/4b/2f032901efyy2d9f9e3df8f123e8534b.png?wh=1673x926" alt="图片"></p><p>这个图是不是看起来逻辑架构清晰一点了？这张图虽然看起来和上一张图差不多，都是一些小方框，但已经开始偏向部署视角了。不过，我还没有画出服务之间的关系。</p><p>我们再换一个视角。</p><p><img src="https://static001.geekbang.org/resource/image/ab/9e/abd7df412998461cd240fe314104a99e.jpg?wh=1920x1590" alt=""></p><p>这样看起来是不是又清楚了一点？没错，这张图已经把部署视角很清晰地描绘出来了。</p><p>我这么详细地给你描述架构的不同视角，是希望你能够理解我下面的动作。我希望你能够分清楚各种不同架构的目的，因为只有理解架构才能理解链路。</p><h2>梳理核心业务链路</h2><p>但是，我们要梳理的是核心业务链路，上面的图却没有办法体现调用关系。那业务链路要怎么从这样的视角体现出来呢。</p><p>这时候我们就要祭出APM工具了。打个比方，对当前比较主流的微服务分布式架构来说，它的链路图是这样的。</p><p><img src="https://static001.geekbang.org/resource/image/cf/74/cf08a14529f7a888a3ddb1348c9f1c74.png?wh=1920x1143" alt="图片"></p><p>在SOA架构的时代，链路图都是记在脑子里的。因为服务没有那么多，倒也记得住。但是在微服务架构中，你很难记得住那么多服务。为了更有条理地梳理核心链路，我们就要借助链路监控工具了。</p><p>只有链路图还不行，要想梳理核心业务链路，还得知道什么是核心业务。核心业务的特点其实主要就两个：一、用得多；二、利润高。</p><p>我们先看符合“用得多”这个特点的业务。</p><p>先做一个业务接口访问的统计。</p><p><img src="https://static001.geekbang.org/resource/image/fa/b9/fa9716d8514b597995ba008cc3ebbeb9.png?wh=252x340" alt="图片"></p><p>截图中展示的是统计出来的业务接口，梳理下来就是这样的业务内容。</p><p><img src="https://static001.geekbang.org/resource/image/ff/82/ffeb1ca2f370f317d671bfacb946b182.jpg?wh=1920x917" alt=""></p><p>知道了是哪些业务，关键的一步就来了：你要在架构图上画出对应的链路。</p><p>我们先拿生成订单信息（在这个系统中，这个接口是最复杂的）的这个接口来说。从接口路径/order/generateOrder上来看，显然是直接访问的order接口。我们来看一下这个接口的泳道图。</p><p><img src="https://static001.geekbang.org/resource/image/8d/8e/8da29a39eda07e833760d8a95b41698e.png?wh=1920x2554" alt="图片"></p><p>这张图是不是乍一看相当复杂？但如果仔细分析一下，你会发现它清晰明了。</p><p>从泳道图上，你可以直观地看到和order直接相关的服务。因为这个图比较大，看不清楚，我们细看其中一段。</p><p><img src="https://static001.geekbang.org/resource/image/ec/6e/ecf1d18ea4295f7411868ce89dfbe06e.png?wh=1023x730" alt="图片"></p><p>从细节图中可以看出，这个接口调用了Cart服务和Member服务。这样一来，这个链路就可以画成下面这样了。</p><p><img src="https://static001.geekbang.org/resource/image/e2/b0/e2aabb4b9185d6f304ea73df5fb429b0.jpg?wh=1920x794" alt=""></p><p>如果接着往下分析，你就能知道这个接口调用的所有链路了。但是从代码中一点点去翻链路关系显然是不理智的，借助APM工具SkyWalking，我们就可以得到下面这样的图。</p><p><img src="https://static001.geekbang.org/resource/image/c2/50/c2bf12532f7bd2caf09314bc0ee73a50.png?wh=1007x680" alt="图片"></p><p>具体用哪种APM工具你可以根据自己的习惯和需要来选，只要看到相应的结构即可。比如用Zipkin梳理出的链路图是这样的。</p><p><img src="https://static001.geekbang.org/resource/image/28/d0/2861380c4fbda55d3bbfe14842c5a1d0.png?wh=1920x1043" alt="图片"></p><p>在这些链路图中，你可以看到所有相关的服务，包括数据库、缓存等。不过，直晃晃地给一个图出来，还是没有办法说明链路。你还要画出箭头表示调用方向。</p><p><img src="https://static001.geekbang.org/resource/image/af/83/af9e53bb6c993852e5bbb377011f2683.jpg?wh=1920x1297" alt="图片"></p><p>为了配合上面泳道图中的内容，我只画了相关的服务。这样我们就有了一个接口的链路图了。</p><p>用这样的方法把所有的接口都梳理一遍，你会得到一组完整的调用链路关系。</p><p><img src="https://static001.geekbang.org/resource/image/43/d3/438832ce2a5a276151917068d89480d3.jpg?wh=1920x1080" alt=""></p><p>这样，我们就把核心业务链路都罗列出来了。</p><p>对于有第三方调用的链路，你可以直接Mock。因为全链路压测中要屏蔽到第三方系统性能对自身系统的影响，在这一点上不用纠结。我看有些企业为了让全链路真的全起来，将第三方系统也纳入到了压测范围。从技术上说不是不可以，但是从协调上来说，就要考虑一下项目进度的风险和协调的难度了。</p><h2>总结</h2><p>好了，到这里，这节课的主要内容就讲完了。对于梳理核心业务链路来说，重点就是去看一个业务涉及到了哪些服务，这些服务又有哪些对应的技术组件。你可以遵照下面的思路：</p><ul>\n<li>先看架构图，看架构图是为了了解一个系统里有哪些具体的服务和组件。</li>\n<li>再看调用链路，看调用链路是为了明确一个业务接口涉及到的具体服务和组件。</li>\n<li>最后将所有流量大的业务接口统计出来，对应每一个业务接口梳理出调用链路。这样，我们就可以确定混合容量场景会涉及到的所有服务和组件了。</li>\n</ul><p>这些都是我们在后续的全链路压测过程中要重点关注的部分。</p><h2>课后题</h2><p>在结束今天的学习之前，我还给你留了两个小问题：</p><ol>\n<li>你知道哪些确定业务接口的链路的方法？</li>\n<li>对不同的容量场景是否需要独立做业务统计？</li>\n</ol><p>欢迎你在留言区与我交流讨论，我们下节课再见！</p>',
        article_title: "04 | 核心链路：如何梳理符合真实业务场景的核心链路？",
      },
      {
        title: "05 | 铺底数据：真实的压测数据应该做成什么样子？",
        id: 431720,
        content:
          '<p>你好，我是高楼。</p><p>这节课，我们来聊一下怎么准备全链路压测的铺底数据。</p><p>做测试少不了与测试数据打交道，做性能测试更不是几条数据就能简单概括的事情。</p><p>在全链路压测过程中，测试数据是必须计算清楚的。通常，测试数据分为<strong>铺底数据</strong>与<strong>参数化数据（流量数据）</strong>，不管是平时的业务功能测试、自动化测试，还是全链路压测等都需要有铺底数据，它们的区别只在于数据量级。另外，铺底数据还是压测清单与压测报告必须说明的内容。</p><p>那么铺底数据与参数化数据从什么地方获取呢？获取的数据又是否符合真实分布？下面我介绍几种构造数据的方法与思路，希望你能有所收获。</p><h1>数据来源分析</h1><p>刚才已经提到了，测试数据分为铺底数据与参数化数据。古人常说，“圣人千虑，必有一失；愚人千虑，必有一得。”我们也来思考一下，铺底数据是怎么获取的？</p><p>造数据这个事情对于工作时间不久的同学来说是一个棘手的事情，但对于一个工作很长时间的老鸟来说，这不是什么大问题。（对于压测数据要造多少量，分析逻辑你可以参考《<a href="https://time.geekbang.org/column/article/359221">性能场景的数据到底应该做成什么样子？</a>》）</p><h3></h3><p>既然想要做铺底数据，就得知道要使用哪些表和字段。在做全链路压测之前，这些内容都要提前梳理出来。</p><p>你可以先把业务场景梳理完，再把相关接口整理出来。做完这些，就可以开始查找证据链了。</p><!-- [[[read_end]]] --><p><img src="https://static001.geekbang.org/resource/image/d1/ec/d1ab72974027decff144205fac06e2ec.jpg?wh=1788x440" alt=""></p><p>我把使用表的链路画成了一个流程图，在图中，我标示出了通过接口找到服务，进而找到 SQL和表的全过程。总之，查找思路就是层层追踪，一直找到表为止。造数据就是体力活。</p><p>现在你可以回想一下，这次链路涉及哪些服务、哪些表？表之间的关系又是什么？你可以复习一下上节课的内容，这里我就不再重复说了。</p><p>下面，我们通过实例演示怎么层层分析查找表。</p><h2>表链路追踪示例</h2><p>先追踪首页链路，打开首页 JMeter 脚本，观察下图文本框。</p><p><img src="https://static001.geekbang.org/resource/image/7e/97/7e0d27yy165aef5f2a6dae0b525b1397.png?wh=1318x808" alt="图片"></p><p>当前接口路径为【 /api/portal/home/content 】，可以知道目前请求的服务为 Portal 服务。</p><p>通过架构图，我们可以很清晰地知道当前在什么位置。</p><p><img src="https://static001.geekbang.org/resource/image/46/32/46ae02d0f31af5831382216700a75632.jpg?wh=1920x1453" alt=""></p><p>上图是用户从网关到应用再到数据库的调用逻辑。</p><p>我在这里用一个电商系统做案例演示。在分析代码之前，我们先回顾下 MVC （Model、View、Controller）开发模式。下图是最常见的一种开发模式，在 PaaS 化平台开发（Platform-as-a-Service，统一提供的平台类服务）中会稍有变动，需要根据实际项目调整分析步骤。</p><p><img src="https://static001.geekbang.org/resource/image/ce/25/cebec3ec1942da42efa4637ebeecc525.jpg?wh=1920x651" alt=""></p><p>有了这种开发模式做基础，我们就可以很轻松地根据请求 url 一步一步分析了。</p><p>首先找到网关服务，再找到服务对应的 controller 层。在这里，你可以很清晰、直观地看到当前的请求，当前的服务。</p><p><img src="https://static001.geekbang.org/resource/image/6f/1b/6ff83d5c08fb6a6c88yy985fc5d2291b.png?wh=1514x485" alt="图片"></p><p>思路参考：</p><p>1、找到接口请求的服务（本次是：Portal 服务）。</p><p>2、打开 Idea，然后打开 mall 源码，找到资源路径为 /content ，接口为：</p><pre><code class="language-java"> @ApiOperation("首页内容页信息展示")\n    @RequestMapping(value = "/content", method = RequestMethod.GET)\n    @ResponseBody\n    public CommonResult&lt;HomeContentResult&gt; content(@RequestParam(value = "pageNum", defaultValue = "4") Integer page,\n                                                   @RequestParam(value = "pageSize", defaultValue = "1") Integer size) {\n        HomeContentResult contentResult = homeService.content(page, size);\n        return CommonResult.success(contentResult);\n    }\n</code></pre><p>3、根据 MVC 开发思路，通过 controller 层找到业务层，然后根据业务层找到 DAO 层，再找到表。我在下面给出了业务层的代码供你参考。</p><p><img src="https://static001.geekbang.org/resource/image/fb/97/fb1eef60a4yy07733169926020c52a97.png?wh=1214x706" alt="图片"></p><p>通过代码，我们可以梳理出链路追踪图，下图这种分析方法能一目了然地让我们知道，目前的方法调用了哪个表。</p><p><img src="https://static001.geekbang.org/resource/image/97/aa/97bfdd4617ec02777252f120449yyaaa.jpg?wh=1881x1756" alt=""></p><p><strong>需要注意的是，即使梳理出了表关系，也要在分析压测链路相关的业务之后，才能决定是否需要铺底数据。</strong></p><p>铺底数据的目的是模拟真实场景与预测业务一段时间后是否还能正常运转。上面的图片只是一个链路追踪分析示意图，它的目的就是找到表。其他的业务模块也可以按这样的方法去找到对应的表。</p><p>假如自己没有权限，你可以找对应的研发，让他梳理接口与表之间的关系，还需要告诉研发什么时候做压测，压测之前做好相关表铺底数据与备份工作，并且告诉他一个表的铺底数据量大概是多少。</p><p>顺便提一下，查找代码逻辑当然也可以借助 IDE 里面的提效工具，比如以下就是 IntelliJ IDEA <a href="https://vanco.github.io/SequencePlugin/">SequencePlugin </a>生成的代码调用图。</p><p><img src="https://static001.geekbang.org/resource/image/16/f1/16b766cdb66a0d762b05ba3f159f4ef1.png?wh=428x456" alt="图片"></p><p>对于铺底数据来说，来源最可靠的就是从生产数据导入。不要认为直接导入就能用，对于用户来说，身份证号、手机号、银行账号、邮箱、密码、教育背景、还有家庭详细地址等这些信息都是敏感信息，一旦泄露出去就是重大信息安全事故，所以都必须做数据脱敏。</p><p>通过上一节梳理出来的场景，再根据上面的代码分析逻辑，我们很快就梳理出了压测链路的表，它们分别是：</p><ol>\n<li>用户表</li>\n<li>用户地址表</li>\n<li>商品表</li>\n<li>库存表</li>\n<li>购物车表</li>\n<li>订单表</li>\n</ol><p>一般用户相关的表、订单相关表需要做铺底数据，商品表与库存表则只需要库存充足就行了。这些我们都需要灵活处理，不用墨守成规。</p><p>下面我们打开用户表观察一下表的结构。</p><pre><code class="language-sql">use 7dnewmall;\ndesc ums_member; --命令看结果\n\n-- 直接打开表看信息\nCREATE TABLE `ums_member` (\n  `id` bigint(20) NOT NULL AUTO_INCREMENT,\n  `member_level_id` bigint(20) DEFAULT NULL COMMENT \'会员等级\',\n  `username` varchar(64) DEFAULT NULL COMMENT \'用户名\',\n  `password` varchar(64) DEFAULT NULL COMMENT \'密码\',\n  `nickname` varchar(64) DEFAULT NULL COMMENT \'昵称\',\n  `phone` varchar(64) DEFAULT NULL COMMENT \'手机号码\',\n  `status` int(11) DEFAULT NULL COMMENT \'帐号启用状态:0-&gt;禁用；1-&gt;启用\',\n  `create_time` datetime DEFAULT NULL COMMENT \'注册时间\',\n  `icon` varchar(500) DEFAULT NULL COMMENT \'头像\',\n  `gender` int(11) DEFAULT NULL COMMENT \'性别：0-&gt;未知；1-&gt;男；2-&gt;女\',\n  `birthday` date DEFAULT NULL COMMENT \'生日\',\n  `city` varchar(64) DEFAULT NULL COMMENT \'所做城市\',\n  `job` varchar(100) DEFAULT NULL COMMENT \'职业\',\n  `personalized_signature` varchar(200) DEFAULT NULL COMMENT \'个性签名\',\n  `source_type` int(11) DEFAULT NULL COMMENT \'用户来源\',\n  `integration` int(11) DEFAULT NULL COMMENT \'积分\',\n  `growth` int(11) DEFAULT NULL COMMENT \'成长值\',\n  `luckey_count` int(11) DEFAULT NULL COMMENT \'剩余抽奖次数\',\n  `history_integration` int(11) DEFAULT NULL COMMENT \'历史积分数量\',\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `idx_username` (`username`),\n  UNIQUE KEY `idx_phone` (`phone`)\n) ENGINE=InnoDB AUTO_INCREMENT=12 DEFAULT CHARSET=utf8 COMMENT=\'会员表\'\n</code></pre><p>在用户表中，你会发现用户名、密码、手机号都是敏感信息，都需要脱敏处理才能使用。那为什么职业与城市不是敏感信息呢？因为用户名与手机号修改之后，职业和城市的覆盖范围就太大了，这些信息自然就不再是敏感信息了。</p><p>你可以再打开地址信息表。</p><pre><code class="language-sql">CREATE TABLE `ums_member_receive_address` (\n  `id` bigint(20) NOT NULL AUTO_INCREMENT,\n  `member_id` bigint(20) DEFAULT NULL COMMENT \'用户表ID\',\n  `name` varchar(100) DEFAULT NULL COMMENT \'收货人名称\',\n  `phone_number` varchar(64) DEFAULT NULL COMMENT \'收货人手机\',\n  `default_status` int(11) DEFAULT NULL COMMENT \'是否为默认\',\n  `post_code` varchar(100) DEFAULT NULL COMMENT \'邮政编码\',\n  `province` varchar(100) DEFAULT NULL COMMENT \'省份/直辖市\',\n  `city` varchar(100) DEFAULT NULL COMMENT \'城市\',\n  `region` varchar(100) DEFAULT NULL COMMENT \'区\',\n  `detail_address` varchar(128) DEFAULT NULL COMMENT \'详细地址(街道)\',\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8 COMMENT=\'会员收货地址表\'\n</code></pre><p>这个表也需要把收货人姓名和手机号做数据脱敏处理。</p><p>最后你还需要打开订单表，这里的一些重要信息也需要脱敏才能使用。哪些信息呢？这个问题不难，你可以自己想一想。</p><p><img src="https://static001.geekbang.org/resource/image/d0/fa/d097d6d8e3b076dc617386d0ff8580fa.png?wh=1540x1300" alt="图片"></p><p>到这里，你知道了压测会涉及到哪些表，也知道了表中哪些数据需要脱敏处理，之后的事情就是找到合理的方法，把生产上面的数据处理后给性能测试库做铺底数据，或者给业务测试使用，那么具体应该怎么处理呢？</p><h2>铺底数据的处理方法</h2><h3>1. SQL替换脱敏数据</h3><p>第一种方法是采用 MySQL 批量替换功能。请注意，只有数据库已经备份或者已经导入到测试库的时候，才能这样操作。</p><p>比如脱敏用户表，一般用户表脱敏用户名、密码、手机号等信息，密码都是加密的。但是脱敏的用户还是需要批量修改密码，不修改密码就不能登陆和获取认证 token ，没有 token 其他业务就没法获取请求。</p><p>假设下图蓝框内的手机号就是需要脱敏的手机号。</p><p><img src="https://static001.geekbang.org/resource/image/55/f5/552c18596cfe026ec2b340008d29b8f5.png?wh=1548x862" alt="图片"></p><p>MySQL 中有 DML(Data Manipulation Language) 操作语言，其中 update 语句可以直接批量更新需要脱敏的表中的字段。</p><p>你可以看一下我给出的语句表达式和操作语句示例。</p><pre><code class="language-sql">UPDATE 表名 SET 字段名= REPLACE( 替换前的字段值, \'替换前关键字\', \'替换后关键字\' )  WHERE 条件；\n</code></pre><pre><code class="language-sql">use 7dnewmall;\n\nSELECT * FROM 7dnewmall.ums_member;\nupdate 7dnewmall.ums_member set phone=replace(phone,\'898\',\'188\')  where username like \'7d%\';\nSELECT * FROM 7dnewmall.ums_member;\n</code></pre><p>再次执行查询 SQL 查看用户表和用户手机号，可以看到，带 where 条件的数据已经修改好了，在下图中，你可以看到具体的查询语句。</p><p><img src="https://static001.geekbang.org/resource/image/cd/f0/cd201b84c625b1be7f24c8d2ddffb1f0.png?wh=1788x1166" alt="图片"></p><h3>2. 用Python脱敏数据</h3><p>你需要先创建一个数据库，再创建一个表，下面是简单的语句：</p><pre><code class="language-sql">CREATE TABLE `user` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `name` varchar(50) DEFAULT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci\n</code></pre><p>下一步，编写 Python 脚本，导入 pymysql 模块。<br>\n在脱敏表数据的时候，可以根据需要修改成想要的数，下面给出的代码是最简单的操作示例。</p><pre><code class="language-python"># -*- coding:utf-8 -*-\n# @Time: 9/17/21 23:09 PM\n# @Author: dunshan\n# @File: mallmysql.py\n\nimport pymysql\n\n# 打开数据库连接\nconn = pymysql.connect(host="localhost", user="root", passwd="li123456", db="7d_api")\n\nif __name__ == \'__main__\':\n    cur = conn.cursor()\n    cur.execute("select * from user;")\n    results = cur.fetchall()\n    for row in results:\n        # print(row[0])\n        # print(row[1])\n\n        sql = "INSERT INTO user(name) VALUES (\'%s\')" % (row[1] + "_测试")\n\n        cur.execute(sql)\n        conn.commit()\n</code></pre><p>执行完上面的代码，查看表中的结果。</p><p><img src="https://static001.geekbang.org/resource/image/2a/48/2a67541a418b2096702dc4f1926a0f48.png?wh=520x210" alt="图片"></p><p>不过，<strong>上面讲的只是单个数据库的实例演示，如果是不同库，就需要新建两个数据库链接进行操作了。</strong></p><h3>3. 用Java脱敏数据</h3><p>这里我们通过原始 JDBC 链接操作数据库。大家可以根据实际情况把数据采集过来，加工后插入新数据库表中。下面的示例没有采用批处理插入数据，只是提供一种采集脱敏的方式。</p><p>你可以参考我给出的代码。</p><pre><code class="language-java">import java.sql.*;\n\n/**\n * @author dunshan\n * @Title: 全链路\n * @Description: 铺底数据\n * @date 2021/9/19 / 11:11\n */\npublic class JdbcOperMysql {\n    private static String url = "jdbc:mysql://localhost:3306/mall_new?useUnicode=true&amp;characterEncoding=utf-8&amp;serverTimezone=Asia/Shanghai&amp;allowMultiQueries=true";\n    private static String username = "root";\n    private static String password = "root";\n    private static String driverClass = "com.mysql.cj.jdbc.Driver";\n \n     //读取数据表\n    public static void main(String[] args) throws SQLException, ClassNotFoundException {\n        //注册数据库驱动\n        Class.forName(driverClass);\n        //通过DriverManager获取连接\n        Connection connection = DriverManager.getConnection(url, username, password);\n        //获取Connection对象获取Statement对象\n        Statement statement = connection.createStatement();\n        //执行SQl语句\n        String sql = "SELECT * FROM ums_member;";\n        ResultSet resultSet = statement.executeQuery(sql);\n        //操作ResultSet\n        System.out.println("id|username|nickname|phone|birthday");\n\n\n\n        while (resultSet.next()) {\n            UmsMember umsMember = new UmsMember();\n            Long id = resultSet.getLong("id");\n            String username = resultSet.getString("username");   //用户脱敏脱敏\n            String nickname = resultSet.getString("nickname");    //昵称需要脱敏\n            String phone = resultSet.getString("phone");        //手机号需要脱敏\n            Date birthday = resultSet.getDate("birthday");\n            System.out.println(id + "|" + username + "|" + nickname + "|" + phone + "|" + birthday);\n            umsMember.setId(id);\n            umsMember.setNickname(username);\n            umsMember.setNickname(nickname);\n            umsMember.setPhone(phone);\n            umsMember.setBirthday(birthday);\n            //插入表\n            insertMall(umsMember);\n        }\n\n\n\n        if (resultSet == null) {\n            try {\n                resultSet.close();\n            } catch (SQLException e) {\n                e.printStackTrace();\n            }\n            resultSet = null;\n        }\n        if (statement == null) {\n            try {\n                statement.close();\n            } catch (SQLException e) {\n                e.printStackTrace();\n            }\n            statement = null;\n        }\n        if (connection == null) {\n            try {\n                connection.close();\n            } catch (SQLException e) {\n                e.printStackTrace();\n            }\n            connection = null;\n        }\n    }\n\n    //插入新数据表\n    public static void insertMall(UmsMember umsMember) throws SQLException, ClassNotFoundException {\n        String url = "jdbc:mysql://localhost:3306/mall?useUnicode=true&amp;characterEncoding=utf-8&amp;serverTimezone=Asia/Shanghai&amp;allowMultiQueries=true";\n        String username = "root";\n        String password = "root";\n        String driverClass = "com.mysql.cj.jdbc.Driver";\n        Connection connection = null;\n        PreparedStatement preparedStatement = null;\n        try {\n            Class.forName(driverClass);\n            //创建应用程序与数据库连接的Connection对象\n            connection = DriverManager.getConnection(url, username, password);\n            //执行SQL\n            String sql = "INSERT INTO users(name,password,phone,brithday)" + "VALUES(?,?,?,?)";\n            preparedStatement = connection.prepareStatement(sql);\n            preparedStatement.setString(1, umsMember.getUsername());\n            preparedStatement.setString(2, umsMember.getNickname());\n            preparedStatement.setString(3, umsMember.getPhone());\n            Date birthday = (java.sql.Date) umsMember.getBirthday();\n            preparedStatement.setDate(4, birthday);\n\n\n\n            preparedStatement.executeUpdate();\n\n\n\n        } catch (ClassNotFoundException e) {\n            e.printStackTrace();\n        } finally {\n            if (preparedStatement == null) {\n                try {\n                    preparedStatement.close();\n                } catch (SQLException e) {\n                    e.printStackTrace();\n                }\n                preparedStatement = null;\n            }\n            if (connection == null) {\n                try {\n                    connection.close();\n                } catch (SQLException e) {\n                    e.printStackTrace();\n                }\n                connection = null;\n            }\n\n        }\n    }\n}\n</code></pre><h3>4. 用ETL Kettle脱敏数据</h3><p>你还可以使用 ETL 操作数据库来脱敏数据。但这个话题比较复杂，受到篇幅限制我没有办法在这里给你展开。你可以在<a href="https://community.hitachivantara.com/docs/DOC-1009855">官方网站</a>了解相关内容，也可以直接<a href="https://sourceforge.net/projects/pentaho/">下载</a>平台，通过实践来了解这方面的知识。</p><p>下图是 Kettle 通过 HTTP 接口获取原始数据，数据处理完后导入到关系型数据库中的例子。</p><p><img src="https://static001.geekbang.org/resource/image/39/bf/395a09cd4442688c32aee39a4yyd53bf.png?wh=1147x345" alt="图片"></p><h3>5.用性能测试工具脱敏数据</h3><p>我们可以通过JMeter、LoadRunner等压力工具，跑业务场景接口，把铺底数据造出来。</p><p>下图是一个 JMeter 跑混合场景的例子。</p><p><img src="https://static001.geekbang.org/resource/image/64/70/643943210908e699db17cc9cfc35df70.png?wh=1520x859" alt="图片"></p><h3>6.其他脱敏数据的方法</h3><p>可以采用存储过程造数据、也可以分析日志，也就是把日志的数据处理完毕后，再脱敏敏感数据。这些处理过的数据可以用作铺底数据或测试数据。</p><p>下图是一个之前我们设计使用存储过程方案清洗生产数据的示意图。</p><p><img src="https://static001.geekbang.org/resource/image/f7/63/f7601c09d2d02abf611eb946c0997363.jpg?wh=1253x745" alt="图片"></p><p>主要实现了以下的功能：</p><ol>\n<li>使用存储过程实现了简单的数据清洗，满足了目前项目测试的数据需求。</li>\n<li>数据清洗解耦了执行和处理逻辑，支持动态扩展清洗规则。</li>\n<li>使用存储过程实现了数据抽取功能，支持规则表动态生成采集 SQL。</li>\n<li>使用临时中间表做数据同步，支持异常回滚。</li>\n<li>建立统一的表映射机制，满足了目前项目上的测试数据清洗需求。</li>\n<li>抽象了目前几种常用的数据核查规则，支持规则表动态扩展。</li>\n<li>满足源数据及存储表的一键数据核查，支持自动生成结果。</li>\n<li>通过日志表监控整个处理链路，实现了结果汇总，异常捕获，调试打印等功能。</li>\n</ol><p>到这里，你已经知道了造铺底数据的各种方法，但还有一个条件需要你注意：<strong>你的项目是新项目还是老项目？</strong></p><p>实际上，新老项目在造铺底数据时还是有一点小区别的。如果是老项目，你可以根据自己项目的实际情况，从我上面所讲的方法中选择最适合自己的。但如果是新项目，其实通过业务场景把铺底数据跑出来才是最安全可靠的。它可以保证数据完整、分布均匀，重要的是不用修改什么造数逻辑，一个人就能做到。但缺点是花的时间长，需要跑每个逻辑，最终还是会把数据插入系统表中。</p><p>你可能会说，直接用存储过程、 SQL 或者其他工具把数据造出来不可以吗？这样造数据没有错，但如果系统复杂、表逻辑复杂，用存储过程就不是一个很好的方法。<strong>对于新项目来说，建议通过业务脚本开多线程跑铺底数据。</strong></p><h1>总结</h1><p>好了，今天的课程到这里就要接近尾声了，我们再回过头来看一看。我们今天学习了怎么通过业务场景查找表，也学到了造数据的几种技术方法与思路。</p><p>铺底数据会直接影响性能测试结果。对于老系统来说，直接通过脱敏生产数据就能使用；但对于新系统来说，还是建议通过业务场景造铺底数据，这样最安全也是最有效。</p><p>对于铺底数据量，我们的目标是找到符合生产量级的数据量。新系统可以通过具体的业务推测未来3-5年系统大概会有多少用户，然后用这个数据做参考去跑铺底数据，跑数据的同时需要考虑数据分布，如果分布不正确会影响压测结果。</p><h1>思考题</h1><p>学完今天的课程，请你思考下面两个问题：</p><p>1、铺底数据的来源有哪些？</p><p>2、生产数据脱敏有几种方式？</p><p>欢迎你在评论区和我分享你的思考成果，也可以把这节课分享给你的朋友一起交流讨论，我们下节课再见！</p>',
        article_title: "05 | 铺底数据：真实的压测数据应该做成什么样子？",
      },
      {
        title: "06 | 流量构建：流量平台如何选型？",
        id: 432143,
        content:
          '<p>你好，我是高楼。</p><p>在第一讲，我提到了全链路线上压测和传统线下压测的主要区别：</p><p><img src="https://static001.geekbang.org/resource/image/f3/b5/f333c161f50604e39b467bcc1df85fb5.jpg?wh=1920x1044" alt=""></p><p>既然全链路线上压测和传统线下压测有这么多差异，那它肯定会对我们的流量平台提出更高的要求。我们希望，一款全链路流量平台能够对整体线上环境进行全方位、真实、安全的压测，这样，我们才能够更好、更快、更精准地进行线上容量评估。</p><h1>流量平台的需求</h1><p>下面让我们来看看，全链路压测对流量平台的要求有哪些：</p><ul>\n<li>能够提供模拟线上真实流量的能力。也就是说，能够模拟真实流量，最好能够直接获取线上的真实流量进行压测。</li>\n<li>能够实现海量数据的并发请求。这就要求平台的网络结构符合线上环境拓扑，特别是能满足有地域分布特点的 CDN 边缘节点需求。</li>\n<li>压测类型需要支撑常用协议，如 RPC、HTTP 协议。这里也要提一下，我们这个课程的微服务电商项目都是标准的 RESTful 风格 HTTP 接口。</li>\n<li>压测不能对线上用户的使用产生任何影响。压测流量的写请求不能对线上产生任何脏数据，也就是说，需要做到压测标记。</li>\n<li>能够提供压测过程中的实时监控以及业务异常保护能力。线上压测是一项对风险管控要求更高的活动，压测平台需要有更及时的异常预警机制，也就是说必须要有压测实时监控和异常熔断能力。</li>\n</ul><p>值得注意的是，为了精准地获取到线上环境的容量，全链路压测有两个非常关键的点：</p><!-- [[[read_end]]] --><ul>\n<li>保证流量的真实性</li>\n<li>保证环境的真实性</li>\n</ul><p>既然，全链路压测对我们的流量平台提出了这么多苛刻的要求，那么我们又如何基于项目实际需求进行选型呢？</p><h2>保证流量的真实性</h2><p>在流量构造方式上，根据主要的原理的不同，会采用不同的方式，下表整理了几种常见的方式：<br>\n<img src="https://static001.geekbang.org/resource/image/46/0a/46d64d772c436ee78bd316f35070590a.jpg?wh=1920x1520" alt=""></p><p>从上表我们可以看到，我们可以使用诸如 JMeter、Locust、nGrinder、Gatling 等传统压测工具模拟流量，也可以使用类似 TCPCopy、GoReplay、JVM-Sandbox-Repeater 等流量复制工具，实时录制生产环境流量，并回放导向目标压力机器。同时，这些流量回放工具可以支持对真实流量进行放大或缩小。</p><p>于是有人说，就是因为这样，才应该直接用真实流量的方式去做嘛，这样就不用管流量模型了，直接就有生产的流量模型了。没错，只要你能通过生产流量放大回放的方式实现压力部分，确实可以不用考虑业务场景了。<strong>但这么做的前提必须是，你录制的生产流量是可以覆盖全链路压测的业务场景</strong>。</p><p>同时这里我也要批驳一个观点，<strong>就是有些人觉得只有通过生产流量录制回放的方式，才是真实地模拟线上的流量</strong>。事实上，这个观点是偏颇的。只能说，生产流量录制的方式，相比用传统的压测工具去模拟生产业务模型，看似在成本和复杂度上都更低。而实际上成本和复杂度上都不会减少。</p><p>我为什么这样说呢？</p><p>我的第二个专栏的<a href="https://time.geekbang.org/column/article/358483">第 6 讲</a>中，主要描述了如何利用工具统计出业务模型，接下来，我会讲到怎样把这个业务模型配置到压测工具中去。</p><p>我刚开始以为，这应该是所有性能测试工程师的日常工作，没必要再啰嗦一遍了。但是，之后有越来越多的人问我这个知识点，我才发现，绝大部分性能测试工程师，并不清楚如何统计出业务模型，也不知道如何将它具体配置到压测工具中。</p><p>这就导致出现了一个严重的现实问题，<strong>目前绝大部分企业在做的线下压测容量场景，都是瞎做一通</strong>。即使线下压测的容量场景结果再怎么好看，也不能回答线上容量的问题。于是乎，大家都把目光转移到了全链路压测身上。</p><p>同时，影响流量真实性的一个重要的方面便是地域性分布，我们可以看到目前国内大部分的压测平台几乎都是只在内网使用的，除了那么几个互联网大厂，很少有涉及到考虑 CDN 分布的。从 DNS 负载均衡以及访问加速的架构角度来看，这块是不能忽略的。</p><p>最后，我再强调一下，由于流量的链路分布差异，流量数据的多样性，流量的地域性，只有努力让我们的项目压测流量更为“真实”，才能让容量场景压测结果更为精准。</p><h2>保证环境的真实性</h2><p>想想我们做性能压测的目标，通常都是这样的说法：保证线上系统的正常运行。而一个系统的整体容量，不仅有上层应用软件，还有硬件、网络、存储、负载均衡、防火墙等等一系列软硬件。当有些大厂线下不能完全满足这样的复杂的环境结构时，就只能后移到线上环境进行压测了。</p><p>于是就有人问了，是不是容量场景就一定得线上做呢？我的回答是，也不一定如此。</p><p>对于绝大部分的中小企业系统，部署架构并没有那么复杂。有条件的可以搭建一个 1:1 的镜像环境，没有条件的也可以通过一些手段做到大概的线上线下容量换算，当然这有着一定的门槛，这里就不展开了。有兴趣的话，可以参考我上个专栏讲过的<a href="https://time.geekbang.org/column/article/379841">《如何确定生产系统配置》</a>，我们需要根据自己系统的特点，选择适合自己企业的方案，而不是一股脑地把所有的压测全搬到线上去进行。</p><h1>流量工具的选型</h1><p>说完这两个重要问题，我来说说如何选择流量工具。因为这次的目标是全链路压测，它包括基准场景、容量场景、稳定性场景等等，所以线上引流这种方式首先可以排除。因为线上引流只能满足单服务的基准场景或者是一个机房（集群）内的功能验证测试。</p><p>至于流量回放和流量模拟，二者都能满足我们的需求。流量回放工具的优势是不用管流量模型了，这就拉平了容量场景脚本编写的复杂度，但目前主流的流量回放工具都无法轻易解决 session 的问题，所以从系统安全的角度来说，工具或系统需要做对应的改造。</p><p>而我们这个专栏的落地项目，对系统的改造是可以接受的，因为流量模拟的方式我已经在<a href="https://time.geekbang.org/column/intro/100074001">《性能工程实战课》</a>中详细介绍过了，在这里我就选择了线上流量回放的方式。</p><p>这里我给出几点建议：</p><ul>\n<li>相较于流量模拟，传统的压测工具学习成本相对较低，几乎所有的性能测试工程师都会掌握那么一两个，同时目前国内压测工具平台化改造的技术还是比较成熟的。关于 session 的问题，传统的压测工具的确也可以很好地解决。</li>\n<li>如果系统链路不复杂，有平台化的需求，改造成本较高，那么还是推荐你使用传统的压测工具。</li>\n<li>如果系统链路稍微有些复杂，能够承受改造成本，同时，它还有除了全链路压测之外其它的用途，比如线下回归测试之类的工作，那么我推荐你选择流量回放工具。</li>\n<li>如果你只是想做线上基准场景，又或者是有线上灰度发布之类的需求，那么线上引流的这种方式是比较适合的。</li>\n<li>如果你在一些特殊行业，比如说金融行业，跟钱有关的数据是受到严格监管的，从管理的角度上来说，线上流量回放和引流这两种方式几乎是不太可能的。</li>\n</ul><p>总之，不同方式的优劣都是相对的，我们只需要根据自己系统的需求及特点，选择适合自己的方式即可。</p><p>接下来，我们来看看常见的流量回放工具。</p><p>首先我们通过一个图梳理下常见的流量回放工具。</p><p><img src="https://static001.geekbang.org/resource/image/12/b6/12ca35195b86ef89825ce95b26a5a4b6.jpg?wh=1920x662" alt=""></p><p>流量回放工具一般分成这几类：</p><ol>\n<li>基于应用层的流量回放工具</li>\n</ol><ul>\n<li>优点：实现简单，扩展性好，可定制化高。</li>\n<li>缺点：会挤占线上应用的资源（比如连接资源、内存资源等），还可能会因为耦合度高。而影响正常业务，需要一定的开发工作量，对工具本身的性能要求高。</li>\n</ul><ol start="2">\n<li>基于网络栈的流量回放工具，直接从链路层抓取数据包。</li>\n</ol><ul>\n<li>优点：安装部署较为方便，对应用影响较小。</li>\n<li>缺点：实现相对复杂一些，只适合无状态的业务，无法轻易处理 session。</li>\n</ul><ol start="3">\n<li>基于 web 服务器的请求复制。</li>\n</ol><ul>\n<li>优点：请求多样化、成本低。</li>\n<li>缺点：不具备通用性、丢失网络延迟、占用在线资源比较严重。</li>\n</ul><p>熟悉了以上三种分类，我们再来梳理一下目前业界主流的几种开源流量回放工具：</p><p><img src="https://static001.geekbang.org/resource/image/00/20/00fa67fef10090efb6cb611244d8ce20.jpg?wh=1920x1920" alt=""></p><p>除了以上这些因素外，通常我们在选型工具的时候大概会考虑这么几个方面：</p><ul>\n<li>压力控制：指压测时流量倍数、TPS 的控制等；</li>\n<li>数据驱动：大量的测试数据的参数化手段；</li>\n<li>分布式支持：是否支持压力机集群；</li>\n<li>测试报告：压测结果是否能够图形化展示，提供美观且丰富的测试报告；</li>\n<li>性能开销：执行机开销、软件可靠性、执行效率、业务处理能力等。</li>\n</ul><p>选定工具，我们最好做下功能、性能、资源开销等方面的验证工作，做下选型 POC（验证）测试，确认选型是否符合我们的需求。</p><p>这里我们主要验证以下几个方面：</p><ul>\n<li>流量录制功能，录制的请求内容是否完整？数量是否正确？大流量下录制的资源开销情况如何？</li>\n<li>流量过滤与改写功能，是否能够过滤指定URL，是否能够改写指定参数？</li>\n<li>流量回放功能，能否处理 Session 问题？</li>\n<li>流量回放功能，是否能正常地回放流量文件？倍数功能是否正常有效？高倍数回放下资源开销如何？</li>\n</ul><p>综合对比评估后，最后我们选择了老牌流量回放工具 <strong>GoReplay</strong>，主要原因是简单、轻量、热度够、完全能满足我们目前项目的要求。</p><h1>流量平台的建设</h1><p>这里我再总结下全链路流量平台必须具备的能力：</p><ul>\n<li>能录制线上真实流量；</li>\n<li>能实现海量数据的并发请求，并覆盖地域性的 CDN 边缘节点；</li>\n<li>能支持常见协议的请求；</li>\n<li>对线上尽量应用透明，也就是说无侵入性；</li>\n<li>避免写请求的脏数据，压测流量能够被识别，方便压测后清理；</li>\n<li>工具使用简单，能够满足压测实时监控，服务安全保护（过载熔断）。</li>\n</ul><p>那么，按照以上的能力需求，我画了一个比较典型的全链路流量平台架构设计图。</p><p><img src="https://static001.geekbang.org/resource/image/71/82/713cb37c39316091a6b6e987cb83c382.jpg?wh=1920x1715" alt="图片"></p><p>平台整体架构分为六大模块：</p><ul>\n<li>压测 web 管理端：负责流量文件的管理、压测任务的管理、压测过程中场景动态控制、日志管理、压测数据实时展示等；</li>\n<li>调度服务：负责压力引擎调度、压测命令分发和执行器的管理等工作；</li>\n<li>压测引擎：负责产生各种压测流量；</li>\n<li>监控服务：实时统计压测结果数据，并监控各种资源及业务指标；</li>\n<li>对象存储：存储流量文件，并支持快速查询、上传下载等功能；</li>\n<li>录制服务：负责线上真实流量的复制，并支持对流量标记等功能。</li>\n</ul><p>这样的一个全链路流量平台基本上就可以覆盖大部分企业的需求了。在后续的课程中，我会就这里面的技术细节一一进行拆解。</p><h1>总结</h1><p>流量平台是一个全链路压测项目的引擎，如果你只是考虑某一个项目的快速压测，可能并不需要设计这么复杂并且重量级的流量平台。但对于一个大企业来说，全链路流量平台就显得极为重要了。因为它融合了整个公司线上压测的过程。</p><p>在全链路流量平台中，我强调了几个重点：保证流量的真实性、保证环境的真实性，它们都会对整全链路压测项目的质量起到关键作用。</p><p>我还带你梳理了目前市面主流的流量工具，当我们在做选型的时候，一定要牢记下面几项关键原则：</p><ul>\n<li>满足业务需求是第一准则；</li>\n<li>工具成熟少踩坑；</li>\n<li>要有高效解决问题的手段。</li>\n</ul><p>紧接着，我把一个全链路流量平台该有的能力以及需要做到什么地步，都给你梳理了一遍。希望能给你一些借鉴。</p><p>当然了，我更希望看到，你可以在后续的项目中尝试去设计一个完整的全链路流量平台。</p><h1>思考题</h1><p>学完这节课，请你思考两个问题：</p><ol>\n<li>常见的构造流量有那几种方式？</li>\n<li>为什么我们一直要强调保证流量的真实性？</li>\n</ol><p>欢迎你在留言区与我交流讨论。当然了，你也可以把这节课分享给你身边的朋友，他们的一些想法或许会让你有更大的收获。我们下节课见！</p>',
        article_title: "06 | 流量构建：流量平台如何选型？",
      },
      {
        title: "07 | 全栈监控：如何设计全栈监控策略？",
        id: 434438,
        content:
          '<p>你好，我是高楼。这节课，我们来看看怎样设计全链路压测的全局监控。</p><p>对于全链路压测来说，因为涉及到的服务比较多，所以分析逻辑难度加大，对监控的要求当然也更加复杂。</p><p>如果我们总是在性能瓶颈出现之后再去做分析，很可能会发现缺少各种数据。这时能做的就只有重新运行一遍场景，再增加监控工具，实现更多的数据采集，以补充分析逻辑中需要的数据。</p><p>但是这样的事情肯定是越少发生越好，所以在全链路压测场景执行之前，我们就要把监控策略考虑清楚。</p><p>怎么样来规划监控策略呢？跟着RESAR性能工程理念，我们从系统架构、性能分析决策树、全局监控几个方面来有节奏、有层次地思考一下。</p><h2>系统架构</h2><p>对于性能分析来说，我们要分析的是整个系统架构中，压测场景中涉及到的每一个技术组件，这些技术组件只有从架构的视角才能看得清楚。</p><p>从服务视角，我们可以知道需要监控的服务有哪些，保证服务的覆盖；从资源视角，可以让我们知道资源使用率应该达到多少才是合理的，同时资源视角也和容量模型有关，是重要的容量模型输入。</p><p>服务视角：</p><p><img src="https://static001.geekbang.org/resource/image/2f/4b/2f032901efyy2d9f9e3df8f123e8534b.png?wh=1673x926" alt="图片"></p><p>资源视角：</p><p><img src="https://static001.geekbang.org/resource/image/67/be/67cee8f3f2b161f2af5e5e65c6e310be.png?wh=871x485" alt="图片"></p><p>看到这样的系统架构，我们可不能只知道里面有几个框，还要清楚四点。</p><ol>\n<li>服务列表和调用关系。</li>\n</ol><p>这一点在系统架构的文档中应该有描述。举例来说，在我们这个系统中，当我们发起一个登录请求时，对应上面的架构就是：gateway - member - auth - mysql(redis)。了解了调用关系之后，等你要分析登录业务的性能时，就可以一层层剥离问题了。</p><!-- [[[read_end]]] --><ol start="2">\n<li>服务的规模。</li>\n</ol><p>服务中都有一个副本数量，看到这个副本数量之后，对应系统的最大容量，就要有概念了。一个副本能支持的最大容量是多少呢？这需要通过压测得知，而整个系统能支持多大的容量，就要根据副本做相应的扩展模型来计算了。</p><p>系统的容量根据每个业务系统的不同有很大的差别，像对一些共用服务（比如说ID服务），由于业务逻辑非常简单，所以单副本的容量就会高。而一些业务服务（比如说电商的订单服务、银行中的转帐服务），由于业务逻辑复杂，单副本的容量就会低一些。做压测的人对这些内容都要有事先的了解和大概的计算，以便判断它和最终的容量是否差距较大。</p><ol start="3">\n<li>硬件投入。</li>\n</ol><p>知道了服务的规模，当然也要知道硬件的规模。由于在K8s+Docker这样的服务编排逻辑中，我们事先并不知道每个服务会被调用到哪个硬件资源上，所以无法直接根据某个硬件资源来做相应的计算。但是在系统架构的规划中，应该明确给出每个服务应该配置多少的资源，如果你是容器化的服务，直接限制就好了。限制资源是为了让整个系统更加稳定，不要产生相互的影响。</p><ol start="4">\n<li>技术栈。</li>\n</ol><p>知道了上面的结构之后，就要分析出整个架构所使用的技术组件有哪些，一一列出。如果有足够的信息支撑，也可以知道每个组件是如何使用的。比如说分库分表、读写分离、单元化部署等等和压测有关的关键技术。</p><p>但是如何使用这些技术呢？先看看我从架构上推导出来的内容。由于上面是宏观的描述，落地到具体的细节中，我们需要知道的是宏观的信息如何在具体的业务调用过程中使用的，所以我们推导出如下内容。</p><ul>\n<li>业务调用链<br>\n我画了一个登录调用链你来直观感受一下。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/35/ff/35b383166b34c94ac067e77b3d4bd8ff.png?wh=1920x1100" alt="图片"><br>\n像这样的调用链，画出来当然清晰，但是所有的业务调用链都画一下，这图也没法看了，所以我们可以列出来一个这样的表格。<br>\n<img src="https://static001.geekbang.org/resource/image/43/d3/438832ce2a5a276151917068d89480d3.jpg?wh=1920x1080" alt=""></p><p>掌握这些调用链，后面的性能分析才不会在拆分时间时感到混乱。</p><ul>\n<li>容量所需的资源<br>\n我们还能大概计算一下容量所需的资源。比如，根据我的经验，登录服务在一个6C12G的容器（这里指的是登录链路上的所有服务都有这样的硬件配置）中，700TPS是稳稳的可以撑得住的，要是极端一点，上1000TPS也不是不可能，但对应的响应时间也会增加。这个时候会需要多少硬件资源呢，上面说了，登录业务涉及member和auth两个服务、一个MySQL、一个Redis。如果我们是想支撑2000TPS的登录业务，那就是近3倍的资源需求（先这样线性计算，实际项目中还是要进行横向扩展测试，同时我们还需要考虑冗余，即水位系数），那么总共需要的资源是多少呢？这里我拿CPU来计算一下，其他资源类似。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/d1/bf/d118bd476f7c5f86043a8f5ed729d1bf.png?wh=745x77" alt=""></p><ul>\n<li>技术栈<br>\n有了上面的架构之后，我们可以列出这样的技术栈来。</li>\n</ul><ol>\n<li>微服务框架：<a href="https://spring.io/projects/spring-cloud">Spring Cloud</a> 、<a href="https://github.com/alibaba/spring-cloud-alibaba">Spring Cloud Alibaba</a></li>\n<li>容器+MVC 框架：<a href="https://spring.io/projects/spring-boot">Spring Boot</a></li>\n<li>认证和授权框架：<a href="https://spring.io/projects/spring-security-oauth">Spring Security OAuth2</a></li>\n<li>ORM 框架：<a href="http://www.mybatis.org/mybatis-3/zh/index.html">MyBatis</a></li>\n<li>数据层代码生成：<a href="http://www.mybatis.org/generator/index.html">MyBatisGenerator</a></li>\n<li>MyBatis 物理分页插件：<a href="http://git.oschina.net/free/Mybatis_PageHelper">PageHelper</a></li>\n<li>文档生产工具：<a href="https://github.com/xiaoymin/swagger-bootstrap-ui">Knife4j</a></li>\n<li>搜索引擎：<a href="https://github.com/elastic/elasticsearch">Elasticsearch</a></li>\n<li>消息队列：<a href="https://www.rabbitmq.com/">RabbitMQ</a></li>\n<li>分布式缓存：<a href="https://redis.io/">Redis</a></li>\n<li>NoSQL 数据库：<a href="https://www.mongodb.com/">MongoDB</a></li>\n<li>应用容器引擎：<a href="https://www.docker.com/">Docker</a></li>\n<li>数据库连接池：<a href="https://github.com/alibaba/druid">Druid</a></li>\n<li>对象存储：<a href="https://github.com/aliyun/aliyun-oss-java-sdk">OSS</a>、<a href="https://github.com/minio/minio">MinIO</a></li>\n<li>JWT 登录支持：<a href="https://github.com/jwtk/jjwt">JWT</a></li>\n<li>日志收集、处理、转发：<a href="https://github.com/logstash/logstash-logback-encoder">LogStash</a></li>\n<li>日志队列和缓冲：<a href="http://kafka.apache.org/">Kafka\t</a></li>\n<li>日志采集：<a href="https://www.elastic.co/cn/beats/filebeat">Filebeat</a></li>\n<li>可视化分析与展示：<a href="https://www.elastic.co/cn/kibana/">Kibana</a></li>\n<li>简化对象封装工具：<a href="https://github.com/rzwitserloot/lombok">Lombok</a></li>\n<li>全局事务管理框架：<a href="https://github.com/seata/seata">Seata</a></li>\n<li>应用容器管理平台：<a href="https://kubernetes.io/">Kubernetes</a></li>\n<li>服务保护：<a href="https://sentinelguard.io/zh-cn/">Sentinel</a></li>\n<li>分布式链路追踪系统：<a href="https://zipkin.io/">Zipkin</a></li>\n<li>基础资源监控： <a href="https://prometheus.io/">Prometheus</a></li>\n<li>容器级链路监控：<a href="https://github.com/weaveworks/scope"> Weave Scope </a></li>\n<li>可视化看板：<a href="https://grafana.com/">Grafana </a></li>\n</ol><p>其实，从架构中得到的信息还远不止上面的三种。请注意，我这里说的架构资料并不限于上面的图，还有架构设计相关的其他各种资料。从这些资料中，我们可以得出企业IT规划、架构设计思路、技术实现逻辑、运维技术的逻辑等等。</p><p>你可能会说，这些有什么用呢？这就是做性能要从工程的视角来解释的原因了。只有我们把性能工程覆盖到系统生命全周期中，压测这个动作才有真正的意义，才能贯穿一个系统从原始需求到线上运维的全流程。</p><p>现在，我们已经拿到了技术栈，接下来就要对应这个技术栈来确定我们的性能分析决策树了。</p><h2>全链路压测性能分析决策树</h2><p>性能分析决策树是什么？在第三讲里我讲过，针对项目中用到的技术栈，你通过分析对应的技术组件以及组件对应的模块，得到全量计数器，然后把这个过程通过树状结构展示出来，并画出相应的关联关系，这就是性能分析决策树了。</p><p>现在我们的技术栈已经有了，那就来看下对应的树状结构吧。</p><p><strong>第一，识别架构中所有的技术组件。</strong></p><p><img src="https://static001.geekbang.org/resource/image/f8/14/f8c3a6961dcedd31201f054c47fe7f14.jpg?wh=1962x1573" alt=""></p><p>在这里，我们要尽量识别出全部的技术组件。当然，如果有遗漏，在后续的工作中你还可以接着补充，只是要耽误些项目时间了。</p><p><strong>第二，细化技术组件的模块。</strong></p><p>这一步如果在专栏正文中全部展示给你的话，肯定会占用大量的篇幅，而且这个动作是重复的，没有必要一一细化，我只是想告诉你这个逻辑。</p><p>这里我拿操作系统CentOS来举个例子。因为做性能分析的时候，你没法绕过操作系统，所以拿操作系统来举例，会更有代入感。细化技术组件对应的模块时，我们必须要知道的是这个组件的架构，我们来看一下操作系统的<a href="http://https://www.google.com/url?sa=i&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFile%3ALinux_kernel_diagram.svg&psig=AOvVaw21IUoYtSDhqUYpvNOrJAw8&ust=1632147307259000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCJDZhOeci_MCFQAAAAAdAAAAABBO">架构图</a>。</p><p><img src="https://static001.geekbang.org/resource/image/d2/93/d252ffd77fbcb48da6f33e5bf910d993.png?wh=1920x1397" alt="图片"></p><p>这是一个简化的Linux内核图，足够我们来判断有哪些内容了。</p><p>先看最上面一列大模块：System、Network、Storage、Memory、Processing。其实看到这一层就已经够了，我们至少可以判断出来有五个模块了。再看竖列，它表示的是每一模块在使用过程中的纵向关系，一直延伸到硬件的元器件。</p><p>有人可能会有疑惑了，为啥没有CPU呢？这是因为，只要系统用起来，就必然会使用CPU。所以CPU这个视角肯定不能遗漏。但是CPU是一个综合资源，不在我们现在的讨论模块范围里。其实你要去监控进程的时候，就一定可以看到CPU相关的计数器了。</p><p><img src="https://static001.geekbang.org/resource/image/6f/e6/6fda8588d5c2ca3d064a7342421e4ce6.jpg?wh=1920x1483" alt=""><br>\n还有一点要说明的是，这个模块是不是可以变动呢？当然是可以的，只要不遗漏，你怎么变动都是可以的，后面你还会看到我做的一个变化。</p><p><strong>第三，细化模块对应的计数器。</strong></p><p>有了模块，当然要有计数器来获取性能数据了。如何确定每个模块有哪些计数器呢？我们拿CPU这个模块来举例，其他模块你可以用同样的思路自己去细化。</p><p>我们知道在CentOS中，看CPU的时候，通常会用top、vmstat之类的命令。在top中可以看到9个CPU计数器，分别是：us/sy/ni/id/hi/si/wa/st/load average，vmstat中的计数器比top中少，所以我们就不用再看了。</p><p>那是不是在CentOS中只有这9个计数器呢，其实也不止，命令mpstat中就有两个计数器%guest和%gnice。把这些计数器都列到CPU模块上去，就得到了一个更加详细的性能分析决策树。</p><p><img src="https://static001.geekbang.org/resource/image/41/44/41d1cdded89dccf7fbe44e0479c58144.jpg?wh=1920x1920" alt=""><br>\n为了方便查看，我把操作系统细化出的性能分析决策树单独拿了出来。</p><p><img src="https://static001.geekbang.org/resource/image/2f/ec/2ff99beac609743a0a10ae6d4c80cdec.jpg?wh=1920x2439" alt=""></p><p>我在这个图里做了几个调整：</p><ol>\n<li>把Processing模块去掉了。因为我觉得在第一层的全局监控计数器中，是不需要细化到进程/线程这个粒度的；另外，在CPU上也可以反映出来Processing模块出现的性能问题，所以它不会被遗漏掉。</li>\n<li>增加了swap模块。虽然swap在性能项目中通常都是关着的，但为了不忘记它的存在，也要标记出来。</li>\n<li>画了关联线。我将需要关联分析的计数器建立起了联系，方便梳理思路。</li>\n</ol><p>看到这里，你就能彻底理解性能分析决策树是什么了。对表中所有的技术组件进行模块、计数器的细化，就可以得到这个项目中全部的性能计数器。这个过程怎么进行要看你的技术功底，如果一个人做不到，也可以组织团队一起来做。</p><p>那有了性能分析决策树，下一步怎么办呢？它当然不只是用来看的，我们要把这些计数器都监控起来。</p><h2>全链路压测全局监控</h2><p>全局监控就是把性能分析决策树中的所有计数器都监控起来。</p><p>还是拿操作系统来说，你可以再看一下上面的细化视图。这么多的计数器，有没有必要全都监控呢？这个要根据情况综合考虑。在上面的视图中，我把我觉得必须要监控到的计数器标成了红色，这些都是根据我的经验标注的，如果你不认可，那就标识出你认为重要的。针对这些重要的计数器，我们到监控工具中去看一下是不是都覆盖全面了。</p><p>我们可以拿现在最常用的node_exporter+Prometheus+Grafana套件来看一下。</p><p><img src="https://static001.geekbang.org/resource/image/60/68/6048c0f6a90cf1ae1b0f4d67f1425168.png?wh=1842x873" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/fc/ca/fcbc2d373b7yy8736c1yyfdbcbe723ca.png?wh=1840x648" alt="图片"></p><p>首先说CPU，前面我们列了11个计数器，这里面现在有的是us、sy、load average，这几个明明确确是一对一覆盖的。其他的，我在这个界面中都没看到。</p><p>这时，应该会有人想反驳我：“那不是有总使用率吗？这个值是从1-%idle计算来的，不就覆盖了所有的CPU计数器了吗？”</p><p>这种问题我建议你不要这么大而化之。我们做性能分析，是要看到“每一个”计数器的，而不是总使用率，这个值没有办法给出依据，指导我们下一步怎么做。</p><p>如果在你的项目中，出现了这种总使用率的计数器，但除此之外就没有其他的计数器了。那可能有些问题体现不到这个总使用率的计数器上来。即便能体现出来，你的下一步动作也只能是去看每一个CPU计数器，而不可能直接知道性能分析的明确方向。</p><p>对于整个项目级的性能分析决策树，你可以自己去一一对一下监控工具，看缺少哪些性能分析决策树中的计数器，少的都要加上。当把整个项目的性能分析决策树，都覆盖到监控工具中之后，全局监控才算是完成了。</p><p>如果实在无法在短时间内加上这些计数器，那你就要在分析的时候记得用其他的工具命令来补充。</p><p>对这样量级的监控，通常会有人提出疑问：监控这么多数据，对性能没有影响吗？请记住，我们的逻辑是，要覆盖全性能分析决策树，但不是一股脑地把所有不重要的计数器都加上。这是不理智的。如果你发现监控工具对性能有很大的影响了，可以降低采样率，但这个风险是数据采集不准确，所以还是要评估好。比如在做性能分析时，把采集粒度调小一点；不分析时，把粒度调大一点。</p><h2>全链路压测的监控逻辑</h2><p>有了上面的监控思路，我们就可以对任何一个性能项目进行全方位的监控策略设计了。</p><p>那全链路压测中又有啥特别的呢？它的特别之处在于，改造的内容产生了新的组件。其实从方案中就可以看出来，我们要做的改造是哪几个模块。</p><p><img src="https://static001.geekbang.org/resource/image/2c/c9/2c41db25b7c4d551e955acac2b6902c9.jpg?wh=1918x853" alt=""></p><p>我们必须把这些改造产生的新组件（影子库等）都监控起来。也就是说，原本我监控一个MySQL数据库就好了，现在分库了，就要监控两个库才可以。</p><p>举个例子，像Zipkin中，之前的链路中只有一个MySQL，现在就多出来一个shadow的MySQL。</p><p><img src="https://static001.geekbang.org/resource/image/cd/44/cd94913bb15891d8811eec8ca70f2b44.png?wh=1486x602" alt="图片"></p><p>在全局监控时，我们要把这些改造产生的组件全部覆盖。</p><p>同时，最好能够结合链路追踪系统，做到识别正常流量与压测流量。这样方便快速分析问题。</p><p>最后，为了应对线上的风险，我们还要着重及时关注异常情况，比如核心业务指标异常、基础资源 Metric 水位过高、异常错误集中等情况。</p><p>结合压测方案章节中的全局监控视图。</p><p><img src="https://static001.geekbang.org/resource/image/71/b5/71e39cd615bb0ac4b322fe638e1918b5.jpg?wh=1960x826" alt="图片"></p><p>你可以看到的是，在技术栈的每个组件都有对应的监控工具。</p><p>当然，如果你所在的企业可以投入成本，可以考虑集成这些开源的监控工具，形成自己的监控平台，这也是现在很多企业在做的。市面上也有集成了这些开源监控工具的商业平台，总之，监控的内容都相差不大。</p><h2>总结</h2><p>这节课的内容就讲完了。</p><p>在这一讲，我给你描述了从系统架构细化到性能分析决策树，再细化到全局监控的全过程。这个过程在任何一个性能项目中都是可以复用的。</p><p>而全链路压测需要格外注意的，首先就是要全，其次要准。 不过也不要觉得做全链路压测就是改变了之前的技术逻辑，监控工具和思路完全不能用了。其实，相比通常压测项目的监控策略，全链路压测的主要的区别都是由于改造产生了新组件和压测流量所引起的。这些组件只是原有技术组件的叠加，在监控时考虑进去就行了。对于压测流量最好能做到自动识别出来。</p><p>只要有了全局监控，才可能在出现性能问题时，知道如何进行定向监控分析。在后面，我会对案例进行更详尽地分析，让你看到分析过程的逻辑。</p><h2>思考题</h2><p>最后，我想请你思考两个问题：</p><ol>\n<li>性能分析决策树要求什么技术背景？如何才能做到不遗漏计数器？</li>\n<li>全局监控对性能瓶颈分析起到什么样的作用？</li>\n</ol><p>欢迎你在评论区和我分享你的思考成果，也可以把这节课分享给你的朋友一起交流、讨论，我们下节课再见！</p>',
        article_title: "07 | 全栈监控：如何设计全栈监控策略？",
      },
      {
        title: "08 | 基础设施：全链路压测的环境有什么特点？",
        id: 436108,
        content:
          '<p>你好，我是高楼。</p><p>这节课，我们来聊一聊全链路压测的环境特点。</p><p>全链路压测在技术市场上叫嚣了好几年了，但是到现在为止还是有很多企业处在懵懂的状态。在开篇词中我们已经提到过，其中一部分原因是会涉及到很高的人工、资金和时间成本。另外，组织协调的问题也不容小觑。但是，为什么会有这么高的成本呢？其实这里的根源，都离不开全链路压测的环境。</p><p>全链路压测涉及到的基础设施范围不仅复杂，而且非常庞大。在复杂的网络结构、应用架构中，有数不胜数的可能影响性能的因素。如果只是简单地这么说，你可能不会有特别强烈的感觉。接下来，我们就具体地看一看。</p><h2>线上环境中复杂的网络结构</h2><p>下面是一个大企业的双数据中心的拓扑图。</p><p><img src="https://static001.geekbang.org/resource/image/27/7c/273b931e7bc05a61f7ffa2b6286c6d7c.jpg?wh=1048x563" alt="图片"></p><p>图中所展示的这个网络结构是为了高可用而设计的。因为是双数据中心，所以在这个结构的中心，两个设备虚拟化成了一个设备，汇聚层同理。</p><p>在这样的网络里，数据进来之后，会先接入防火墙（FW）、负载均衡（SLB），但它们还没到具体的分区。如果要进入分区，还要接入另一层网络设备，然后才能到达生产业务区、办公业务区、开发测试区、NAS资源区等。</p><p>这就完了吗？其实还没有。比方说，生产业务区就还有不同的子分区，你要经过子分区才能到达具体的系统里。</p><!-- [[[read_end]]] --><p>而现在的压测市场，通常都接触不到这么完整的网络架构，很多企业只是在内网的一些服务器上做些压测的操作，大一点的也就是跨几个VLAN（虚拟局域网）了事。还有一些大的企业，据我所知，虽然对外宣传全链路压测做得多么完备，但是在实际执行的过程中，还是分段玩的。</p><p>可以看到，两中心的网络结构已经这么复杂了，但这还远不是最复杂的。我们再来看看两地三中心的网络结构是什么样子。</p><p>我依旧用拓扑图来展示，为了方便大家理解起见，外网的部分我就不画了。</p><p><img src="https://static001.geekbang.org/resource/image/ba/5e/bae1238cebff5eeyye0142497933bf5e.jpg?wh=1031x574" alt="图片"></p><p>在这种两地三中心的网络架构中，各中心之间是用专线连接的，数据中心A、B会同时提供服务，数据中心C主要是用于异地灾备。</p><p>在这个基础上，我们再添加一下网络结构就会成为下面的样子。</p><p><img src="https://static001.geekbang.org/resource/image/0e/37/0ed1a732fde4631abcb89cb1fda04037.jpg?wh=1031x574" alt="图片"></p><p>在这个网络结构中，数据中心之间使用的是BGP（Border Gateway Protocol，边界网关协议）协议，数据中心A、B之间使用的是OSPF（Open Shortest Path First、开放式最短路径优先）协议，各分区之间使用的是二层VLAN。</p><p>我们不仅要在二层VLAN的网络结构中做压测，数据中心之间的网络也不能忽略。</p><p>那再来延伸一下。由于微服务分布式技术的发展，以及云计算基础设施服务的支撑，两地三中心也快成为过去式了，随之而来的是<strong>多地多中心的网络结构</strong>：</p><p><img src="https://static001.geekbang.org/resource/image/26/f7/26174507df0f6465a65e2f8yy5526ff7.jpg?wh=1920x877" alt="图片"></p><p>图中有四个中心，对于一些重要的业务来说，四个中心都需要灵活部署的。</p><p>你还可以看到，数据中心A、B可以组成一个联邦，数据中心C、D是另一个联邦。每个数据中心都有两个生产业务区，这两个生产数据区之间是可以数据同步的。不过，数据中心A、B和数据中心C、D之间是数据异步的。这些都是为了保证整个企业的业务高可用。</p><p>我们把这些数据中心对应到地域上再来看一下。</p><p><img src="https://static001.geekbang.org/resource/image/58/6b/58e9136b51d8f05bb3849121f627cd6b.jpg?wh=1920x1353" alt="图片"></p><p>这四个数据中心分布在不同的城市。每个数据中心需要的硬件设备总量完全取决于这个企业的体量，有的几十万台，有的几万台，也有的几千台。</p><p>之所以需要这么多设备，是为了满足这么大的容量要求，当然这里也包括了容灾冗余的要求。</p><p>如果只是考虑接口级的请求，这个网络结构大概就够了，无非是再加上一些ISP的DNS服务器。但是，我还想再多讲一点，因为对于一些企业来说，还有一个需要关注的大头：CDN。</p><p>对于CDN网络架构来说，像上面这样规划显示是过于集中了。如果我们使用CDN架构，它的地域分布应该是这样的：</p><p><img src="https://static001.geekbang.org/resource/image/e7/b9/e793d1510149e442b677e920a8344db9.jpg?wh=1920x1442" alt="图片"></p><p>可以看到，针对全国所有的省市，我们都要放CDN的节点。这样，一些静态资源就可以从最近的CDN服务器上获取了，比如电商系统中的商品图片、视频系统中的视频等。这样不仅速度快，也可以节省更多的主干网络资源。</p><p>不过，现在各厂商的全链路压测的逻辑中，其实是不包括CDN的部分的，我之所以讲它是希望你可以对线上环境有个更全面的认知。由于做全链路压测的重点在业务系统，而CDN是由独立的系统完成的，并且CDN服务的稳定性由CDN厂商负责，所以CDN没有必要包含在业务系统中，也就没有必要包含在全链路压测中了。</p><p>讲到这里估计你已经能够感受到了，真正的线上压测要组织起来，从环境上来讲确实是太复杂了。</p><h2>线上环境中复杂的应用架构</h2><p>说了这么多网络结构的问题，我们再来看看应用架构。先来看下我给你画的这张图。</p><p><img src="https://static001.geekbang.org/resource/image/92/83/921792c70cd8d4dcafb6a5c90bb90d83.png?wh=1920x1832" alt="图片"></p><p>可以看得出，在应用架构方面，一个系统要最终展现到用户面前，还是需要很多技术支撑的。</p><p>这些技术大体分为三个层级，也就是 IaaS、PaaS 和 SaaS 。这个层级的划分在不同的企业中并不完全相同，还好无伤大雅，即便划分不同说出来大家也都能理解。</p><p>上面这个图可能还是太过笼统了，没有应用、服务细节，看起来还是不太像一个应用系统的架构。我们就再换一个视角，看看一个应用系统架构该有的样子：</p><p><img src="https://static001.geekbang.org/resource/image/ab/9e/abd7df412998461cd240fe314104a99e.jpg?wh=1920x1590" alt=""></p><p>这张图是不是更容易理解了一些？</p><p>在这个应用中，用户会从Nginx这样的入口进来（请记住，一个终端用户要想进到这个入口，已经经过了漫长的网络结构），经过网关，然后根据具体的业务访问不同的服务。</p><p>这还只是一个系统，而大部分的企业都有很多系统。在我已知的企业级IT架构中，一些大的企业怎么也有几百个系统，一些中小型企业也能达到几十个系统。在这样的量级下，要想把系统完整地搭建起来，可想而知需要多少干体力活的劳工不眠不休地干上几个月。</p><p>再加上，企业的整个IT架构也不是一成不变的，每个系统都需要反复的迭代。这就是全链路压测的整体环境为什么那么复杂的另一个原因了。</p><p>其实，只是像我们专栏这个项目的量级，搭建的部分最快也得近一个月才能完成。如果再遇到一些鸡毛蒜皮、鸡飞狗跳的技术问题，成本就更加不可控了。</p><h2>线上性能的影响因素</h2><p>前面我们了解了网络和应用架构的复杂度，从刚才的分析中我们可以知道，全链路压测出现的前提是，测试环境中构建基础设施和应用资源的成本非常高，所以我们才需要把全链路压测放到线上环境中去执行。</p><p>既然要在线上环境中执行，就需要知道影响线上性能的因素有哪些。我们先以应用为核心，对线上资源做个拆解。</p><p>基础设施资源方面：</p><p><img src="https://static001.geekbang.org/resource/image/0b/c3/0b44ba67f492275af711763494ed03c3.jpg?wh=1920x960" alt="图片"></p><p>可以看到，从服务器到操作系统、从域名到网络设备、从IDC到机柜等，都属于基础设施资源的范畴。</p><p>应用资源方面：</p><p><img src="https://static001.geekbang.org/resource/image/72/aa/72608004b9ca6bd2f25efd77c43438aa.jpg?wh=1621x746" alt="图片"></p><p>我们知道，很多应用程序的性能问题都可能是第三方依赖引起的，与此同时，应用的运行参数以及部署方式也对性能有着很大的影响。</p><p>找齐了这些要素，我们就来总结归纳一下：影响线上性能问题的因素有哪些？</p><p>我给你画了张图，你可以参考一下。</p><p><img src="https://static001.geekbang.org/resource/image/b8/34/b86b778a07d8ffe00a95afac28a9f134.jpg?wh=1920x918" alt="图片"></p><p>看到这么多影响线上性能的因素，你是不是有点懵？实际上我这里还只列举了我认为最重要的一小部分。那么，是不是所有项目的线上性能都会被这么多因素影响呢？</p><p>当然也不是。我这里举的例子只是上了规模的互联网项目，如果你所在的项目只是 ToB 的企业内网项目，系统拓扑肯定是没有这么复杂的。你也可以尝试着按照“横向-纵向”的逻辑把你所在项目的拓扑图画一画。</p><p>从以上的描述中就可以看到，虽然我们在线上执行全链路压测是符合压测目标的，但是仍然会有不少需要考虑的因素，这些因素也构成了全链路线上压测的复杂性，当然这些也是我们必须要面对并解决的问题。</p><h2>全链路压测线下环境亦有挑战</h2><p>好了，上面我们已经看到了，大型项目的线上环境，在性能方面会面临非常多的挑战。那如果我们知难而退，不做线上压测了，考虑对线上环境做镜像环境、做线下测试，我们又会面临什么挑战呢？</p><p>我简要总结了几点：</p><ul>\n<li>复制线上环境网络环境；</li>\n<li>复制线上环境基础资源；</li>\n<li>复制线上环境应用架构；</li>\n<li>保证网络带宽；</li>\n<li>准备所有的基础数据；</li>\n<li>…</li>\n</ul><p>显然，这对于性能工程师来说，压力太大了。要完成上面这些需求，面临巨大的困难：</p><ul>\n<li>需要沟通协调几乎所有的相关部门(开发、运维、网络……)；</li>\n<li>成本也是个大问题，一些大的企业通常都是多地多中心的，涉及到公网出入口、各层网络设备、防火墙等等内容，搭建一套一样的环境的成本，纵然是大企业也得掂量掂量。可以想象，如果只是使用一两次，那么就是劳民伤财，成本根本兜不住；如果持续维护，维护成本也同样不可持续。</li>\n</ul><p>所以，我们很少看到有企业进行这样的“镜像环境”操作。即便做线下的全链路压测，环境问题依旧很复杂，所以在线上做全链路压测仍然是首选的方案。</p><h2>总结</h2><p>好了，这节课就讲到这里，我们总结一下。全链路压测的环境一大特点就是非常复杂，它的复杂性主要体现在以下几个方面：</p><p>首先是互联网环境复杂，这不仅仅体现在层级复杂上，而且还具有显著的地域性。</p><p>另外，基础设施也是个很大的问题，分布式、云计算、云原生的广泛应用导致现如今的基础设施越来越庞大，其中的人力和时间成本不可低估。</p><p>还有，影响线上性能的因素也很多，包括代码、应用、服务器及云环境、业务逻辑及应用行为等等，这都给大型互联网项目在性能方面提出了非常多的挑战。</p><p>我们知道，互联网最大的趋势和变化就是海量数据、海量用户导致架构对分布式、云计算、云原生更加依赖，高速发展的独角兽企业会因此遇到很多挑战和困难，但也催生了许多成熟、成功的全链路压测方案。但是，我们还是得着眼实际，打造适合自己项目的特殊的全链路压测。毕竟，技术太过纯粹，适用才是王道。</p><h2>课后题</h2><p>学完这节课，请你思考两个问题：</p><ol>\n<li>除了线下镜像环境，你还知道哪几种搭建环境的方式？</li>\n<li>在线上做全链路压测的过程中，你遇到过哪些环境问题？</li>\n</ol><p>欢迎你在留言区与我交流讨论。当然了，你也可以把这节课分享给你身边的朋友，他们的一些想法或许会让你有更大的收获。我们下节课见！</p>',
        article_title: "08 | 基础设施：全链路压测的环境有什么特点？",
      },
      {
        title: "09 | 压测模型：如何建立一套完整的全链路压测模型？",
        id: 437781,
        content:
          '<p>你好，我是高楼。</p><p>这一讲，我们开始学习模型部分。</p><p>在完整的性能工程中，模型到底有哪些内容呢？这个问题，估计每一个做性能项目的人都没有看到过什么定论。</p><p>这节课呢，我想跟你分享一下我总结出来的性能项目中的模型，我把它称为<strong>性能五模型</strong>。</p><p><img src="https://static001.geekbang.org/resource/image/59/44/59d9845e775edce954d26c7da0cb1944.jpg?wh=1786x796" alt=""></p><p>简单解释一下，它分为<strong>业务模型、容量模型、监控模型、失效模型</strong>和<strong>分析模型</strong>五个部分。</p><ol>\n<li>业务模型确定了压力工具中的设置；</li>\n<li>容量模型确定了系统的设计容量计算方式；</li>\n<li>监控模型确定了监控的内容；</li>\n<li>分析模型确定了分析的逻辑；</li>\n<li>失效模型确定了异常场景。</li>\n</ol><p>那么，在全链路压测项目中，这些模型产生了什么变化呢？</p><p>从分类和内容上来说，没有任何变化。主要的变化是，所有的模型都乘以了2。</p><p><img src="https://static001.geekbang.org/resource/image/d7/f3/d70f8yy9c9395140144ef7587d023bf3.jpg?wh=1920x788" alt=""></p><p>因为全链路压测强调的是<strong>线上执行</strong>，所以不管你用什么样的方式来组织线上的架构，除了正在运行的真实模型之外，你还要从全链路压测的视角把这些模型再考虑一遍。</p><p>在具体描述每个模型之前，我先把整体的架构图展示一下，以便让你看到我们是在哪些地方做了改造的动作。</p><p><img src="https://static001.geekbang.org/resource/image/ab/9e/abd7df412998461cd240fe314104a99e.jpg?wh=1920x1590" alt=""></p><p>在这个架构图中，我们对微服务、缓存、队列、数据库做了改造，用灰色标示的是做了改造的部分。其中，微服务、缓存、队列部分没有用新的硬件设备，是在同样的硬件中实现的；数据库创建了影子库，所以增加了同配置的硬件。</p><!-- [[[read_end]]] --><p>现在我们来说一下每一个模型的具体内容。</p><h2>业务模型</h2><p>业务模型就是将真实场景的业务比例复制或推算出来的过程。关于复制，我们要考虑不同的手段。</p><p><strong>手段一：线上流量复制的方式复制出业务模型。</strong></p><p>简而言之就是流量复制，这也是现在全链路压测中经常讨论的一个话题。流量复制使用的工具，我在<a href="https://time.geekbang.org/column/article/432143">第6讲</a>中已经有过详细的论述，如果你记不清了，也可以回过头去温习一下。这里就不复述了。</p><p>这里我们要讨论的是：复制出来的流量，是不是回放或放大回放就可以满足我们的目标了呢？</p><p>当然没有那么简单，因为我们先要确定<strong>目标</strong>是什么。如果我们的目标就是要测试录制出来的那一段流量产生的业务比例，这样做确实是可以的。</p><p>但如果我们的目标是通过放大录制的流量来评估未来的场景，那基本上就靠不住了。因为复制出来的业务模型，是不能代表未来业务模型的变化的。</p><p><strong>手段二：通过统计线上流量的方式复制出业务模型。</strong></p><p>我把详细的流程给你梳理了一下：<br>\n<img src="https://static001.geekbang.org/resource/image/f8/f3/f8ac34d8df065b1ac26935a7c3aa89f3.jpg?wh=1544x2424" alt=""></p><p>只要业务比例和生产环境一致，通过这种方式复制出的业务模型，就和录制回放手段产生的压力结果没有区别。</p><p>不过，如果要想推算未来的模型，也存在同样的弊端。</p><p>谈到这里，我们就不得不说说推算业务模型这件事了。假设我们拿到了这样的生产业务模型。<br>\n<img src="https://static001.geekbang.org/resource/image/44/44/441a212620cc64b1736464ea5407e344.jpg?wh=1920x1080" alt=""><br>\n但是由于未来业务的变化，业务部门评估认为，未来购买的比例会增加。那从接口上来说，就是图表中4-9之间的业务比例会增加，于是我们得到了下面这个模型。</p><p><img src="https://static001.geekbang.org/resource/image/42/d7/42f577a7905055722babc17c116786d7.jpg?wh=1920x1080" alt=""><br>\n如果我们想实现这样的业务模型，那用复制出来的流量回放显然是不可以的。要实现这样的业务模型大概只有一个选择，那就是用压力工具来实现压力。</p><p>所以，<strong>对于业务模型来说，关键是要看测试目标。</strong></p><h2>容量模型</h2><p>在执行测试场景之前，要先计算一下系统大概能支持多少容量（本节课后面的容量都用TPS这个概念来承载）。容量模型的计算离不开架构图，这张图我在前面已经给你展示过了，我再放在这里加深一下你的印象，希望你能把它牢牢地记下来。</p><p><img src="https://static001.geekbang.org/resource/image/ab/9e/abd7df412998461cd240fe314104a99e.jpg?wh=1920x1590" alt=""></p><p>有了架构图，我们再来输入一些数据。</p><ol>\n<li>微服务应用在总资源40C80G的硬件资源，可以支撑混合场景1500TPS，且硬件资源已饱和。</li>\n<li>在上述条件下，2台数据库(8C16G)使用率最大的硬件资源CPU，只用到了50%左右；2台缓存（4C8G）和2台队列服务器（4C8G）最大的资源使用率（CPU使用率）只有20%左右；2台网关（4C8G）最大资源使用率是40%。</li>\n</ol><p>请注意，上面的信息并不需要臆测，只要通过正常的性能容量场景就可以得到这样的数据。那么，如果现在我们希望系统支持3000TPS，需要多少资源呢？</p><p>如果用线性计算，我们可以得到图片中这些变化。</p><p><img src="https://static001.geekbang.org/resource/image/2f/71/2fyyff46c11ea9aae5606aeaf1aed271.jpg?wh=1920x856" alt=""></p><p>也就是说，应用类的资源要增加一倍，而数据库、缓存、队列服务器的资源可以不用增加。</p><p>可是，真的就这么简单吗？你是不是也觉得这样的计算非常不靠谱？</p><p>试想一下，当压力增加的时候，数据库资源用完，响应会变慢，那显然TPS就会下降，所以不可能达到3000TPS。而缓存和队列服务器就还好，因为还有比较大的空间。</p><p>如果这么做不合理，那我们是不是再增加一些数据库服务器资源就好了呢？其实即便是这样，我们也无法让系统容量成倍地增加，因为还有另一个问题：微服务应用资源已经用完了。</p><p>如果我们增加了成倍的资源，在不考虑其他服务器的情况下，这成倍资源的增加会带来TPS的成倍增加吗？这是不能确定的。因为资源的增加，会带来服务节点的增加，紧接着会带来远程调用时网络带宽的增加。请注意，这个增加不止是网络资源，还包括网关和负载均衡压力的增加。</p><p><strong>所以我不建议这样线性地计算</strong>，怎么来计算呢？最好的方式就是拿真实的数据说话，采用递增加压的方式，先看一下TPS增加的幅度和资源消耗之间的关系。相应地创建下面这样的表格：</p><p><img src="https://static001.geekbang.org/resource/image/71/9f/71e639c3aa9a4bee677de184d0b8219f.jpg?wh=1920x1000" alt=""></p><p>我们先增加到2000TPS，再增加到2500TPS，再增加到3000TPS，然后看看硬件资源的增长率。这里我只把硬件的增加列出来，就不再详细记录使用率的百分比了。</p><p><img src="https://static001.geekbang.org/resource/image/67/bb/67445f03cc783d0d5a671bce98083cbb.jpg?wh=1920x858" alt=""></p><p>通过数据来看，网关的资源增加主要是CPU，微服务应用和数据库也都有增加，但并不是完全线性的，TPS越往上增加，微服务应用和数据库的增幅也越大，成本越高。缓存和队列由于没有达到使用的上限，所以不需要增加。</p><p>在这样的示例中，我们得到最重要的启发就是：<strong>容量的规划不能通过简单的线性计算得到合理的结果，而是要通过容量场景的执行得到一个合理的增长模型。</strong></p><h2>监控模型</h2><p>监控模型在全链路压测过程中是非常重要的模型。根据RESAR性能工程理念，全局监控的关键是“全”。怎么才能全呢。那就要在监控之前做几个动作：</p><ol>\n<li>分析技术栈。我这里主要列出了涉及到改造的技术栈。</li>\n</ol><p><img src="https://static001.geekbang.org/resource/image/35/be/356729706577fc912c084d08681552be.jpg?wh=1920x852" alt=""><br>\n2. 创建性能分析决策树。</p><p>接下来，我们要为上面提到的网关、微服务应用、缓存、队列和数据库一一创建性能分析决策树。</p><p>由于网关和微服务应用都是基于Java编写的，所以这里我们直接创建一个Java应用的性能分析决策树：</p><p><img src="https://static001.geekbang.org/resource/image/34/7c/34e707f9dc15af1b85242244db4eaa7c.jpg?wh=1188x1131" alt=""></p><p>数据库性能分析决策树：</p><p><img src="https://static001.geekbang.org/resource/image/9d/84/9d5d5a569bd00f92b21543e05c0f8584.jpg?wh=1466x1655" alt=""><br>\n缓存性能分析决策树：</p><p><img src="https://static001.geekbang.org/resource/image/24/31/24551ab2df5bf316b7a44fcccd732731.jpg?wh=1300x1336" alt=""></p><p>队列性能分析决策树：</p><p><img src="https://static001.geekbang.org/resource/image/02/24/0222226751fc36fed83a07f01yyyyf24.jpg?wh=1608x1615" alt=""></p><p>这样，我们几个关键技术组件的性能分析决策树就完成了。其他的技术栈，你可以自己创建一下，组织的形式和分类的方式并不是固定的，只要你能理解就可以。关键在于监控数据的完整性。</p><ol start="3">\n<li>有了整个项目的性能分析决策树之后，就要把这些需要监控的计数器，都体现到监控工具中了。对应每一个技术组件，我们用下图表示一下覆盖关系。</li>\n</ol><p><img src="https://static001.geekbang.org/resource/image/71/b5/71e39cd615bb0ac4b322fe638e1918b5.jpg?wh=1960x826" alt=""><br>\n看完这张图你会发现，我们在设计监控模型的时候，是每一个技术组件都会覆盖到的。<br>\n这个逻辑在所有的性能项目中都是一样的。</p><p>而在全链路压测的项目中，由于做了改造，必然会涉及到一些监控计数器的增加。具体有哪些呢？我们还是拿关键组件来说明。</p><p><img src="https://static001.geekbang.org/resource/image/bd/b5/bd2a05fd13a6ac2dd41b7bbe73abf3b5.jpg?wh=1920x856" alt=""></p><p>从表格上可以看出，在我们这个全链路压测项目中，相比较之前的监控内容，只是增加了新的数据库节点而已。</p><p>在这里，如果你说：“我以前监控得都不全，加这些怕是不够吧”。那就是你之前的工作有疏漏了，请后退出去把门带上，面壁反省。</p><h2>分析模型</h2><p>分析模型是建立在监控模型基础之上的，所以我们必须把性能分析决策树再拿出来，接着往下拆解。</p><p>我们来举例说明一下：</p><ul>\n<li>对数据库的查询和排序报表类计数器的分析</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/b0/ce/b09ed6fddfyy738a3d4f4aeec8ac4cce.jpg?wh=1920x859" alt=""><br>\n我们要做的是：分析查询类型 - 统计具体的SQL语句 - 查看执行计划和Profiling信息 - 分析业务逻辑 - 提出解决方案。</p><ul>\n<li>对InnoDB锁报表计数器的分析</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/7a/05/7a19342af0yya5f6508b04d4b5d71f05.jpg?wh=1920x548" alt=""></p><p>我们要分析的逻辑是：分析锁信息 - 统计具体的SQL语句 - 查看执行计划和Profiling信息 - 分析业务逻辑 - 提出解决方案。</p><ul>\n<li>对InnoDB缓存池报表计数器的分析</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/b2/a1/b211b90a0fbda56989c939024c96d4a1.jpg?wh=1920x1374" alt=""><br>\n之所以这个图截得大一点，是分析SQL和查询排序报表之间有了关联关系。</p><p>我们要分析的逻辑是：分析缓存使用率 - 分析内存读写和磁盘读写 - 分析SQL（分析SQL的类型） - 提出解决方案。</p><p>再拿java的性能分析决策树来举个例子。</p><ul>\n<li>对Java线程类计数器的分析：</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/13/97/135a251fa5c019b66d6bd1c5ff631597.jpg?wh=1919x789" alt=""></p><p>对Java的线程池，分析过程是：分析所有线程状态 - 确定繁忙程序/是否阻塞 - 分析栈信息 - 提出解决方案。</p><p>但是因为分析线程状态要结合CPU使用率来分析，所以在图片中你可以看到，他们之间也有一条线。</p><p>把所有的分析路径都分析出来，我们的分析模型就创建完成了。其实在这一步，最需要的就是技术功底和耐心。</p><p>可能有人会说，这么庞大的分析模型是一个人能完成的吗？根据我的经验，如果你认真学习，多则十年八年，少则五六年，确实是可以一个人完成的。</p><p>不过，其实你也不用这么纠结是不是一个人，毕竟性能项目从来都不是一个人的事情，而是一个团队、一个企业的事情。</p><p>由于每一类、每一个计数器都需要创建一个分析逻辑，而且往往还需要多个计数器关联分析，所以很难在文章里让你看到我们这一步做得有多壮观。如果我把所有的分析路径都创建出来放在文章里，那图片会糊得像马赛克一样。</p><p>不过，我还是建议你自己去创建针对项目的完整的性能分析决策树和分析模型。因为只有亲手做一遍，才能慢慢理解它的博大精深，也能知道自己会卡在什么地方。</p><h2>异常（失效）模型</h2><p>最后，我们再看下异常模型。异常模型又叫失效模型，其实在性能领域中，我真是不想提它，但又不得不提。</p><p>在很多企业中，异常、故障、失效这样的场景，都被列为非功能测试范畴，而非功能的案例在性能领域中其实只会蜻蜓点水似的做上几个，不完整不说，效果也很一般。如果想做得完整呢，那就不是一个性能项目的成本能cover得住的了。</p><p>为什么成本这么高？我们先来看一下异常场景有哪些类型。</p><p><img src="https://static001.geekbang.org/resource/image/b1/6c/b1d44b128ac8b6f8820d05277a506f6c.jpg?wh=1920x1249" alt=""></p><p>在这个图中，我根据技术栈从下到上的层级罗列了一下，大概有20多个测试点。请注意，这里的测试点只是针对一个节点来说的，如果你有成百上千的节点，那就是乘法关系。想一想吧，要做完整的异常场景，靠手工是很难做得下去的。</p><p>这个时候，你是不是自然而然地想到了“混沌工程”这个概念？</p><p>确实，你可以借鉴混沌工程的理念和工具来完成其中一部分内容。但是当前市场上几个用得比较多的开源混沌工具，基本上是停留在微服务分布式应用、容器编排、操作系统这几个角度，而且部分模拟的手段和真实生产环境中出现的故障并不一致。</p><p>举例来说，在阿里开源的 <a href="https://github.com/chaosblade-io/chaosblade-operator">ChaosBlade-Operator</a> 中，模拟CPU使用率高的方式是启动了一个进程来消耗掉CPU，其实跟你写一个do while或者sleep是同样的功能。</p><pre><code class="language-bash">top - 11:13:32 up  1:29,  1 user,  load average: 1.28, 0.58, 0.36\nTasks: 130 total,   2 running, 128 sleeping,   0 stopped,   0 zombie\n%Cpu0  : 78.4 us,  2.3 sy,  0.0 ni, 18.9 id,  0.0 wa,  0.0 hi,  0.3 si,  0.0 st\n%Cpu1  : 77.9 us,  3.3 sy,  0.0 ni, 18.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem :  8008972 total,  2815456 free,  1042748 used,  4150768 buff/cache\nKiB Swap:        0 total,        0 free,        0 used.  6657892 avail Mem\n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n26633 root      20   0  108552  12488      4 R 147.8  0.2   0:40.34 chaos_burncpu\n 2450 root      20   0  830076  93308  29864 S   1.7  1.2   1:48.33 dockerd\n 1459 root      10 -10  193480  25652  14620 S   1.0  0.3   0:57.01 AliYunDun\n 3500 root      20   0 1594788  81096  35772 S   1.0  1.0   1:06.78 kubelet\n 1949 root      20   0 1172680  52168  14772 S   0.7  0.7   0:06.45 containerd\n 4881 root      20   0 1041280  40388  17168 S   0.7  0.5   0:31.43 calico-node\n    9 root      20   0       0      0      0 S   0.3  0.0   0:02.45 rcu_sched\n 3770 root      20   0  107688   6492   2784 S   0.3  0.1   0:00.09 containerd-shim\n 4811 root      20   0  109096  10564   3100 S   0.3  0.1   0:02.88 containerd-shim\n</code></pre><p>你看，上面的chaos_burncpu进程就是来干这个坏事的。</p><p>有过线上运维经验的人可以回忆一下，在线上遇到过的CPU使用率高的性能问题，是不是很少是因为新产生了一个进程而抢掉了CPU导致的？通常情况下，CPU消耗高，都是一个正在使用的应用进程内部产生的性能缺陷导致的，这才是常见的性能缺陷。所以这种模拟手段，并不能覆盖我们在线上看到的问题，只能起个心理安慰的作用。</p><p>那应该怎样设计异常模型呢？我的建议是，把你的生产环境中出现的问题罗列出来，分析原因，把相应的缺陷实现到异常模型中去。比如说，由于某个方法写得不好而导致了CPU的长期消耗，那你可以写一段方法性能差的代码打包到应用中去，通过开关触发；或者直接attach到正在运行的Java进程中，将消耗CPU的代码直接注入到进程中去并运行起来。这样的异常模型才是符合真实缺陷的设计。</p><p>如果针对异常场景的每一个分类，我们都做出符合真实场景中的缺陷的设计，就可以得到完整的异常模型啦了。</p><h2>总结</h2><p>你也看到了，这节课，我并没有给你一套拿去就能用的压测模型。当然，这也是做不到的，因为模型只有在一个具体的项目中去细化和创建才有价值。</p><p>相比拿来即用的压测模型，我更想传递给你的是思考的逻辑。我希望在这样的逻辑指导下，你可以去创建属于自己的模型，如果有困难，可以来找我沟通。</p><p>好了，我们最后总结一下知识点。这节课，我们重点分析了业务模型、容量模型、监控模型、分析模型和异常模型：</p><ul>\n<li>业务模型的重点是，根据全链路压测的目标来确定如何实现业务的比例关系。你是要复现历史的容量峰值场景，还是推算未来的容量峰值场景？这个问题很关键，而具体使用什么样的压测工具反而不是重点。</li>\n<li>容量模型的重点是，计算的逻辑要合理。你可以单点估算，也可以用混合容量估算。但前提是，一定是符合具体项目的估算逻辑的。我看到有些企业用“TPC-C”或“日交易量x峰值系数/交易时长”来估算TPS。在我看来，还是太单薄了。</li>\n<li>监控模型的重点是“全”。现在很多企业用云服务器，厂商会提供一些监控视图。在我的经验中，厂商提供的监控视图就没有完整的，通常都是分析时发现缺这少那的。所以别觉得有了监控视图，你就有了完整的监控模型，那是不现实的。还是创建自己的性能分析决策树，再来比对一下缺少什么更为合理。</li>\n<li>分析模型的重点是分析逻辑，而分析逻辑靠的是技术功底。所以要想创建分析模式，当能力不足时，可以借助团队的力量。</li>\n<li>异常模型的重点是“真实”，不真实的异常，做得再多也没有意义。</li>\n</ul><p>在性能领域中，我把以上的内容称为性能五模型，在全链路压测模型的创建中，也是这五个模型，不会有偏差。</p><h2>课后题</h2><p>在结束今天的学习之前，我还想请你思考两个问题：</p><ol>\n<li>尝试在自己的项目中创建监控模型和分析模型，告诉我你创建时的难点在哪里？</li>\n<li>异常模型可不可以独立到压测项目之外进行？</li>\n</ol><p>欢迎你在留言区与我交流讨论，我们下节课见！</p>',
        article_title: "09 | 压测模型：如何建立一套完整的全链路压测模型？",
      },
      {
        title: "10 | 场景执行：压测执行过程中的关键步骤是什么？",
        id: 439862,
        content:
          '<p>你好，我是高楼。</p><p>上节课，我们重点讨论了压测模型。今天我们一起来看一下，在全链路压测执行过程中有哪些关键步骤。</p><p>这两节课的内容是密切相关的。因为通常情况下，有了全链路压测模型之后，我们就需要把这些性能需求指标一一对应到容量场景业务中去了。</p><p>关于场景设计和执行，首先有两个重点需要你格外关注：</p><ul>\n<li>场景分为四类：<strong>基准、容量、稳定性、异常</strong>；</li>\n<li>执行过程中要保持<strong>连续、递增</strong>。</li>\n</ul><p>接下来，我会从全链路压测角度把场景执行做更为详细的补充。</p><p>有的同学可能会想，全链路压测的场景执行是有多重要，至于拿一节课来讲吗？其实，你可以把场景执行理解为是全链路压测方案的落地，它是全链路压测活动实施的核心，更是性能分析的起点。我大概能写出 10 个这样的句子来说明场景的<strong>地位</strong>。</p><p>下面这张图列出了全链路压测场景执行过程中，一些关键步骤所包含的内容。</p><p><img src="https://static001.geekbang.org/resource/image/0b/c7/0b6dbe188d016dc69c2e5a02daf4d4c7.jpg?wh=1920x935" alt=""></p><p>在全链路压测中，只要场景跑起来，图中这些内容就都是我们要观察的部分。</p><p>下面我就拆开来讲一讲，希望你能明白它们的重要性。</p><h2>压测预热</h2><p>我们首先要关注的就是压测预热。</p><p>高并发系统有几种常用的手段：缓存、队列、限流、降级、熔断等。</p><p>但其实这里还有一个经常被人忽视的动作：<strong>预热</strong>。</p><p>你有没有看到过这样的情况，压测开始时响应时间长，但随着压力的持续，响应时间会缩短。</p><!-- [[[read_end]]] --><p><img src="https://static001.geekbang.org/resource/image/1e/cc/1ec29b88efaa622240dec85dcc00f7cc.png?wh=1920x591" alt="图片"></p><p>出现这样的问题显然都是因为没做好压测预热。系统如果没有预热，在压力开始时，由于缓存、工作线程池、连接池等资源没有分配，响应时间就会比较长。例如，缓存中没有数据，需要从磁盘中直接读取数据时，就会有磁盘 IO 寻址的过程，这显然会增加响应时间。</p><p>所以，我们一定要知道，开始压测的前置动作是预热而不是冷启动。当我们开始压测时系统容量处于低水位的情况下，突然遇到大量压测流量，会直接把系统容量拉升到高水位，这时由于后端反应较慢，会导致前端超时，从而出错。请注意，<strong>这种状态下，并不是系统宕掉了，而是反应慢导致的超时</strong>。</p><p>这里要啰嗦几句，很多人描述系统时就老说系统“打挂”、“死掉”了之类的话，这种感性的描述完全没办法给出明确的技术上的原因，会让沟通的效率非常低。</p><p>关于压测要不要预热，你其实还可以从另一个角度去判断，你只要确保它和生产环境中的逻辑一致就可以了。生产环境在提供用户服务之前如果要预热，那压测也要预热，预热时最好可以覆盖所有的数据；生产环境如果不预热，那么压测也不用预热。</p><p>既然压测预热这么重要，我们应该怎么入手呢？</p><p>最简单直接的思路就是把容量场景中涉及到的业务都提前预压测一遍，通过容量场景的逻辑，实现预热的过程，让系统对资源进行提前准备。需要做到遍历容量场景中所有的业务接口，让一些资源加载进来。</p><h2>执行控制</h2><p>解决了预热的问题，接下来，我们就要来看下执行的部分了。</p><p>根据 RESAR 性能工程理论，如果你想让性能场景循序渐进、没有遗漏地覆盖所有生产场景，就要尽可能地按照<strong>“基准-容量-稳定性-异常”</strong>的逻辑执行。请注意，除了这四类性能场景之外，就再也没有其他类型的场景了。只不过每一个类别的场景，我们都可以设计多个具体的场景来对应不同的压测目标。</p><p>在这四个类别的场景中，基准场景是容量场景的前奏，容量场景是线上业务的真正模拟。</p><p>在基准场景执行过程中，第一阶段的目标就是把资源用光，第二阶段的目标是将系统优化到满足业务容量。</p><p>但你要知道，无论哪个系统，要调优都是没有止境的，而我们的目标是要保证线上系统的正常运行。因为我们这个课程的示例项目只有一个系统，所以，我们先做接口级的，然后把接口拼装成完整的业务量，并实现复制流量模型，然后在容量场景中执行。在基准场景执行阶段，我们将执行测试范围中业务接口级的基准场景。</p><p>在容量场景中，我们还是要继续秉承<strong>“连续、递增”</strong>的执行逻辑。最重要的是，要实现我们前面提到的流量模型，来回放真实线上的业务场景。</p><p>接着我们来看稳定性场景，这里只有两个关注重点：</p><ol>\n<li>\n<p>稳定性场景的时长；</p>\n</li>\n<li>\n<p>用多大的 TPS 来执行。</p>\n</li>\n</ol><p>在执行稳定性场景时，完全可以用最大的稳定 TPS 来运行，只要覆盖了容量规划周期之内的业务容量即可。</p><p>对于异常场景呢，有些企业是把它放到非功能场景分类中的。我之所以把异常场景放在性能部分，是因为我认为，这些异常场景需要在有压力的情况下执行。当然，你也可以用一些所谓的“混沌工程”的工具来实现对容器的随机删除、网络丢包、模拟 CPU 高等操作。不过，这个话题就太大了。在后面的课程里，我会设计几个常用的异常性能场景来带你看一下效果。</p><p>另外，在场景执行的过程中，还有几个点需要你关注一下：</p><ul>\n<li>要查看实时的性能数据曲线，并判断是停下来还是继续，要分析哪些内容。这可以让我们清楚下一步要干什么事情。</li>\n<li>在一开始的压测执行中，不要过度上监控工具。最好是先上全局监控工具，等有了问题之后，我们再重复执行场景，上定向监控工具。</li>\n</ul><p>说明一下，全局监控主要关注的是压测应用系统的性能分析决策树的第一层计数器。比如：基础性能监控（CPU 性能、磁盘性能、网络性能）等。定向监控主要关注的是有问题的计数器。它是在分析根本原因的过程中所使用的监控手段和思路。比如，当CPU使用率较高时，我们可以通过定向监控来寻找根本的原因。</p><ul>\n<li>需要关注业务指标。这些关键的业务指标都有具体的业务含义，比如单位时间内的下单成功率等。在整个全链路压测过程中，性能测试人员在压测启动后就需要紧盯监控看板，任何性能指标异常，都需要第一时间进行性能分析。</li>\n</ul><h2>场景调整</h2><p>场景执行起来之后，我们还需要进行场景调整。在这一步，我们需要明确很多内容，比如流量放大的倍数到底应该从多少开始？最大是多少？要解决这些问题，最好的方式就是<strong>保持连续递增加压</strong>。</p><p>这里我以 JMeter 为例。</p><p><img src="https://static001.geekbang.org/resource/image/09/2f/0956658112f32d02fa187ab6f522d52f.png?wh=1920x697" alt="图片"></p><p>图中画红框的地方其实就是压力递增呈现出来的性能问题。在递增过程中，被测系统的资源要动态分配。系统会不会在这个时候抖动，我们完全可以从这样的图中看出来，而这样的场景也才算是真实的线上场景。</p><p>如果不连续递增，就不会有图中红框这样的部分，也就不能复制出线上的真实场景了。</p><p>此外，我们还需要考虑：场景需不需要递减策略？场景执行持续时间是多长？等等。</p><h2>预案演练</h2><p>在正常全链路压测之前，我们一定要线下环境做好压测预演，预演主要确认下面几个方面：</p><ul>\n<li>压测流量是否写入到了正确的目的地。例如，压测流量写入到影子库，压测日志写到影子目录，生成压测 Cache 等等。</li>\n<li>做好压测异常的降级预案，确认降级策略是否完整且有效。</li>\n<li>进一步确保所有的监控都已到位，确保压测的时候能够准确、及时地发现问题。\n<ul>\n<li>核心链路的流量和耗时监控是否准确；</li>\n<li>中间件、缓存、数据库的监控报警是否及时；</li>\n<li>基础资源的指标报警是否有效；</li>\n<li>业务异常监测指标是否正常且及时。</li>\n</ul>\n</li>\n<li>有了以上的几步之后，还要模拟常见的性能瓶颈，确认可以快速恢复环境。常见的性能瓶颈有：CPU 使用率高、内存溢出、IO 等待、网络出现队列等。</li>\n</ul><h2>问题记录</h2><p>在压测中遇到性能问题的时候，随时记录问题数据和相关现象是一个很必要的步骤。</p><p>在全链路压测过程中，通过标记透传、流量隔离等一系列功能，我们可以快速发现并定位一批性能问题。我一直提倡在性能工程中做分析调优，所以这里我们也要对应地产出性能调优报告。</p><p>在发现并解决问题后，先随手就记录到问题清单里面，后续有时间再补充完善下调优报告。</p><p>关于问题清单，你可以参考下面这个示例：</p><p><img src="https://static001.geekbang.org/resource/image/aa/be/aa72992a9cfb0216898d58dbf4f2bbbe.jpg?wh=1920x1080" alt=""></p><p>我们通常将问题等级设置为 P0~P3 这么 4 个级别，P0 级别最高，P3 级别最低。对于我们这个电商项目来说，主要以下单失败、支付失败、资损等这些跟钱相关的指标为主要衡量标准。如果是其他项目，就需要根据业务类型，制定符合实际业务特点的等级标准了。</p><p>整理问题清单还有一个好处，那就是，如果出现的一些性能问题需要其他人员配合解决，我们可以第一时间把这些问题现象记录下来，然后配合迭代计划、绩效和工单系统，将问题指派到相关负责人，提高项目的性能优化效率。</p><p>在每次压测后，我们还要进行复盘，总结压测中的遇到的问题。将压测结果、监控指标等结果数据形成报告发给相关人员，制定好后续的工作以及跟进的负责人。</p><p>如果这些流程都能够落到实处，后续我们也就更容易开展常态化的全链路压测活动，推动建立公司项目的性能运营体系，将一些模糊的、松散的性能优化工作转变为日常工作了。</p><h2>紧急问题处理</h2><p>在压测过程中，最怕的就是线上出现紧急故障。线上故障带来的影响不仅仅是技术方面的，还有业务方面的，比如出现了大量的用户投诉、资损等情况。如果问题上了微博热搜，情况就更糟糕了。这时候性能团队承受着非常大的压力，如果没做好故障应对机制，整个团队很容易陷入混乱。</p><p>这也暴露出一个难点，那就是，全链路压测除了需要花时间梳理核心链路、做流量隔离等工作外，还要协同整个链路涉及到的多个团队，比如业务团队、架构团队、开发团队、运维团队，不同系统的团队等。</p><p>我们要保证每个业务线统一作战，让大家能够在给定的时间点进行联调和压测值守。在压测推进的时候，按照核心链路所在的模块进行跟进，每个模块出一个值班人员，各个值班人员快速跟进出现的紧急问题。</p><p>在处理紧急故障的时候也有几个重要的原则：</p><ul>\n<li>优先恢复业务，暂不分析解决问题；</li>\n<li>快速地组织协调：\n<ul>\n<li>快速确认故障级别及业务影响面；</li>\n<li>组织应急团队，冷静且有序地处理问题；</li>\n<li>保持信息公开，及时向社会通报。</li>\n</ul>\n</li>\n</ul><p>等到问题解决之后，我们再对故障进行复盘，原则是对事不对人。其实这时候公司大领导的角色很重要，因为出现线上故障后，扯皮的例子简直不要太多。</p><h2>总结</h2><p>好了，到这里，我们整个全链路压测的关键步骤就介绍完了。这节课的要点有下面三个：</p><p>首先，整个性能团队要对全链路压测实施有一个正确和理性的认识，科学地与实际业务结合，制定出对应合理的场景执行方式；</p><p>其次，结合运维体系，我们要在压测前要做好各类预案演练，当线上压测故障真实发生时，能够做到冷静正确地处理问题，以及高效地组织协调应对；</p><p>最后，在压测复盘中，我们需要总结出不足之处，为下一步的改进打下基础。这样，整个全链路压测活动质量才能不断提升，技术方案也将不断完善。</p><p>当然了，只记住这些“道理”是远远不够的，我更希望你吃透我这节课所讲的内容，把它们都灵活地应用起来。</p><h2>思考题</h2><p>在课程的最后，我还是照例给你留两道思考题：</p><ol>\n<li>在你的工作经验中，有哪些性能问题扯皮的案例？是否能够在后续的工作中合理地规避？</li>\n<li>总结本节课中全链路压测场景中的关键内容，对比一下你的项目实践，是否有缺失？</li>\n</ol><p>欢迎你在留言区和我交流讨论，我们下节课见！</p>',
        article_title: "10 | 场景执行：压测执行过程中的关键步骤是什么？",
      },
      {
        title: "11 | 链路追踪：如何选择一款适合自己项目的工具？",
        id: 441338,
        content:
          '<p>你好，我是高楼。</p><p>从这里开始，我们进入链路追踪改造部分，这节课，我们来看一下怎么在全链路压测项目中选择合适的链路追踪方案。</p><h2>为什么需要链路追踪？</h2><p>首先，我们看下这个课程的开源电商项目的部署架构。</p><p><img src="https://static001.geekbang.org/resource/image/2f/4b/2f032901efyy2d9f9e3df8f123e8534b.png?wh=1673x926" alt="图片"></p><p>从部署架构图中你可以很直观地看到，在微服务架构下，系统的功能是由大量的微服务协调组成的。例如：电商下单业务就需要会员系统服务（mall-member）、认证中心服务（mall-auth）、订单系统服务（mall-order）、购物车系统服务（mall-cart）逐级调用才能完成。而实际在电商企业，每个服务可能是由不同的团队进行开发，部署在成百上千台服务器上的。</p><p>这么复杂的调用链路就带来一系列问题：</p><ol>\n<li>如何快速发现性能问题？</li>\n<li>如何精准判断故障影响范围？</li>\n<li>如何梳理服务依赖以及依赖的合理性？</li>\n<li>如何分析链路性能问题以及容量规划？</li>\n</ol><p>当系统发生故障的时候，我们需要一种机制对故障点进行快速定位，确认到底是哪个服务、哪个接口出了问题，那么分布式链路追踪技术由此产生。</p><p><strong>所谓的分布式链路追踪，就是运行时通过某种方式记录下各服务之间的调用过程，再通过可视化UI方式帮助相关人员快速定位到故障点。</strong> 现如今，分布式链路追踪已经成为微服务架构性能监控的底层基础设施，没有它，性能分析人员就像盲人摸象，根本无法彻底了解服务间链路通信的全过程。</p><!-- [[[read_end]]] --><p>而在我的<strong>RESAR 性能分析七步法</strong>中，<strong>分析架构图、拆分响应时间、定向分析</strong>都需要借助链路追踪来梳理服务依赖，分析业务核心链路也要用到它。可想而知，链路追踪对性能项目的作用有多大。</p><p><img src="https://static001.geekbang.org/resource/image/7f/4b/7f9f638aaf350eyy04efab3d0b85944b.jpg?wh=1428x439" alt="图片"></p><h2>链路追踪的设计需求</h2><p>知道了链路追踪的重要性，那么，链路追踪应该做成什么样子呢？</p><p>总的来说，我们希望链路追踪能够从<strong>全局监控</strong>到<strong>定向监控</strong>，从<strong>整体维度</strong>到<strong>局部维度</strong>展示各项指标，将跨服务的所有调用链性能信息集中展现出来。这些信息有利于我们度量整体和局部性能，方便我们找到问题产生的源头，极大缩短性能分析时间。</p><p>首先，我们需要关注在请求处理期间各个调用的各项性能指标，比如：TPS、响应时间及错误数等。</p><ol>\n<li>TPS，收集链路拓扑可计算相应组件、服务的实时数据；</li>\n<li>响应时间，整体调用和各个服务的响应时间等；</li>\n<li>错误数，统计单位时间服务异常次数。</li>\n</ol><p>我们希望从链路追踪中，就可以清晰地看到一个请求对应的每一段的耗时。比如说，一个接口调用另一个接口、Redis、MySQL 等组件时的耗时。当我们发现哪一段耗时比较长的时候，就可以到耗时长的那个组件上，根据定向监控的数据接着往下分析了。</p><p>另外，我们对全链路压测标识透传也是有要求的，那就是：</p><blockquote>\n<p>对于跨服务的调用，我们需要对所有涉及到的服务进行一一改造。添加压测标识的属性，以此保证传输中始终带着压测标识。</p>\n</blockquote><p>也就是说，这个压测标记需要在链路追踪中被<strong>识别、记录、并传递</strong>，这样我们就能做到区分请求(正常流量 &amp; 压测流量)了。</p><p>好了，我们先来总结一下，我们希望通过链路追踪达到的目标：</p><ol>\n<li>请求链路追踪，快速定位问题：可以通过调用链结合业务日志快速定位错误信息；</li>\n<li>可视化UI： 细化服务各个阶段耗时并可视化展示，方便进行性能分析；</li>\n<li>依赖优化：统计各个调用组件的可用性、梳理服务依赖关系，可做到精准优化；</li>\n<li>数据分析，优化链路：可以得到流量的请求路径，汇总分析各业务场景；</li>\n<li>区分流量：可以识别、记录、传递压测标记，做到快速区分请求(正常流量 &amp; 压测流量)。</li>\n</ol><h2>链路追踪的选型</h2><p>上面我们介绍了链路追踪需要达到的几个主要目标，那么我们又该怎样选择适合自己项目的链路追踪组件呢？</p><p><a href="http://bigbully.github.io/Dapper-translation/">Google Dapper </a>中已经提到了需要关注的几个主要的方面：</p><ol>\n<li>探针的性能消耗：对服务的影响要尽可能小，服务埋点本身会带来一定的性能损耗，这对于性能敏感的应用是致命伤；</li>\n<li>代码的侵入性：对应用尽量少侵入，对业务尽可能透明；</li>\n<li>可扩展性：支持集群部署，支持的组件越多越好；</li>\n<li>数据的分析：数据分析要快，分析的维度要多。</li>\n</ol><p>目前业务主流的链路追踪系统，大致功能可以分为以下四大模块：</p><ol>\n<li>埋点与生成日志：可分为客户端、服务端、以及客户端和服务端双向埋点；</li>\n<li>搜集与存储日志：分布式日志采集，一般都支持消息队列作为缓存，支持常见的 ElasticSearch 数据储存；</li>\n<li>分析与统计调用链数据：调用链数据汇总，还原调用链路，并分析最终的调用形态；</li>\n<li>可视化 UI 展示：直观地了解业务处理过程中服务间的依赖关系与处理时间、处理状态等信息。</li>\n</ol><p>目前业界流行的链路追踪组件有 Jaeger、Zipkin、SkyWalking 和 Pinpoint 等。但是，这些组件对业务代码的侵入性和不同系统的兼容性都各有其特点。</p><p>为了让你有个更为直观的认识，我给你画了个表格，我们一起来看看不同组件的优劣。</p><p><img src="https://static001.geekbang.org/resource/image/9b/a5/9b22e625d8e20d06326b2593a85e62a5.jpg?wh=1920x1080" alt="图片"></p><p>从这张对比表可以看出：</p><ol>\n<li><a href="https://zipkin.io/">Zipkin</a> 和 <a href="https://www.jaegertracing.io/">Jaeger</a> 在各个方面的指标都差不多（Jaeger 是 Zipkin Plus（改良版））；</li>\n<li>相对其它三种组件，<a href="http://skywalking.apache.org/">Skywalking</a> 的功能较为齐全，性能损耗也较低，同时客户端也支持多种主流语言；</li>\n<li><a href="https://pinpoint-apm.github.io/pinpoint/">Pinpoint</a> 是一个完整的性能监控解决方案，有从探针、收集器、存储到 Web 可视化界面等全套体系，但它在性能和可扩展性方面有所不足；</li>\n<li>四种组件都支持 <a href="https://opentracing.io/">OpenTracing</a>，这样开发者可以无缝切换追踪组件，使得通用链路追踪的实现成为可能。</li>\n</ol><p>总之，每种组件都有它的优缺点，我建议你在选择链路追踪组件的时候，一定要根据自身项目的实际情况，仔细考量哪些地方可以妥协，哪些地方可以放弃，最终选择出一款最适合自己项目的组件。</p><p>考虑到我们的电商项目是 Spring Cloud 技术栈，在压测标记跨服务透传上又存在刚性需求，而 <a href="https://spring.io/projects/spring-cloud-sleuth">Spring Cloud Sleuth </a>是一套原生的解决方案。</p><blockquote>\n<p>Sleuth 是 Spring Cloud 提供的服务治理模块，在其标准生态下内置了 Sleuth 这个组件。它通过扩展 Logging 日志的方式实现微服务的链路追踪。</p>\n</blockquote><p>利用 Sleuth 服务间原生传递上下文特性，在原有传输上下文的基础上，添加了压测标记的属性，以保证后续跨服务传输中始终带着压测标记。这样也就免去了代码的硬耦合。</p><p>Sleuth + Zipkin 是一套成熟的链路追踪解决方案，基于经典 X-B3 Trace 协议 的 Java 接口实现 Brave，使用 Brave SDK，提供了基本的操作 API，手动埋点生成 Trace，如果需要与框架或者项目集成的话，就需要手动添加配置文件或增加代码，它的 <a href="https://github.com/openzipkin/brave/tree/master/instrumentation">instrumentation</a> 子项目，已经提供了 SpringMVC、MySQL、Dubbo 等等的常用组件链路追踪的功能。但是 SDK 埋点的方式，对业务代码存在侵入性，当升级埋点时，必须要做代码的变更。</p><p>而性能更好的 Skywalking，依赖 Java Agent 探针，通过<strong>字节码注入</strong>的方式修改目标方法的字节码，实现调用拦截和数据收集，可以做到真正的无侵入的埋点，在应用程序启动时使用 -javaagent 参数，就可以将探针包注入目标应用程序，完成埋点的植入。对业务代码无侵入的方式，可以做到无感的热升级。用户不需要理解深层的原理，就可以使用完整的监控服务。</p><p>但是，从改造难度和成本方面来说，<strong>Brave 比字节码注入更容易上手，</strong>简单阅读 Skywalking 和 Brave 插件的代码，你就可以发现两者的实现难度有天壤之别。Brave 的代码量很少，核心功能都集中在 <a href="https://github.com/brave/brave-core">brave-core</a> 这个模块下，一个中等水平的开发人员，可以在一天之内读懂它的内容，并且能对 API 的结构有非常清晰的认识。</p><p>作为一个专栏项目，我们希望能够尽量能覆盖绝大部分人群，所以<strong>最终我们选择了 Sleuth + Zipkin作为链路追踪解决方案</strong>。</p><p>如果你对 Zipkin 还不是很了解的话，可以参考这篇文章《 <a href="https://mp.weixin.qq.com/s/zNGhUNTlLkxvFTc24UKlzw">快速了解分布式链路追踪系统 Zipkin</a> 》，文中对 Zipkin 的入门知识做了详细的介绍。</p><h2>链路追踪的改造</h2><p>说完了链路追踪组件选型，紧接着我们看看如何让它在我们的电商项目中落地？</p><p>首先，我们来看一下电商项目集成 Sleuth+ Zipkin 的应用架构。</p><p><img src="https://static001.geekbang.org/resource/image/5a/56/5a92b6d7266a4b9b335d79be0d05cd56.jpg?wh=1920x1210" alt="图片"></p><p>从架构图中可以看到：我们构建了一个服务网关，通过 API 网关调用具体的微服务，所有的服务都注册到 Nacos 上；当客户端的请求到来之时，网关作为服务端的门户，会根据配置的规则，从 Nacos 中获取对应服务的信息，并将请求反向代理到指定的服务实例。</p><p>系统涉及的需要新增/改造业务服务与组件包含以下 10 个：</p><ol>\n<li>Zipkin，链路追踪系统</li>\n<li>Kafka，消息队列</li>\n<li>ElasticSearch，搜索引擎</li>\n<li>mall-gateway，API 网关</li>\n<li>mall-auth，认证中心服务</li>\n<li>mall-member，会员系统服务</li>\n<li>mall-order，订单系统服务</li>\n<li>mall-cart，购物车系统服务</li>\n<li>mall-protal，商城后台系统服务</li>\n<li>mall-admin，后台管理系统服务</li>\n</ol><p>顺便提下，Zipkin 包括如下的 Transport 方式：</p><ol>\n<li>HTTP，通过 <a href="https://github.com/openzipkin/zipkin-reporter-java/blob/master/okhttp3/">okhttp3</a> 或 <a href="https://github.com/openzipkin/zipkin-reporter-java/tree/master/urlconnection">urlconnection</a> 实现。</li>\n<li>ActiveMQ，通过 <a href="https://github.com/openzipkin/zipkin-reporter-java/tree/master/activemq-client">activemq-client</a> 实现。</li>\n<li>RabbitMQ，通过 <a href="https://github.com/openzipkin/zipkin-reporter-java/blob/master/amqp-client/">amqp-client</a> 实现。</li>\n<li>Kafka，通过 <a href="https://github.com/openzipkin/zipkin-reporter-java/blob/master/kafka">kafka</a> 或 <a href="https://github.com/openzipkin/zipkin-reporter-java/blob/master/kafka08">kafka08</a> 实现。</li>\n<li>Thrift，通过 <a href="https://github.com/openzipkin/zipkin-reporter-java/blob/master/libthrift/">libthrift</a> 实现。</li>\n</ol><p>请求量级小的时候，选择 HTTP 方式即可。量级比较大的时候，<strong>我们推荐使用 Kafka 消息队列。</strong></p><p>而 Zipkin Server Storage 负责存储链路数据，目前支持 Memory、MySQL、Cassandra、ElasticSearch 等数据库，不同数据库的适用环境也不一样：</p><ol>\n<li>Memory：默认存储器。主要用于简单演示，生产环境下不推荐。</li>\n<li>MySQL：小规模使用时，可以考虑用 MySQL。</li>\n<li>ElasticSearch：<strong>主流数据库。通常情况下都会采用 ElasticSearch 存储链路数据。</strong></li>\n<li>Cassandra：在 Twitter 内部被大规模使用，因为 Cassandra 易跨站，支持灵活的 schema。</li>\n</ol><p>我们这里将链路日志推送到 Kafka，然后启动 Zipkin Server 聚合日志，监听 Kafka ，如果有新的消息则进行拉取存入到 ElasticSeach，最后再用 Zipkin UI 展示链路过程。</p><p>最后，我们再来梳理下整个系统链路追踪改造部分，它大概分为五大部分：</p><ol>\n<li>在服务中加入 Spring Cloud Sleuth 生成链路追踪日志；</li>\n<li>使用 <a href="https://github.com/openzipkin/brave">Brave</a> 库，集成 Zipkin 客户端埋点。使用 <a href="https://github.com/openzipkin/brave/tree/master/instrumentation">instrumentation</a> 插件，实现对 SpringMVC、MySQL 等组件实现链路追踪的功能；</li>\n<li>将链路日志推送到 Kafka；</li>\n<li>启动 Zipkin Server 聚合日志，监听 Kafka ，如果有新的消息则进行拉取存入到 ElasticSeach；</li>\n<li>最后使用 Zipkin UI 展示链路过程、使用 Kibana 查询链路数据。</li>\n</ol><p>完成这些内容后，整体展示出来的内容如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/cd/44/cd94913bb15891d8811eec8ca70f2b44.png?wh=1486x602" alt="图片"></p><h2>总结</h2><p>好了，这节课我们讲到这里，我来总结一下：</p><ul>\n<li>现如今链路追踪是微服务架构的基础设施，也是性能分析人员不可缺少的“神兵利器”；</li>\n<li>链路追踪、快速定位问题、可视化UI、数据分析等功能已成为众多链路追踪工具的标配；</li>\n<li>探针性能损耗低、对应用透明、可扩展性强，数据分析快且丰富等功能是用户重点关注的内容；</li>\n<li>另外我们还一起梳理了全链路压测中链路追踪的需求、目标和几种常见的组件，确定了系统最后的选型方案。</li>\n</ul><p>总之，每种组件都有它的优缺点，在做链路追踪组件的选型时，一定要根据自身项目的实际情况，仔细衡量哪些地方可以妥协，哪些地方可以放弃，从而选择一款适合自己项目的组件。</p><p>下一节课，我们将进入实践环节，我会通过案例演示如何应用  Sleuth+Zipkin  来实现链路追踪改造。</p><h2>思考题</h2><p>在课程的最后，我还是照例给你留两道思考题：</p><ol>\n<li>你知道链路追踪和系统监控的区别是什么？</li>\n<li>除了我列出的链路追踪组件，你还知道哪些？</li>\n</ol><p>欢迎你在留言区和我交流讨论，我们下节课见！</p>',
        article_title: "11 | 链路追踪：如何选择一款适合自己项目的工具？",
      },
      {
        title: "12 | 链路追踪：如何对一个具体的项目进行追踪改造？",
        id: 443442,
        content:
          '<p>你好，我是高楼。</p><p>在上一讲，我给你梳理了链路追踪的背景、目标、几种常见的组件，我们还确定了系统最后的选型方案。</p><p>在这一讲，我会通过案例演示在应用  Sleuth+Zipkin  来追踪我们电商微服务项目请求的过程中，需要关注的一些技术细节。</p><p>首先，我们来回顾一下电商项目集成 Sleuth+ Zipkin 的应用架构。​<img src="https://static001.geekbang.org/resource/image/5a/56/5a92b6d7266a4b9b335d79be0d05cd56.jpg?wh=1920x1210" alt="图片"></p><p>系统涉及的需要新增或改造的服务与组件包含以下 10 个：</p><ol>\n<li>Zipkin，链路追踪系统</li>\n<li>Kafka，消息队列</li>\n<li>ElasticSearch，搜索引擎</li>\n<li>mall-gateway，API 网关</li>\n<li>mall-auth，认证中心服务</li>\n<li>mall-member，会员系统服务</li>\n<li>mall-order，订单系统服务</li>\n<li>mall-cart，购物车系统服务</li>\n<li>mall-protal，商城后台系统服务</li>\n<li>mall-admin，后台管理系统服务</li>\n</ol><p>结合技术栈，可以得出下面这张表格。表格整合了涉及追踪的技术组件，可以指导我们后续的改造工作。</p><p><img src="https://static001.geekbang.org/resource/image/22/1b/22d025a14dce6134a14a578c790a681b.jpg?wh=1920x1080" alt="图片"></p><p>接下来，我们通过 Demo 预演来一一做一下组件追踪的技术验证工作。</p><h2>demo 预演</h2><h3>SpringMVC、Fegin、Logback</h3><p>一般而言，我们使用 <a href="https://github.com/openzipkin/brave">Brave</a> 库，作为 Zipkin 客户端。同时它的 <a href="https://github.com/openzipkin/brave/tree/master/instrumentation">instrumentation</a> 子项目，已经提供了 SpringMVC、Fegin 等组件的链路追踪功能。</p><!-- [[[read_end]]] --><p>所以通过引入 Spring Cloud Sleuth + Zipkin 相关依赖，可以实现对它们的自动配置，从而实现链路追踪的功能。</p><pre><code class="language-xml">   &lt;!--添加 Sleuth 依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;\n            &lt;version&gt;2.2.6.RELEASE&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;!--Zipkin 客户端--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;\n            &lt;version&gt;2.2.6.RELEASE&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre><p>从我给出的截图可以看出，已经成功引入 Zipkin、Sleuth 和 Brave 相关的依赖。</p><p><img src="https://static001.geekbang.org/resource/image/3f/cd/3f95c3d14cf9288bab3e72a6a7e96bcd.png?wh=732x794" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/2a/ec/2ac1eb0abd845b44d24acd528eb7b8ec.png?wh=720x794" alt="图片"></p><p>具体的示例，你可以参考这篇文章： <a href="https://mp.weixin.qq.com/s/7Xqk_1xGlLZom9hkfyW9hg">Sleuth+Zipkin 实现 Spring Cloud 链路追踪</a> ​。</p><p>最后，Zipkin 可视化 UI 会出现对应的链路调用图及详细的链路。</p><p><img src="https://static001.geekbang.org/resource/image/64/b9/64436fd08e6f241215f2a1205b32c7b9.png?wh=1912x996" alt="图片"></p><h3>Spring Cloud Gateway</h3><p>因为 <a href="https://github.com/openzipkin/brave">Brave</a> 库默认提供了 Gateway 链路追踪的功能，所以要想实现 Sleuth 对 <a href="https://spring.io/projects/spring-cloud-gateway">Spring Cloud Gateway</a> 的代理请求的链路追踪，我们只需要集成就可以了。</p><p>这里我们结合上面示例中的三个服务，实现一个 API 网关，转发请求到 A 服务，即 Gateway -&gt; A -&gt; B -&gt; C，各服务间通过 Fegin 实现远程调用。</p><p>注意，Spring Cloud Gateway 是基于 <a href="https://docs.spring.io/spring-framework/docs/current/reference/html/web-reactive.html">WebFlux</a> 实现的，而 Spring Cloud Sleuth 的 <a href="https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/web/">instrument/web</a> 模块提供的插件，实际是针对 WebFlux 框架，同样也适用于 Spring Cloud Gateway。</p><p>具体操作如下：</p><p>第一步，创建 pom.xml 文件，引入相关依赖。</p><pre><code class="language-xml">&lt;dependencies&gt;\n        &lt;!-- 引入 Spring Cloud Gateway 相关依赖，使用它作为网关，并实现对其的自动配置 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 引入 Zipkin 依赖--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--添加 Sleuth 依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n</code></pre><p>​第二步，在 application.yml 中，添加服务路由相关配置。</p><pre><code class="language-yaml">server:\n  port: 8888\n\nspring:\n  application:\n    name: demo-gateway-application\n  sleuth:\n    sampler: #采样器\n      probability: 1.0 #采样率，采样率是采集 Trace 的比率，默认 0.1\n      rate: 10000 #每秒数据采集量，最多 n 条/秒 Trace\n    web:  # Web 组件的配置项，例如说 SpringMVC\n      enabled: true\n  zipkin: #设置 zipkin 服务端地址\n    base-url: http://127.0.0.1:9411\n\n  cloud:\n    # Spring Cloud Gateway 配置项，对应 GatewayProperties 类\n    gateway:\n      # 路由配置项，对应 RouteDefinition 数组\n      routes:\n        - id: feign-service-route\n          uri: http://127.0.0.1:7000\n          predicates:\n            - Path=/**\n</code></pre><p>因为使用的是 <a href="https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/web/">instrument/web</a> 模块提供的插件，所以和 SpringMVC 一样，WebFlux 也是使用 spring.sleuth.web 配置项。</p><p>配置项中，我们创建了一个编号为 feign-service-route 的路由，转发到 a-service 服务。</p><p>第三步，网关启动类。</p><pre><code class="language-java">package com.dunshan.gatewaydemo;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n/**\n * @author dunshan\n */\n@SpringBootApplication\npublic class GatewayApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(GatewayApplication.class, args);\n    }\n}\n</code></pre><p>最后启动所有应用，我们使用 Postman 测试一下接口，执行一次请求操作，尝试追踪该链路。</p><p><img src="https://static001.geekbang.org/resource/image/f2/6d/f261dec214449e92fcd8a98277ba906d.png?wh=1920x1125" alt="图片"></p><p>在 Zipkin 可视化 UI 就可以看到刚才我们调用接口的链路数据了。</p><p><img src="https://static001.geekbang.org/resource/image/0a/81/0a8f9035f47e868f75f0804cd1c41b81.png?wh=1912x583" alt="图片"></p><p>这条链路经过 gateway、a-service、b-service、c-service 四个服务。</p><p>我们点开该链路，可以看到一个 Trace 明细。</p><p><img src="https://static001.geekbang.org/resource/image/57/8e/57fde9b813b3276a96ca23cf2edeyy8e.png?wh=1917x969" alt="图片"></p><h3>MySQL</h3><p>这里需要实现一个简单的 MySQL 查询，接下来我会使用 MySQL8 驱动进行数据库的操作，这也是我们项目目前采用的方式。</p><p>目前 Brave 支持通过三种插件实现 MySQL 链路数据收集，它们分别是：</p><ul>\n<li><a href="https://github.com/openzipkin/brave/tree/master/instrumentation/mysql">brave-instrumentation-mysql</a></li>\n<li><a href="https://github.com/openzipkin/brave/blob/master/instrumentation/mysql6/">brave-instrumentation-mysql6</a></li>\n<li><a href="https://github.com/openzipkin/brave/blob/master/instrumentation/mysql8/">brave-instrumentation-mysql8</a></li>\n</ul><p>我简单演示一下用 Spring JDBC Template 的方式进行 MySQL 操作的步骤。</p><p>第一步，创建 pom.xml 文件，引入相关依赖。</p><pre><code class="language-xml">  &lt;dependencies&gt;\n        &lt;!-- 引入 SpringMVC 相关依赖，并实现对其的自动配置 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 实现对数据库连接池的自动化配置 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--Mysql 数据库驱动--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;mysql&lt;/groupId&gt;\n            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;\n            &lt;version&gt;8.0.15&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 引入 Zipkin 依赖--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--添加 Sleuth 依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- Brave 对 MySQL8 的支持 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;io.zipkin.brave&lt;/groupId&gt;\n            &lt;artifactId&gt;brave-instrumentation-mysql8&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n</code></pre><p>这里引入 brave-instrumentation-mysql8 依赖，实现对 MySQL 的链路追踪。</p><p>第二步，在 application.yml 中，添加数据库相关配置。</p><pre><code class="language-yaml">spring:\n  application:\n    name: demo-service # 服务名\n  sleuth:\n    sampler: #采样器\n      probability: 1.0 #采样率，采样率是采集 Trace 的比率，默认 0.1\n      rate: 10000 #每秒数据采集量，最多 n 条/秒 Trace\n    web:  # Web 组件的配置项，例如说 SpringMVC\n      enabled: true\n  zipkin: #设置 zipkin 服务端地址\n    base-url: http://127.0.0.1:9411\n\n  # datasource 数据源配置内容\n  datasource:\n    url: jdbc:mysql://127.0.0.1:3306/test?useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;queryInterceptors=brave.mysql8.TracingQueryInterceptor&amp;exceptionInterceptors=brave.mysql8.TracingExceptionInterceptor&amp;zipkinServiceName=mysql-demo\n    driver-class-name: com.mysql.jdbc.Driver\n    username: root\n    password: root\n\nlogging:\n  level:\n    root: debug #为演示需要，开启 debug 级别日志\n</code></pre><p>在这里，Brave 通过实现类 <a href="https://github.com/openzipkin/brave/blob/master/instrumentation/mysql8/src/main/java/brave/mysql8/TracingQueryInterceptor.java">TracingQueryInterceptor</a> 拦截 SQL 请求，进行 MySQL 的链路追踪。切记，在 spring.datasource.url 配置项上的 queryInterceptors、exceptionInterceptors 和 zipkinServiceName 属性上，一定要分别设置拦截器和该 MySQL 在 Zipkin 中展示的服务名。</p><p>第三步，在 MySQL 数据库中，创建用户表并插入数据。</p><pre><code class="language-sql">CREATE TABLE `t_user` (\n  `id` int(8) NOT NULL AUTO_INCREMENT COMMENT \'主键自增\',\n  `username` varchar(50) NOT NULL COMMENT \'用户名\',\n  `password` varchar(50) NOT NULL COMMENT \'密码\',\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT=\'用户表\';\n\nINSERT INTO `t_user`(`id`, `username`, `password`) VALUES (1, \'7d\', \'123456\');\n\n</code></pre><p>​第四步，创建 DemoController 类，提供示例 API 接口。</p><pre><code class="language-java">package com.dunshan.mysql8demo.controller;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.jdbc.core.BeanPropertyRowMapper;\nimport org.springframework.jdbc.core.JdbcTemplate;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestParam;\nimport org.springframework.web.bind.annotation.RestController;\n\n@RestController\n@RequestMapping("/7d")\npublic class DemoController {\n    @Autowired\n    private JdbcTemplate template;\n    @GetMapping("/get")\n    public String get(@RequestParam("id") Integer id) {\n        this.selectById(1);\n        return "success";\n    }\n    public Object selectById(Integer id) {\n        return template.queryForObject("SELECT id, username, password FROM t_user WHERE id = ?",\n                new BeanPropertyRowMapper&lt;&gt;(Object.class), // 结果转换成对应的对象。\n                id);\n    }\n}\n</code></pre><p>第五步，创建 DemoServiceApplication 启动类。</p><pre><code class="language-java">package com.dunshan.mysql8demo;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n@SpringBootApplication\npublic class DemoServiceApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(DemoServiceApplication.class, args);\n    }\n}\n</code></pre><p>这样在 /7d/get 接口会执行一次 MySQL 的查询操作。<br>\n最后启动应用，我们使用 Postman 测试一下接口，尝试追踪该链路。</p><p><img src="https://static001.geekbang.org/resource/image/2f/52/2fdb79acbbe86076a9eeb542f8905e52.png?wh=1920x1398" alt="图片"></p><p>同样在 Zipkin 可视化 UI 也可以看到刚才调用接口的链路数据。</p><p><img src="https://static001.geekbang.org/resource/image/0f/c8/0f8ce28181143847aef94b64397bb0c8.png?wh=1916x996" alt="图片"></p><p>点开这个链路，可以看到一个 Trace 明细。</p><p><img src="https://static001.geekbang.org/resource/image/33/a7/33afd454a0f38fe5yy66f0fdb52b37a7.png?wh=1915x996" alt="图片"></p><p>到这里，我们的 MySQL 组件链路追踪就成功了。</p><h3>Redis</h3><p>这一部分需要实现简单的 Redis 操作，接下来我们使用 Spring Data Redis + Jedis 进行 Redis 的操作（项目部分服务使用的方式）。目前 Brave 暂未支持 Jedis 客户端的方式，所以我们只能考虑其它的办法。</p><p>在 <a href="https://github.com/opentracing">opentracing-contrib</a> 项目中，有一个 <a href="https://github.com/opentracing-contrib/java-redis-client">java-redis-client</a> 子项目，提供了 OpenTracing 针对 Jedis、Lettuce 等客户端的链路追踪功能。这样，我们搭配上 <a href="https://github.com/openzipkin-contrib/brave-opentracing">brave-opentracing</a> 项目，可以使用 OpenTracing API 收集的链路数据，发送给 Zipkin。</p><p>而 Lettuce 基于 Brave 实现了 <a href="https://github.com/lettuce-io/lettuce-core/blob/main/src/main/java/io/lettuce/core/tracing/BraveTracing.java">BraveTracing</a> ，从而可以实现对 Redis 操作的链路追踪。并且，Spring Cloud Sleuth 的 <a href="https://github.com/spring-cloud/spring-cloud-sleuth/tree/main/spring-cloud-sleuth-autoconfigure/src/main/java/org/springframework/cloud/sleuth/autoconfig/instrument/redis">instrument/redis</a> 模块对它实现了自动配置。如果项目中是使用 Lettuce 作为 Redis 客户端的话，可以考虑采用这种方式。</p><p>我们还是来看下具体的操作步骤。</p><p>第一步，创建 pom.xml 文件，引入相关依赖。</p><pre><code class="language-xml">   &lt;dependencies&gt;\n        &lt;!-- 引入 SpringMVC 相关依赖，并实现对其的自动配置 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 实现对 Spring Data Redis 的自动化配置 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;\n            &lt;exclusions&gt;\n                &lt;!-- 去掉对 Lettuce 的依赖，因为 Spring Boot 优先使用 Lettuce 作为 Redis 客户端 --&gt;\n                &lt;exclusion&gt;\n                    &lt;groupId&gt;io.lettuce&lt;/groupId&gt;\n                    &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt;\n                &lt;/exclusion&gt;\n            &lt;/exclusions&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 引入 Jedis 的依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;redis.clients&lt;/groupId&gt;\n            &lt;artifactId&gt;jedis&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 引入 Zipkin 相关依赖--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--引入 Sleuth 相关依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--  Brave 对 Opentracing 的实现 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;io.opentracing.brave&lt;/groupId&gt;\n            &lt;artifactId&gt;brave-opentracing&lt;/artifactId&gt;\n            &lt;version&gt;0.35.0&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;!-- Opentracing 对 Redis 的支持 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;\n            &lt;artifactId&gt;opentracing-redis-jedis3&lt;/artifactId&gt;\n            &lt;version&gt;0.1.16&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;\n            &lt;artifactId&gt;opentracing-redis-spring-data&lt;/artifactId&gt;\n            &lt;version&gt;0.1.16&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n</code></pre><p>这里引入 <a href="https://mvnrepository.com/artifact/io.opentracing.brave/brave-opentracing">brave-opentracing</a> 依赖，也就是 Brave 对 Opentracing 的实现。</p><p>注意，Opentracing 和 JDBC 一样是一个通用标准，因此需要有 Brave 对 Opentracing 做具体实现，从而将链路数据写入到 Zipkin 中。就好比 JDBC 对 MySQL Driver 实现，将数据写入到 MySQL 数据库中一样。</p><p>同时，我们要引入 <a href="https://mvnrepository.com/artifact/io.opentracing.contrib/opentracing-redis-spring-data">opentracing-redis-spring-data</a> 和 <a href="https://mvnrepository.com/artifact/io.opentracing.contrib/opentracing-redis-jedis3">opentracing-redis-jedis3</a> 依赖，实现对 Jedis 操作 Redis 的链路追踪。</p><p>第二步，在 application.yml 中，添加redis相关配置。</p><pre><code class="language-yaml">spring:\n  application:\n    name: demo-service # 服务名\n  sleuth:\n    sampler: #采样器\n      probability: 1.0 #采样率，采样率是采集 Trace 的比率，默认 0.1\n      rate: 10000 #每秒数据采集量，最多 n 条/秒 Trace\n  zipkin: #设置 zipkin 服务端地址\n    base-url: http://127.0.0.1:9411\n    \n  # 对应 RedisProperties 类\n  redis:\n    host: 127.0.0.1\n    port: 6379\n    password: # Redis redis密码，默认为空。\n    database: 0 # Redis redis中的数据库号，默认为 0。\n    timeout: 6000 # Redis 连接超时时间，单位：毫秒。\n    # 对应 RedisProperties.Jedis 内部类\n    jedis:\n      pool:\n        max-active: 8 # 连接池最大连接数，默认为 8。使用负数表示没有限制。\n        max-idle: 8 # 默认连接数最小空闲的连接数，默认为 8。使用负数表示没有限制。\n        min-idle: 0 # 默认连接池最小空闲的连接数，默认为 0。允许设置 0 和 正数。\n        max-wait: -1 # 连接池最大阻塞等待时间，单位：毫秒。默认为 -1，表示不限制。\nlogging:\n  level:\n    root: debug #为演示需要，开启 debug 级别日志\n</code></pre><p>第三步，创建 SleuthConfiguration 配置类，创建一个 <a href="https://github.com/opentracing-contrib/java-redis-client/blob/master/opentracing-redis-spring-data2/src/main/java/io/opentracing/contrib/redis/spring/data2/connection/TracingRedisConnectionFactory.java">TracingRedisConnectionFactory</a> Bean 对象。这样，我们就能拦截到 Redis 操作，进行相应的链路跟踪了。</p><pre><code class="language-java">package com.dunsan.redisdemo.config;\n\nimport io.opentracing.Tracer;\nimport io.opentracing.contrib.redis.common.TracingConfiguration;\nimport io.opentracing.contrib.redis.spring.data.connection.TracingRedisConnectionFactory;\nimport org.springframework.boot.autoconfigure.data.redis.RedisProperties;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.data.redis.connection.RedisConnectionFactory;\nimport org.springframework.data.redis.connection.jedis.JedisConnectionFactory;\n\n@Configuration\npublic class SleuthConfiguration {\n\n    @Bean\n    public RedisConnectionFactory redisConnectionFactory(Tracer tracer, RedisProperties redisProperties) {\n        // 创建 JedisConnectionFactory 对象\n        RedisConnectionFactory connectionFactory = new JedisConnectionFactory();\n        // 创建 TracingConfiguration 对象\n        TracingConfiguration tracingConfiguration = new TracingConfiguration.Builder(tracer)\n                // 设置拓展 Tag，设置 Redis 服务器地址。因为默认情况下，不会在操作 Redis 链路的 Span 上记录 Redis 服务器的地址，所以这里需要设置。\n                .extensionTag("Server Address", redisProperties.getHost() + ":" + redisProperties.getPort())\n                .build();\n        // 创建 TracingRedisConnectionFactory 对象\n        return new TracingRedisConnectionFactory(connectionFactory, tracingConfiguration);\n    }\n    \n}\n</code></pre><p>第四步，创建 DemoController 类，提供示例 API 接口。</p><pre><code class="language-java">package com.dunsan.redisdemo.controller;\n\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.data.redis.core.StringRedisTemplate;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestParam;\nimport org.springframework.web.bind.annotation.RestController;\n\n@RestController\n@RequestMapping("/7d")\npublic class DemoController {\n\n    @Autowired\n    private StringRedisTemplate redisTemplate;\n\n    @GetMapping("/get")\n    public String get(@RequestParam("id") Integer id) {\n        this.get("demo");\n        return "success";\n    }\n\n    public void get(String key) {\n        redisTemplate.opsForValue().get(key);\n    }\n\n}\n</code></pre><p>这样在 /7d/get 接口中，就会执行一次 Redis 的查询。</p><p>第五步，创建 DemoServiceApplication 启动类。</p><pre><code class="language-java">package com.dunsan.redisdemo;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n@SpringBootApplication\npublic class DemoServiceApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(DemoServiceApplication.class, args);\n    }\n\n}\n</code></pre><p>最后启动应用。我们用 Postman 测试一下口，执行一次 Redis 查询操作，尝试跟踪该链路。</p><p>在 Zipkin 可视化 UI 中就可以看到刚才我们调用接口的链路数据了。</p><p><img src="https://static001.geekbang.org/resource/image/fc/0e/fc5a5d1423aa5651daebfcd6bfb8590e.png?wh=1915x434" alt="图片"></p><p>点开该链路，可以看到一个 Trace 明细。</p><p><img src="https://static001.geekbang.org/resource/image/9f/ff/9f1fe4da7f1e7bf7b155e3d787f74cff.png?wh=1918x756" alt="图片"></p><p>到这里，我们的 Redis 组件链路跟踪也已经成功了。</p><h3>MongoDB</h3><p>这里还是实现简单的 MongoDB 操作，我会使用 Spring Data MongoDB + MongoTemplate 进行 MongoDB 的操作。目前，Brave 默认提供了对 MongoDB 操作的链路跟踪，通过 <a href="https://github.com/openzipkin/brave/tree/master/instrumentation/mongodb">brave-instrumentation-mongo</a> 库实现收集链路数据。</p><p>我们来看下具体的实现路径：</p><p>第一步，还是引入相关依赖。</p><pre><code class="language-xml">   &lt;dependencies&gt;\n        &lt;!-- 引入 SpringMVC 相关依赖，并实现对其的自动配置 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 自动化配置 Spring Data Mongodb --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 引入 Zipkin 相关依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--添加 Sleuth 依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--添加 brave mongodb 依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;io.zipkin.brave&lt;/groupId&gt;\n            &lt;artifactId&gt;brave-instrumentation-mongodb&lt;/artifactId&gt;\n            &lt;version&gt;5.13.3&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre><p>第二步，创建全局配置文件，添加MongoDB配置。</p><pre><code class="language-yaml">spring:\n  application:\n    name: demo-service # 服务名\n\n  sleuth:\n    sampler: #采样器\n      probability: 1.0 #采样率，采样率是采集 Trace 的比率，默认 0.1\n      rate: 10000 #每秒数据采集量，最多 n 条/秒 Trace\n    web:  # Web 组件的配置项，例如说 SpringMVC\n      enabled: true\n\n  zipkin: #设置 zipkin 服务端地址\n    base-url: http://127.0.0.1:9411\n\n  data:\n    # MongoDB 配置项，对应 MongoProperties 类\n    mongodb:\n      host: 127.0.0.1\n      port: 27017\n      database: demo\n#      username:\n#      password:\nlogging:\n  level:\n    root: debug #为演示需要，开启 debug 级别日志\n</code></pre><p>第三步，创建 SleuthConfiguration 配置类，创建一个 <a href="https://github.com/openzipkin/brave/blob/master/instrumentation/mongodb/src/main/java/brave/mongodb/TraceMongoCommandListener.java">TraceMongoCommandListener</a> Bean 对象。这样，我们就能拦截到 MongoDB 操作，进行相应的链路跟踪了。</p><pre><code class="language-java">package com.dunshan.mongodbdemo.config;\n\nimport brave.Tracer;\nimport brave.Tracing;\nimport brave.mongodb.MongoDBTracing;\nimport com.mongodb.MongoClientSettings;\nimport com.mongodb.client.MongoClient;\nimport com.mongodb.client.MongoClients;\nimport com.mongodb.event.CommandListener;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\n@Configuration\npublic class SleuthConfiguration {\n\n    @Bean\n    public MongoClient mongoClient(Tracer tracer) {\n        CommandListener listener = MongoDBTracing.create(Tracing.current())\n                .commandListener();\n        MongoClientSettings settings = MongoClientSettings.builder()\n                .addCommandListener(listener)\n                .build();\n        MongoClient client = MongoClients.create(settings);\n        return client;\n    }\n\n}\n</code></pre><p>第四步，创建 DemoController 类，提供示例 API 接口。</p><pre><code class="language-java">package com.dunshan.mongodbdemo.controller;\n\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.data.mongodb.core.MongoTemplate;\nimport org.springframework.data.mongodb.core.query.Criteria;\nimport org.springframework.data.mongodb.core.query.Query;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestParam;\nimport org.springframework.web.bind.annotation.RestController;\n\n@RestController\n@RequestMapping("/7d")\npublic class DemoController {\n\n    @Autowired\n    private MongoTemplate mongoTemplate;\n\n    @GetMapping("/get")\n    public String get(@RequestParam("id") Integer id) {\n        this.findById(1);\n        return "success";\n    }\n\n    public DemoDO findById(Integer id) {\n        return mongoTemplate.findOne(new Query(Criteria.where("_id").is(id)), DemoDO.class);\n    }\n\n}\n\n</code></pre><p>我们可以请求 /7d/get 接口，执行一次 MongoDB 的查询。</p><p>第五步，创建 DemoServiceApplication 启动类。</p><pre><code class="language-java">package com.dunshan.mongodbdemo;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n@SpringBootApplication\npublic class DemoServiceApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(DemoServiceApplication.class, args);\n    }\n\n}\n</code></pre><p>最后启动应用。我们使用 Postman 测试接口，执行一次 MongoDB 查询操作，尝试跟踪该链路。</p><p>在 Zipkin 可视化 UI 中可以看到刚才我们调用接口的链路数据。</p><p><img src="https://static001.geekbang.org/resource/image/bf/bd/bfbfb1767558068524726c361d95d1bd.png?wh=1909x556" alt="图片"></p><p>点开该链路，可以看到一个 Trace 明细。</p><p><img src="https://static001.geekbang.org/resource/image/93/45/936dd0396e3233228917ec0351f95445.png?wh=1909x997" alt="图片"></p><p>我们的 MongoDB 组件链路跟踪也已经成功了。</p><h3>RabbitMQ</h3><p>因为 <a href="https://github.com/openzipkin/brave">Brave</a> 库也默认提供了 RabbitMQ 链路追踪的功能，所以同样只需要集成就可以了。</p><p>顺便提下 ，Brave 主要通过两个插件实现链路数据收集，它们分别是：</p><ul>\n<li><a href="https://github.com/openzipkin/brave/tree/master/instrumentation/messaging">brave-instrumentation-messaging</a></li>\n<li><a href="https://github.com/openzipkin/brave/blob/master/instrumentation/spring-rabbit/">brave-instrumentation-spring-rabbit</a></li>\n</ul><p><strong>搭建生产者示例</strong></p><p>第一步，还是引入相关依赖。</p><pre><code class="language-xml">    &lt;dependencies&gt;\n        &lt;!-- 引入 SpringMVC 相关依赖，并实现对其的自动配置 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 引入 Spring Cloud Stream RabbitMQ 相关依赖，将 RabbitMQ 作为消息队列，并实现对其的自动配置 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 引入 Zipkin 依赖--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--添加 Sleuth 依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n</code></pre><p>第二步，创建全局配置文件，添加RabbitMQ配置。</p><pre><code class="language-yaml">spring:\n  application:\n    name: demo-producer\n  cloud:\n    # Spring Cloud Stream 配置项，对应 BindingServiceProperties 类\n    stream:\n      # Binder 配置项，对应 BinderProperties Map\n      binders:\n        demo:\n          type: rabbit # 设置 Binder 的类型\n          environment: # 设置 Binder 的环境配置\n            # 如果是 RabbitMQ 类型的时候，则对应的是 RabbitProperties 类\n            spring:\n              rabbitmq:\n                host: 127.0.0.1 # 服务地址\n                port: 5672 # 服务端口\n                username: guest # 服务账号\n                password: guest # 服务密码\n      # Binding 配置项，对应 BindingProperties Map\n      bindings:\n        demo-output:\n          destination: DEMO-TOPIC # 目的地,使用 RabbitMQ Exchange\n          content-type: application/json # 内容格式\n          binder: demo # 设置使用的 Binder 名字\n  sleuth:\n    sampler: #采样器\n      probability: 1.0 #采样率，采样率是采集 Trace 的比率，默认 0.1\n      rate: 10000 #每秒数据采集量，最多 n 条/秒 Trace\n    messaging:\n      # Spring Cloud Sleuth 针对 RabbitMQ 组件的配置项\n      rabbit:\n        enabled: true # 是否开启\n        remote-service-name: rabbitmq # 远程服务名，默认为 rabbitmq\n  zipkin: #设置 zipkin 服务端地址\n    base-url: http://127.0.0.1:9411\n\nserver:\n  port: 18080\n</code></pre><p>第三步，创建 MySource 接口，声明名字为 Output Binding。</p><pre><code class="language-java">package com.dunshan.rabbitmqdemo.producerdemo.message;\n\nimport org.springframework.cloud.stream.annotation.Output;\nimport org.springframework.messaging.MessageChannel;\n\npublic interface MySource {\n\n    @Output("demo-output")\n    MessageChannel demoOutput();\n\n}\n</code></pre><p>第四步，创建 DemoMessage 类，示例 Message 消息。</p><pre><code class="language-java">package com.dunshan.rabbitmqdemo.producerdemo.message;\n\n/**\n * 示例 Message 消息\n */\npublic class DemoMessage {\n\n    /**\n     * 编号\n     */\n    private Integer id;\n\n    public DemoMessage setId(Integer id) {\n        this.id = id;\n        return this;\n    }\n\n    public Integer getId() {\n        return id;\n    }\n\n    @Override\n    public String toString() {\n        return "DemoMessage{" +\n                "id=" + id +\n                \'}\';\n    }\n\n}\n</code></pre><p>​第五步，创建 DemoController 类，提供示例 API 接口。</p><pre><code class="language-java">package com.dunshan.rabbitmqdemo.producerdemo.controller;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.messaging.Message;\nimport org.springframework.messaging.support.MessageBuilder;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RestController;\n\nimport java.util.Random;\n\n@RestController\n@RequestMapping("/7d")\npublic class DemoController {\n\n    private Logger logger = LoggerFactory.getLogger(getClass());\n\n    @Autowired\n    private MySource mySource;\n\n    @GetMapping("/send")\n    public boolean send() {\n        // 创建 Message\n        DemoMessage message = new DemoMessage()\n                .setId(new Random().nextInt());\n        // 创建 Spring Message 对象\n        Message&lt;DemoMessage&gt; springMessage = MessageBuilder.withPayload(message)\n                .build();\n        // 发送消息\n        return mySource.demoOutput().send(springMessage);\n    }\n\n}\n</code></pre><p>我们可以请求 /7d/send 接口，执行一次发送消费。</p><p>第六步，创建 DemoServiceApplication 启动类。</p><pre><code class="language-java">package com.dunshan.rabbitmqdemo.producerdemo;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.stream.annotation.EnableBinding;\n\n@SpringBootApplication\n@EnableBinding(MySource.class)\npublic class ProducerApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(ProducerApplication.class, args);\n    }\n\n}\n</code></pre><p>这样，我们的生产者示例就搭建完了。</p><p><strong>搭建消费者示例</strong></p><p>第一步，还是引入相关依赖。</p><pre><code class="language-xml">    &lt;dependencies&gt;\n        &lt;!-- 引入 SpringMVC 相关依赖，并实现对其的自动配置 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 引入 Spring Cloud Stream RabbitMQ 相关依赖，将 RabbitMQ 作为消息队列，并实现对其的自动配置 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 引入 Zipkin 依赖--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--添加 Sleuth 依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n</code></pre><p>第二步，创建全局配置文件，添加相关数据库配置。</p><pre><code class="language-yaml">spring:\n  application:\n    name: demo-consumer\n  cloud:\n    # Spring Cloud Stream 配置项，对应 BindingServiceProperties 类\n    stream:\n      # Binder 配置项，对应 BinderProperties Map\n      binders:\n        demo:\n          type: rabbit # 设置 Binder 的类型\n          environment: # 设置 Binder 的环境配置\n            # 如果是 RabbitMQ 类型的时候，则对应的是 RabbitProperties 类\n            spring:\n              rabbitmq:\n                host: 127.0.0.1 # RabbitMQ 服务的地址\n                port: 5672 # RabbitMQ 服务的端口\n                username: guest # 服务账号\n                password: guest # 服务密码\n      # Binding 配置项，对应 BindingProperties Map\n      bindings:\n        demo01-input:\n          destination: DEMO-TOPIC # 目的地。这里使用 RabbitMQ Exchange\n          content-type: application/json # 内容格式。这里使用 JSON\n          group: demo-consumer-group-DEMO-TOPIC # 消费者分组\n          binder: demo  # 设置使用的 Binder 名字\n\n  sleuth:\n    sampler: #采样器\n      probability: 1.0 #采样率，采样率是采集 Trace 的比率，默认 0.1\n      rate: 10000 #每秒数据采集量，最多 n 条/秒 Trace\n    messaging:\n      # Spring Cloud Sleuth 针对 RabbitMQ 组件的配置项\n      rabbit:\n        enabled: true # 是否开启\n        remote-service-name: rabbitmq # 远程服务名，默认为 rabbitmq\n  zipkin: #设置 zipkin 服务端地址\n    base-url: http://127.0.0.1:9411\n\nserver:\n  port: ${random.int[10000,19999]} # 随机端口，方便启动多个消费者\n</code></pre><p>spring.sleuth.messaging.rabbit 是 Spring Cloud Sleuth 针对 RabbitMQ 组件的配置项，对应 <a href="https://github.com/spring-cloud/spring-cloud-sleuth/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/messaging/SleuthMessagingProperties.java#L150-L175">SleuthMessagingProperties.Rabbit</a> 类。</p><ul>\n<li>enabled 配置项，是否开启，默认为 true。</li>\n<li>remote-service-name 配置项，远程服务名，默认为 rabbitmq。</li>\n</ul><p>第三步，创建 MySink 接口，声明名字为 Intput Binding。</p><pre><code class="language-java">package com.dunshan.rabbitmqdemo.consumerdemo.listener;\n\nimport org.springframework.cloud.stream.annotation.Input;\nimport org.springframework.messaging.SubscribableChannel;\n\npublic interface MySink {\n\n    String DEMO_INPUT = "demo-input";\n\n    @Input(DEMO_INPUT)\n    SubscribableChannel demoInput();\n\n}\n</code></pre><p>第四步，创建 DemoMessage 类，示例 Message 消息。</p><pre><code class="language-java">package com.dunshan.rabbitmqdemo.consumerdemo.message;\n\n/**\n * 示例 Message 消息\n */\npublic class DemoMessage {\n\n    /**\n     * 编号\n     */\n    private Integer id;\n\n    public DemoMessage setId(Integer id) {\n        this.id = id;\n        return this;\n    }\n\n    public Integer getId() {\n        return id;\n    }\n\n    @Override\n    public String toString() {\n        return "DemoMessage{" +\n                "id=" + id +\n                \'}\';\n    }\n\n}\n</code></pre><p>​第五步，创建 DemoConsumer 类，消费消息。</p><pre><code class="language-java">package com.dunshan.rabbitmqdemo.consumerdemo.listener;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.cloud.stream.annotation.StreamListener;\nimport org.springframework.messaging.handler.annotation.Payload;\nimport org.springframework.stereotype.Component;\n\n@Component\npublic class DemoConsumer {\n\n    private Logger logger = LoggerFactory.getLogger(getClass());\n\n    @StreamListener(MySink.DEMO_INPUT)\n    public void onMessage(@Payload DemoMessage message) {\n        logger.info("[onMessage][线程编号:{} 消息内容：{}]", Thread.currentThread().getId(), message);\n    }\n\n}\n</code></pre><p>第六步，创建 DemoServiceApplication 启动类。</p><pre><code class="language-java">package com.dunshan.rabbitmqdemo.consumerdemo;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.stream.annotation.EnableBinding;\n\n@SpringBootApplication\n@EnableBinding(MySink.class)\npublic class ConsumerApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(ConsumerApplication.class, args);\n    }\n\n}\n</code></pre><p>最后，启动应用。我们使用 Postman 测试一下接口，发送一条消息，尝试跟踪该链路。</p><p><img src="https://static001.geekbang.org/resource/image/ef/be/ef4dfa07248e4c3c0a1c16abyy2a84be.png?wh=1920x1110" alt="图片"></p><p>查看结果，在 Zipkin 已经可以看到刚才我们调用接口的链路数据了。</p><p><img src="https://static001.geekbang.org/resource/image/41/61/41492e413b4772bcc3409c7f9fc04c61.png?wh=1912x579" alt="图片"></p><p>点开该链路可以看到一个 Trace 明细。</p><p><img src="https://static001.geekbang.org/resource/image/68/de/68d35a1c927d1b543d5b3e58386e07de.png?wh=1914x920" alt="图片"></p><p>到这里，我们的 RabbitMQ 组件链路跟踪也成功了。</p><p>刚才，我对所有的技术组件都单独进行了 demo 预演。单就这个过程来说，改造的复杂度还是可以接受的。接下来我们就一起来看看，怎么让它们在真实项目中落地。</p><h2></h2><h2>系统改造</h2><h3>服务改造</h3><p>我这里主要介绍三个服务的改造过程（其他服务是类似的操作），它们分别是：</p><ol>\n<li>mall-gateway，API 网关</li>\n<li>mall-auth，认证中心服务</li>\n<li>mall-member，会员系统服务</li>\n</ol><p>mall-gateway 主要使用的是 Spring Cloud Gateway 技术，我们这个项目的主要用途是路由匹配、请求统一校验认证和鉴权。</p><p>大致的执行流程可以参考<a href="https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html">下图</a>。</p><p><img src="https://static001.geekbang.org/resource/image/42/15/42ab6bd5e6de4a937d418a62c4e85415.jpg?wh=503x595" alt=""></p><p>通过上面的介绍，我们知道 Sleuth 原生就支持对 Spring Cloud Gateway 链路追踪，所以我们只需要集成就可以了。</p><p>我们在 pom.xml 文件引入相关依赖。</p><pre><code class="language-xml">&lt;dependencies&gt;\n        &lt;!-- 引入 Spring Cloud Gateway 相关依赖--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 引入 Zipkin 依赖--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--添加 Sleuth 依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n</code></pre><ul>\n<li>mall-auth</li>\n</ul><p>mall-auth 是认证中心服务，主要结合 Oauth2 实现用 JWT 令牌存储信息、刷新令牌功能还有权限认证等工作，涉及的组件主要为 Redis （非 Jedis 客户端方式）和 MySQL。</p><p>所以在 pom.xml 文件，引入相关依赖即可。</p><pre><code class="language-java">  &lt;!--添加 Sleuth 依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--Zipkin 客户端--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- Brave 对 MySQL8 的支持 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;io.zipkin.brave&lt;/groupId&gt;\n            &lt;artifactId&gt;brave-instrumentation-mysql8&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n</code></pre><p>​启动应用后，我们使用 API 文档测试下接口，尝试跟踪该链路。</p><p><img src="https://static001.geekbang.org/resource/image/18/46/18cab3ba4c2d9fa202602d8d262ff846.png?wh=1912x996" alt="图片"></p><p>在 Zipkin 就可以看到刚才我们调用接口的链路数据了，它分别涉及了四个组件。</p><p><img src="https://static001.geekbang.org/resource/image/4d/47/4d3814b90bd397aca6baafd8e8917e47.png?wh=1911x993" alt="图片"></p><p>我们点开该链路，可以看到一个 Trace 明细。</p><p><img src="https://static001.geekbang.org/resource/image/27/yy/27e86aae5d5882e049d7ebd7509b2eyy.png?wh=1918x1001" alt="图片"></p><p>同样，我们也可以查看拓扑关系。</p><p><img src="https://static001.geekbang.org/resource/image/f6/4f/f68c61215de55649a42acc9cdfd1764f.png?wh=1915x996" alt="图片"></p><ul>\n<li>mall-member</li>\n</ul><p>mall-member 是我们的会员服务，这里以核心接口【会员登录】为例，梳理一下它涉及到的组件。</p><p><img src="https://static001.geekbang.org/resource/image/0d/12/0d5bd029e555f860e31a60f5dba2ff12.png?wh=1920x1069" alt="图片"></p><p>通过分析代码调用链，我们知道它主要涉及 MySQL、Redis、MongoDB 等组件。</p><p>所以我们只需要在 pom.xml 文件引入相关依赖即可。</p><pre><code class="language-xml">   &lt;!--添加 Sleuth 依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--Zipkin 客户端--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- Brave 对 MySQL8 的支持 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;io.zipkin.brave&lt;/groupId&gt;\n            &lt;artifactId&gt;brave-instrumentation-mysql8&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--添加 brave mongodb 依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;io.zipkin.brave&lt;/groupId&gt;\n            &lt;artifactId&gt;brave-instrumentation-mongodb&lt;/artifactId&gt;\n            &lt;version&gt;5.13.3&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;!--  Brave 对 Opentracing 的实现 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;io.opentracing.brave&lt;/groupId&gt;\n            &lt;artifactId&gt;brave-opentracing&lt;/artifactId&gt;\n            &lt;version&gt;0.35.0&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;!-- Opentracing 对 Redis 的支持 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;\n            &lt;artifactId&gt;opentracing-redis-jedis3&lt;/artifactId&gt;\n            &lt;version&gt;0.1.16&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;io.opentracing.contrib&lt;/groupId&gt;\n            &lt;artifactId&gt;opentracing-redis-spring-data&lt;/artifactId&gt;\n            &lt;version&gt;0.1.16&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre><p>同时，要在 config 下新增两个配置类。</p><p><img src="https://static001.geekbang.org/resource/image/9d/22/9d8864e4cbe9c4e22d0438e895b22922.png?wh=624x534" alt="图片"></p><p>BraveConfiguration 为 MongoDB 链路追踪配置类，OpentracingConfiguration 为 Redis 组件链路追踪配置类。</p><p>启动应用后，我们使用 API 文档测试【会员登录】接口，尝试追踪该链路。</p><p><img src="https://static001.geekbang.org/resource/image/99/af/99959835f5316c4a62d13e2a8d13c2af.png?wh=1904x992" alt="图片"></p><p>在 Zipkin 中可以看到刚才我们调用接口的链路数据，它分别涉及了五个组件。</p><p><img src="https://static001.geekbang.org/resource/image/25/c8/2589ff807a90609f80b08209e1b85ac8.png?wh=1913x997" alt="图片"></p><p>我们点开这条链路，可以看到 Trace 明细。</p><p><img src="https://static001.geekbang.org/resource/image/2c/af/2ce6b93d67eaa781e45772b7a245b8af.png?wh=1917x998" alt="图片"></p><p>也可以查找依赖，看到拓扑关系。</p><p><img src="https://static001.geekbang.org/resource/image/cd/6b/cd8e3ebf8fded10bc62dbe251760f26b.png?wh=1913x992" alt="图片"></p><p>接着我们使用 API 文档测试一下【添加品牌关注】接口，尝试追踪该链路。</p><p><img src="https://static001.geekbang.org/resource/image/b4/4d/b49a43e149ce7de2ec2yyc763e66604d.png?wh=1915x1000" alt="图片"></p><p>在 Zipkin 可以看到刚才我们调用接口的链路数据，它分别涉及了三个组件。</p><p><img src="https://static001.geekbang.org/resource/image/7a/17/7a81b796da4d1dc86e4c7d73754b1417.png?wh=1913x996" alt="图片"></p><p>我们点开这条链路，也可以看到详细的 Trace 明细。</p><p><img src="https://static001.geekbang.org/resource/image/7e/a0/7e7d92b90da5ac1d65f7b00b7d172ca0.png?wh=1915x845" alt="图片"></p><h3>Zipkin 改造</h3><p>改造完服务后，我们还有一个需求，那就是对 Zipkin 生产环境的功能增强。也就是说， 我们这里将链路日志推送到 Kafka，然后启动 Zipkin Server 聚合日志，监听 Kafka ，如果有新的消息则进行拉取存入到 ElasticSeach，最后再用 Zipkin UI 展示链路过程。</p><p>这里的改造大概分为两个部分：</p><ul>\n<li>将链路日志数据写入 Kafka 进行削峰；</li>\n<li>Kafka 写入 Zipkin Server 进行聚合并存储。</li>\n</ul><p>第一步，我们需要在各个服务引入 Kafka 依赖。</p><pre><code class="language-xml">  &lt;!-- 引入 Spring Cloud Stream Kafka--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-stream-kafka&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n</code></pre><p>​第二步，修改 bootstrap.yml 配置文件，增加 Kafka 系列配置。</p><pre><code class="language-yaml">spring:\n  zipkin: #设置zipkin服务端地址\n    sender:\n      type: kafka  #指定发送到kafka,还可以指定Rabbit、Web\n    service:\n      name: ${spring.application.name} #Zipkin链路日志中收集的服务名称\n    kafka:\n      topic: zipkin\n  kafka:\n    bootstrap-servers: kafka:9092 #Kubernetes中Kakfa地址,当然也可以指定Kubernetes集群外的Kafk\n</code></pre><p>第三步，Zipkin 安装时增加 Storage 参数，比如 Docker 安装可以参考我给出的命令。</p><pre><code class="language-bash">docker run -it -d --restart=always -e KAFKA_BOOTSTRAP_SERVERS=192.168.3.58:9092 -e STORAGE_TYPE=elasticsearch -e ES_HOSTS=http://192.168.3.58:9200 -p 9411:9411  openzipkin/zipkin\n</code></pre><p>第四步，在 Kafka Manager 验证一下 Topic 情况。</p><p><img src="https://static001.geekbang.org/resource/image/de/a7/dea1fc60cd3b14999984da7eb72b8ea7.png?wh=1214x595" alt="图片"></p><p>最后，我们使用 ES Head 客户端插件查询一下链路日志。</p><p><img src="https://static001.geekbang.org/resource/image/e4/19/e4e06fbyye7d9ef2f4b50d9ebe282919.png?wh=1912x996" alt="图片"></p><p>可以看到，链路追踪数据已经成功保存了。</p><h2>总结</h2><p>好了，这节课的内容到这里就全部讲完了。</p><p>刚才，我们进行了 demo 预演和系统实战，演示了如何在微服务项目中集成 Sleuth + Zipkin 落地链路追踪，希望能够给你一些启发。</p><p>这里有几个重点我们再一起回顾一下：</p><ul>\n<li>在 <a href="https://github.com/openzipkin/brave">Brave</a> 库，常见的 Spring Cloud Gateway、SpringMVC、Fegin、Logback、MongoDB、MySQL、RabbitMQ 等组件均已默认支持；</li>\n<li>目前 Brave 支持通过三种插件实现 MySQL 链路追踪，具体需要适配 MySQL 驱动版本；</li>\n<li>目前 Brave 暂未支持 Jedis 客户端的方式，所以我们可以考虑替代用 OpenTracing 的方式；</li>\n<li>从经验性角度，前置 kafka，一方面作为队列和缓冲，另一方面提供了统一的入口渠道，通过将链路日志数据写入 Kafka 进行削峰，再由 Kafka 写入 Zipkin Server 进行聚合，所以较适合数据量大、服务多的环境。</li>\n</ul><p>链路追踪现如今已成为微服务架构性能监控的标配，如何去选型并结合系统本身的特点做到成功的改造是不容易的。希望你可以动手实践起来，因为只有实践才是检验真理的唯一标准。</p><h2></h2><h2>思考题</h2><p>在课程的最后，我还是照例给你留两道思考题：</p><ol>\n<li>你觉得使用 Brave SDK，手动埋点生成 Trace 的难点在哪里？为什么？</li>\n<li>在生产环境中为什么考虑把链路追踪日志先推送到 Kafka 消息队列？</li>\n</ol><p>欢迎你在留言区和我交流讨论，我们下节课见！</p>',
        article_title: "12 | 链路追踪：如何对一个具体的项目进行追踪改造？",
      },
      {
        title: "13 | 标记透传：微服务系统如何做标记透传方案选型？",
        id: 444794,
        content:
          '<p>你好，我是高楼。</p><p>接下来的两节课，我们会详细讲讲标记透传。这节课呢，我会带你看看，在微服务系统中如何对标记透传方案进行选型。下节课我们会进入实战，讲解如何基于微服务技术进行标记透传的落地。</p><p>在微服务系统中，服务之间可以通过各种方式和协议进行通信，而且一般链路都很长。在全链路压测的系统中，线上压测要保证压测安全且可控，不会对真实用户产生影响，也不会对线上环境造成数据的污染，我们首要解决的就是压测标记在整条链路中透传和识别的问题。</p><p>分布式系统的压测流量透传主要包含两大方面：</p><ul>\n<li><strong>跨线程间的透传：</strong>能够做到跨线程间对压测标记进行传递，同时可以兼容不同的协议类型，支持对标记的存取操作；</li>\n<li><strong>跨服务间的透传：</strong>能够通过网络在上下游服务间进行标记传递，有点像“击鼓传花”的感觉，而且可以做到全链路双向传递。</li>\n</ul><p>接下来，我们分别看看这两大方面都有哪些可供选择的标记透传方案。</p><h2>跨线程间的透传</h2><p>我们先来看下跨线程间的透传。对于涉及多线程调用的服务来说，一个重点就是要保证压测标识在跨线程的情况下不丢失。</p><p>这个时候，我们就不得不提到本地线程专属变量 ThreadLocal 了。ThreadLocal 能够提供线程局部专属变量，这些变量和普通变量的不同之处在于，我们访问的每个变量(通过  Get 或 Set 的方法)的线程都有独立初始化的变量副本。ThreadLocal将状态与线程关联起来的私有静态字段(例如Request ID 或 TraceID)保存起来。</p><!-- [[[read_end]]] --><p>我们通过这张图片快速了解下 ThreadLocal 的内部存储结构。</p><p><img src="https://static001.geekbang.org/resource/image/3e/3d/3e56a00e590fb56365cd5b3b6f64de3d.jpg?wh=1920x1754" alt="图片"></p><p>在这张图里，我们可以看到 ThreadLocal 的存储结构是这样的：</p><ul>\n<li>每个 Thread 维护一个 ThreadLocalMap 映射表，这个映射表的 key 是 ThreadLocal 实例本身，value 是真正需要存储的 Object；</li>\n<li>ThreadLocal 本身并不存储值，它只是作为一个 key 来让线程从 ThreadLocalMap 中获取 value；</li>\n<li>ThreadLocal 能在每个线程间进行隔离，这主要是靠在每个 Thread 对象中维护一个 ThreadLocalMap 来实现的。</li>\n</ul><p>虽然 ThreadLocal 能够提供线程局部专属变量，但也有它的局限性，那就是它无法在父子线程之间传递。</p><p>你可以参考一下我给出的代码。</p><pre><code class="language-java">package com.dunshan.threadlocaldemo.demo;\n\n/**\n * @author: dunshan\n * @date: 2021-4-2 13:13\n */\n\npublic class ThreadLocalDemo {\n    private static final ThreadLocal&lt;Integer&gt; flagThreadLocal = new ThreadLocal&lt;&gt;();\n    public static void main(String[] args) {\n        Integer flagId = new Integer(5);\n        ThreadLocalDemo threadLocalExample = new ThreadLocalDemo();\n        threadLocalExample.setRequestId(flagId);\n    }\n\n    public void setRequestId(Integer flagId) {\n        flagThreadLocal.set(flagId);\n        doRun();\n    }\n\n    public void doRun() {\n        System.out.println("首先打印 flagId:" + flagThreadLocal.get());\n        (new Thread(new Runnable() {\n            @Override\n            public void run() {\n                System.out.println("子线程启动");\n                System.out.println("在子线程中访问 flagId:" + flagThreadLocal.get());\n            }\n        })).start();\n    }\n}\n</code></pre><p>在代码中你可以看到，我在 doRun 方法中又启动了一个子线程来执行业务(模拟异步处理)。</p><p>运行结果如下：</p><pre><code class="language-plain">首先打印 flagId:5\n子线程启动\n在子线程中访问 flagId:null\n</code></pre><p>从运行结果中也能看出来，在子线程中是不能获取父线程中的变量值的。就像前面分析存储原理时提到的，因为子线程拥有自己的 ThreadLocalMap，所以不能获取父线程 ThreadLocalMap 中的值。</p><p>但是在实际业务中呢，很多业务都是需要异步操作的，所以我们需要父子线程能够直接共享 ThreadLocal 中的值。怎么解决这个问题呢？</p><p>这个时候，我们可以考虑引入另外一个线程对象 <strong>InheritableThreadLocal</strong>。</p><p>我们通过下面这段代码，看看它是怎么实现父子线程之间共享 ThreadLocal 值的。</p><pre><code class="language-java">package com.dunshan.threadlocaldemo.demo;\n\n/**\n * @author: dunshan\n * @date: 2021-4-2 13:13\n */\n\npublic class InheritableThreadLocalDemo {\n    private static final InheritableThreadLocal&lt;Integer&gt; flagThreadLocal = new InheritableThreadLocal&lt;&gt;();\n    public static void main(String[] args) {\n        Integer flagId = new Integer(5);\n        InheritableThreadLocalDemo threadLocalExample = new InheritableThreadLocalDemo();\n        threadLocalExample.setRequestId(flagId);\n    }\n\n    public void setRequestId(Integer flagId) {\n        flagThreadLocal.set(flagId);\n        doBussiness();\n    }\n\n    public void doRun() {\n        System.out.println("首先打印 flagId:" + flagThreadLocal.get());\n        (new Thread(new Runnable() {\n            @Override\n            public void run() {\n                System.out.println("子线程启动");\n                System.out.println("在子线程中访问 flagId:" + flagThreadLocal.get());\n            }\n        })).start();\n    }\n}\n</code></pre><p>运行结果如下：</p><pre><code class="language-plain">首先打印 flagId:5\n子线程启动\n在子线程中访问 flagId:5\n</code></pre><p>从运行结果可以看出，子线程成功获取到了父线程 ThreadLocal  的值，这样也就解决了父子线程值传递的问题。<br>\n不过，在大部分业务场景下，业务应用不可能每一个异步请求都要 new 一个单独的子线程来处理，这样会导致内存被撑爆。所以，通常情况下，我们还会使用到线程池，而线程池中又存在线程复用的情况。假设线程池复用线程变量值，就会导致父子线程变量复制混乱。</p><p>你可以看下这段示例代码：</p><pre><code class="language-java">package com.dunshan.threadlocaldemo.demo;\n\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\n\n/**\n * 演示 InheritableThreadLocal 的缺陷\n * @author: dunshan\n * @date: 2020-4-4\n */\n\npublic class InheritableThreadLocalWeaknessDemo {\n\n    private static final InheritableThreadLocal&lt;Integer&gt; INHERITABLE_THREAD_LOCAL = new InheritableThreadLocal&lt;&gt;();\n    //模拟业务线程池\n    private static final ExecutorService threadPool = Executors.newFixedThreadPool(5);\n\n    public static void main(String[] args) throws InterruptedException {\n        //模拟同时 10 个 web 请求，一个请求一个线程\n        for (int i = 0; i &lt; 10; i++) {\n            new TomcatThread(i).start();\n        }\n\n        Thread.sleep(3000);\n        threadPool.shutdown();\n    }\n\n    static class TomcatThread extends Thread{\n        //线程下标\n        int index;\n\n        public TomcatThread(int index) {\n            this.index = index;\n        }\n\n        @Override\n        public void run() {\n            String parentThreadName = Thread.currentThread().getName();\n            //父线程中将 index 值塞入线程上下文变量\n            System.out.println( parentThreadName+ ":" + index);\n            INHERITABLE_THREAD_LOCAL.set(index);\n\n            threadPool.submit(new BusinessThread(parentThreadName));\n        }\n    }\n\n    static class BusinessThread implements Runnable{\n        //父进程名称\n        private String parentThreadName;\n\n        public BusinessThread(String parentThreadName) {\n            this.parentThreadName = parentThreadName;\n        }\n\n        @Override\n        public void run() {\n            System.out.println("parent:"+parentThreadName+":"+INHERITABLE_THREAD_LOCAL.get());\n        }\n    }\n}\n</code></pre><p>这段代码模拟了同时有 10 个 web 请求（启动 10 个线程），每个线程内部都向线程池中提交一个异步任务的情况。</p><p>运行结果：</p><pre><code class="language-plain">Thread-1:1\nThread-4:4\nThread-5:5\nThread-6:6\nThread-7:7\nThread-8:8\nThread-9:9\nparent:Thread-1:1\nparent:Thread-0:0\nparent:Thread-4:4\nparent:Thread-3:0\nparent:Thread-8:1\nparent:Thread-7:7\nparent:Thread-9:0\nparent:Thread-2:2\nparent:Thread-5:4\nparent:Thread-6:1\n</code></pre><p>在运行结果中我们也能看到，子线程中输出的父线程名称和它们下标的 index 无法一一对应，在子线程中出现了线程本地变量混乱的现象。在全链路压测中，出现这种情况是致命的。</p><p>那我们要怎么解决这个问题呢？</p><p>当然你可以实现自己的工具类，将要传递的变量封装到对象里，在启动子线程时将对象传递进去，这样子线程就可以拿到父线程的变量了。</p><p>我们这里选择的是 <a href="https://github.com/alibaba/transmittable-thread-local">TransmittableThreadLocal</a> （TTL）。</p><p>TransmittableThreadLocal 是阿里开源的一个增强 InheritableThreadLocal 的库，能够很好地解决使用线程池在线程之间复制值混乱的问题。</p><p>接下来，我们一起验证一下， TransmittableThreadLocal 是不是真的能解决上面的问题。</p><p>首先需要引包：</p><pre><code class="language-xml">&lt;dependency&gt;\n     &lt;groupId&gt;com.alibaba&lt;/groupId&gt;\n     &lt;artifactId&gt;transmittable-thread-local&lt;/artifactId&gt;\n     &lt;version&gt;2.12.0&lt;/version&gt;\n &lt;/dependency&gt;\n</code></pre><p>示例代码如下：</p><pre><code class="language-java">package com.dunshan.threadlocaldemo.demo;\n\nimport com.alibaba.ttl.TransmittableThreadLocal;\nimport com.alibaba.ttl.TtlRunnable;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\n\n/**\n * @author: dunshan\n * @date: 2021-4-2 13:13\n */\n\npublic class TransmittableThreadLocalDemo {\n\n    private static final TransmittableThreadLocal&lt;Integer&gt; INHERITABLE_THREAD_LOCAL = new TransmittableThreadLocal&lt;&gt;();\n    //模拟业务线程池\n    private static final ExecutorService threadPool = Executors.newFixedThreadPool(5);\n\n    public static void main(String[] args) throws InterruptedException {\n        //模拟同时 10 个 web 请求，一个请求一个线程\n        for (int i = 0; i &lt; 10; i++) {\n            new TomcatThread(i).start();\n        }\n\n        Thread.sleep(3000);\n        threadPool.shutdown();\n    }\n\n    static class TomcatThread extends Thread{\n        //线程下标\n        int index;\n\n        public TomcatThread(int index) {\n            this.index = index;\n        }\n\n        @Override\n        public void run() {\n            String parentThreadName = Thread.currentThread().getName();\n            //父线程中将 index 值塞入线程上下文变量\n            System.out.println( parentThreadName+ ":" + index);\n            INHERITABLE_THREAD_LOCAL.set(index);\n\n            threadPool.submit(TtlRunnable.get(new BusinessThread(parentThreadName)));\n        }\n    }\n\n    static class BusinessThread implements Runnable{\n        //父进程名称\n        private String parentThreadName;\n\n        public BusinessThread(String parentThreadName) {\n            this.parentThreadName = parentThreadName;\n        }\n\n        @Override\n        public void run() {\n            System.out.println("parent:"+parentThreadName+":"+INHERITABLE_THREAD_LOCAL.get());\n        }\n    }\n}\n</code></pre><p>运行结果如下：</p><pre><code class="language-bash">Thread-1:0\nThread-3:2\nThread-2:1\nThread-4:3\nThread-5:4\nThread-6:5\nThread-7:6\nThread-8:7\nThread-9:8\nThread-10:9\nparent:Thread-5:4\nparent:Thread-2:1\nparent:Thread-6:5\nparent:Thread-10:9\nparent:Thread-8:7\nparent:Thread-1:0\nparent:Thread-7:6\nparent:Thread-3:2\nparent:Thread-9:8\nparent:Thread-4:3\n</code></pre><p>我们可以看到，子线程中输出的内容和父线程一致，<strong>没有出现线程变量复制混乱的情况</strong>。</p><p>通过刚才的学习，我们能够知道，在跨线程间透传的场景下，使用 ThreadLocal 库友好地解决了线程专属变量的问题，但是它还不能真正解决父子线程值传递丢失的问题，于是 JDK 又引入了 InheritableThreadLocal 对象。然后呢，这又引出了下一个问题，那就是涉及到线程池等复用线程场景时，还是会存在变量复制混乱的缺陷。我们的解决方案是，在全链路压测标记透传改造方案中直接引入 TransmittableThreadLocal  来增强 InheritableThreadLocal 对象。</p><p>虽然跨线程间透传的过程有点复杂，我们也看到了，问题一个接着一个，但这些问题都被我们很好地解决了。压测标识在跨线程的情况下始终保持不丢失，我们的目的就达到了。</p><h2>跨服务间的透传</h2><p>好了，讲完了跨线程间的透传，我们再来看下跨服务间的透传有哪些可供选择的方案。</p><p>跨服务透传的方案有很多，其中，基于 HTTP 请求的数据传递类型主要有两种：一、作为参数传递；二、作为 Header 传递。</p><p>而作为  Header 传递的类型，细分下来大概有下面这四种方案：</p><ul>\n<li>放入 HttpRequest Header</li>\n<li>改造 TraceId</li>\n<li>使用字节码增强技术</li>\n<li>使用 Sleuth Baggage</li>\n</ul><p>下面，我们就来仔细说一说这几种方案。</p><h4>方案一：作为接口参数</h4><p>实现思路就是把压测标记追加到接口参数里面。这样做的优点是思路比较简单，开发改起来也没有学习的成本。但缺点还是比较明显的：对业务侵入性强，代码高度耦合，后续维护会有一定困难，如果我们想要增加一个参数，那么所有的接口都需要跟着改动，工作量很大。</p><h4>方案二：放入 HttpRequest Header</h4><p>我们都知道现在的微服务结构大部分情况下都是通过 HTTP 调用的，所以说， HTTP Header 好像天生就是做标记透传载体的料。</p><p>一般我们的实现思路是：先自定义一个 Filter，获取 Request 中自定义的 request header。然后将这些信息放入 ThreadLocal 中。最后实现 feign.Client(暂时忽略 RestTemplate)的 execute() 方法，在调用下级服务前把标记塞入 Request 的 Header 中。</p><p>我们可以通过 Spring 提供的方法从任意地方获取 HttpServletRequest 里的 Header，当然了，因为我们使用了ThreadLocal，所以上述操作要保证在同一线程中。</p><pre><code class="language-java"> ServletRequestAttributes requestAttributes = (ServletRequestAttributes) RequestContextHolder\n                .getRequestAttributes();\n        HttpServletRequest request = requestAttributes.getRequest();\n        //获取头标记\n        String header = request.getHeader("headerKey");\n</code></pre><p>这种方式的优点是显而易见的，因为它对业务是透明的。</p><p>但缺点也比较多，比如链路中有父子线程的话，我们就没有办法从 RequestContextHolder 中拿到标记信息了。同时， Request Header 和 HTTP 是绑定的，就算大部分业务都在用 HTTP 协议进行交互，总还是有些应用会使用 TCP 协议的 RPC 框架，比方说 Thrift、Dubbo等，这时候，这个方案就不适用了。</p><p>除此之外，还有许多应用间使用消息队列来做异步任务的情况，它们也希望做到标记透传。另外，大多数情况下，利用这个方案我们只能对 Request Header 做取得操作，而不能对 HttpServletRequest 进行修改。</p><p>所以说，如果使用这个方案的话，这些问题都需要考虑到，做到根据项目的实际需求灵活选择，切不可蒙头转向干活，到最后发现方案满足不了要求又推翻返工。</p><h4>方案三：改造 TraceId</h4><p>分布式系统中的服务调用链路追踪在理论上并不复杂，它有两个关键点，一个是为请求链路创建唯一的追踪标识，二是统计各个处理单元的延迟时间。</p><p>主要实现原理如下图：</p><p><img src="https://static001.geekbang.org/resource/image/75/4a/7528e659e35788cf6f257bb8993aba4a.png?wh=970x536" alt="图片"></p><p>在<a href="https://docs.spring.io/spring-cloud-sleuth/docs/2.2.8.RELEASE/reference/html/#propagating-extra-fields">这张图</a>中，每个颜色的注解表明一个 Span(总计 7 个 Span，从 A 到 G )，如果注解显示：Trace Id = X；Span Id = D；Client Sent。这就表明当前 Span 将 Trace Id 设置为了 X，将 Span Id 设置为了 D，同时它还表明了 Client Sent（客户端发起一个请求）事件。</p><p>我们将这些 Span 的关系（Parent/Child）图形化：</p><p><img src="https://static001.geekbang.org/resource/image/05/b0/05ca7dacd2e73005c08a84bb4e26dbb0.png?wh=716x247" alt="图片"></p><p>从这张图中我们可以看到，一次链路调用 TraceId 作为唯一的请求 ID，而 Span 标识了各节点发起的请求信息，最后各个子 Span 通过 Parent Id 与父 Span 关联起来。</p><p>那么 Sleuth 在服务内部是如何对TraceId进行处理的呢？</p><p>下面是一个使用多 Header 的 HTTP 请求传递流程图:</p><pre><code class="language-bash">   Client Tracer                                                  Server Tracer     \n┌───────────────────────┐                                       ┌───────────────────────┐\n│                       │                                       │                       │\n│   TraceContext        │          Http Request Headers         │   TraceContext        │\n│ ┌───────────────────┐ │         ┌───────────────────┐         │ ┌───────────────────┐ │\n│ │ TraceId           │ │         │ X-B3-TraceId      │         │ │ TraceId           │ │\n│ │                   │ │         │                   │         │ │                   │ │\n│ │ ParentSpanId      │ │ Inject  │ X-B3-ParentSpanId │ Extract │ │ ParentSpanId      │ │\n│ │                   ├─┼────────&gt;│                   ├─────────┼&gt;│                   │ │\n│ │ SpanId            │ │         │ X-B3-SpanId       │         │ │ SpanId            │ │\n│ │                   │ │         │                   │         │ │                   │ │\n│ │ Sampling decision │ │         │ X-B3-Sampled      │         │ │ Sampling decision │ │\n│ └───────────────────┘ │         └───────────────────┘         │ └───────────────────┘ │\n│                       │                                       │                       │\n└───────────────────────┘                                       └───────────────────────┘\n</code></pre><p>在这张图中，Sleuth 通过 Filter 对 Header 进行处理，先检测 Header 中是否存在 “X-B3-TraceId” 标识，如果存在就传入新的 TraceId，如果不存在就生成新的值。</p><p>我们再来看下，在跨服务过程中 Sleuth 具体是如何处理 TraceId 的？</p><p>首先网关接收请求后，TraceWebFilterr 中将 TraceId 添加到 Header 中，以便所转发的请求对应的服务能从头中获取到 Header。同时网关还将 TraceId 放到 MDC (Mapped Diagnostic Context)中，以便应用在输出日志时携带 TraceId。</p><p>然后，服务在 execute 请求前，Sleuth 将 TraceId 存放到 X-B3-TraceId 头中来实现 Feign.Client（具体参见 <a href="https://github.com/vaquarkhan/spring-cloud-sleuth-Zipkins/blob/master/spring-cloud-sleuth-core/src/main/java/org/springframework/cloud/sleuth/instrument/web/client/feign/TraceFeignClient.java">TraceFeignClient</a> ），具体的步骤是：</p><ul>\n<li>用 Sleuth 创建 TraceFilter，对所有的网络请求进行拦截，如果请求的 Header 中没有 Span 信息，那就创建 Span 对象，生成 Span id、Trace id 等。如果 Header 中有Span信息，就直接使用 Header 中的数据创建 Span 对象，之后将 Span id、Trace id 设置到 MDC 中。</li>\n<li>当使用 RestTemplate 发送请求时，RestTemplate 和 Interceptor Injector 会拦截请求。我们可以将新生成的 Span id、Trace id 等信息设置到请求的 Header 中，这样服务端收到请求后就可以从 Header 中解析出 Span 信息了。</li>\n</ul><p>其他场景的执行原理都是差不多的，这里就不多介绍了 。</p><p>我们看到，微服务调用链框架 <a href="https://github.com/spring-cloud/spring-cloud-sleuth.git">Sleuth</a> 的核心功能就是跨服务追踪调用全过程，它原生就可以对Traceld进行标记、识别并传递。所以，我们可以复用 Sleuth 的相关功能同时顺带修改 Sleuth 源码，将 TraceId 识别并改造后一起往下游服务透传。</p><p>讲完了原理，我们再来看下具体如何改造 TraceId。首先我们需要了解下 TraceId 的组成结构。</p><p>比如下面这个示例。</p><pre><code class="language-bash">X-B3-TraceId: 80f198ee56343ba864fe8b2a57d3eff7\nX-B3-ParentSpanId: 05e3ac9a4f6e3b90\nX-B3-SpanId: e457b5a2e4d86bd1\nX-B3-Sampled: 1\n</code></pre><p>可以看到，在 Header 中有 4 个属性：</p><ul>\n<li>X-B3-SpanId：一次链路调用唯一 ID（可以是 RPC，DB 等，没有特定的限制）；</li>\n<li>X-B3-ParentSpanId：上层服务的调用 Span_ID，最上层服务 Parent_ID 为 null；</li>\n<li>X-B3-TraceId：用于标示一次完整且唯一的请求 ID，一般为 64 位或 128 位的数字；</li>\n<li>X-B3-Sampled：是否被抽样为输出的标志，1 为需要被输出，0 为不需要被输出。</li>\n</ul><p>所以，做 TraceId 改造我们可以这样考虑，如果是正常标记，则是以 1 开头，后面全为零。</p><pre><code class="language-plain">“b3”、“1000000000000000e457b5a2e4d86bd1-e457b5a2e4d86bd1”\n</code></pre><p>如果是压测标记，那就全部是以 2 开头，后面全为零。</p><pre><code class="language-plain">“b3”、“2000000000000000e457b5a2e4d86bd1-e457b5a2e4d86bd1”\n</code></pre><p>你可以使用符合 TraceId 格式的数字，只要有效就可以了。<br>\n到这里，我们已经解决了获取标记的技术问题，通常的做法就是实现一个 Filter 就行了，而后面就是重写 TraceId 并传递给下游服务。</p><p>在 Sleuth 2.2中， 我们可以这样实现：</p><pre><code class="language-bash">@Bean \nExtraFieldPropagation.Factory customPropagationFactory() {\n  return ExtraFieldPropagation.newFactory(\n      CustomTraceIdPropagation.create(B3Propagation.FACTORY, "my_trace_id"));\n}\n</code></pre><p>在 Sleuth 3.0中， 我们可以这样实现：</p><pre><code class="language-bash">@Bean \nBaggagePropagation.Factory customPropagationFactory() {\n  return BaggagePropagation.newFactory(\n      CustomTraceIdPropagation.create(B3Propagation.FACTORY, "my_trace_id"));\n}\n</code></pre><p>这种方式的优点是原理比较简单，不用考虑底层实现，也不用考虑兼容性等问题，因为 Sleuth 都已经实现好了，实现起来比较快。<br>\n但是实际上，TraceId 加零是一个坏主意，因为它跟正常 TraceId 没有明显区别，还有可能会随机出现重复的情况。另外，后期维护也很困难，我们很容易忘了以前修改了哪些地方，移交给别人维护就更加困难了。而且程序升级也比较困难，以后每次 Spring 或者 Sleuth 要升级的时候，都要重新修改源码。</p><p>最后，我们放弃了这个方案，主要的原因就是可能会影响现有的正常 TraceId，对我们来说并不是性价比最高的选择。</p><h4>方案四：使用字节码增强技术</h4><p>Java 还有一种基于字节码增强技术的埋点方式，就是依赖 Java Agent 技术在目标程序启动时加上 -javaagent 参数，或者运行时 attach 进程，两种做法都可以做到将对应的 SDK 注入到目标应用，完成埋点。它们整体来说对服务应用是透明的，对业务代码无侵入。</p><p>关于字节码的基础知识你可以参考美团的<a href="https://tech.meituan.com/2019/09/05/java-bytecode-enhancement.html">《字节码增强技术探索》</a>这篇文章，已经讲得很清楚了，我就不多赘述了。</p><p>在项目具体落地的过程中，我们可以考虑在框架或中间件层做统一的 SDK Jar 包托管，将 SDK 包直接打入 Base 镜像内，然后，借助 Jar 包容器提供的入口，将封装好的 TransmittableThreadLocal SDK 在应用启动之前完成埋点工作，这样的话，就能够实现应用无感知透传了。</p><p>整个 SDK 带起过程你可以参考这张<a href="https://www.infoq.cn/article/z503AvLZyA0SXJCkY61o">示意图</a>。</p><p><img src="https://static001.geekbang.org/resource/image/f9/20/f90c9473584fd8f4a0ba51b0402byy20.png?wh=1080x777" alt="图片"></p><p>这样做的优点是，对业务代码无侵入，可以做到用户无感的热升级；缺点是对开发人员要求比较高，开发成本也比较高，同时，SDK 引入可能还会遇到包冲突等问题。</p><p>在实践方面更加具体的例子你可以参考这篇文章：<a href="https://mp.weixin.qq.com/s/gf8-2DXSNLJ5lhFv7bmN6Q">《JVM 字节码增强技术之 Java Agent 入门》。</a></p><h4>方案五：使用 Sleuth Baggage</h4><p>既然我们可以在程序里获取到 Trace 和 Span 相关信息，那为什么不把信息直接放到 Span 里呢？在 Span 中能放点额外信息，这样就不用自己实现了。事实上，只要 Sleuth 里有 Baggage ，我们确实可以这样做。</p><p>Baggage 是一组存储在 Span Context（上下文）中的 key:value（键值对）。Baggage 和 Trace 一起传递并附加在每个 Span 上。Spring Cloud Sleuth 可以识别以 Baggage 为前缀的 Header，消息传递以 baggage_ 开始。</p><p>需要注意的是，Baggage 的数据和大小没有明显的限制，但是太多会拖慢整个系统的性能。</p><p>Baggage 跟随 Trace 一起传递(每个子 Span 都包含父 Span 的 Baggage)。默认情况下，因为 Zipkin 不知道 Baggage，所以也不接收这些信息。</p><p>需要注意的是，从 Sleuth 2.0.0 开始，我们就必须在项目配置中明确传递 baggage keys 的名称了。Tags 会附加到指定的 Span，也就是该标签只在指定的 Span 中呈现。但是，如果包含 Tag 的 Span 存在，我们可以根据 Tag 搜索对应的 Trace。所以说，你如果希望通过 Baggage 查找 Span ，就应该在 root span 中添加相应的 Tag。</p><p>比如下面这个例子。</p><pre><code class="language-java">spring.sleuth.baggage-keys=baz,bizarrecase\nspring.sleuth.propagation-keys=foo,upper_case\ninitialSpan.tag("foo",ExtraFieldPropagation.get(initialSpan.context(), "foo"));\ninitialSpan.tag("UPPER_CASE",ExtraFieldPropagation.get(initialSpan.context(), "UPPER_CASE"));\n</code></pre><p>这里有关 Baggage 的配置，还有几点需要说明：</p><ul>\n<li>我们在使用 spring.sleuth.baggage-keys 时，可以设置以 baggage-为前缀的键，用于 HTTP 调用和 baggage- 的消息传递。</li>\n<li>还可以使用该 spring.sleuth.propagation-keys 属性传递带前缀的键列表，这些键被传播到远程服务而没有任何前缀。</li>\n<li>另外，还可以使用该 spring.sleuth.local-keys 属性来传递将在本地服务传播但不会通过网络传播的列表键。</li>\n</ul><p>使用 Sleuth  Baggage 的优点是它很容易实现，而且也支持 RestTemplate 的调用，同时它还原生就兼容其他的 SpringCloud 组件。但它也存在和 Sleuth 一样的缺点，也就是对业务代码有侵入性、维护有些困难，而且程序升级后都要重新修改源码。注意哦，Sleuth 底层使用的是 ThreadLocal，后续在跨线程透传方面我们还是需要单独做增强处理的。</p><h2>总结</h2><p>好了，这节课就讲到这里。我们刚才一起梳理了标记透传的背景、目标和几种常见的方案，这里我们做个总结。</p><p>从标记透传的对象来说，我们主要可以分为两个方面，也就是跨线程间的透传和跨服务间的透传。</p><p>跨线程透传主要解决的是线程间的变量复制传递的问题，比如父子线程、线程池复用等场景，最后我们看到 TransmittableThreadLocal （TTL）是 Java 语言一个比较优雅且通用的解决方案。</p><p>而跨服务透传主要的方式就是参数传递和 Header 传递，在 Header 传递方案内，有诸如 HttpRequest Header、改造 TraceId、Java Agent、Sleuth Baggage等技术方案，它们都各有特点，各有用场。你可以看看我给你画的这张思维导图，上面有非常详细的概述和总结，希望对你有帮助。</p><p><img src="https://static001.geekbang.org/resource/image/4a/d6/4a68c7365ab94b8c2509fed93160b5d6.jpg?wh=1920x3103" alt="图片"></p><p>最后，需要强调是，在做具体标记透传技术选型时，我们还是要根据自身项目特点，仔细衡量各各项指标，选择一款适合自己项目的技术方案，技术过于纯粹，适用才是王道。</p><p>下一节课，我们进入实践环节，我会通过案例给你演示如何实现标记透传改造。</p><h2>思考题</h2><p>在课程的最后，我还是照例给你留两道思考题：</p><ol>\n<li>除了我上面列出的这些技术方案，你还接触过哪些？</li>\n<li>如果是异构系统，说说你对技术方案选型的考虑？</li>\n</ol><p>欢迎你在留言区和我交流讨论，我们下节课见！</p>',
        article_title: "13 | 标记透传：微服务系统如何做标记透传方案选型？",
      },
      {
        title: "14 | 标记透传：如何基于微服务技术进行标记透传？",
        id: 446320,
        content:
          '<p>你好，我是高楼。</p><p>这节课，我们详细讲讲怎样基于微服务技术进行标记透传的落地。</p><p>微服务已经成为现在主流的分布式架构了。在微服务架构中，服务之间的调用越来越复杂，问题排查的难度也越来越高，这让链路追踪成为微服务架构的基础设施，甚至可以说是性能分析人员不可缺少的“神兵利器”。</p><p>在全链路压测场景中，链路追踪技术需要额外解决的是压测标记的<strong>识别、记录、并传递的</strong>问题。上节课我们讲过，在微服务架构下标记透传需要满足以下两个需求：</p><ul>\n<li>跨线程间的透传要保证压测标记在跨线程的情况下不丢失，这里面包括异步场景、线程池复用等情况；</li>\n<li>跨服务间的透传要保证压测标记在跨服务网络调用的情况下不丢失，而且可以做到全链路双向传递，这里面包括服务内异步任务（MQ）、服务内其它基于 TCP 协议的 RPC 框架等情况。</li>\n</ul><h2>方案设计</h2><p>考虑到我们的电商项目是 Spring Cloud 技术栈，所以我们可以使用 <a href="https://spring.io/projects/spring-cloud-sleuth">Spring Cloud Sleuth </a>这一套跨服务间透传的原生方案。但是 Sleuth 在跨线程间透传的实现上底层使用的是 ThreadLocal，而项目又有 MQ 异步透传的需求，所以它并不能完全满足我们标记透传的需求。所以在这里，我们考虑引入<strong>数据上下文</strong>对象来解决这个问题。</p><!-- [[[read_end]]] --><p>所谓引入数据上下文，就是引入一个能承载压测标记的对象，我们希望它能够兼容不同的应用协议，同时能够支持跨线程间的透传，最重要的是能支持随时存取。所以，我们抽象出一层数据上下文（Context ）对象。</p><p>数据上下文逻辑如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/9e/8f/9e7875372bf0f25cbf9338966a0d2e8f.jpg?wh=1920x2272" alt="图片"></p><p>创建数据上下文后，跨线程间的标记透传我们就有了 Transmittable ThreadLocal 对象，这样，我们就可以实现随用随取了，它同时也解决了跨协议标记透传的问题。</p><p>至于跨服务间的标记透传，我们考虑把标记数据直接放到 Span Baggage 扩展字段中，这样就不用自己实现了。</p><p>你可以看下我画的这张压测标记透传设计图：</p><p><img src="https://static001.geekbang.org/resource/image/4b/72/4bdfde47b247f3fe494e9c0c40e49a72.jpg?wh=1920x1922" alt=""></p><p>我们可以看到，网关接收请求后，在 Filter 中将压测标记添加到 Span 中，这样，所转发的请求对应的服务就能从 Header 中获取到标记数据了。同时在 Filter 中判断并赋值到 Logback MDC 中，以便应用在输出日志时区分是正常流量还是压测流量。</p><h2>技术预演</h2><p>为了降低风险呢，我们先在开发环境搭建一个示例微服务项目（参考电商项目）进行链路追踪与标记透技术预演。</p><p>针对这个预演方案，我给你画了一张调用关系逻辑图。</p><p><img src="https://static001.geekbang.org/resource/image/ba/27/ba3d569dc9e4f2b98f21b60c46b7dc27.jpg?wh=1920x378" alt="图片"></p><p>通过分析架构图，可以很清楚地看到，这次 demo 微服务需要搭建的 4 个服务分别是网关、会员、购物车和订单。这 4 个微服务采用 Nacos 做注册中心；但是为了降低 demo 项目难度，我们没有添加数据库操作，只做了一个简单请求；配置中心使用本地做配置，没有采用 Nacos 做配置中心。</p><p>下面我们就来具体地看一看，这 4 个微服务是怎么一步一步搭建起来的。</p><p>首先，因为我们需要用 Sleuth 与  Zipkin 来完成项目跟踪，所以就要先搭建这两个服务。搭建 Sleuth 和 Zipkin 的方法可以参考我给出的这篇文章《 <a href="https://mp.weixin.qq.com/s/7Xqk_1xGlLZom9hkfyW9hg">Sleuth+Zipkin 实现 Spring Cloud 链路追踪</a> 》，受到篇幅限制呢，我这里就不展开了。</p><h3>搭建微服务 demo</h3><p>搭建好 Sleuth 和 Zipkin 之后，我们开始正式搭建微服务 demo。</p><ul>\n<li>新建父项目 pom 文件</li>\n</ul><p>首先，新建空项目，在 pom.xml  加入相关依赖包。</p><pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;\n&lt;project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n    \n    &lt;groupId&gt;com.dunshan&lt;/groupId&gt;\n    &lt;artifactId&gt;springboot-sleuth-zipkin&lt;/artifactId&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n    &lt;modules&gt;\n        &lt;module&gt;dunshan-sz-gateway&lt;/module&gt;\n        &lt;module&gt;dunshan-member&lt;/module&gt;\n        &lt;module&gt;dunshan-cart&lt;/module&gt;\n        &lt;module&gt;dunshan-order&lt;/module&gt;\n        &lt;module&gt;dunshan-common&lt;/module&gt;\n    &lt;/modules&gt;\n    &lt;packaging&gt;pom&lt;/packaging&gt;\n    \n    &lt;properties&gt;\n        &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;\n        &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;\n        &lt;spring.boot.version&gt;2.3.0.RELEASE&lt;/spring.boot.version&gt;\n        &lt;spring.cloud.version&gt;Hoxton.SR5&lt;/spring.cloud.version&gt;\n        &lt;spring.cloud.alibaba.version&gt;2.2.0.RELEASE&lt;/spring.cloud.alibaba.version&gt;\n        &lt;zipkin.version&gt;2.2.6.RELEASE&lt;/zipkin.version&gt;\n        &lt;sleuth.version&gt;2.2.6.RELEASE&lt;/sleuth.version&gt;\n        &lt;transmittable.version&gt;2.12.1&lt;/transmittable.version&gt;\n        &lt;swagger.version&gt;2.7.0&lt;/swagger.version&gt;\n    &lt;/properties&gt;\n\n      &lt;!--\n          引入 Spring Boot、Spring Cloud、Spring Cloud Alibaba 三者 BOM 文件，进行依赖版本的管理，防止不兼容。\n       --&gt;\n    &lt;dependencyManagement&gt;\n        &lt;dependencies&gt;\n            &lt;dependency&gt;\n                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n                &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;\n                &lt;version&gt;${spring.boot.version}&lt;/version&gt;\n                &lt;type&gt;pom&lt;/type&gt;\n                &lt;scope&gt;import&lt;/scope&gt;\n            &lt;/dependency&gt;\n            &lt;dependency&gt;\n                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;\n                &lt;version&gt;${spring.cloud.version}&lt;/version&gt;\n                &lt;type&gt;pom&lt;/type&gt;\n                &lt;scope&gt;import&lt;/scope&gt;\n            &lt;/dependency&gt;\n            &lt;dependency&gt;\n                &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;\n                &lt;version&gt;${spring.cloud.alibaba.version}&lt;/version&gt;\n                &lt;type&gt;pom&lt;/type&gt;\n                &lt;scope&gt;import&lt;/scope&gt;\n            &lt;/dependency&gt;\n        &lt;/dependencies&gt;\n    &lt;/dependencyManagement&gt;\n    \n&lt;/project&gt;\n</code></pre><ul>\n<li>创建网关服务</li>\n</ul><p>然后，在网关服务的 pom 文件中添加配置文件，内容包含 Gateway、Nacos、Zipkin、Sleuth 组件，这些内容共同组成网关服务。</p><pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;\n&lt;project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;\n    &lt;parent&gt;\n        &lt;artifactId&gt;dunshan-sleuth-zipkin&lt;/artifactId&gt;\n        &lt;groupId&gt;com.dunshan&lt;/groupId&gt;\n        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n    &lt;/parent&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;artifactId&gt;dunshan-sz-gateway&lt;/artifactId&gt;\n\n    &lt;properties&gt;\n        &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;\n        &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;\n    &lt;/properties&gt;\n\n    &lt;dependencies&gt;\n        &lt;!-- 引入 Spring Cloud Gateway 相关依赖，使用它作为网关，并实现对其的自动配置 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--Nacos 客户端 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 引入 Zipkin 依赖--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--添加 Sleuth 依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n\n&lt;/project&gt;\n</code></pre><ul>\n<li>创建全局过滤器 Filter</li>\n</ul><p>我们知道，Filter 过滤器能过滤所有请求，还能获取请求资源信息。所以，我们在工程中新增一个全局过滤器（ Filter ），这样，它就可以对全部的请求加以过滤和判断了。如果一个标记带有请求，过滤器就能够获取请求中的标记，放入 Span Baggage 扩展字段（Tag）中去，如果这个请求没有标记，就直接返回走正常路径。</p><p>你可以参考我给出的代码。</p><pre><code class="language-java">import brave.Tracer;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.slf4j.MDC;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.cloud.gateway.filter.GatewayFilterChain;\nimport org.springframework.cloud.gateway.filter.GlobalFilter;\nimport org.springframework.http.server.reactive.ServerHttpRequest;\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.server.ServerWebExchange;\nimport reactor.core.publisher.Mono;\n\n/**\n * @author dunshan\n * @description: 网关过滤\n * @date 2021-10-02 14:18:33\n */\n \n@Component\npublic class globalTagFilter implements GlobalFilter {\n    private final static Logger logger = LoggerFactory.getLogger(globalTagFilter.class);\n\n    private Tracer tracer;\n    @Autowired\n    public void setTracer(Tracer tracer) {\n        this.tracer = tracer;\n    }\n\n    @Override\n    public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) {\n        String dunshan = exchange.getRequest().getHeaders().getFirst("dunshan");\n        if (dunshan != null) {\n            logger.info("dunshan：" + dunshan);\n            tracer.currentSpan().tag("dunshan", dunshan); //透传标签\n            ServerHttpRequest request = exchange.getRequest().mutate().header("dunshan", dunshan).build();\n            exchange = exchange.mutate().request(request).build();\n        }\n        return chain.filter(exchange);\n    }\n}\n</code></pre><p>要想实现 GlobalFilter 接口，你可以在 Filter 类中获取标记，把标记赋值给 tracer.currentSpan().tag() 。</p><ul>\n<li>在 application.yml 中添加配置</li>\n</ul><p>在网关服务中的配置文件输入文件内容，内容包含：项目名称、端口号、Sleuth 采集数据比例、Zipkin 服务地址还有其他配置信息。</p><p>需要注意的是，propagation-keys 参数需要配置 value，同时，你也可以根据需要配置其他参数，但是为了前后服务调用一致性，这里选择配置为 dunshan 关键字。</p><pre><code class="language-yaml">server:\n  port: 6000\n\nspring:\n  application:\n    name: dunshan-gateway\n  sleuth:\n    sampler: #采样器\n      probability: 1.0 #采样率，采样率是采集 Trace 的比率，默认 0.1，百分之百采集\n      rate: 10000 #每秒数据采集量，最多 n 条/秒 Trace\n    propagation:\n      tag:\n        whitelisted-keys:\n          - dunshan\n        enabled: true\n    propagation-keys:\n      - dunshan\n    web: # Web 组件的配置项，例如说 SpringMVC\n      enabled: true\n  zipkin: #设置 zipkin 服务端地址\n    base-url: http://127.0.0.1:9411\n\n  cloud:\n    # Spring Cloud Gateway 配置项，对应 GatewayProperties 类\n    nacos:\n      discovery:\n        server-addr: localhost:8848\n        username: nacos\n        password: nacos\n    gateway:\n      discovery:\n        locator:\n          enabled: true\n          lower-case-service-id: true #使用小写 service-id\n      # 路由配置项，对应 RouteDefinition 数组\n      routes:\n        - id: dunshan-member\n          uri: lb://dunshan-member\n          predicates:\n            - Path=/api/member/**\n          filters:\n            - StripPrefix=2\n</code></pre><p>完成网关服务配置后，可以启动 Nacos、Zipkin 还有网关服务，这时打开 Nacos，就能看到网关服务了。</p><p><img src="https://static001.geekbang.org/resource/image/ed/8e/ed6f5619fc119b5ea4ab188f145bfa8e.png?wh=1762x562" alt="图片"></p><p>对于整个标记透传过程来说，我们还需要搭建会员系统、购物车系统、订单系统来完成整个链路跟踪预演。</p><p>所以接下来，我们就依次搭建和部署一下其他服务。在这里，我用会员服务系统做例子，其他服务的搭建过程是一样的，我就不重复演示了。你只需要注意修改每个服务配置文件中的服务名字和端口号就可以了。</p><ul>\n<li>新建会员系统</li>\n</ul><p>新建 dunshan-member 会员应用系统，在 pom.xml 配置文件中添加如下配置：</p><pre><code class="language-xml">&lt;dependencies&gt;\n        &lt;!--Spring Web 应用 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--Nacos 客户端 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!--服务间通信组件 OpenFeign --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\n            &lt;version&gt;2.2.6.RELEASE&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;!--添加 Sleuth 依赖 --&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;\n            &lt;version&gt;${sleuth.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;!--Zipkin 客户端--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;\n            &lt;version&gt;${zipkin.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n</code></pre><p>添加依赖包后，在 Controller 控制器下新建 MemberController 类，新建这个类的主要功能是暴露一个 get 接口请求，同时，它还会调用购物车服务接口，会员系统请求资源路径为 /member/uer/{id}  ，响应为 Object。</p><pre><code class="language-java">package com.member.controller;\n\nimport com.member.feign.CartServiceFeignClient;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RestController;\nimport javax.annotation.Resource;\nimport javax.servlet.http.HttpServletRequest;\nimport java.util.HashMap;\n\n/**\n * @author dunshan\n * @description: 会员服务 index\n * @date 2021-10-02 14:47:29\n */\n \n@RestController\npublic class MemeberController {\n    @Resource\n    CartServiceFeignClient cartFeignClient;\n\n    @Autowired\n    HttpServletRequest request;\n    @GetMapping("/member/user/{id}")\n    public Object memberUser(@PathVariable(value = "id") Integer id) {\n        String dunshan = request.getHeader("dunshan");\n        String result = cartFeignClient.openCartInfo(id);\n        result = "-&gt; dunshan-member" + result;\n        HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;();\n        map.put("data", result);\n        map.put("dunshan", dunshan);\n        return map;\n    }\n}\n</code></pre><p>你可以采用 OpenFeign 来完成会员服务和购物车服务、订单服务之间的调用。</p><p>例如，你可以参考我给出的代码，在会员服务中新建 CartServiceFeignClient 来远程调用 Cart 服务。</p><pre><code class="language-java">package com.dunshan.member.feign;\n\nimport org.springframework.cloud.openfeign.FeignClient;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\n\n/**\n * @author dunshan\n * @description: 购物车 feign\n * @date 2021-10-02 14:45:01\n */\n \n@FeignClient("dunshan-cart")\npublic interface CartServiceFeignClient {\n\n    @GetMapping("/cart/user/{id}")\n    String openCartInfo(@PathVariable(value="id") Integer id);\n\n}\n</code></pre><p>会员服务系统配置文件如下：</p><pre><code class="language-yaml">server:\n  port: 7000\nspring:\n  cloud:\n    nacos:\n      discovery:\n        server-addr: localhost:8848\n        username: nacos\n        password: nacos\n  application:\n    name: dunshan-member\n  sleuth:\n    propagation:\n      tag:\n        whitelisted-keys:\n          - dunshan\n        enabled: true\n    sampler: #采样器\n      probability: 1.0 #采样率，采样率是采集 Trace 的比率，默认 0.1\n      rate: 10000 #每秒数据采集量，最多 n 条/秒 Trace\n    propagation-keys:\n      - dunshan\n  zipkin: #设置 zipkin 服务端地址\n    base-url: http://127.0.0.1:9411\nlogging:\n  level:\n    root: info #为演示需要，开启 debug 级别日志\n</code></pre><p>到这里，我们的会员服务系统就建好了。我来总结一下会员服务系统开发的主要步骤：</p><ol>\n<li>新建会员服务工程；</li>\n<li>在 pom.xml 文件中加入依赖；</li>\n<li>在控制层新增 get 请求，把请求数据响应给前端用户。根据 demo 设计思路，这里需要调用购物车接口，所以需要注入购物车接口；</li>\n<li>在 Feign 包下新增可以远程调用购物车请求的资源接口；</li>\n<li>在 application.yml 配置中添加 Zipkin 服务地址还有其他信息。</li>\n</ol><p>上面 5 个步骤完成之后，我们的会员服务系统就开发好了。接下来，我们要重复上面 5 个步骤，把购物车服务和订单服务都搭建起来。</p><p>你也可以看一下我们搭建出来的整个项目的截图。</p><p><img src="https://static001.geekbang.org/resource/image/8b/86/8b6731e368642987b3edeb2a6c2c8486.png?wh=594x274" alt="图片"></p><p>项目搭建好之后，启动项目服务，你就能在 Nacos 中看到全部的服务注册情况了。下面是 Nacos 注册中心显示的服务。</p><p><img src="https://static001.geekbang.org/resource/image/03/9c/03b1eb7567e80a03bc1b8c42f00e949c.png?wh=1670x690" alt="图片"></p><p>好了，现在我们已经把整个微服务 demo 搭建好了，下面的问题是，要怎么实现它呢？</p><h3>获取压测标记</h3><p>首先，我们要获取压测标记。</p><p>前面我们在网关服务中设计了全局过滤器，只要添加标记到  Span Baggage 就能在每个微服务中使用了。怎么使用呢，下面我用订单服务做个实验：</p><p>在网关服务的 Span Context 中设置 Baggage，具体如下：</p><pre><code class="language-java"># 放入\ntracer.currentSpan().tag("dunshan", dunshan); \n\n# 取出\nBaggageField dunshan = BaggageField.getByName("dunshan");\n</code></pre><p>如果在下游服务中获取标记成功，就证明微服务 demo 实验成功了。<br>\n那么我们在订单服务中的 Controller 层新增 OrderController 类，尝试获取压测标记。</p><p>具体代码参考如下：</p><pre><code class="language-java">package com.dunshan.order.controller;\n\nimport brave.Tracer;\nimport brave.baggage.BaggageField;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RestController;\nimport javax.servlet.http.HttpServletRequest;\n\n/**\n * @author dunshan\n * @date 2021-11-12 17:47:52\n */\n \n@RestController\npublic class OrderController {\n    @Autowired\n    HttpServletRequest request;\n    @Autowired\n    Tracer tracer;\n    @GetMapping("/order/user/{id}")\n    public String orderInfo(@PathVariable(value = "id") Integer id) {\n        String result = null;\n        //使用 brave 类获取透传标记信息\n        BaggageField dunsha = BaggageField.getByName("dunshan");\n        String tagInfo = dunsha.getValue();\n        System.out.println("=============\\n" + tagInfo);\n        //使用 HttpServletRequest 获取标记\n        String dunshan = request.getHeader("dunshan");\n        if ("7DGroup".equals(dunshan)) {\n            result = tagInfo + "-&gt;-order-&gt;" + id;\n            return result;\n        }\n        result = " --&gt; 我是没有头信息-&gt; dunshan-order";\n        return result;\n    }\n}\n</code></pre><p>可以使用 &nbsp;HttpRequest Header&nbsp;和 Sleuth Baggage 这两种方式来获取标记信息，你可以根据实际情况灵活选择，这里采用 BaggageField.getByName(“dunshan”) 获取标记，获取压测标记后，可以直接放入数据上下文中。</p><h3>测试接口</h3><p>完成整个服务搭建 demo 后，我们在 JMeter 中添加 Http request 请求，验证一下订单服务能不能获取标记。</p><p><img src="https://static001.geekbang.org/resource/image/2y/5c/2yy235be2dc9c1dd1c8b861556471e5c.png?wh=1920x638" alt="图片"></p><p>在 JMeter HTTP Header Manager 中设置压测标记。</p><p><img src="https://static001.geekbang.org/resource/image/f9/4d/f9b4d8c367a9120e111c4d212353d84d.png?wh=1252x376" alt="图片"></p><p>然后，执行 JMeter 脚本来验证订单服务。</p><p><img src="https://static001.geekbang.org/resource/image/74/c5/748b327964de9af708067afe8797f5c5.png?wh=1445x653" alt="图片"></p><p>经过上面的 JMeter 操作之后，我们再来观察一下程序日志，日志显示了 JMeter 添加的标记。说明 demo 标记透传成功。</p><h3>抽取数据上下文对象</h3><p>​在 demo 预演获取压测标记验证通过后，我在前面介绍过，我们需要抽象出一层数据上下文（Context ）对象，我用订单系统做一下演示。</p><p>在订单系统中新建 config 包，再新建 AppContext 类：</p><pre><code class="language-java">package com.dunshan.order.config;\n\nimport com.alibaba.ttl.TransmittableThreadLocal;\nimport java.io.Serializable;\n\n/**\n * @author dunshan\n * @description: 数据上下文\n * @date 2021-11-12 17:53:39\n */\n \npublic class AppContext implements Serializable {\n    private static final TransmittableThreadLocal&lt;AppContext&gt; contextdunshan = new TransmittableThreadLocal&lt;&gt;();\n    private String flag;\n    public static AppContext getContext() {\n        return contextdunshan.get();\n    }\n    public static void setContext(AppContext context) {\n        contextdunshan.set(context);\n    }\n    public static void removeContext() {\n        contextdunshan.remove();\n    }\n    public String getFlag() {\n        return flag;\n    }\n    public void setFlag(String flag) {\n        this.flag = flag;\n    }\n}\n</code></pre><p>有了 TransmittableThreadLocal 类后，需要在 Filter 类中获取 Tag，然后存放到 AppContext 里面。只有这样， 才能确保 AppContext.getContext().getFlag() 方法在不同类中使用。</p><p>Filter 参考如下：</p><pre><code class="language-java">import brave.Tracer;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.cloud.gateway.filter.GatewayFilterChain;\nimport org.springframework.cloud.gateway.filter.GlobalFilter;\nimport org.springframework.http.server.reactive.ServerHttpRequest;\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.server.ServerWebExchange;\nimport reactor.core.publisher.Mono;\n\n/**\n * @author dunshan\n * @description: 全局过滤器\n * @date 2021-10-02 14:18:33\n */\n \n@Component\npublic class globalTagFilter implements GlobalFilter {\n\n    private final static Logger logger = LoggerFactory.getLogger(globalTagFilter.class);\n    private Tracer tracer;\n    \n    @Autowired\n    public void setTracer(Tracer tracer) {\n        this.tracer = tracer;\n    }\n\n    @Override\n    public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) {\n        String dunshan = exchange.getRequest().getHeaders().getFirst("dunshan");\n        if (dunshan != null) {\n            logger.info("dunshan：" + dunshan);\n            tracer.currentSpan().tag("dunshan", dunshan); //透传标签\n            ServerHttpRequest request = exchange.getRequest().mutate().header("dunshan", dunshan).build();\n            exchange = exchange.mutate().request(request).build();\n        }\n        return chain.filter(exchange);\n    }\n}\n</code></pre><p>有了数据上下文 Threadlocal 对象后，用户就可以随用随取。使用参考如下：</p><pre><code class="language-java">AppContext.getContext().setFlag("value"); # 放入压测标记\nAppContext.getContext().getFlag();        # 取出压测标记\n</code></pre><p>最后，我们就可以在订单服务中的 Controller 中获取数据上下文存储的压测标记。运行结果如下图：</p><p><img src="https://static001.geekbang.org/resource/image/fa/9e/fa5bb1c9ed6b02f3d6a8732e01673f9e.png?wh=1429x780" alt="图片"></p><h3>Zipkin 链路追踪</h3><p>刚才，我们已经看到业务应用成功获取了压测标记，下面我们打开链路追踪系统 Zipkin ，看看能不能正常显示透传的压测标记。</p><p>我们在浏览器中输入 <a href="http://127.0.0.1:9411/zipkin/">http://127.0.0.1:9411/zipkin</a> 点击 【找到一个痕迹】，再点击【run query】， 数据就显示出来了。</p><p><img src="https://static001.geekbang.org/resource/image/e6/1a/e606057ce9ac9d7be00ca8f28652391a.png?wh=1920x671" alt="图片"></p><p>选择一个请求，点击 show ，看到标签下面的自定义标签为 dunshan，它的值为 7DGroup。通过这里我们也能知道，压测标记能在每个服务中显示出来。</p><p><img src="https://static001.geekbang.org/resource/image/0c/56/0cbc103d3fe62d66bcaf18edce692f56.png?wh=1920x951" alt="图片"></p><p>为了验证网关服务、会员服务、购物车服务和订单服务标签是不是也会显示自定义标签，我们依次打开每个请求查看标签是不是已经改好了。</p><ul>\n<li>网关（Gateway）</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/70/bf/7047ed3002cfbb81eb725a1ee845afbf.png?wh=1920x620" alt="图片"></p><ul>\n<li>会员服务（Member）</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/2a/9b/2aec91864e2a859dde4f1bba75797f9b.png?wh=1920x956" alt="图片"></p><ul>\n<li>购物车服务（Cart）</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/7a/7e/7ab00df62560af4d39214c1ace56a57e.png?wh=1920x913" alt="图片"></p><ul>\n<li>订单服务（Order）</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/36/1d/3633401bc5ec82b4e00215780870611d.png?wh=1920x925" alt="图片"></p><p>在网关中添加压测标记到  Span Baggage 中，在 Zipkin 链路中也可以看到每个服务添加了自定义标签。</p><p>打开链路跟踪图，可以看到服务之间的链路调用是完整的。</p><p><img src="https://static001.geekbang.org/resource/image/b7/e6/b731177c0c4bba8b302df1738e5581e6.png?wh=1920x277" alt="图片"></p><p>经过本地 demo 预演，我们知道了，只要在网关中拦截并添加压测标记并放入  Span Baggage 中，其他下游服务就能够通过 Span Baggage 获取压测标记来区分流量了。我们还知道，在 Zipkin 中也是能显示压测标记的。</p><p>基于刚才 demo 的预演，接下来，我们尝试对真实系统进行改造。</p><h1>系统改造</h1><ul>\n<li>网关服务改造</li>\n</ul><p>首先，我们对网关服务进行改造。</p><p>有了本地 demo 的改造经验后，我们可以直接在 mall 项目中找到网关服务，然后在它的全局过滤类中添加 Tag 标签。其他服务只要加入 Zipkin 依赖 Jar 包就可以了。</p><p>具体操作是，打开 mall 项目，找到 mall-gateway 服务，再找到 AuthGlobalFilter 类，添加之前 demo 网关自定义的 Tag 标记信息。</p><p><img src="https://static001.geekbang.org/resource/image/ed/3b/ed50945112ef563b526a724a5e4b5d3b.png?wh=1920x1027" alt="图片"></p><p>全局配置修改完毕后，还需要在 application.yml 配置文件里增加配置。</p><p>因为整个 mall 项目比较复杂，我们先对 mall-member 和 mall-auth 服务进行改造，如果这两项改造成功，那其他服务只要增加配置文件就可以了。</p><ul>\n<li>认证服务改造</li>\n</ul><p>接下来，我们对改造的服务进行认证。</p><p>找到 mall-member 服务和 mall-auth 认证服务，在配置文件中添加如下配置内容。</p><p><img src="https://static001.geekbang.org/resource/image/ef/7c/efe51a341254ec3ec3da27aac34bc97c.png?wh=1644x1060" alt="图片"></p><p>上面是添加认证服务的过程示意图，可以看到，添加mall-member 服务配置文件和添加认证服务没有什么区别。添加配置文件后，我们就可以启动 mall-member 、 mall-auth 和 mall-gateway 服务了：</p><p><img src="https://static001.geekbang.org/resource/image/78/d3/78ff691b0da89b69c4d1e2541a303bd3.png?wh=1434x256" alt="图片"></p><ul>\n<li>JMeter 接口测试</li>\n</ul><p>启动项目后，打开 JMeter 注册接口。项目的注册脚本开发过程我就不在这里演示了，你可以参考这篇文章《 <a href="https://mp.weixin.qq.com/s/KHGfK7DUbSBcNOF6J8mb6Q">高楼的性能工程实战课之脚本开发</a> 》。</p><p>需要注意的是，执行注册接口需要增加 Header 信息，这样在 Zipkin 中就能识别自定义 tag 标签了。脚本如下：</p><p><img src="https://static001.geekbang.org/resource/image/44/95/44ee17bf77876e285f294b6d2f53c795.png?wh=1794x586" alt="图片"></p><p>头信息的位置我已经标注在图片中了。</p><p><img src="https://static001.geekbang.org/resource/image/20/4d/204cb6f258d041c14189dbcccf455d4d.png?wh=1518x382" alt="图片"></p><p>脚本执行结果：</p><p><img src="https://static001.geekbang.org/resource/image/07/0e/077a5b504b0620fb0d108ea4ffaf560e.png?wh=1082x268" alt="图片"></p><ul>\n<li>Zipkin 链路追踪</li>\n</ul><p>最后，我们打开 Zipkin 查看有没有自定义标记，如果有压测标记，说明标记透传改造成功。</p><p><img src="https://static001.geekbang.org/resource/image/bf/99/bf8e3bc248c735f93c691fbf1cd3f099.png?wh=1920x1317" alt="图片"></p><p>打开链路图也可以很清晰地看到目前的调用关系。</p><p><img src="https://static001.geekbang.org/resource/image/e5/3b/e5909b83d243866b3d1aaae8e30ea83b.png?wh=1920x410" alt="图片"></p><p>到这里，我们微服务电商项目的压测标记改造就完成了。</p><p>需要强调的是，你必须时刻记住我们做全链路压测标记追踪的目的。那就是，区分压测流量和线上流量，方便后续流量识别和隔离的动作。</p><h1>总结</h1><p>好了，这节课就讲到这里，我们使用 demo 预演和系统改造，完整演示了标记透传的改造过程。这节课有几个要点，我希望你能够记住：</p><ul>\n<li>通过抽象出一层数据上下文（Context ）对象，我们能够让压测标记兼容不同的应用协议，并支持跨线程间的透传、随时存取等功能；</li>\n<li>在跨服务间的标记透传方面，我们可以利用 Sleuth 服务间原生传递上下文的特性，但是在此基础上，我们可以在网关拦截并添加压测标记的属性到 Span Baggage 中，以保证后续跨服务传输中始终带着压测标记，这样也就免去了代码的硬耦合；</li>\n<li>下游服务可以使用 &nbsp;HttpRequest Header&nbsp;或 Sleuth Baggage 这两种方式来获压测标记信息，获取标记后，我们可以把它直接放到数据上下文中，方便后续业务的使用。</li>\n</ul><p>总之，标记透传使用的场景比较单一，它除了在全链路压测中用于区分流量，通常还会应用在线上灰度发布场景中。不过从技术实现来看，这两者逻辑是一致的。</p><p>下一节课，我们进入流量隔离环节，我会通过案例给你演示怎样实现各个组件的流量隔离。</p><h1>思考题</h1><p>在课程的最后，我还是照例给你留两道思考题：</p><ol>\n<li>为什么要引入数据上下文对象，它解决了什么问题？</li>\n<li>除了 Java 中的 ThreadLocal 对象，你还知道哪些语言有类似的方案？</li>\n</ol><p>欢迎你在留言区和我交流讨论，我们下节课见！</p>',
        article_title: "14 | 标记透传：如何基于微服务技术进行标记透传？",
      },
      {
        title: "15 | 流量隔离：MySQL数据库隔离是怎么做的？",
        id: 447129,
        content:
          '<p>你好，我是高楼。</p><p>这节课，我们详细讲讲怎样基于微服务技术进行 MySQL 数据库隔离的落地。</p><p>对线上压测来说，最重要的环节就是<strong>流量隔离</strong>了，而这其中最核心的又是<strong>数据库隔离</strong>，只有识别并隔离正式流量与压测流量才能避免产生<strong>脏数据</strong>。</p><h2>为什么要做数据库隔离？</h2><p><strong>为什么做数据库隔离呢？</strong>我给你举个例子。记得几年前，我当时所在的公司做线上压测。当时团队并没有做数据库隔离，而是直接在线上生产库进行压测，导致产生了很多脏数据。结果花费了很长时间去清理数据，但最终也没有完全清理干净，它不仅污染了生产数据还对线上业务产生了不好的影响，影响了用户体验。<strong>所以说做线上压测，数据库隔离是必须做的一个环节</strong>。</p><p>当然了，如果你的公司不差钱，也可以直接隔离出一套线下1:1镜像环境做压测。但是大部分公司并没有这么“土豪”，还得使用目前的生产坏境做压测，那么数据库隔离技术就必不可少了。</p><p>首先，我梳理了一下目前业界对于数据库隔离的主要解决方案，以及它们的优缺点和适用场景。</p><p><img src="https://static001.geekbang.org/resource/image/e9/dd/e96da5730379c3c94215f327069674dd.jpg?wh=1920x1080" alt="图片"></p><p>我们可以看到，根据不同的项目情况，可以选择不同的技术方案，<strong>这里最优、最安全的方案当然首推影子库</strong>，具体的优缺点上面表格已经写得非常清楚了。​</p><p>其实，上面这三种数据库隔离技术现在都已经很成熟了，要做到并不太难。但是出于各方面的原因，我们在网上很少看到过完整的技术解决方案，而有些公司把它们视作“独门秘方”，所以数据库隔离还远远谈不上普及。不过随着市场的发展，数据库隔离的面纱迟早会缓缓揭开。</p><!-- [[[read_end]]] --><p>对于我们这个电商项目，我选择采用影子库。因为数据偏移缺点很明显，不值得选择，而影子表代码修改量大，风险大不好控制。影子库呢，风险小，改动量小。综合权衡后，我们认为采用影子库性价比更高。</p><p>在进行影子库的改造之前，让我们先来看下目前项目的系统链路图。心中有链路图，才能知道在哪些地方改造。</p><p><img src="https://static001.geekbang.org/resource/image/76/67/7619f4a70f6f8ee62dfa8fd43711dc67.png?wh=1920x826" alt="图片"></p><p>上面是简单的链路图，你可以很清楚地知道，哪些服务与 MySQL 数据库有关系。为了方便你更直观地理解，我给你画了个思维导图。</p><p><img src="https://static001.geekbang.org/resource/image/96/6c/964818d5c9363fb9eab7d26fc40f0b6c.jpg?wh=1876x1688" alt="图片"></p><p>有了链路图做指导，接下来就是修改各个模块，进行具体的代码改造工作了。为了减少风险，我们可以先写一个 demo 做分库实例演示。如果 demo 没问题，再修改一个线上实例做实验。验证完毕并权衡风险后，再同步其他服务，只有这样反复验证才能把改造风险降到最低。</p><h2>影子库技术预演</h2><p>在做影子库改造之前，需要调研可以用什么技术做，心中有思路才好下手实践。</p><h3>准备工作</h3><p>首先，打开开发工具（如 Idea）新建 SpringBoot 工程（说明：这里用什么工具与什么工程无所谓，只要达到分库的目的就行）。既然是分库工作，就需要在 MySQL 数据库中新建两个数据库，如下图：</p><p><img src="https://static001.geekbang.org/resource/image/8d/e8/8d3f9d1e7506333533d673c52de7f3e8.png?wh=1722x534" alt="图片"></p><p>新建数据库参考语句如下：</p><pre><code class="language-sql">--正式库\nCREATE SCHEMA `mall_master` DEFAULT CHARACTER SET utf8mb4 ;\n--影子库\nCREATE SCHEMA `mall_shadow` DEFAULT CHARACTER SET utf8mb4 ;\n</code></pre><p>新建成功后如下图：<br>\n<img src="https://static001.geekbang.org/resource/image/fa/3d/faafe66d254a94a18936867c97d92a3d.png?wh=590x152" alt="图片"></p><p>到这里，数据库就建完了，但是光有库没有表是不行的，所以需要在两个新的数据库中新建一样的表，参考语句如下：</p><pre><code class="language-sql">DROP TABLE IF EXISTS `ums_admin`;\n\nCREATE TABLE `ums_admin` (\n  `id` bigint(20) NOT NULL AUTO_INCREMENT,\n  `user_name` varchar(64) DEFAULT NULL COMMENT \'用户名\',\n  `pass_word` varchar(64) DEFAULT NULL  COMMENT \'密码\',\n  `icon` varchar(500) DEFAULT NULL COMMENT \' Header 像\',\n  `email` varchar(100) DEFAULT NULL COMMENT \'邮箱\',\n  `nick_name` varchar(200) DEFAULT NULL COMMENT \'昵称\',\n  `note` varchar(500) DEFAULT NULL COMMENT \'备注信息\',\n  `create_time` datetime DEFAULT NULL COMMENT \'创建时间\',\n  `login_time` datetime DEFAULT NULL COMMENT \'最后登录时间\',\n  `status` int(1) DEFAULT \'1\' COMMENT \'帐号启用状态：0-&gt;禁用；1-&gt;启用\',\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT=\'管理员用户表\';\n</code></pre><p>打开项目，在 resources 目录下新建 application.yml文件，因为这次 demo 的目的是要使用两个数据源做数据库隔离，所以要配置两个数据源。你可以参考我给你提供的 application.yml 配置文件。</p><pre><code class="language-yaml">spring:\n  datasource:\n    master: # 数据源1\n      url: jdbc:mysql://localhost:3306/mall_master?characterEncoding=utf8&amp;useUnicode=true&amp;useSSL=false&amp;serverTimezone=GMT%2B8\n      username: root\n      password: lw123root\n      driver-class-name: com.mysql.cj.jdbc.Driver\n      type: com.alibaba.druid.pool.DruidDataSource\n      druid:\n        initialSize: 5\n        minIdle: 5\n        maxActive: 20\n        maxWait: 60000\n        timeBetweenEvictionRunsMillis: 60000\n        minEvictableIdleTimeMillis: 300000\n        validationQuery: SELECT 1 FROM DUAL\n        testWhileIdle: true\n        testOnBorrow: false\n        testOnReturn: false\n        poolPreparedStatements: true\n        #   配置监控统计拦截的filters，去掉后监控界面sql无法统计，\'wall\'用于防火墙\n        filters: stat,wall,log4j\n        maxPoolPreparedStatementPerConnectionSize: 20\n        useGlobalDataSourceStat: true\n        connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500\n    shadow: # 数据源2\n      url: jdbc:mysql://localhost:3306/mall_shadow?characterEncoding=utf8&amp;useUnicode=true&amp;useSSL=false&amp;serverTimezone=GMT%2B8\n      username: root\n      password: lw123root\n      driver-class-name: com.mysql.cj.jdbc.Driver\n      type: com.alibaba.druid.pool.DruidDataSource\n      druid:\n        initialSize: 5\n        minIdle: 5\n        maxActive: 20\n        maxWait: 60000\n        timeBetweenEvictionRunsMillis: 60000\n        minEvictableIdleTimeMillis: 300000\n        validationQuery: SELECT 1 FROM DUAL\n        testWhileIdle: true\n        testOnBorrow: false\n        testOnReturn: false\n        poolPreparedStatements: true\n        #   配置监控统计拦截的filters，去掉后监控界面sql无法统计，\'wall\'用于防火墙\n        filters: stat,wall,log4j\n        maxPoolPreparedStatementPerConnectionSize: 20\n        useGlobalDataSourceStat: true\n        connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500\n</code></pre><p>有了数据源配置文件，还需要读取配置文件的配置类。如果是单库实例数据源，用下面的代码读取数据源配置，项目就可以直接操作数据库了。</p><pre><code class="language-java"> @ConfigurationProperties(prefix = "spring.datasource")\n    @Bean\n    public DataSource druid() {\n        return new DruidDataSource();\n    }\n</code></pre><p>但是如果是多库数据源，就不能按这个方式读取数据源了。那么应该怎么读取数据源呢？我们来仔细看一下下面的代码。</p><p>SpringBoot 在读取配置文件时，大量采用 @ConfigurationProperties 注解，从代码中可以得知，单个读取实例采用这一注解就可以读取到数据配置文件。</p><p>我们还可以通过查看源码知道，该注解带了一个 “prefix ”属性，可以使用该属性读取数据源。另外，为了区分数据源，可以给不同读取数据源取不同的方法名字。</p><p><img src="https://static001.geekbang.org/resource/image/63/ac/63d5c6e03485a782195925e5f7bf51ac.png?wh=708x402" alt="图片"></p><p>你可以参考我的代码。</p><pre><code class="language-java">package com.dunshan.data.config;\n\nimport com.alibaba.druid.pool.DruidDataSource;\nimport com.alibaba.druid.support.http.StatViewServlet;\nimport com.alibaba.druid.support.http.WebStatFilter;\nimport lombok.extern.log4j.Log4j2;\nimport org.springframework.boot.context.properties.ConfigurationProperties;\nimport org.springframework.boot.web.servlet.FilterRegistrationBean;\nimport org.springframework.boot.web.servlet.ServletRegistrationBean;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.context.annotation.Primary;\nimport javax.sql.DataSource;\nimport java.util.HashMap;\nimport java.util.Map;\n\n/**\n * @author dunshan\n * @description: 数据库配置文件\n * @date 2021-08-15 11:01:31\n */\n \n@Log4j2\n@Configuration\npublic class DynamicDataSourceConfig {\n\n    /**\n     * 创建 shadow 数据源\n     */\n    @Bean(name = "shadowDataSource")\n    @ConfigurationProperties(prefix = "spring.datasource.shadow")\n    public DataSource shadowDataSource() {\n        return new DruidDataSource();\n    }\n    \n    /**\n     * 创建 master 数据源\n     */\n    @Bean(name = "masterDataSource")\n    @ConfigurationProperties(prefix = "spring.datasource.master")\n    public DataSource masterDataSource() {\n        return new DruidDataSource();\n    }\n    \n}\n</code></pre><p>数据源配置读取成功后，还得去思考怎么把数据源注入到上下文中，只有成功注入到上下文中才能使用该数据，操作数据库中的表。但是注入成功后，要根据什么切换数据源呢？<br>\n在切换数据源之前，我们先来回顾一下之前做的 demo。我们做 demo 是为了识别流量标记，那怎么产生流量标记呢？</p><p>我们只需要在 Http 请求中增加 Header 信息，业务层会根据 Header 信息判断特定标记来切换数据源，如 JMeter 中 HTTP Header Manager 就可以增加 Header 信息标记：</p><p><img src="https://static001.geekbang.org/resource/image/59/a7/5932e537d1338b0a2c09010cfdbeb6a7.png?wh=1622x366" alt="图片"></p><h3>数据源上下文的实现</h3><p>在目前的 Spring 开发中，多多少少都会涉及 AOP（Aspect-Oriented Programming：面向切面编程)，这里只用其中一个特性，大家慢慢往下看就知道了。</p><p>另外，在切换数据源中还需要使用一个技术就是&nbsp;ThreadLocal。但是因为 ThreadLocal 有点缺陷，所以我们这个场景使用 TransmittableThreadLocal 做数据源保存对象信息。这个在 <a href="https://time.geekbang.org/column/article/444794">第13讲</a>已经讲得很清楚了，我就不多赘述了。</p><p>动态数据源 TransmittableThreadLocal 的代码实现，你可以看看我给出的例子。</p><pre><code class="language-java">package com.dunshan.data.config;\n\nimport com.alibaba.ttl.TransmittableThreadLocal;\nimport org.springframework.jdbc.datasource.lookup.AbstractRoutingDataSource;\nimport javax.sql.DataSource;\nimport java.util.Map;\n\n/**\n * @author dunshan\n * @description: 动态数据源切换\n * @date 2021-08-15 12:35:14\n */\npublic class DynamicDataSource extends AbstractRoutingDataSource {\n   \n     private static final TransmittableThreadLocal&lt;String&gt; contextHolder = new TransmittableThreadLocal&lt;&gt;();\n    \n    /**\n     * 配置DataSource, defaultTargetDataSource为主数据库\n     */\n    public DynamicDataSource(DataSource defaultTargetDataSource, Map&lt;Object, Object&gt; targetDataSources) {\n        super.setDefaultTargetDataSource(defaultTargetDataSource);\n        super.setTargetDataSources(targetDataSources);\n        super.afterPropertiesSet();\n    }\n    \n    @Override\n    protected Object determineCurrentLookupKey() {\n        return getDataSource();\n    }\n    \n    public static void setDataSource(String dataSource) {\n        contextHolder.set(dataSource);\n    }\n    \n    public static String getDataSource() {\n        return contextHolder.get();\n    }\n    \n    public static void clearDataSource() {\n        contextHolder.remove();\n    }\n}\n\n</code></pre><p>接下来要把数据源注入上下文中，只有这样数据源才能在系统中灵活使用。</p><pre><code class="language-java">    /**\n     * 如果还有数据源,在这继续添加 DataSource Bean\n     */\n     \n    @Bean\n    @Primary\n    public DynamicDataSource dataSource(DataSource masterDataSource, DataSource shadowDataSource) {\n        Map&lt;Object, Object&gt; targetDataSources = new HashMap&lt;&gt;(2);\n      targetDataSources.put(DataSourceNames.SHADOW, shadowDataSource);\n      targetDataSources.put(DataSourceNames.MASTER, masterDataSource);\n        // 还有数据源,在targetDataSources中继续添加\n        log.info("DataSources:" + targetDataSources);\n        return new DynamicDataSource(masterDataSource, targetDataSources);\n    }\n</code></pre><p>做好以上这些准备工作后，我们就可以开始实现具体的标记识别和数据库隔离动作了。</p><p>下面，我们主要做以下两种获取标记方案的技术预演：</p><ul>\n<li>HttpRequest Header：从 HttpRequest Header 中获取标记，适合单服务、单一 HTTP 协议的场景；</li>\n<li>数据上下文：从数据上下文对象中获取标记，<strong>这是更为推荐的微服务标记透传方案</strong>。</li>\n</ul><h3>第一种：HttpRequest Header 方案</h3><p>主要逻辑如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/3d/a9/3d52080337e01b054be1aa3eb821dca9.jpg?wh=617x877" alt="图片"></p><p>数据源注入上下文后，刚才提到的 AOP 就派上用场了。这里你需要先学习 AOP 切面编程，使用切面编程拦截并获取 Header 信息。再结合 TransmittableThreadLocal 的特性进行数据源切换。</p><pre><code class="language-java">package com.dunshan.data.config;\n\nimport lombok.extern.log4j.Log4j2;\nimport org.aspectj.lang.JoinPoint;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.aspectj.lang.annotation.Before;\nimport org.aspectj.lang.annotation.Pointcut;\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.context.request.RequestContextHolder;\nimport org.springframework.web.context.request.ServletRequestAttributes;\n\nimport javax.servlet.http.HttpServletRequest;\nimport java.util.Arrays;\n\n/**\n * @author dunshan\n * @description: 数据源切换\n * @date 2021-08-15 12:46:32\n */\n \n@Log4j2\n@Aspect\n@Component\npublic class DataSourceAspect {\n\n\n\n    /**\n     * 切点: 所有配置 DataSource 注解的方法\n     */\n    @Pointcut("execution(public * com.dunshan.data.controller..*.*(..))")\n    public void controllerAspect() {\n    }\n\n    // 请求method前打印内容\n    @Before(value = "controllerAspect()")\n    public void methodBefore(JoinPoint joinPoint) {\n        ServletRequestAttributes requestAttributes = (ServletRequestAttributes) RequestContextHolder\n                .getRequestAttributes();\n        HttpServletRequest request = requestAttributes.getRequest();\n       \n         //获取 Header 标记\n        String header = request.getHeader("dunshan");\n        \n        // 打印请求内容\n        log.info("===============请求内容===============");\n        log.info("请求地址:" + request.getRequestURL().toString());\n        log.info("请求方式:" + request.getMethod());\n        log.info("请求类方法:" + joinPoint.getSignature());\n        log.info("请求类方法参数:" + Arrays.toString(joinPoint.getArgs()));\n        log.info("请求类方法 Header 信息:" + header);\n\n        // 通过 Header 信息判断\n        if (header != null &amp;&amp; header.equals("7DGroup")) {\n              DynamicDataSource.setDataSource(DataSourceNames.SHADOW);\n        } else {\n              DynamicDataSource.setDataSource(DataSourceNames.MASTER);\n        }\n\n    }\n</code></pre><p>我们可以看到，这里根据 Header 信息中包含的关键字“7DGroup”做流量判断。<br>\n上面的技术预演工作已经包含了整个 demo 的关键部分，之后就要验证 demo 能不能运行成功了。如果能成功，就可以移植到目前的工程中去了。</p><ul>\n<li>结果验证</li>\n</ul><p>好了，我们这就来验证一下。</p><p>先验证数据源读取是否正常：启动工程，查看日志。</p><p><img src="https://static001.geekbang.org/resource/image/20/ed/20e48d67b4ac08a19a2964635174c4ed.png?wh=1320x711" alt="图片"></p><p>观察启动信息，标示两个数据源已经读取成功。好，接下来就要验证操作能不能成功了。</p><ul>\n<li>正式请求（未加标记的请求）</li>\n</ul><p>上面的代码已经修改完成了，下面我们要来验证一下是否修改成功。目前我们使用 JMeter 做验证工具，在 JMeter 中增加插入接口请求，并且写入下面数据：</p><p><img src="https://static001.geekbang.org/resource/image/7a/8c/7ac95dbc0d1ac174yye8d518fc373c8c.png?wh=1920x610" alt="图片"></p><p>在 body 中的“昵称”中写入“性能测试-我是正常流量”，请求成功显示如下：</p><p><img src="https://static001.geekbang.org/resource/image/5e/f5/5ea87c5418d16448168bd861a968ccf5.png?wh=1404x570" alt="图片"></p><p>为了验证是否插入成功，需要打开 MySQL 输入查询语句。在查询前，要先通过日志判断出要使用哪个数据库，这样才能查到结果。</p><p><img src="https://static001.geekbang.org/resource/image/85/32/85cc2af8da0ef0yydb8d5d3befab1332.png?wh=1511x212" alt="图片"></p><p>上面的日志中， Header 信息为 null，根据早先的设计， Header 信息为 null 会走 master（正式库） 数据源 ，也就是 mall_master 库。</p><p>接下来，打开数据库工具执行 SQL，验证数据是否已经插入成功。</p><pre><code class="language-sql">SELECT * FROM mall_master.ums_admin;\n</code></pre><p><img src="https://static001.geekbang.org/resource/image/f2/d8/f248b5bb921a22f8688aa18de1655dd8.png?wh=1500x211" alt="图片"></p><p>从界面截图中可以看出，显示的数据与压力工具执行的数据一致。</p><ul>\n<li>压测请求（加标记的请求）</li>\n</ul><p>在 JMeter 中增加 HTTP Header Manager 组件，并且在 Header 信息增加如下标志，再次执行操作，验证数据是否进入压测数据库。如果成功，说明目前的 demo 是有效的。</p><p><img src="https://static001.geekbang.org/resource/image/74/62/742584f24110d6025b50693b98d23d62.png?wh=1242x400" alt="图片"></p><p>打开 JMeter，在 View Results Tree 中查看结果。</p><p><img src="https://static001.geekbang.org/resource/image/85/a3/854be53a09b5b366596cf72da3d4d5a3.png?wh=1398x572" alt="图片"></p><p>在 SpringBoot 工程中查看请求日志，验证是否成功切换数据源，日志显示目前的标记为 7DGroup ，因而应该走 shadow（影子库）数据源，也就是数据源中的 mall_shadow 库。</p><p><img src="https://static001.geekbang.org/resource/image/yy/d1/yy268e668bbec292069312dc10b92ed1.png?wh=1502x247" alt="图片"></p><p>根据日志显示的信息，打开 mall_shadow 数据库执行 SQL ，验证数据是否已经插入该数据库。</p><pre><code class="language-sql">SELECT * FROM mall_shadow.ums_admin;\n</code></pre><p><img src="https://static001.geekbang.org/resource/image/4d/15/4d36793754b8f2bfd2b9dd4c3f5f3f15.png?wh=1490x225" alt="图片"></p><p>从上面的结果可以看出，目前的数据已经进入预期的数据库，也达成了 demo 技术预演目标。之前已经说过，只要 demo 能完成数据库切换，并且数据正常，那么就需要把目前的配置文件和相关类移植到正式系统中去，只有这样才能低风险完成系统改造。</p><h3>第二种：数据上下文方案</h3><p>在第一种方案中，最关键的一点是要通过 AOP 拦截切换，而我们下面这个方案，则是通过数据上下文切换获取压测标记和 AOP 切换数据源，其它的内容都是一样的。</p><p>在开始演示之前，我们先回顾一下上一讲提到过的方案图：</p><p><img src="https://static001.geekbang.org/resource/image/99/05/99ee4d4daa9fe5fe78d99c2d8fdc8105.jpg?wh=1920x1922" alt="图片"></p><p>这里实现的核心逻辑图如下：</p><p><img src="https://static001.geekbang.org/resource/image/4e/bf/4e09a1ca5e528b306697eccc9f0eedbf.jpg?wh=1165x2065" alt="图片"></p><p>我们还是使用上一节课搭建的环境，如果忘了这个环境怎么搭，你可以到上一讲复习一下。</p><p>首先，启动项目，显示如下：</p><p><img src="https://static001.geekbang.org/resource/image/38/e1/3818be42a830468aa5147b1bfbcee3e1.png?wh=300x98" alt="图片"></p><p>可以看到，Nacos 注册中心有网关、会员、购物车、订单服务。</p><p><img src="https://static001.geekbang.org/resource/image/b9/ac/b9c1c74f093289ee0d0e5a8152d110ac.png?wh=1920x504" alt="图片"></p><p>在上一节课，我们已经把标记传入到了每个服务中，同时还存入了对应的数据上下文里，所以在这里，我们只要在业务层获取标记，判断是不是压测标记，然后做对应的数据源切换动作就可以了。</p><p>这里，我们打开上一节课的项目，在订单服务中的 application.yml 文件中添加双数据库链接，你可以参考下面的配置：</p><pre><code class="language-yaml">spring:\n  datasource:\n    master: # 数据源1\n      username: root\n      password: dunshan123root\n      url: jdbc:mysql://localhost:3306/mall_master?useUnicode=true&amp;characterEncoding=UTF-8&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC&amp;queryInterceptors=brave.mysql8.TracingQueryInterceptor&amp;zipkinServiceName=mall_master\n      type: com.alibaba.druid.pool.DruidDataSource\n      druid:\n        initialSize: 5\n        minIdle: 5\n        maxActive: 20\n        maxWait: 60000\n        timeBetweenEvictionRunsMillis: 60000\n        minEvictableIdleTimeMillis: 300000\n        validationQuery: SELECT 1 FROM DUAL\n        testWhileIdle: true\n        testOnBorrow: false\n        testOnReturn: false\n        poolPreparedStatements: true\n        #   配置监控统计拦截的filters，去掉后监控界面sql无法统计，\'wall\'用于防火墙\n        filters: stat,wall,log4j\n        maxPoolPreparedStatementPerConnectionSize: 20\n        useGlobalDataSourceStat: true\n        connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500\n    shadow: # 数据源2\n      url: jdbc:mysql://localhost:3306/mall_shadow?useUnicode=true&amp;characterEncoding=UTF-8&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC&amp;queryInterceptors=brave.mysql8.TracingQueryInterceptor&amp;zipkinServiceName=mall_shadow\n      username: root\n      password: dunshan123root\n      druid:\n        initial-size: 10 #连接池初始化大小\n        min-idle: 10 #最小空闲连接数\n        max-active: 20 #最大连接数\n        web-stat-filter:\n          exclusions: "*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/*" #不统计这些请求数据\n        stat-view-servlet: #访问监控网页的登录用户名和密码\n          login-username: druid\n          login-password: druid\n</code></pre><p>想想上节课我们讲的，应该在什么地方获取和设置标记。这样可以方便下一步操作。具体操作可以参考我在文稿中给出的截图。</p><p><img src="https://static001.geekbang.org/resource/image/1f/55/1fc2d15d66b86e9d780f5a7f70b8d155.png?wh=1920x924" alt="图片"></p><p>简单解释下，这里我们添加了一个全局 Filter 拦截器，拦截全部请求，通过 BaggageField.getByName(“dunshan”) 获取透传标记，再把标记放入数据上下文中。</p><p>接下来，我们查看一下数据上下文类，这个类的主要目的是把压测标记保存到 TransmittableThreadLocal 中去。</p><pre><code class="language-java">\n/**\n * @author dunshan\n * @description: 数据上下文\n * @date 2021-11-18 23:12:10\n */\npublic class AppContext implements Serializable {\n    private static final TransmittableThreadLocal&lt;AppContext&gt; contextdunshan = new TransmittableThreadLocal&lt;&gt;();\n    private String flag;\n    public static AppContext getContext() {\n        return contextdunshan.get();\n    }\n    public static void setContext(AppContext context) {\n        contextdunshan.set(context);\n    }\n    public static void removeContext() {\n        contextdunshan.remove();\n    }\n    public String getFlag() {\n        return flag;\n    }\n    public void setFlag(String flag) {\n        this.flag = flag;\n    }\n}\n\n</code></pre><p>下一步还是使用 AOP 判断请求，这次，我们通过数据上下文获取标记来切换数据源链接，你可以参考我给出的代码：</p><pre><code class="language-java">@Log4j2\n@Component\n@Aspect\npublic class AopMyDbSwitch {\n    /**\n     * 拦截入口下所有的 public方法\n     */\n    @Pointcut("execution(public * com.dunshan.order.controller..*(..))")\n    public void pointCutAround() {\n    }\n    /**\n     *根据数据上下文切换数据源\n     */\n    @Before(value = "pointCutAround()")\n    public void around(JoinPoint point) {\n        AppContext context = AppContext.getContext();\n        String flag = context.getFlag();\n        \n        if (StringUtils.isNotEmpty(flag) &amp;&amp; flag.equals(DataSourceNames.HEAD)) {\n            //影子库\n            MDC.put("dunshan", "shadow");\n            DynamicDataSource.setDataSource(DataSourceNames.SHADOW);\n        } else {\n            //正式库\n            MDC.put("dunshan", "produce");\n            DynamicDataSource.setDataSource(DataSourceNames.MASTER);\n        }\n    }\n}\n</code></pre><p><strong>注意：除了刚才所讲的这些不一样的地方以外，其他配置与第一种方案都是一样的。</strong><br>\n有上面的配置后，我们再查看一下数据库表是否做了区分。先看 mall_master 中的 ums_admin表，再看 mall_shadow 中的 ums_admin 表。</p><p>mall_master 数据库中 ums_admin 表显示一条内容为 master：</p><pre><code class="language-sql">use mall_master;\nselect * from ums_admin;\n</code></pre><p><img src="https://static001.geekbang.org/resource/image/cc/75/cc57a1046256f398dda747ca31762475.png?wh=1600x360" alt="图片"></p><p>mall_shadow 数据库中的 ums_admin 数据显示有一条记录包含 shadow：</p><pre><code class="language-sql">use mall_shadow;\nSELECT * FROM ums_admin;\n</code></pre><p><img src="https://static001.geekbang.org/resource/image/e3/46/e357e0000c110d16674bd31db2d08746.png?wh=1498x398" alt="图片"></p><p>数据库区分完毕后，我们再设计一个请求，用来模拟正式流量与压测流量查询这两个表，在会员系统中增加查询请求，参考如下：</p><pre><code class="language-java">/**\n * 查询用户信息\n *\n * @return\n */\n@GetMapping("/admin/info")\npublic Object selectAdmin() {\n    return cartFeignClient.selectInfo();\n}\n</code></pre><p>链路调用关系为网关 -&gt; 用户-&gt; 购物车 -&gt; 订单 -&gt; 数据库。<br>\n这里我们还是使用 JMeter 模拟压测流量与正常流量，你可以参考我给出的请求信息。</p><ul>\n<li>不添加请求 Header 标记（正式请求）</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/68/37/68a37aa327227c471d328d386eb40637.png?wh=1920x518" alt="图片"></p><ul>\n<li>添加请求 Header 标记（压测请求）</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/2c/95/2c03d9e1dac6bba26597aa65af01e295.png?wh=1920x396" alt="图片"></p><p>接下来，我们要验证结果是否跟预期想的一致。点击 JMeter 请求，在响应结果中查看结果。</p><ul>\n<li>不添加请求 Header 标记（正式请求）</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/1e/ce/1e9512fdf1db5b43168ea9c3ceccfbce.png?wh=1134x572" alt="图片"></p><ul>\n<li>添加请求 Header 标记（压测请求）</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/55/a3/554656e57b399f4794da157e049b7aa3.png?wh=1244x670" alt="图片"></p><p>可以看到，数据已经按预期显示出来了。</p><p>我们再打开 Zipkin 连接跟踪，验证下链路追踪结果。</p><ul>\n<li>不添加请求 Header 标记（正式库）</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/64/e2/648f9be5f21048e2ba920516973797e2.png?wh=1920x438" alt="图片"></p><ul>\n<li>添加请求 Header 标记（影子库）</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/74/3a/7427433a234112c83453212cdc59123a.png?wh=1920x481" alt="图片"></p><p>Zipkin 链路全貌图为：</p><p><img src="https://static001.geekbang.org/resource/image/6b/f4/6bd3dca5622bd201b0505424470364f4.png?wh=1920x434" alt="图片"></p><p>从上面的结果可以看出，目前的数据已经进入到了预期的数据库中，也达成了通过获取数据上下文切换数据库技术预演的目标。之前已经说过了，只要 demo 能完成数据库切换，并且数据正常，那么就可以把目前的配置文件和相关类移植到正式系统中去了。</p><h2>系统改造</h2><p>接下来我们就看看真实系统的改造。</p><p>首先对 mall-member（会员系统）进行改造。接下来我们快速把 demo 的类与配置移植到对应工程中去。</p><h3>添加全局 Filter 过滤器</h3><p>因为我们上一讲已经演示过在全局 Filter 中把标记加入到数据上下文中的逻辑，所以这里的改造就比较轻松了。我们可以快速在服务中添加全局 Filte 过滤器 。</p><p><img src="https://static001.geekbang.org/resource/image/9a/4b/9ab23782cb816823a2989f732254764b.png?wh=1274x1184" alt="图片"></p><h3>配置 AOP 切面</h3><p>然后，在 AOP 类中修改 Pointcut 类中的注解。需要注意修改中间的文本框，其它地方与 demo 中保持一致即可。</p><p><img src="https://static001.geekbang.org/resource/image/38/c9/3870db5171b3726536e4477e03143ec9.png?wh=1400x266" alt="图片"></p><p>具体的代码你可以参考我给出的图片。</p><p><img src="https://static001.geekbang.org/resource/image/6e/2d/6e2fa4716ebca9507edd64e6f9d70e2d.png?wh=1486x1682" alt="图片"></p><p>我们通过 AOP 拦截服务请求，实时判断数据上下文的标记类型，并存入设置对应的数据源上下文。</p><h3>配置多数据源</h3><p>AOP 配置类替换成功后，再把 application.yml 文件修改成多数据源，因为只有这样才能实现数据源切换。具体操作如下：</p><p><img src="https://static001.geekbang.org/resource/image/19/0b/19c6a67f0db592d6bb434336f404ee0b.png?wh=1920x1015" alt="图片"></p><p>改造完成后，我们启动工程，验证数据库是否成功。因为这个工程比较复杂，所以还需要启动网关服务和认证服务，启动的数据库启动日志如下：</p><p><img src="https://static001.geekbang.org/resource/image/2c/34/2c9d888af95a3a8d02b2d0659ff7c734.png?wh=1160x459" alt="图片"></p><p>启动系统如下：</p><p><img src="https://static001.geekbang.org/resource/image/f2/03/f2d3f60cc9496204aec733878ea3a903.png?wh=801x255" alt="图片"></p><h3>验证改造结果</h3><p>打开接口文档，在 JMeter 中模拟注册用户接口，关于注册接口怎么开发，你可以参考《<a href="https://mp.weixin.qq.com/s/KHGfK7DUbSBcNOF6J8mb6Q">高楼的性能工程实战课之脚本开发</a>》，验证 mall-member （会员系统）数据源是否切换成功。</p><ul>\n<li>压测请求（带 Header 标记）</li>\n</ul><p>脚本编写成功后，执行注册流程，执行成功后打开工程查看日志：</p><p><img src="https://static001.geekbang.org/resource/image/ed/16/ed791084acb7b389a0f9f93f5a6e5116.png?wh=1500x366" alt="图片"></p><p>在我给出的日志截图中，7dTest005 是新注册的用户名。7DGroup 是 HTTP Header 标记，这是压测标记，带有它的数据都要进入 shadow（影子）数据库。</p><p>根据提示与配置信息，再到数据库中查询数据是否注册成功，仔细观察目前的用户名为7dTest005。执行 SQL 语句之前，先分析下目前执行的数据库是哪个？</p><p>上面的日志信息提示的是压测流量标记，找到数据源为 shadow，执行用户名 SQL 语句。检查一下日志中用户 7dTest005 是否已经注册成功。</p><pre><code class="language-sql">select * from shadow.ums_member;\n</code></pre><p><img src="https://static001.geekbang.org/resource/image/6b/c5/6bd630049yydcdb64ba3d52109cd92c5.png?wh=1334x584" alt="图片"></p><p>刚才讲的是带 Header 标记验证，我们根据 SQL 查询结果知道已经注册成功了。这说明流量带压测标记进入了影子库，下面我们再来验证不带标记是否也会正常进入正式库。</p><ul>\n<li>正常请求（无 Header 标记）</li>\n</ul><p>我们把注册脚本中的 Header 标记去掉，修改用户名执行脚本请求，成功后查看工程日志。如果没有Header 标记也能成功注册脚本，表示 mall-member 系统改造成功。</p><p>执行成功后查看日志：</p><p><img src="https://static001.geekbang.org/resource/image/3d/76/3dbf27fd57f57c99e4b3fafb6af71a76.png?wh=1500x362" alt="图片"></p><p>从我给出的日志截图可以看出，目前流量正常，而且注册的用户名是 7dTest1188，根据代码可知目前走的数据源是 master（正式库）。执行用户 SQL，验证数据是否已经走入 master 数据库，</p><p>从下图执行结果可以看出，目前数据已经插入预期的数据库中。</p><pre><code class="language-sql">select * from master.ums_member;\n</code></pre><p><img src="https://static001.geekbang.org/resource/image/13/10/1316923170f0fda99cc1e5e9e267e410.png?wh=1495x592" alt="图片"></p><p>完成了上述一系列操作后，对 mall-member （会员系统）的改造就完成了。有了成功改造 mall-member 影子库的经验，我们就可以沿用这种方法落地改造到其他系统中去了。</p><h2>总结</h2><p>好了，我们今天就讲到这里了。在课程的最后，我再带你回顾一下课程内容。这节课，我们主要讲了落地 MySQL 数据库隔离的方法。</p><p>这里有几个要点希望你能记住：</p><ol>\n<li>\n<p>多数据源是实现影子库的基础；</p>\n</li>\n<li>\n<p>从数据上下文对象中获取标记，这是更为推荐的微服务标记透传方案；</p>\n</li>\n<li>\n<p>AOP 结合 TransmittableThreadLocal 是实现数据源动态切换的好组合；</p>\n</li>\n<li>\n<p>从风险管控的角度，我建议你先做 demo 技术预演，再做单个系统改造，最后再同步其余系统。</p>\n</li>\n</ol><p>可以说，在全链路压测中，<strong>数据库隔离是最重要的一个改造环节了</strong>。因为做得不到位，压测流量就会污染生产数据，导致的后果非常严重。这节课给出的影子库方案，是改造成本低，效果最好的一种方式。希望你能够充分地理解并用好它。</p><h2>思考题</h2><p>最后，我想请你思考几个问题：</p><ol>\n<li>\n<p>为什么说数据库隔离是全链路压测中改造的核心环节？</p>\n</li>\n<li>\n<p>除了我列出的这三种，你还有没有其他方式可以实现数据库隔离？</p>\n</li>\n</ol><p>欢迎你在留言区和我交流讨论。当然，你也可以把这节课分享给你身边的朋友，或许可以碰撞出更多新的想法。我们下节课再见！</p>',
        article_title: "15 | 流量隔离：MySQL数据库隔离是怎么做的？",
      },
      {
        title: "16 | 流量隔离：Redis 缓存隔离是怎么做的？",
        id: 448848,
        content:
          '<p>你好，我是高楼。</p><p>这节课，我们详细来讲讲如何基于微服务技术落地 Redis 缓存隔离。</p><p>在全链路压测的流量隔离中，有一个很重要的部分就是缓存隔离，即区分对应的 Cache。</p><p>说到缓存肯定离不开 Redis，因为高性能架构设计中都离不开它，在互联网和传统行业中呢，也都有它的身影。可以说，Redis 是性能项目中的必备知识点。</p><p>Redis 是一个 NoSQL 数据库，它使用的是内存存储的非关系型数据库，Redis 不使用表，使用 K-V 存储。</p><p>根据前面的经验我们知道，要改造什么就得知道它和其它组件的依赖关系，只有这样才能知道要对谁做改造。</p><p>所以，我又得祭出这个项目链路图了，脑中有策，心中有图，手中有码，改造才有路。</p><p><img src="https://static001.geekbang.org/resource/image/45/92/45cf5ddd861f77569f0bcc98c0437892.png?wh=1920x1149" alt="图片"></p><p>从链路图中我们可以看到，几乎所有业务系统（除了搜索）都和 Redis 组件有关系，所以，相关业务系统都得做缓存隔离改造，以保证正常流量与压测流量的缓存分离。</p><p>为了方便你更直观地理解，我给你画了个简要的思维导图。</p><p><img src="https://static001.geekbang.org/resource/image/cf/e3/cfe0663041e4591f71868f0718ba63e3.jpg?wh=1860x1685" alt="图片"></p><p>好了，搞懂了组件间依赖关系后，我们就要进入相关技术预演了。</p><h2>技术预演</h2><p>在正式动工之前，我们先回顾一下数据库隔离的方式。上一讲我们提过， MySQL 数据库隔离通常有三种方式，分别是数据偏移、影子库、影子表，Redis 和 MySQL 数据结构不同，所以 Redis 的缓存隔离技术会有些区别。</p><!-- [[[read_end]]] --><p>首先，我们来了解下目前业界对于缓存隔离的主要解决方案，以及它们的优缺点和适用场景。</p><p><img src="https://static001.geekbang.org/resource/image/13/1b/1325d9f1ab08582290eab3445874d41b.jpg?wh=1920x1080" alt="图片"></p><p>我们可以看到，根据不同的项目情况，可以选择不同的技术方案，<strong>这里最优、最安全的方案当然首推影子缓存（多实例）</strong>，具体的优缺点上面表格已经写得非常清楚了。​</p><p>下面我们来对缓存隔离里面的核心技术做下 demo 预演。</p><h3>缓存隔离落地</h3><p>在这里呢，我会主要介绍影子缓存和影子 key 两种方案。</p><ul>\n<li>第一种方案：影子缓存（多实例）</li>\n</ul><p>影子缓存（多实例）方案，首先要满足多数据源，然后确保它们分属两个物理上不同的Redis实例。在全链路压测过程中，业务层会识别标记，并选择对应的的 Redis 数据源，操作不同的 Redis 实例。</p><p>这里我给你画了一个简单的逻辑图：</p><p><img src="https://static001.geekbang.org/resource/image/8f/yy/8f3944b186162e0aa2e15aa13a4e81yy.jpg?wh=1290x417" alt=""></p><p>实现的具体操作步骤是：</p><p><strong>第一步：添加依赖。</strong></p><p>在 pom 中添加相关依赖。</p><pre><code class="language-xml">&lt;dependency&gt;\n   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n   &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n   &lt;groupId&gt;redis.clients&lt;/groupId&gt;\n   &lt;artifactId&gt;jedis&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre><p><strong>第二步：添加全局配置文件。</strong></p><p>新建工程，在配置文件 &nbsp;application.yml 中新增两个数据源配置，配置文件参考如下：</p><pre><code class="language-yaml">spring:\n  redis:\n    database: 1   # Redis数据库索引（默认为0）\n    host: 9.15.21.27  # Redis服务器地址\n    port: 16379  # Redis服务器连接端口\n    password:    # Redis服务器连接密码（默认为空）\n    timeout: 0  # 连接超时时间（毫秒）\n    pool:\n      max-active: -1 # 连接池最大连接数（使用负值表示没有限制）\n      max-wait: -1  # 连接池最大阻塞等待时间（使用负值表示没有限制）\n      max-idle: 8  # 连接池中的最大空闲连接\n      min-idle: 0  # 连接池中的最小空闲连接\n  redis2:\n    database: 2   # Redis数据库索引（默认为0）\n    host: 19.15.201.27  # Redis服务器地址\n    port: 16379  # Redis服务器连接端口\n    password:    # Redis服务器连接密码（默认为空）\n    timeout: 0  # 连接超时时间（毫秒）\n    pool:\n      max-active: -1 # 连接池最大连接数（使用负值表示没有限制）\n      max-wait: -1  # 连接池最大阻塞等待时间（使用负值表示没有限制）\n      max-idle: 8  # 连接池中的最大空闲连接\n      min-idle: 0  # 连接池中的最小空闲连接\n</code></pre><p><strong>第三步：添加读取配置类。</strong></p><p>再创建两个方法读取不同的数据源配置，参考代码如下：</p><pre><code class="language-java">import org.springframework.beans.factory.annotation.Value;\nimport org.springframework.cache.annotation.EnableCaching;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.context.annotation.Primary;\nimport org.springframework.data.redis.connection.RedisConnectionFactory;\nimport org.springframework.data.redis.connection.jedis.JedisConnectionFactory;\nimport org.springframework.data.redis.core.StringRedisTemplate;\nimport redis.clients.jedis.JedisPoolConfig;\n\n/**\n * @description:\n * @author: dunshan\n * @create: 2021-08-19 23:18\n **/\n@EnableCaching\n@Configuration\npublic class RedisDevConfiguration {\n    @Primary\n    @Bean(name = "slaveDataSource")\n    public StringRedisTemplate slaveDataSource(@Value("${spring.redis.host}") String hostName,\n                                             @Value("${spring.redis.port}") int port, @Value("${spring.redis.password}") String password,\n                                             @Value("${spring.redis.pool.max-idle}") int maxIdle, @Value("${spring.redis.pool.max-active}") int maxTotal,\n                                             @Value("${spring.redis.database}") int index, @Value("${spring.redis.pool.max-wait}") long maxWaitMillis, @Value("${spring.redis.pool.min-idle}") int minIdle) {\n        StringRedisTemplate temple = new StringRedisTemplate();\n        temple.setConnectionFactory(connectionFactory(hostName, port, password, maxIdle, maxTotal, index, maxWaitMillis, minIdle));\n        return temple;\n    }\n\n    @Bean(name = "masterDataSource")\n    public StringRedisTemplate masterDataSource(@Value("${spring.redis2.host}") String hostName,\n                                                @Value("${spring.redis2.port}") int port, @Value("${spring.redis2.password}") String password,\n                                                @Value("${spring.redis.pool.max-idle}") int maxIdle, @Value("${spring.redis.pool.max-active}") int maxTotal,\n                                                @Value("${spring.redis2.database}") int index, @Value("${spring.redis.pool.max-wait}") long maxWaitMillis, @Value("${spring.redis.pool.min-idle}") int minIdle) {\n        StringRedisTemplate temple = new StringRedisTemplate();\n        temple.setConnectionFactory(\n                connectionFactory(hostName, port, password, maxIdle, maxTotal, index, maxWaitMillis, minIdle));\n        return temple;\n    }\n    public RedisConnectionFactory connectionFactory(String hostName, int port, String password, int maxIdle,\n                                                    int maxTotal, int index, long maxWaitMillis, int minIdle) {\n        JedisConnectionFactory jedis = new JedisConnectionFactory();\n        jedis.setHostName(hostName);\n        jedis.setPort(port);\n        if (password != null) {\n            jedis.setPassword(password);\n        }\n        if (index != 0) {\n            jedis.setDatabase(index);\n        }\n        jedis.setPoolConfig(poolCofig(maxIdle, maxTotal, maxWaitMillis, minIdle));\n        // 初始化连接pool\n        jedis.afterPropertiesSet();\n        RedisConnectionFactory factory = jedis;\n        return factory;\n    }\n    public JedisPoolConfig poolCofig(int maxIdle, int maxTotal, long maxWaitMillis, int minIdle) {\n        JedisPoolConfig poolCofig = new JedisPoolConfig();\n        poolCofig.setMaxIdle(maxIdle);\n        poolCofig.setMaxTotal(maxTotal);\n        poolCofig.setMaxWaitMillis(maxWaitMillis);\n        poolCofig.setMinIdle(minIdle);\n        return poolCofig;\n    }\n}\n</code></pre><p><strong>第四步：验证结果。</strong></p><p>为了验证 Redis 的配置是否生效，我们新建一个测试类，在类里面中注入两个数据源：</p><pre><code class="language-java">   @Resource(name = "slaveDataSource")\n   private StringRedisTemplate slavetemplate;\n    \n   @Resource(name = "masterDataSource")\n   private StringRedisTemplate masterTemplate;\n\n   @Test\n   void contextLoads() {\n        slavetemplate.opsForValue().set("one", System.currentTimeMillis() + "我是正常流量标记");\n        System.out.println(slavetemplate.opsForValue().get("one"));\n        System.out.println("-----------");\n        masterTemplate.opsForValue().set("two", System.currentTimeMillis() + "我是压测流量标记");\n        System.out.println(masterTemplate.opsForValue().get("two"));\n    }\n</code></pre><p>最后，我们就可以把整个工程运行起来了，你可以参考文稿中的运行结果。</p><p><img src="https://static001.geekbang.org/resource/image/28/b7/2803e1b409a11f6a71c159636ba23cb7.png?wh=533x139" alt="图片"></p><p>需要说明是，影子缓存（单实例）方案跟上面的实现逻辑是一致的，只不过单实例的数据库数量是有上限的，一般为 16 个。</p><p><img src="https://static001.geekbang.org/resource/image/28/83/28ffd1af65a6e9cd00b5b9052a53ab83.png?wh=806x870" alt="图片"></p><p>不过在正式线上压测的时候，我还是推荐你使用多实例，这样能从物理上完全隔离掉生产缓存。</p><p>到这里，我们的第一种方案就已经验证通过了。接下来我们再来看看第二种方案。</p><ul>\n<li>第二种方案：影子 key</li>\n</ul><p>所谓影子 key，就是在同一个库中，用不同的 key 存储不同的 value。</p><p>它大致的逻辑是下面的样子。</p><p><img src="https://static001.geekbang.org/resource/image/5d/2f/5d1a3935ff0daed92e2e57817939ce2f.jpg?wh=881x275" alt=""></p><p>你可以看一下我给出的demo 代码。</p><pre><code class="language-java">    @Autowired\n    private RedisTemplate redisTemplate;\n\n    @GetMapping("/redis/{key}")\n    public void setRedisDemo(HttpServletRequest request,@PathVariable String key) {\n        String header = request.getHeader("7d");\n        if (header！= null &amp;&amp;"7dGroup".equals(header)) {\n            redisTemplate.opsForValue().set(key, "压测流量");\n            log.info("压测流量");\n        } else {\n            redisTemplate.opsForValue().set(key, "线上流量");\n        }\n    }\n</code></pre><p>它的基本原理就是，接口在 Header 信息中携带“ 7DGroup” 压测标记。然后，后台通过 HttpRequest 获取 Header 信息，业务层通过 Header 标记判断用对应 key 来操作缓存。</p><h3>数据源上下文实现</h3><p>上一节课，我们采用 AOP 技术完成了 MySQL 数据库的切换，你可以思考一下， Redis 是不是也可以用这种方式完成数据源切换。</p><p>其实确实是可以的。下面，我们就来演示怎么通过 AOP 技术完成 Redis 数据源的切换。</p><p>下面是一张简单的逻辑图：</p><p><img src="https://static001.geekbang.org/resource/image/82/91/8262b0af1bbf0135d5d36a51ab514491.jpg?wh=1512x635" alt="图片"></p><p><strong>第一步：添加全局配置文件。</strong></p><p>新建工程，在配置文件中添加数据源配置，参考代码如下：</p><pre><code class="language-yaml">spring:\n  http:\n    multipart:\n      max-file-size: 100MB\n      max-request-size: 100MB\n      enabled: true\n  redis:\n    database: 3\n    host: 9.105.1.27\n    port: 16379\n    password:    # 密码（默认为空）\n    timeout: 6000ms  # 连接超时时长（毫秒）\n    jedis:\n      pool:\n        max-active: 1000  # 连接池最大连接数（使用负值表示没有限制）\n        max-wait: -1ms      # 连接池最大阻塞等待时间（使用负值表示没有限制）\n        max-idle: 10      # 连接池中的最大空闲连接\n        min-idle: 5       # 连接池中的最小空闲连接\n  cache:\n    type: none\n\nresar:\n    oneDatabase: 5  #压测数据库\n</code></pre><p><strong>第二步：创建读取Redis配置文件类。</strong></p><p>有了配置文件，还得有读取配置文件类，只有这样才能正常读取自定义配置信息：</p><pre><code class="language-java">import org.springframework.boot.autoconfigure.AutoConfigureAfter;\nimport org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration;\nimport org.springframework.boot.autoconfigure.data.redis.RedisProperties;\nimport org.springframework.boot.context.properties.EnableConfigurationProperties;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.context.annotation.Primary;\nimport org.springframework.data.redis.connection.RedisPassword;\nimport org.springframework.data.redis.connection.RedisStandaloneConfiguration;\nimport org.springframework.data.redis.connection.jedis.JedisClientConfiguration;\nimport org.springframework.data.redis.connection.jedis.JedisConnectionFactory;\nimport redis.clients.jedis.JedisPoolConfig;\n\nimport java.time.Duration;\n\n/**\n * @description:读取数据源配置\n * @author: dunshan\n * @create: 2021-08-21 12:02\n **/\n@Configuration\n@AutoConfigureAfter(RedisAutoConfiguration.class)// 自动获取application.yml中的配置\n@EnableConfigurationProperties(RedisProperties.class)\npublic class RedisConfig {\n    private RedisProperties properties;\n\n    public RedisConfig(RedisProperties properties){\n        this.properties = properties;\n    }\n\n    @Bean\n    @Primary\n    public JedisConnectionFactory jedisConnectionFactory(){\n        RedisStandaloneConfiguration config = new RedisStandaloneConfiguration();\n        config.setHostName(properties.getHost());\n        config.setPort(properties.getPort());\n        config.setPassword(RedisPassword.of(properties.getPassword()));\n        config.setDatabase(properties.getDatabase());\n        return new JedisConnectionFactory(config, getJedisClientConfiguration());\n    }\n\n    private JedisClientConfiguration getJedisClientConfiguration() {\n        JedisClientConfiguration.JedisClientConfigurationBuilder builder = JedisClientConfiguration.builder();\n        if (properties.isSsl()) {\n            builder.useSsl();\n        }\n        if (properties.getTimeout() != null) {\n            Duration timeout = properties.getTimeout();\n            builder.readTimeout(timeout).connectTimeout(timeout);\n        }\n        RedisProperties.Pool pool = properties.getJedis().getPool();\n        if (pool != null) {\n            builder.usePooling().poolConfig(jedisPoolConfig(pool));\n        }\n        return builder.build();\n    }\n\n    private JedisPoolConfig jedisPoolConfig(RedisProperties.Pool pool) {\n        JedisPoolConfig config = new JedisPoolConfig();\n        config.setMaxTotal(pool.getMaxActive());\n        config.setMaxIdle(pool.getMaxIdle());\n        config.setMinIdle(pool.getMinIdle());\n        if (pool.getMaxWait() != null) {\n            config.setMaxWaitMillis(pool.getMaxWait().toMillis());\n        }\n        return config;\n    }\n\n    @Bean(name = "redisTemplate")\n    @Primary\n    public SelectableRedisTemplate redisTemplate() {\n        SelectableRedisTemplate redisTemplate = new SelectableRedisTemplate();\n        redisTemplate.setConnectionFactory(jedisConnectionFactory());\n        return redisTemplate;\n    }\n}\n</code></pre><p><strong>第三步：创建数据源上下文类</strong></p><p>我们这个场景还是使用 TransmittableThreadLocal 来保存数据源对象信息。这个在 <a href="https://time.geekbang.org/column/article/444794">第13讲</a> 已经讲得很清楚了，我就不多赘述了。</p><pre><code class="language-java">/**\n * @description: 数据源上下文\n * @author: dunshan\n * @create: 2021-08-21 11:50\n **/\npublic class RedisSelectSupport {\n    private static final TransmittableThreadLocal&lt;Integer&gt; SELECT_CONTEXT = new TransmittableThreadLocal&lt;&gt;();\n\n    public static void select(int db){\n        SELECT_CONTEXT.set(db);\n    }\n\n    public static Integer getSelect(){\n        return SELECT_CONTEXT.get();\n    }\n}\n</code></pre><p><strong>第四步：编写StringRedisTemplate 继承类。</strong></p><p>我们实现一个 RedisTemplate 来创建对应的 Redis 连接。</p><p>关键代码参考如下：</p><pre><code class="language-java">import org.springframework.data.redis.connection.RedisConnection;\nimport org.springframework.data.redis.core.StringRedisTemplate;\n\n/**\n * @description: 创建数据源连接\n * @author: dunshan\n * @create: 2021-08-21 11:49\n **/\npublic class SelectableRedisTemplate extends StringRedisTemplate {\n\n    @Override\n    protected RedisConnection createRedisConnectionProxy(RedisConnection pm) {\n        return super.createRedisConnectionProxy(pm);\n    }\n\n    @Override\n    protected RedisConnection preProcessConnection(RedisConnection connection, boolean existingConnection) {\n        Integer db;\n        if((db = RedisSelectSupport.getSelect()) != null){\n            connection.select(db);\n        }\n        return super.preProcessConnection(connection, existingConnection);\n    }\n\n}\n\n</code></pre><p>做好以上这些准备工作后，我们就可以开始实现后续的标记识别和缓存隔离动作了。</p><h3>标记数据上下文实现</h3><p>在做数据上下文的改造实现之前，我们还是先来回顾以下标记透传架构图。</p><p><img src="https://static001.geekbang.org/resource/image/99/05/99ee4d4daa9fe5fe78d99c2d8fdc8105.jpg?wh=1920x1922" alt="图片"></p><p>我们在<a href="https://time.geekbang.org/column/article/446320"> 14 讲</a>已经把标记透传逻辑讲得很清楚了，所以接下来，我们基于上面的标记透传方案，在业务层使用 AOP 拦截请求，做对应的数据源上下文设置即可。</p><p>这里使用的 demo 工程还是 14 讲的示例，主要包括网关、会员系统、购物车系统和订单系统 4 个服务：</p><p><img src="https://static001.geekbang.org/resource/image/ba/27/ba3d569dc9e4f2b98f21b60c46b7dc27.jpg?wh=1920x378" alt="图片"></p><p>首先，我们还是需要实现一个全局 Filter 过滤器，用它获取标记信息，然后将标记存放到数据上下文中。</p><p><img src="https://static001.geekbang.org/resource/image/1b/26/1bb36ac034c4367150392fbe9e250726.png?wh=1039x394" alt="图片"></p><p>标记存放到数据上下文后，我们就可以使用 AOP 拦截请求和判断对应的目标数据源了。</p><p>关键代码如下：</p><pre><code class="language-java">@Value("${spring.redis.master.database}")\nprivate int defaultDataBase;\n@Value("${spring.redis.shadow.database}")\nprivate int shadowDataBase;\n\n/**\n * 拦截入口下所有的 public方法\n */\n@Pointcut("execution(public * com.dunshan.order.controller..*(..))")\npublic void pointCutAround() {\n}\n\n/**\n * @param point\n * @throws Throwable\n */\n@Around(value = "pointCutAround()")\n@ConditionalOnBean(SelectableRedisTemplate.class)\npublic Object configRedis(ProceedingJoinPoint point) throws Throwable {\n    AppContext context = AppContext.getContext();\n    String flag = context.getFlag();\n    int db = defaultDataBase;\n    try {\n        if (flag != null &amp;&amp; flag.equals(DataSourceNames.HEAD)) {\n            db = shadowDataBase;\n            log.info("redis 压测流量：" + db);\n        } else {\n            db = defaultDataBase;\n            log.info("redis 正常流量: " + db);\n        }\n        RedisSelectSupport.select(db);\n        return point.proceed();\n    } finally {\n        RedisSelectSupport.select(defaultDataBase);\n        log.debug("redis switch {} to {}", defaultDataBase, db);\n    }\n}\n</code></pre><p>之所以说这段代码关键，是因为它能让我们从 &nbsp; AppContext.getContext() （数据上下文）中获取标记，从而判断是使用正式缓存还是影子缓存。<br>\n改造完成后，我们就可以验证缓存隔离效果是否达到预期了。</p><p>这里，我们测试同一个接口，切换对应两个不同的数据库，同 key 但不同 value ，具体而言：</p><ul>\n<li>正式缓存：db0，value为线上流量；</li>\n<li>影子缓存：db5，value为压测流量。</li>\n</ul><p>Redis 查询结果如下：</p><pre><code class="language-shell">127.0.0.1:6379[5]&gt; select 5\nOK\n127.0.0.1:6379[5]&gt; get 7d\n"\\xe5\\x8e\\x8b\\xe6\\xb5\\x8b\\xe6\\xb5\\x81\\xe9\\x87\\x8f"\n127.0.0.1:6379[5]&gt; select 0\nOK\n127.0.0.1:6379&gt; get 7d\n"\\xe7\\xba\\xbf\\xe4\\xb8\\x8a\\xe6\\xb5\\x81\\xe9\\x87\\x8f"\n127.0.0.1:6379&gt;\n</code></pre><p>这里，我们还是使用 JMeter 做下接口测试。</p><ul>\n<li>正常流量（不带 Header 标记）</li>\n</ul><p>脚本中不设置 Header 信息。</p><p><img src="https://static001.geekbang.org/resource/image/9e/90/9e1eb204939d5aab6a42a74b09074290.png?wh=800x656" alt="图片"></p><p>测试完成后，查看测试结果数据，可以看到，返回的结果为“线上流量”。</p><p><img src="https://static001.geekbang.org/resource/image/df/58/df95ccf8cce8e9a7d8f57584dd3b1d58.png?wh=1158x618" alt="图片"></p><ul>\n<li>压测流量（带 Header 标记）</li>\n</ul><p>脚本中设置 Header， 具体为 “dunshan:7DGroup”：</p><p><img src="https://static001.geekbang.org/resource/image/3d/e0/3d7db08d3f3cf56c79c1d1b44f7f7be0.png?wh=1830x322" alt="图片"></p><p>测试完成后，查看测试结果数据，我们看到返回的结果为“压测流量”。</p><p><img src="https://static001.geekbang.org/resource/image/15/d9/1592f60e2930c0595ebb3710b3a8f8d9.png?wh=1094x594" alt="图片"></p><p>最后，我们打开订单服务查看日志，从日志可以看到打印 Redis 数据库切换成功。</p><p><img src="https://static001.geekbang.org/resource/image/76/78/76b884c1044c1ff87f0e22ec0a67a678.png?wh=1920x910" alt="图片"></p><p>之前已经说过了，只要 demo 能完成数据源切换，并且数据隔离正常，那么就可以把目前的配置文件和相关类移植到真实系统中去了。</p><h2>真实系统改造</h2><p>我们先来对 mall-member（会员系统） 进行改造验证，如果改造并验证成功，我们就可以同步到其他系统了。</p><h3><strong>第一步：移植代码</strong></h3><p>打开工程，选择 mall-member 模块，在配置文件新建一个包，然后把目前调试成功的配置文件复制到该工程中。</p><p>如果你仔细看一下就可以发现，系统在 mall-common 中有封装 Redis 公共模块，非常方便供其他模块调用。所以，我们可以把 RedisSelectSupport 与SelectableRedisTemplate 两个类放到公共配置类中：</p><p><img src="https://static001.geekbang.org/resource/image/7e/2b/7e4031f26c39f399dff4923c1a654a2b.png?wh=1233x207" alt="图片"></p><p>把上面两个类放到公共配置后，我们还要配置下 AOP 切面：</p><p><img src="https://static001.geekbang.org/resource/image/d7/4d/d71c5e0ce9448afb53dfe7030ec5fa4d.png?wh=743x898" alt="图片"></p><p>这里要注意哦， public 中的包名路径需要修改正确，不然拦截不会生效。</p><p>紧接着，我们把 RedisConfig 类移植到工程中去：</p><p><img src="https://static001.geekbang.org/resource/image/2f/86/2fdd4496424d0e8b0193cfa2ac144086.png?wh=1156x465" alt="图片"></p><p>为了方便验证结果，我们还需要在 Controller 中添加一个测试接口，代码参考如下:</p><pre><code class="language-java">/**\n * @author 7DGroup\n * @program: dunshan-mall\n * @description:\n * @date 2021-08-22 11:05:24\n */\n@Log4j2\n@Controller\n@RequestMapping("/index")\npublic class IndexController {\n\n    @Autowired\n    private StringRedisTemplate redisTemplate;\n\n    @GetMapping("/test/{name}")\n    @ResponseBody\n    public Object set(@PathVariable String name) {\n        System.out.println("参数" + name);\n        redisTemplate.opsForValue().set("student", name);\n        HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;();\n        map.put("ok", "调试");\n        map.put("data", redisTemplate.opsForValue().get("student"));\n        return map;\n    }\n}\n</code></pre><p>上面是一个很简单的 Get 请求的写接口，主要演示的是当流量过来的时候，如何生成对应的 Cache。</p><h3><strong>第二步：测试接口验证</strong></h3><p>改造完成后，我们还是使用 JMeter 来做下接口测试，验证改造是否达到预期吧。</p><p>验证步骤主要有四个：</p><p>1、配置好数据源；</p><p>2、核实初始化 Cache；</p><p>3、通过 JMeter 接口测试；</p><p>4、验证日志及生成的 Cache。</p><p>首先，配置好对应的数据源，这里 db1 为正式缓存，db5 为影子缓存。</p><pre><code class="language-plain">    database: 0 # Redis数据库索引（默认为0）\n    port: 6379 # Redis服务器连接端口\n    password:  # Redis服务器连接密码（默认为空）\n    timeout: 3000ms # 连接超时时间（毫秒）\n  resar:\n    oneDatabase: 5\n</code></pre><p>接下来，我们核实下数据。可以看到，这两个库的 key 的都没有 student 。<br>\n<img src="https://static001.geekbang.org/resource/image/85/9f/85e9039d603f661d51e5b14073b7c79f.png?wh=898x560" alt="图片"></p><p>接下来，我们通过 JMeter 来做下接口测试验证。</p><ul>\n<li><strong>压测流量（带Header标记）</strong></li>\n</ul><p>开发第一个请求脚本，在 HTTP Header Manager 里面添加压测标记。</p><p><img src="https://static001.geekbang.org/resource/image/4e/58/4e4yy5d17da7bf844f6869cce2c09a58.png?wh=1352x718" alt="图片"></p><p>注意，JMeter 中的 Header 标记需要与代码中判断的标记保持一致。</p><p><img src="https://static001.geekbang.org/resource/image/f7/c8/f73d74dfebf3516cb445241fa0a7e0c8.png?wh=848x350" alt="图片"></p><p>然后我们打开JMeter 结果查看树，点击第一个请求日志查看响应结果。可以看到提示已经成功了。</p><p><img src="https://static001.geekbang.org/resource/image/91/ae/913969efb073bb5ceaa20d88531711ae.png?wh=1256x206" alt="图片"></p><p>我们再打开 Redis 客户端工具查看 db0 ，显示的结果也与预期一致。</p><p><img src="https://static001.geekbang.org/resource/image/00/7d/00cb339c71953d34yy9da60b832b8e7d.png?wh=1158x326" alt="图片"></p><ul>\n<li><strong>正式流量（不带Header标记）</strong></li>\n</ul><p>我们在 JMeter 脚本中去掉 HTTP Header Manager 组件。</p><p><img src="https://static001.geekbang.org/resource/image/80/b5/80b6f17a4046b965515f6ab9bd0fd9b5.png?wh=1920x496" alt="图片"></p><p>同样还是打开JMeter 结果查看树，点击第二个请求日志查看响应结果。<img src="https://static001.geekbang.org/resource/image/30/8c/30yyf4566aea56a770bc465a198bb28c.png?wh=1496x314" alt="图片"></p><p>然后我们再打开 Redis db5，显示数据与请求数据也是一致的。</p><p><img src="https://static001.geekbang.org/resource/image/97/eb/978280f722f5ce96f41dc3acd7df56eb.png?wh=1374x410" alt="图片"></p><p>好了，测试接口验证通过之后，我们就可以验证真实接口是否有效了。</p><h3><strong>第三步：真实接口验证</strong></h3><p>这里我们验证的接口为【会员登录】接口，使用的工具还是 JMeter 。</p><ul>\n<li><strong>压测流量（带 Header 标记）</strong></li>\n</ul><p>这里需要注意的是，要提前在脚本增加对应的 Header 标记。</p><p><img src="https://static001.geekbang.org/resource/image/a9/00/a91ba20737f5ae7a1023bba466f74a00.png?wh=1504x766" alt="图片"></p><p>我们可以看到压测流量已经成功了，在 db5（影子缓存）中生成了对应的数据。</p><ul>\n<li><strong>正式流量（不带 Header 标记）</strong></li>\n</ul><p>Redis 中 db0（正式缓存） 的数据也生成正常：</p><p><img src="https://static001.geekbang.org/resource/image/3d/c6/3d351f15e9224f65f16470017c5127c6.png?wh=1504x636" alt="图片"></p><p>到这里，我们的真实接口测试验证也通过了。</p><p>刚才，我们演示了 mall-member 模块的改造过程，通过区分正常流量和压测流量生成对应的Cache，我们知道这部分系统改造已经成功了。</p><p>有了上面的改造经验之后，其他模块我们也可以按这个步骤来进行。我建议你一步一步地改造，这样能让改造的风险处于更加可控的范围内。</p><h2>总结</h2><p>好了，到这里，我们就把整个缓存隔离的改造介绍完了。这节课的要点有下面三个：</p><p>1、隔离方案选型：做好复杂度识别、风险管控、成本计算等前置工作。由于全链路压测本来就是在压力大的前提下产生的需求，所以缓存的隔离，使用多实例物理分离的方式来做隔离方案是最为合理的。</p><p>2、demo 预演：在技术预演中，我们也给出了两种实现方案，那就是根据\bHeader识别和根据数据上下文识别。</p><p>3、真实系统改造：主要涉及到移植代码和验证两大环节。</p><p>大家都知道现在处理集中的大访问量的一些常见手段：缓存、队列、限流、熔断、降级等。而缓存作为一个系统大幅提升性能的重要手段，在全链路压测的逻辑中是绝对不能忽视的。这也是我这节课讲得这么详细的原因。我希望你能从文本中看到在全链路压测中，具体每一步的改造过程。</p><p>因为改造涉及的代码比较多，在这个专栏结束之后，我也会把所有改造过的代码都公布出来，方便大家一起交流讨论。</p><h2>思考题</h2><p>在这节课结束之前，我还是给你留两道思考题 ：</p><p>1、Redis 缓存隔离和 MySQL 隔离方案上有什么区别？</p><p>2、如果线上做了 Redis 缓存隔离，压测的时候我们需要注意些什么？</p><p>欢迎你在留言区与我交流讨论，我们下节课再见！</p>',
        article_title: "16 | 流量隔离：Redis 缓存隔离是怎么做的？",
      },
      {
        title: "17 | 流量隔离：MongoDB 数据库隔离是怎么做的？",
        id: 452218,
        content:
          '<p>你好，我是高楼。</p><p>这节课，我们详细来讲讲如何基于微服务技术落地 MongoDB 数据库隔离。</p><h2>快速了解 MongoDB</h2><p>MongoDB 是目前比较主流的一款 NoSQL 文档数据库，它能够解决关系数据库中的 Schema 问题，其特点是 No-Schema，任意存储和读取任意数据。</p><p>MongoDB 的数据格式主要是 JSON/BSON，因为用它新增字段简单，历史数据容错性强，比较容易存储复杂数据，所以很多电商系统都会用它，我们这个开源电商项目里面的会员服务（mall-member）也用到了它。</p><p>目前 SpringBoot 对 MongoDB 使用的支持，主要通过 Spring Data MongoDB 模块，有以下两种方式：</p><ul>\n<li>MongoTemplate</li>\n<li>MongoRepository</li>\n</ul><p>既然 Template 和 Repository 都可以做增删查改操作，它们俩的区别和优劣势在哪里呢？</p><p>到目前为止，Repository 比 Template 方便得多，但是后者可以提供对执行内容更细粒度的控制。可以说，“方便”和 “功能强大”在某种程度上是相互矛盾的目标。</p><p>Template 能做各式各样的操作，比较灵活，但是什么都得自己写。而 Repository 做了很好的封装，Repository 的实现和 JPA 类似，SQL 是动态生成出来的，对应关系是在实体类上定义的，用起来也比较方便，但是对于更复杂的查询操作，最后还是得靠 Template 来实现。</p><!-- [[[read_end]]] --><p>我们这个电商项目里面，主要是使用 MongoTemplate 方式。</p><h2>数据库隔离方案</h2><p>刚才，我们简单了解了什么是 MongoDB 以及 SpringBoot 中如何使用 MongoDB，接下来进入这节课的重点，如何在 SpringBoot 中落地数据隔离方案。</p><p>首先，我们来回顾一下目前业界对于数据库隔离的主要解决方案，以及它们的优缺点和适用场景。</p><p><img src="https://static001.geekbang.org/resource/image/d3/1d/d34b2104398a74fe530b9c072c44991d.jpg?wh=1920x1080" alt="图片"></p><p>毋庸置疑，这里最优最安全的方案是<strong>影子库</strong>。​所以在这个项目里，我们选择的 MongoDB 数据库隔离方案还是影子库，也就是说，将压测流量对 MongoDB 的读写打到影子库上。</p><p>既然选定了改造方案，我们就梳理一下 MongoDB 影子库改造的需求：</p><ol>\n<li>服务能获取压测请求的标记；</li>\n<li>MongoDB 能支持多数据源；</li>\n<li>MongoDB 能根据压测标记动态切换数据源；</li>\n<li>对业务代码的侵入性尽可能少。</li>\n</ol><p>接下来，我们看看如何去落地这些需求。</p><h2>技术预演</h2><p>首先，我们写个 demo 尝试做一下技术预演。</p><p>这里使用的 demo 工程还是 14 讲的示例，主要包括网关、会员系统、购物车系统和订单系统 4 个服务：</p><p><img src="https://static001.geekbang.org/resource/image/ba/27/ba3d569dc9e4f2b98f21b60c46b7dc27.jpg?wh=1920x378" alt="图片"></p><p>这里我们主要改造会员系统，实现 MongoDB 影子库技术预演。</p><h3>集成 MongoDB</h3><p>在member 工程的 pom 文件引入 MongoDB 依赖：</p><pre><code class="language-xml">  &lt;!--引入 mongodb 依赖--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n</code></pre><p>​在 application.yml 中配置 MongoDB 连接信息：</p><pre><code class="language-yaml">spring:\n  data:\n    mongodb:\n      shadow: mongodb://localhost:27017/shadow\n      produce: mongodb://localhost:27017/produce\n</code></pre><p>可以看到，这里配置了多数据源，分别配置 shadow 为影子库，而 produce 为生产库。</p><h3>实现获取压测标记</h3><p>接下来我们就要考虑如何获取压测请求 Header 标记了。</p><p>首先还是实现一个全局 Filter 过滤器，只要从  Span Baggage 获取压测标记放入 Context （数据上下文），就能在服务中使用了。</p><p>全局 Filter 过滤器代码如下：</p><pre><code class="language-java">package com.dunshan.member.config;\n\nimport brave.Span;\nimport brave.Tracer;\nimport brave.baggage.BaggageField;\nimport lombok.extern.log4j.Log4j2;\nimport org.apache.commons.lang3.StringUtils;\nimport org.springframework.stereotype.Component;\nimport javax.servlet.*;\nimport java.io.IOException;\n\n/**\n * @author dunshan\n * @description: 全局过滤器\n * @date 2021-10-03 17:45:30\n */\n@Log4j2\n@Component\npublic class ContextFilter implements Filter {\n    private final Tracer tracer;\n    ContextFilter(Tracer tracer) {\n        this.tracer = tracer;\n    }\n\n    @Override\n    public void init(FilterConfig filterConfig) throws ServletException {\n        Filter.super.init(filterConfig);\n    }\n\n    @Override\n    public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException {\n        BaggageField dunshan = BaggageField.getByName("dunshan");\n        String flag = dunshan.getValue();\n        Span currentSpan = this.tracer.currentSpan();\n        AppContext appContext = new AppContext();\n        if (StringUtils.isNotBlank(flag)) {\n            log.info("压测流量: " + flag);\n            currentSpan.tag("flag", flag);\n            appContext.setFlag(flag);\n            AppContext.setContext(appContext);\n        } else {\n            log.info("正常流量: " + flag);\n            AppContext.setContext(appContext);\n        }\n        filterChain.doFilter(servletRequest, servletResponse);\n    }\n\n    @Override\n    public void destroy() {\n        AppContext.removeContext();\n        Filter.super.destroy();\n    }\n}\n</code></pre><p>数据上下文代码如下：</p><pre><code class="language-java">package com.dunshan.member.config;\n\nimport com.alibaba.ttl.TransmittableThreadLocal;\nimport java.io.Serializable;\n\n/**\n * @author dunshan\n * @description: 数据上下文\n * @date 2021-11-12 17:53:39\n */\npublic class AppContext implements Serializable {\n    private static final TransmittableThreadLocal&lt;AppContext&gt; contextdunshan = new TransmittableThreadLocal&lt;&gt;();\n    private String flag;\n    public static AppContext getContext() {\n        return contextdunshan.get();\n    }\n    public static void setContext(AppContext context) {\n        contextdunshan.set(context);\n    }\n    public static void removeContext() {\n        contextdunshan.remove();\n    }\n    public String getFlag() {\n        return flag;\n    }\n    public void setFlag(String flag) {\n        this.flag = flag;\n    }\n}\n</code></pre><p>后面，我们只要在业务方通过从 getContext().getFlag() 中取出 Header 标记即可。</p><p>代码如下：</p><pre><code class="language-java">// 获取标记\nString header = AppContext.getContext().getFlag();\n</code></pre><h3><strong>实现动态数据源</strong></h3><p>要让 MongoDB 实现多数据源，网上其实有很多文章可以参考，原理都是利用 Spring IOC 容器实例化出多个 MongoTemplate。</p><p>比如这段代码，一个 Primary 库和一个 Secondary 库，其中 Primary 为主库，在使用的过程中必须指定主库，不然会报错：</p><pre><code class="language-java">@Configuration\npublic class MultipleMongoConfig {\n\n\t@Autowired\n\tprivate MultipleMongoProperties mongoProperties;\n\n\t@Primary\n\t@Bean(name = "primaryMongoTemplate")\n\tpublic MongoTemplate primaryMongoTemplate() throws Exception {\n\t\treturn new MongoTemplate(primaryFactory(this.mongoProperties.getPrimary()));\n\t}\n\n\t@Bean\n\t@Qualifier("secondaryMongoTemplate")\n\tpublic MongoTemplate secondaryMongoTemplate() throws Exception {\n        return new MongoTemplate(secondaryFactory(this.mongoProperties.getSecondary()));\n\t}\n\n\t@Bean\n\t@Primary\n\tpublic MongoDbFactory primaryFactory(MongoProperties mongo) throws Exception {\n\t\tMongoClient client = new MongoClient(new MongoClientURI(mongoProperties.getPrimary().getUri()));\n\t\treturn new SimpleMongoDbFactory(client, mongoProperties.getPrimary().getDatabase());\n\t}\n\n\t@Bean\n\tpublic MongoDbFactory secondaryFactory(MongoProperties mongo) throws Exception {\n\t\tMongoClient client = new MongoClient(new MongoClientURI(mongoProperties.getSecondary().getUri()));\n\t\treturn new SimpleMongoDbFactory(client, mongoProperties.getSecondary().getDatabase());\n\t}\n}\n\n</code></pre><pre><code class="language-java">@Configuration\n@EnableConfigurationProperties(MultipleMongoProperties.class)\n@EnableMongoRepositories(basePackages = "com.dunshan.repository.primary",\n\t\tmongoTemplateRef = "primaryMongoTemplate")\npublic class PrimaryMongoConfig {\n}​\n</code></pre><p>这里我们总结一下主要的实现方式：</p><ul>\n<li>通过 @Autowired 注入 DataSource 实例，以此操作不同的 Mongo 数据库；</li>\n<li>通过 MongoTemplate 配置的 basePackages，来指定继承了 MongoRepository 的 model 接口，在操作数据库时，使用所配置的 MongoTemplate 实例，以操作不同的数据库。</li>\n</ul><p>如上所述，这样一层一层注入。</p><p>首先，创建 DataSource，然后创建 SimpleMongoDbFactory，最后包装到 MongoTemplate 中。其中需要指定分库的 MongoRepository 以及分库 DAO 层代码。</p><p>这是最常见的 SpringBoot Mongo 多数据配置实现方法。这种方式，多数据源确实可用，但不足之处在于，需要根据不同数据源建立不同的 package，一旦数据源发生变更，需要更改所在的 package。另外，这种方式也无法做到动态切换数据源，不是我们想要的方案。</p><p>我们可以看到，实现 Mongo 多数据库的核心原理是，需要操作不同的 MongoTemplate 实例。</p><p>于是这里我们考虑通过 Spring AOP 技术，在调用方法前后动态替换 SimpleMongoClientDatabaseFactory 值，让每个 factory 都维护自己需要连接的库。在操作之前，先判断请求 Header 标记，然后选择对应 factory，切换完成后再放回去，也就是说将相关数据源的 Template 删除。</p><p><img src="https://static001.geekbang.org/resource/image/a1/ff/a1bd0db0838e9fcf209e9952f9a43aff.jpg?wh=966x2145" alt=""></p><p>具体怎么操作呢？</p><p>首先，添加 AOP 和 TransmittableThreadLocal 依赖：</p><pre><code class="language-xml">     &lt;!--引入 AOP 依赖--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;!-- 引入 TransmittableThreadLocal 依赖--&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;\n            &lt;artifactId&gt;transmittable-thread-local&lt;/artifactId&gt;\n            &lt;version&gt;2.12.1&lt;/version&gt;\n            &lt;scope&gt;compile&lt;/scope&gt;\n        &lt;/dependency&gt;\n</code></pre><p>接着创建 AOP 切面：</p><pre><code class="language-java">package com.dunshan.member.config;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.aspectj.lang.annotation.Pointcut;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.data.mongodb.core.MongoTemplate;\nimport org.springframework.data.mongodb.core.SimpleMongoClientDatabaseFactory;\nimport org.springframework.stereotype.Component;\nimport java.lang.reflect.Field;\n\n/**\n * Created by dunshan on 2021/8/29.\n *\n * @return\n */\n\n@Component\n@Aspect\n@Slf4j\npublic class MongoSwitch {\n\n    //获取配置文件的副本集连接\n    @Value("${spring.data.mongodb.produce}")\n    private String produceUri;\n\n    @Value("${spring.data.mongodb.shadow}")\n    private String shadowUri;\n\n    @Pointcut("execution(public * com.dunsan.member.service.MongotemplteService.*(..))")\n    public void routeMongoDB() {\n    }\n\n    @Around("routeMongoDB()")\n    public Object routeMongoDB(ProceedingJoinPoint joinPoint) {\n        Object result = null;\n        Object o = joinPoint.getTarget();\n        Field[] fields = o.getClass().getDeclaredFields();\n        MultiMongoTemplate mongoTemplate = null;\n\n        // 获取标记\n        String header = AppContext.getContext().getFlag();\n\n        try {\n            for (Field field : fields) {\n                field.setAccessible(true);\n                Class fieldclass = field.getType();\n\n                // 找到 Template 的变量\n                if (fieldclass == MongoTemplate.class || fieldclass == MultiMongoTemplate.class) {\n                    // 查找项目对应的 MongFactory\n                    SimpleMongoClientDatabaseFactory simpleMongoClientDbFactory = null;\n\n                    // 实例化\n                    if (header != null &amp;&amp; header.equals("7DGroup")) {\n                        simpleMongoClientDbFactory =\n                                new SimpleMongoClientDatabaseFactory(this.shadowUri);\n                    } else {\n                        simpleMongoClientDbFactory =\n                                new SimpleMongoClientDatabaseFactory(this.produceUri);\n                    }\n\n                    // 如果第一次，赋值成自定义的 MongoTemplate 子类\n                    if (fieldclass == MongoTemplate.class) {\n                        mongoTemplate = new MultiMongoTemplate(simpleMongoClientDbFactory);\n                    } else if (fieldclass == MultiMongoTemplate.class) {\n                        Object fieldObject = field.get(o);\n                        mongoTemplate = (MultiMongoTemplate) fieldObject;\n                    }\n                    // 设置 MongoFactory\n                    mongoTemplate.setMongoDbFactory(simpleMongoClientDbFactory);\n                    // 重新赋值\n                    field.set(o, mongoTemplate);\n                    break;\n                }\n            }\n            try {\n                result = joinPoint.proceed();\n                // 清理 ThreadLocal 的变量\n                mongoTemplate.removeMongoDbFactory();\n            } catch (Throwable t) {\n                log.error("", t);\n                mongoTemplate.removeMongoDbFactory();\n            }\n        } catch (Exception e) {\n            log.error("", e);\n        }\n\n        return result;\n    }\n}\n</code></pre><p>这里，我再补充一下 Spring AOP 和 AspectJ 的关系。</p><p>AspectJ 也是 Spring 的 AOP 实现方式之一，可以用 CGLIB 对类做动态代理，也可以用 Java 自己的基于接口的动态代理，也可以用 AspectJ。Spring 提供了两个有关 AOP 的包，一个是 spring-aop，一个是 spring-aspect。</p><p>然后，我们创建相关配置类:</p><pre><code class="language-java">package com.dunshan.member.config;\n\nimport com.mongodb.client.MongoDatabase;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.data.mongodb.core.MongoDatabaseFactorySupport;\nimport org.springframework.data.mongodb.core.MongoTemplate;\n\n/**\n * Created by dunshan on 2021/8/29.\n *\n * @return\n */\n \npublic class MultiMongoTemplate extends MongoTemplate {\n\n    //用来缓存当前 MongoDbFactory\n    private static ThreadLocal&lt;MongoDatabaseFactorySupport&gt; mongoDbFactoryThreadLocal;\n\n    public MultiMongoTemplate(MongoDatabaseFactorySupport mongoDbFactory) {\n        super(mongoDbFactory);\n        if (mongoDbFactoryThreadLocal == null) {\n            mongoDbFactoryThreadLocal = new ThreadLocal&lt;&gt;();\n        }\n    }\n\n    public void setMongoDbFactory(MongoDatabaseFactorySupport factory) {\n        mongoDbFactoryThreadLocal.set(factory);\n    }\n\n    public void removeMongoDbFactory() {\n        mongoDbFactoryThreadLocal.remove();\n    }\n\n    @Override\n    public MongoDatabase getDb() {\n        return mongoDbFactoryThreadLocal.get().getMongoDatabase();\n    }\n}\n</code></pre><p>这里主要是为了实现多线程下存储 MongoDbFactory 副本变量的，也就是 MongoDbFactory 的上下文。</p><p>最后，我们需要写一个简单的接口测试一下。</p><p>新建一个实体类 Student：</p><pre><code class="language-java">package com.dunshan.member.entity;\n\nimport lombok.Data;\nimport org.springframework.data.mongodb.core.mapping.Document;\n/**\n * Created by dunshan on 2020/8/29.\n * \n * @return\n */\n\n@Data\n@Document("student")\npublic class Student {\n    private String name;\n    private String addr;\n    private String email;\n}\n\n</code></pre><p>​再新建 MongotemplteService：</p><pre><code class="language-java">package com.dunshan.member.service;\n\nimport org.springframework.data.mongodb.core.MongoTemplate;\nimport org.springframework.stereotype.Service;\n\n/**\n * Created by dunshan on 2021/8/29.\n *\n * @return\n */\n\n@Service\npublic class MongotemplteService {\n    private MongoTemplate mongoTemplate;\n\n    public &lt;T&gt; T insert(T var1) {\n        return mongoTemplate.insert(var1);\n    }\n}\n\n</code></pre><p>这里，我们实现了一个 insert 方法。<br>\n接下来，新建控制器 HomeController：</p><pre><code class="language-java">package com.dunshan.member.api;\n\nimport com.dunshan.member.service.MongotemplteService;\nimport com.dunshan.member.entity.Student;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.PutMapping;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestParam;\nimport org.springframework.web.bind.annotation.RestController;\n\n@RequestMapping("/home")\n@RestController\npublic class HomeController {\n\n    @Autowired\n    private MongotemplteService mongotemplteService;\n\n    @PutMapping\n    public String addDataByDynamic(@RequestParam(value = "name") String name,@RequestParam(value = "addr") String addr,@RequestParam(value = "email") String email){\n        Student student = new Student();\n        student.setAddr(addr);\n        student.setName(name);\n        student.setEmail(email);\n        mongotemplteService.insert(student);\n        return "添加成功";\n    }\n}\n</code></pre><p>这样，我们就创建了一个添加 student 的接口。<br>\n好了，接下来我们验证一下改造效果。</p><h3>接口测试</h3><p>启动网关和会员服务，我们再用 Postman 验证一下这个 demo。</p><p><img src="https://static001.geekbang.org/resource/image/71/ca/71d8efa3a192019e70338bdef3995eca.png?wh=1829x457" alt="图片"></p><ul>\n<li>正常流量（不带 Header 压测标记）</li>\n</ul><p>首先，我们测试请求接口不带 Header 压测标记的情况：</p><p><img src="https://static001.geekbang.org/resource/image/a6/90/a6130b22e709dc897fbd7de5d6843990.png?wh=1920x1464" alt="图片"></p><p>我们可以看到对应的服务端日志：</p><p><img src="https://static001.geekbang.org/resource/image/7e/85/7e07cee7df25b4500c5454a68b0f9485.png?wh=1615x168" alt="图片"></p><p>这里，数据已经正常落到正式库里面了：</p><p><img src="https://static001.geekbang.org/resource/image/99/f3/99f65050aaac16f13025c181b47fe1f3.png?wh=1920x657" alt="图片"></p><ul>\n<li>压测流量（带 Header 压测标记）</li>\n</ul><p>我们再来测试一下接口带 Header 压测标记的情况：</p><p><img src="https://static001.geekbang.org/resource/image/87/7b/8756a7ab15922f9f1b2f0ff3923ef67b.png?wh=1920x1086" alt="图片"></p><p>下面是对应的服务端日志：</p><p><img src="https://static001.geekbang.org/resource/image/a5/3d/a5cc91b4yydcd4b630ec37dd51597b3d.png?wh=1607x177" alt="图片"></p><p>我们可以看到，数据已经正常落到影子库里面了，这说明数据已经隔离成功了。</p><p><img src="https://static001.geekbang.org/resource/image/24/dd/2448671b43440f697ef01435f64f09dd.png?wh=1920x966" alt="图片"></p><p>到这里，我们就已经初步实现对于影子库的需求了。</p><p>我再总结下 demo 预演的主要步骤：</p><ol>\n<li>实现多数据源；</li>\n<li>通过全局 Filter 获取标记，并存入数据上下文；</li>\n<li>通过 AOP 拦截请求，并判断压测标记；</li>\n<li>选择对应 Factory；</li>\n<li>操作对应的 MongoDB。</li>\n</ol><p>可以看到，这里面的改造量还是挺大的，那么，我们如何把它在真实系统落地呢？</p><h2>真实系统改造</h2><p>首先，我们需要梳理一下 mall 电商项目里 MongoDB 的使用方式。</p><p>通过仔细阅读项目代码，我们发现只有在 member（会员服务）中会使用 MongoDB ，这里我们以 【添加品牌关注】接口为例，梳理一下代码的实现逻辑。</p><p>下面是控制器的代码。</p><pre><code class="language-java">    @ApiOperation("添加品牌关注")\n    @RequestMapping(value = "/add", method = RequestMethod.POST)\n    @ResponseBody\n    public CommonResult add(@RequestBody MemberBrandAttention memberBrandAttention) {\n        int count = memberAttentionService.add(memberBrandAttention);\n        if(count&gt;0){\n            return CommonResult.success(count);\n        }else{\n            return CommonResult.failed();\n        }\n    }\n</code></pre><p>接口逻辑处理如下：</p><p><img src="https://static001.geekbang.org/resource/image/53/bf/53afde7a111a6e4e7e22f54e038841bf.png?wh=1486x1152" alt="图片"></p><p>这个逻辑比较简单，主要是通过注入 memberAttentionService 中的 DAO 接口，通过实现 MemberAttentionServiceImpl 去具体操作数据库。</p><p>紧接着，我们来看下 MemberAttentionServiceImpl 的代码：</p><pre><code class="language-java">@Service("MemberAttent")\npublic class MemberAttentionServiceImpl implements MemberAttentionService {\n\n    @Autowired\n    private MemberService memberService;\n\n    @Autowired\n    private MongoTemplate mongoTemplate;\n\n    @Override\n    public int add(MemberBrandAttention memberBrandAttention) {\n        int count = 0;\n        UmsMember member = memberService.getCurrentMember();\n        memberBrandAttention.setMemberId(member.getId());\n        memberBrandAttention.setMemberNickname(member.getNickname());\n        memberBrandAttention.setMemberIcon(member.getIcon());\n        memberBrandAttention.setCreateTime(new Date());\n\n        Query query = new Query(Criteria.where("memberId").is(memberBrandAttention.getMemberId()).and("brandId").is(memberBrandAttention.getBrandId()));\n        List&lt;MemberBrandAttention&gt; findAttention = mongoTemplate.find(query,MemberBrandAttention.class);\n\n        if (findAttention.isEmpty()) {\n            mongoTemplate.save(memberBrandAttention);\n            count = 1;\n        }\n        return count;\n    }\n    \n    ......\n }\n</code></pre><p>可以看到，我们主要还是通过 MongoTemplate 的方式实现了一个 save 的操作。</p><p>到这里就可以确定， mall 项目操作数据库的方式与我们在 demo 的逻辑是一致的了。</p><p>通过 demo 的例子，我们发现需要改动的代码还是不少的。那么问题来了，如果做真实系统改造，为了减少原有服务代码的改动，能不能把这些影子库配置、动态切换的逻辑抽取做成一个 SDK，在压测的时候项目直接引入 SDK 依赖呢？</p><p>带着这个问题，我研究了一下 SpringBoot 的特性，发现还真有可能，这就要说到 Starter了。</p><blockquote>\n<p>SpringBoot 中的 Starter 是一种非常重要的机制，能够抛弃以前繁杂的配置，将其统一集成进 Starter，应用者只需要在 Maven 中引入 Starter 依赖，SpringBoot 就能自动扫描到要加载的信息并启动相应的默认配置。Starter 让我们不用再处理各种依赖库、配置各种信息。SpringBoot 会自动通过 classpath 路径下的类发现需要的 Bean，并注册进 IOC 容器。SpringBoot 提供了针对日常企业应用研发各种场景的 spring-boot-starter 依赖模块。所有这些依赖模块都遵循着约定成俗的默认配置，并允许我们调整这些配置，即遵循“<strong>约定大于配置</strong>”的理念。</p>\n</blockquote><p>既然 SpringBoot 原生就支持 Starter 机制，那么我们就得考虑如何实现影子库的自定义 Starter 功能了。</p><h3>实现影子库 Starter</h3><p>创建一个 Maven 项目：dynamicmongo-starter。</p><p>将 demo 中的以下文件拷贝到项目中：</p><p><img src="https://static001.geekbang.org/resource/image/30/0e/30aa2a4402d239aa45a455acc388880e.png?wh=326x349" alt="图片"></p><p>注意，我们需要修改 AOP 切面拦截点到对应真实系统的位置。</p><p>创建自动装配文件：</p><pre><code class="language-java">package com.dunshan.dynamicmongo.config;\n\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\n/**\n * @author dunshan\n * @date 2021/8/29 mongo 自动配置\n */\n\n@Configuration(proxyBeanMethods = false)\npublic class MongodbAutoConfiguration {\n\n\t@Bean\n\tpublic MongoSwitch mongoSwitch() {\n\t\treturn new MongoSwitch();\n\t}\n\n\t@Bean\n\tpublic MongotemplteService mongotemplteService() {\n\t\treturn new MongotemplteService();\n\t}\n\n}\n</code></pre><p>这里，我们将 MongoSwitch 和  MongotemplteService 类定义为 Bean，交给 IOC 容器。<br>\n在 resources 下新建 META-INF 文件夹，然后创建 spring.factories 文件。</p><pre><code class="language-java">org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\\n    com.dunshan.dynamicmongo.config.MongodbAutoConfiguration\n</code></pre><p>至此，Starter 就已经写完了，是不是很简单？</p><p>接下来我们通过 maven install 命令操作把 Starter 的包打入 maven local repository （maven本地仓库）里面。</p><p>最后，我们总结一下自定义 Starter，我们要做的事情主要是两个：</p><ul>\n<li>确定依赖；</li>\n<li>编写自动配置。</li>\n</ul><p>这里重点要做的就是编写自动配置，特别是注解配置的使用，它包括：</p><ul>\n<li><strong>@Configuration</strong>：指定这个类是一个配置类；</li>\n<li><strong>@ConditionalOnXXX</strong>：在指定条件成立的情况下自动配置类生效；</li>\n<li><strong>@AutoConfigureAfter</strong>：指定自动配置类的顺序；</li>\n<li><strong>@Bean</strong>：给容器中添加组件；</li>\n<li><strong>@ConfigurationPropertie</strong>：结合相关 xxxProperties 类来绑定相关的配置；</li>\n<li><strong>@EnableConfigurationProperties</strong>：让 xxxProperties 生效加入到容器中。</li>\n</ul><p>按照这些注解写好自动配置类后，我们还需要进行自动配置的加载，加载方式是将需要启动就加载的自动配置类，配置在 META-INF/spring.factories 里。</p><h3>真实项目引入 Starter</h3><p>在 member 项目中引入该依赖：</p><pre><code class="language-java">      &lt;dependency&gt;\n            &lt;groupId&gt;com.dunshan&lt;/groupId&gt;\n            &lt;artifactId&gt;dynamicmongo-starter&lt;/artifactId&gt;\n            &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre><p>​接下来配置 MongoDB 连接信息，在 application.yml 中配置：</p><pre><code class="language-yaml">spring:  \n  data:\n    mongodb:\n      produce: mongodb://localhost:27017/mall-port\n      shadow: mongodb://localhost:27017/mall-port_shadow\n</code></pre><p>这里分开配置 mall-port_shadow 为影子库，mall-port 为生产库。</p><h3>接口测试</h3><p>上面我们已经对系统进行了改造，接下来我们以 member （会员服务）中的 【添加品牌关注】 接口来做下测试，验证影子库功能是否达到了我们的预期。</p><p>下面这张图是接口文档中对该接口的描述：</p><p><img src="https://static001.geekbang.org/resource/image/91/d3/911c7a35c5b0dac65ee67a757fc789d3.png?wh=1596x931" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/02/8a/02628867026b618c95e59878330dda8a.png?wh=1480x649" alt="图片"></p><p>这是一个 Post HTTP 接口，在测试前我们首先需要获取 token。</p><ul>\n<li>正常流量（不带 Header 压测标记）</li>\n</ul><p>在接口管理平台中测试正常接口请求：</p><p><img src="https://static001.geekbang.org/resource/image/d9/68/d9f1edce3cc9881ec67c06ff366b6368.png?wh=1904x959" alt="图片"></p><p>我们可以看到，正常请求的数据已经成功落在正常库里面了。</p><p><img src="https://static001.geekbang.org/resource/image/91/58/91109a82b84ed1766f68877d76c29b58.png?wh=1920x1228" alt="图片"></p><ul>\n<li>压测流量（带 Header 压测标记）</li>\n</ul><p>接下来，我们通过 curl 命令测试一下，带入 Header 压测标记请求接口：</p><pre><code class="language-bash">~  curl -X POST -H  "Accept:*/*" -H  "Authorization:eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX25hbWUiOiJsaXNpIiwic2NvcGUiOlsiYWxsIl0sImlkIjo1LCJleHAiOjE2MzAzMDQ2NjIsImF1dGhvcml0aWVzIjpbIuWJjeWPsOS8muWRmCJdLCJqdGkiOiJiYjBjZGUwMS05NDdkLTQwMGUtOTAxOC00MzIyMDg3MjFmNDkiLCJjbGllbnRfaWQiOiJwb3J0YWwtYXBwIn0.OTw0gQpb3vpUUmWgE7K0VyVdC5T0sb1AilXpl3UmNtgzEl2JLEKDzv-iiIDG0bgGYU6FfGmIrh-2JrX5oZTQRMMAmA123Gyz2F19y6mnR_mnjw0VdnloFneHXtBDsZfUQa0hyrdyXBhOWrTlyfeV8gtJqpcIpAnylbwSPRw29UA" -H  "Request-Origion:Knife4j" -H  "Content-Type:application/json" -H  "dunshan:7DGroup" -d "{\\"brandCity\\":\\"beijing\\",\\"brandId\\":8,\\"brandLogo\\":\\"\\",\\"brandName\\":\\"test\\",\\"createTime\\":\\"\\",\\"id\\":\\"8\\",\\"memberIcon\\":\\"\\",\\"memberId\\":8,\\"memberNickname\\":\\"test\\"}" "http://127.0.0.1:8201/mall-member/member/attention/add"\n\n{"code":200,"message":"操作成功","data":1}%\n</code></pre><p>我们可以看到， Header 带压测标记的请求数据已经成功落在影子库里了：</p><p><img src="https://static001.geekbang.org/resource/image/71/f3/71082ccd5018805d19ed4bf5a51462f3.png?wh=1920x1237" alt="图片"></p><p>到这里，我们的真实系统 MongoDB 影子库功能就改造成功了。</p><h2>总结</h2><p>好了，这节课就讲到这里，我们使用 demo 预演和系统改造，完整演示了 MongoDB 数据库隔离技术方案的改造过程。这节课有两个要点，我希望你能够记住：</p><p>1、MongoDB 数据隔离技术本质上跟 MySQL 是一样的；</p><p>2、对于常见的 SDK 注入方案，除了 AOP 字节码增强，我们还可以考虑 SpringBoot 中基于 IOC 的 Starter 机制。</p><p>最后，再次强调一遍，在全链路压测中，<strong>数据库隔离是最重要的一个改造环节了</strong>。因为做得不到位，压测流量就会污染生产数据，导致的后果非常严重。这节课给出的影子库方案，是改造成本低，效果最好的一种方式。希望你能够充分地理解并用好它。</p><h2>课后题</h2><p>学完这节课，我想请你思考两个问题：</p><ol>\n<li>你知道 Spring 中 AOP 和 IOC 的区别是什么？</li>\n<li>相比字节码 AOP 增强方案，你觉得 Starter 的优缺点是什么？</li>\n</ol><p>欢迎你在留言区与我交流讨论。你也可以把这节课分享给你身边的朋友，他们的一些想法或许会让你有更大的收获。我们下节课见！</p>',
        article_title: "17 | 流量隔离：MongoDB 数据库隔离是怎么做的？",
      },
      {
        title: "18 | 流量隔离：RabbitMQ 消息隔离是怎么做的？",
        id: 457043,
        content:
          '<p>你好，我是高楼。</p><p>这节课，我们详细来讲讲如何基于微服务技术落地 RabbitMQ 消息隔离。</p><p>我们的项目中选择的消息中间件是 RabbitMQ。它是最受欢迎的开源消息中间件之一。RabbitMQ 量级轻，而且易于部署，能支持多种消息协议。它还可以部署在分布式系统中，满足大规模、高可用、削峰填谷的要求。所有消息中间件的根本目标是相同的，那就是：<strong>将同步处理转成异步处理。</strong></p><p>首先，我们来了解一下 Spring 中 RabbitMQ 的消息传递解决方案。</p><p>Spring 生态中提供了<a href="https://spring.io/projects/spring-amqp">Spring-AMQP</a> 项目，可以让我们更简便地使用 AMQP。它提供了一个“template”作为发送消息的高级抽象。同时它还通过“Listener Container”为消息驱动的 POJO 提供支持。这些库简化了 AMQP 资源的开发使用。</p><p>这个项目主要包括两个部分：</p><ul>\n<li><a href="https://mvnrepository.com/artifact/org.springframework.amqp/spring-amqp">Spring-AMQP</a> ：是 AMQP 的基础抽象；</li>\n<li><a href="https://mvnrepository.com/artifact/org.springframework.amqp/spring-rabbit">Spring-Rabbit</a> ：是基于 RabbitMQ 对 AMQP 的具体实现。</li>\n</ul><p>它的主要功能包括：</p><ul>\n<li>Listener Container：异步处理接收到的消息；</li>\n<li><a href="https://github.com/spring-projects/spring-amqp/blob/master/spring-rabbit/src/main/java/org/springframework/amqp/rabbit/core/RabbitTemplate.java">RabbitTemplate</a> ：发送和接收消息；</li>\n<li><a href="https://github.com/spring-projects/spring-amqp/blob/master/spring-rabbit/src/main/java/org/springframework/amqp/rabbit/core/RabbitAdmin.java">RabbitAdmin</a> ​：自动创建队列、交换机、绑定器。</li>\n</ul><!-- [[[read_end]]] --><p>因为我们的项目采用 SpringBoot，而它默认提供了 AMQP 和 RabbitMQ 的自动化配置，所以我们仅需引入 spring-boot-starter-amqp 依赖，即可快速装配使用了。</p><h2>消息隔离方案</h2><p>刚才，我们简单介绍了 RabbitMQ，并了解了如何在 SpringBoot 中使用 RabbitMQ。接下来，我们回到这节课的主题：如何在 SpringBoot 中落地 MQ 消息隔离方案。</p><p>在这里我梳理了目前业界对于消息中间件隔离的主要解决方案，你可以参考下面的表格：<br>\n<img src="https://static001.geekbang.org/resource/image/1b/3f/1b11259057e92939e336ef01b748183f.jpg?wh=1920x1080" alt="图片"></p><p>其中，数据偏移方式会让压测消息跟正常消息都进入到生产队列。压测消息在发送端加上标记，然后在接收端加上识别。</p><p>而影子队列方式则会把压测消息发送到另一个影子队列里，跟生产队列完全隔离开。接受端同时监听生产队列和影子队列，然后从接收的消息判断是哪个队列发过来的，再根据判断的结果做对应的处理。</p><p>根据不同的项目情况，我们可以选择不同的技术方案，当然，<strong>这里我们首推影子队列，因为它相对安全</strong>。</p><p>为了方便你参考，我会对影子队列和数据偏移方案分别进行演示。</p><h2>技术预演</h2><p>在正式预演之前，我们先要知道 RabbitMQ 消息模式的实现方式，<a href="https://www.rabbitmq.com/getstarted.html">RabbitMQ 的 7 种消息模式</a>是构建消息应用的基础，我们一定要牢牢掌握它们。</p><p><img src="https://static001.geekbang.org/resource/image/5c/b0/5ccdcca63d340c8dc1179eb6a1acb7b0.png?wh=999x602" alt="图片"></p><p>学过 RabbitMQ 的朋友应该了解过这些消息模式的 Java 实现方式，这里的 demo 预演我们使用 Spring AMQP 的形式来实现它们。</p><p>在 AMQP 中，发布者将消息发送到交换机，再由交换机将消息路由到一个或多个消息队列中（或者丢弃）。</p><p>交换机会根据路由键和绑定的键将消息路由到消息队列。目前常用的交换器类型有 Direct、Topic、Fanout、Headers 四种类型。</p><h3>影子队列实现</h3><p>我们先来看一下影子队列的实现方式。</p><p>影子队列我们主要通过 RabbitMQ 的路由模式（Direct）实现：</p><blockquote>\n<p>路由模式是可以根据路由键选择性给多个消费者发送消息的模式。它包含一个发布者、两个消费者、两个队列和一个交换机。两个消费者同时绑定到不同的队列上去，两个队列通过路由键绑定到交换机上去，发布者发送消息到交换机，交换机通过路由键转发到不同队列，队列绑定的消费者接收并消费消息。</p>\n</blockquote><p>为了方便你更直观地理解路由模式的原理，我给你画了一张示意图。</p><p><img src="https://static001.geekbang.org/resource/image/51/42/51c5cacb48d3f00edcf24ea389e0ca42.jpg?wh=1920x569" alt="图片"></p><p>通过这张图我们可以很直观的看到：</p><ul>\n<li>如果我们以 routingKey=“produce” 发送消息到交换机，消息会路由到 7dQueue。</li>\n<li>如果我们以 routingKey=“shadow” &nbsp;发送消息到交换机，消息会路由到 7dQueue_shadow。</li>\n</ul><p>也就是说，如果我们在交换机设置对应路由键，那么消息就只会路由到最多一个消息队列中。如果没有相应的匹配，消息就会发送到“空气”（丢弃）中，不会进入任何消息队列中了。</p><p>我们还是来看一下具体的操作步骤吧。</p><p>这里使用的 demo 工程还是 14 讲的示例，主要包括网关和消息系统 2 个服务：</p><p><img src="https://static001.geekbang.org/resource/image/9d/51/9dfd614cb25639683ea8f441cb431551.jpg?wh=847x226" alt=""></p><p>下面我们主要改造消息系统，实现 RabbitMQ 消息隔离技术预演。</p><p><strong>第一步，引入相关依赖。</strong></p><pre><code class="language-xml">&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;com.dunshan&lt;/groupId&gt;\n        &lt;artifactId&gt;dunshan-common&lt;/artifactId&gt;\n        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n        &lt;scope&gt;compile&lt;/scope&gt;\n        &lt;exclusions&gt;\n            &lt;exclusion&gt;\n                &lt;artifactId&gt;jsr305&lt;/artifactId&gt;\n                &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt;\n            &lt;/exclusion&gt;\n        &lt;/exclusions&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;\n        &lt;scope&gt;test&lt;/scope&gt;\n    &lt;/dependency&gt;\n    &lt;!--Swagger-UI API 文档生产工具--&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;io.springfox&lt;/groupId&gt;\n        &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt;\n        &lt;version&gt;2.7.0&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;io.springfox&lt;/groupId&gt;\n        &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt;\n        &lt;version&gt;2.7.0&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;!--hutool 工具包--&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;cn.hutool&lt;/groupId&gt;\n        &lt;artifactId&gt;hutool-all&lt;/artifactId&gt;\n        &lt;version&gt;4.6.3&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;!--Spring AMQP 依赖--&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;\n        &lt;artifactId&gt;lombok&lt;/artifactId&gt;\n        &lt;version&gt;1.18.10&lt;/version&gt;\n        &lt;scope&gt;provided&lt;/scope&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre><p><strong>第二步，在 resources 目录下，创建 application.yaml 配置文件。</strong></p><pre><code class="language-yaml">server:\n  port: 8008\n\nspring:\n  rabbitmq:\n    host: localhost\n    port: 5672\n    virtual-host: /\n    username: guest\n    password: guest\n    publisher-confirms: true #消息发送到交换器确认\n    publisher-returns: true #消息发送到队列确认\n</code></pre><p>这里，我们在 spring.rabbitmq 配置项，设置了 RabbitMQ 的配置，对应的是 RabbitProperties 配置类。</p><p>然后，我们可以利用 SpringBoot 提供的 RabbitAutoConfiguration 自动化配置类，实现 RabbitMQ 的自动配置，创建相应的发布者和消费者。</p><p><strong>第三步，创建 RabbitConfig 配置类，添加和 Direct Exchange 相关的 Exchange、Queue、Binding 配置。</strong></p><p>这一步主要是添加和路由模式相关 Java 配置，创建一个名为 exchange.7d 的交换机、一个发布者、两个消费者和两个消息队列。其中，队列通过路由键都绑定到交换机。</p><ul>\n<li>消息队列 7dQueue 的路由键为 produce；</li>\n<li>消息队列 7dQueue_shadow 的路由键为 shadow。</li>\n</ul><pre><code class="language-java">package com.dunshan.direct;\n\nimport org.springframework.amqp.core.Binding;\nimport org.springframework.amqp.core.BindingBuilder;\nimport org.springframework.amqp.core.DirectExchange;\nimport org.springframework.amqp.core.Queue;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\n/**\n *\n * @author dunshan\n * @date 2021/10/1\n */\n@Configuration\npublic class DirectRabbitConfig {\n\n    @Bean\n    public DirectExchange direct() {\n        return new DirectExchange("exchange.7d");\n    }\n    \n    @Bean\n    public Queue directQueue() {\n        return new Queue("7dQueue");\n    }\n    \n    @Bean\n    public Queue directQueueShadow() {\n        return new Queue("7dQueue_shadow");\n    }\n    @Bean\n    public Binding directBinding1a(DirectExchange direct, Queue directQueue) {\n        return BindingBuilder.bind(directQueue).to(direct).with("produce");\n    }\n    \n    @Bean\n    public Binding directBinding1b(DirectExchange direct, Queue directQueueShadow) {\n        return BindingBuilder.bind(directQueueShadow).to(direct).with("shadow");\n    }\n    \n    @Bean\n    public DirectReceiver receiver() {\n        return new DirectReceiver();\n    }\n    \n    @Bean\n    public DirectSender directSender() {\n        return new DirectSender();\n    }\n}\n</code></pre><p>从这段代码中可以看到，在 DirectExchangeDemoConfiguration 内部静态类中，我们创建了 Exchange、Queue、Binding 三个 Bean，后续 RabbitAdmin 会自动创建交换机、消息队列、绑定器。</p><p><strong>第四步，实现获取压测标记。</strong></p><p>接下来我们就要考虑如何获取压测请求 Header 标记了。</p><p>首先还是实现一个全局 Filter 过滤器，只要从  Span Baggage 获取压测标记放入 Context （数据上下文），就能在服务中使用了。</p><p>全局 Filter 过滤器代码如下：</p><pre><code class="language-java">package com.dunshan.config;\n\nimport brave.Span;\nimport brave.Tracer;\nimport brave.baggage.BaggageField;\nimport lombok.extern.log4j.Log4j2;\nimport org.apache.commons.lang3.StringUtils;\nimport org.springframework.stereotype.Component;\nimport javax.servlet.*;\nimport java.io.IOException;\n\n/**\n * @author dunshan\n * @description: 全局过滤器\n * @date 2021-10-03 17:45:30\n */\n@Log4j2\n@Component\npublic class ContextFilter implements Filter {\n\n    private final Tracer tracer;\n    ContextFilter(Tracer tracer) {\n        this.tracer = tracer;\n    }\n    \n    @Override\n    public void init(FilterConfig filterConfig) throws ServletException {\n        Filter.super.init(filterConfig);\n    }\n    \n    @Override\n    public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException {\n        BaggageField dunshan = BaggageField.getByName("dunshan");\n        String flag = dunshan.getValue();\n        Span currentSpan = this.tracer.currentSpan();\n        AppContext appContext = new AppContext();\n        if (StringUtils.isNotBlank(flag)) {\n            log.info("压测流量: " + flag);\n            currentSpan.tag("flag", flag);\n            appContext.setFlag(flag);\n            AppContext.setContext(appContext);\n        } else {\n            log.info("正常流量: " + flag);\n            AppContext.setContext(appContext);\n        }\n        filterChain.doFilter(servletRequest, servletResponse);\n    }\n    \n    @Override\n    public void destroy() {\n        AppContext.removeContext();\n        Filter.super.destroy();\n    }\n    \n}\n</code></pre><p>数据上下文代码如下：</p><pre><code class="language-java">package com.dunshan.config;\nimport com.alibaba.ttl.TransmittableThreadLocal;\nimport java.io.Serializable;\n\n/**\n * @author dunshan\n * @description: 数据上下文\n * @date 2021-11-12 17:53:39\n */\npublic class AppContext implements Serializable {\n\n    private static final TransmittableThreadLocal&lt;AppContext&gt; contextdunshan = new TransmittableThreadLocal&lt;&gt;();\n    private String flag;\n    \n    public static AppContext getContext() {\n        return contextdunshan.get();\n    }\n    \n    public static void setContext(AppContext context) {\n        contextdunshan.set(context);\n    }\n    \n    public static void removeContext() {\n        contextdunshan.remove();\n    }\n    \n    public String getFlag() {\n        return flag;\n    }\n    \n    public void setFlag(String flag) {\n        this.flag = flag;\n    }\n    \n}\n</code></pre><p>后面，我们只要在业务方通过从 getContext().getFlag() 中取出 Header 标记即可。</p><p>代码如下：</p><pre><code class="language-java">// 获取标记\nString header = AppContext.getContext().getFlag();\n</code></pre><p><strong>第五步，创建 DirectSender 类。</strong></p><p>它会使用 Spring-AMQP 封装提供的 RabbitTemplate 来发送消息。发布者通过 send 方法向交换机 exchange.7d 发送消息。交换机通过判断是否为压测标记，来选择使用不同的路由键，然后，这些消息会根据不同的路由键被转发到不同的队列。</p><pre><code class="language-java">package com.dunshan.direct;\n\nimport com.dunshan.config.AppContext;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.amqp.rabbit.core.RabbitTemplate;\nimport org.springframework.beans.factory.annotation.Autowired;\n\n/**\n *\n * @author dunshan\n * @date 2021/10/1\n */\n@Slf4j\npublic class DirectSender {\n   \n   @Autowired\n   private RabbitTemplate template;\n   \n   private static final String exchangeName = "exchange.7d";\n   \n   public void send(int index) {\n      // 获取标记\n      String header = AppContext.getContext().getFlag();\n      StringBuilder builder = new StringBuilder("Hello to ");\n      String message = builder.toString();\n     \n       // 判断标记\n      if ("7DGroup".equals(header)) {\n         template.convertAndSend(exchangeName, "shadow", message);\n      } else {\n         template.convertAndSend(exchangeName, "produce", message);\n      }\n      log.info(" [x] Sent \'{}\'", message);\n   }\n   \n}\n</code></pre><p>在具体的代码实现上，因为 RabbitTemplate 是 AmqpTemplate 接口的实现类，所以使用 AmqpTemplate 也可以。不过因为 RabbitTemplate 还实现了其它接口，操作会更为丰富一些。所以这里我们还是选择注入了 RabbitTemplate 属性。</p><p><strong>第六步，创建 DirectReceiver 类，消费消息。</strong></p><p>消费者从自己绑定的匿名队列中获取消息，由于该消费者可以从两个队列中获取并消费消息，所以可以看做是两个消费者，名称分别是 instance 1 和 instance 2。</p><pre><code class="language-java">package com.dunshan.direct;\n\nimport cn.hutool.core.thread.ThreadUtil;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.amqp.rabbit.annotation.RabbitListener;\nimport org.springframework.util.StopWatch;\n\n/**\n *\n * @author dunshan\n * @date 2021/10/1\n */\n@Slf4j\npublic class DirectReceiver {\n    \n    @RabbitListener(queues = "#{directQueue.name}")\n    public void receive1(String in){\n        receive(in, 1);\n    }\n    \n    @RabbitListener(queues = "#{directQueueShadow.name}")\n    public void receive2(String in){\n        receive(in, 2);\n    }\n   \n    private void receive(String in, int receiver){\n        StopWatch watch = new StopWatch();\n        watch.start();\n        log.info("instance {} [x] Received \'{}\'", receiver, in);\n        doWork(in);\n        watch.stop();\n        log.info("instance {} [x] Done in {}s", receiver, watch.getTotalTimeSeconds());\n    }\n    \n    private void doWork(String in){\n        for (char ch : in.toCharArray()) {\n            if (ch == \'.\') {\n                ThreadUtil.sleep(1000);\n            }\n        }\n    }\n    \n}\n</code></pre><p>可以看到，我们在类上添加了 <a href="https:///RabbitListener">@RabbitListener </a> 注解，声明了消费的队列。</p><p><strong>第七步，在 Controller 中添加测试接口。</strong></p><p>这里主要实现调用该接口，开始发送消息。</p><pre><code class="language-java">package com.dunshan.controller;\n\nimport cn.hutool.core.thread.ThreadUtil;\nimport com.dunshan.common.api.CommonResult;\nimport com.dunshan.direct.DirectSender;\nimport com.dunshan.directheader.DirectHeaderSender;\nimport io.swagger.annotations.Api;\nimport io.swagger.annotations.ApiOperation;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestMethod;\nimport org.springframework.web.bind.annotation.ResponseBody;\n\n/**\n * Created by dunshan on 2020/5/19.\n */\n@Api(tags = "RabbitController", description = "RabbitMQ 消息隔离测试")\n@Controller\n@RequestMapping("/rabbit")\n@Slf4j\npublic class RabbitController {\n\n    @Autowired\n    private DirectSender directSender;\n    \n    @ApiOperation("路由模式")\n    @RequestMapping(value = "/direct", method = RequestMethod.GET)\n    @ResponseBody\n    public CommonResult directTest() {\n        for(int i=0;i&lt;10;i++){\n            directSender.send(i);\n            ThreadUtil.sleep(1000);\n        }\n        return CommonResult.success(null);\n    }\n    \n}\n</code></pre><p><strong>第八步，接口测试验证。</strong></p><p>我们可以通过 Postman 做接口测试，往队列中发送包含不同路由键的消息。</p><ul>\n<li>发送正常请求（不带压测标记）<br>\n<img src="https://static001.geekbang.org/resource/image/21/1d/2164868fba308d6c696c2e154805391d.png?wh=1920x1461" alt="图片"></li>\n</ul><p>服务端的控制台日志如下：</p><p><img src="https://static001.geekbang.org/resource/image/b2/3a/b21c23d49yy292cbc76518d59cc5223a.png?wh=1401x604" alt="图片"></p><ul>\n<li>发送压测请求（带压测标记）<br>\n<img src="https://static001.geekbang.org/resource/image/bb/3a/bb11d1acf0685bf4046194bc87812c3a.png?wh=1920x1474" alt="图片"></li>\n</ul><p>服务端的控制台日志如下：</p><p><img src="https://static001.geekbang.org/resource/image/d0/44/d0f5c3eed95edd56bfcf541a3c9e4144.png?wh=1407x623" alt="图片"></p><p>可以看到，instance 1 获取到了正常消息，instance 2 获取到了压测消息，这说明测试成功了。</p><p>这样的话，后续业务我们就可以判断消息是不是从影子队列发过来的了。我们还可以将压测标记设置到数据上下文，最后根据这个标记来写数据库或进行其他操作。</p><h3>数据偏移实现</h3><p>好了，接下来，我们再来看下数据偏移的实现方式。</p><p>数据偏移方式会让压测请求和正常请求都进入到生产队列。我们要在压测消息的发布端加上标记，消费端加上识别。</p><p>你可以通过下面这张图片更直观地理解数据偏移的工作原理。</p><p><img src="https://static001.geekbang.org/resource/image/8b/b1/8b37915e1d056fbb989916f978e025b1.jpg?wh=1920x575" alt="图片"></p><p><strong>第一步，改造 RabbitConfig 配置类。</strong></p><p>在影子队列方案的基础上，我们还需要改造 RabbitConfig 配置类，创建一个名为 exchange.7d 的交换机、一个生产者、两个消费者和一个消息队列。队列需要通过路由键绑定到交换机，其中队列 7dQueue 的路由键为 produce。</p><p>因为消息的标记识别实际上就是对发布和消费做一个拦截处理，所以我们还要配置 1 个 Bean，在 Bean 的方法里面实现拦截的逻辑。</p><pre><code class="language-java">package com.dunshan.directheader;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.amqp.AmqpException;\nimport org.springframework.amqp.core.*;\nimport org.springframework.amqp.rabbit.config.SimpleRabbitListenerContainerFactory;\nimport org.springframework.amqp.rabbit.connection.CachingConnectionFactory;\nimport org.springframework.amqp.rabbit.connection.ConnectionFactory;\nimport org.springframework.amqp.rabbit.core.RabbitTemplate;\nimport org.springframework.boot.autoconfigure.amqp.SimpleRabbitListenerContainerFactoryConfigurer;\nimport org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.web.context.request.RequestContextHolder;\nimport org.springframework.web.context.request.ServletRequestAttributes;\n\nimport javax.servlet.http.HttpServletRequest;\nimport java.util.Map;\n\n/**\n * @author dunshan\n */\n\n\n\n@Configuration\n@Slf4j\npublic class DirectRabbitHeaderConfig {\n\n    @Bean\n    public DirectExchange directheader() {\n        return new DirectExchange("exchange.7dHeader");\n    }\n\n    @Bean\n    public Queue directheaderQueue() {\n        return new Queue("7dHeaderQueue");\n    }\n\n    @Bean\n    public Binding directheaderBinding1a(DirectExchange directheader, Queue directheaderQueue) {\n        return BindingBuilder.bind(directheaderQueue).to(directheader).with("produce");\n    }\n\n\n\n    @Bean\n    public DirectHeaderReceiver headerreceiver() {\n        return new DirectHeaderReceiver();\n    }\n\n\n\n    @Bean\n    public DirectHeaderSender directHeaderSender() {\n        return new DirectHeaderSender();\n    }\n\n    @Bean(name = "rabbitListenerContainerFactory")\n    @ConditionalOnProperty(prefix = "spring.rabbitmq.listener", name = "type", havingValue = "simple", matchIfMissing = true)\n    public SimpleRabbitListenerContainerFactory simpleRabbitListenerContainerFactory(SimpleRabbitListenerContainerFactoryConfigurer configurer,\n                                                                                     ConnectionFactory connectionFactory) {\n        SimpleRabbitListenerContainerFactory factory = new SimpleRabbitListenerContainerFactory();\n        factory.setAfterReceivePostProcessors(new MessagePostProcessor() {\n            @Override\n            public Message postProcessMessage(Message message) throws AmqpException {\n                Map header = message.getMessageProperties().getHeaders();\n                //判断是否压测消息，是的话要动态切换影子库跟后续操作\n                if (header.containsKey("test")){\n                    log.info("带压测标记！");\n                }\n                return message;\n            }\n        });\n        configurer.configure(factory, connectionFactory);\n        return factory;\n    }\n\n}\n</code></pre><p>每次接收消息都会调用拦截器对消息进行拦截处理，有压测消息标记的，就先存到数据上下文中，后续数据库会根据这个上下文切换影子库。</p><p><strong>第二步，改造 DirectSender 类。</strong></p><p>然后改造 DirectSender 类，发布消息时判断是否为压测标记，使用 MessageProperties 构建自定义 Header，以达到数据偏移的目的。</p><pre><code class="language-java">package com.dunshan.directheader;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.amqp.core.Message;\nimport org.springframework.amqp.core.MessageProperties;\nimport org.springframework.amqp.rabbit.core.RabbitTemplate;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.context.request.RequestContextHolder;\nimport org.springframework.web.context.request.ServletRequestAttributes;\n\nimport javax.servlet.http.HttpServletRequest;\n\n/**\n *\n * @author dunshan\n * @date 2021/10/1\n */\n\n@Slf4j\npublic class DirectHeaderSender {\n\n\t@Autowired\n\tprivate RabbitTemplate template;\n\n\tprivate static final String exchangeName = "exchange.7dHeader";\n\tprivate final String key = "produce";\n\n\tpublic void send(int index) {\n\n\t\t// 获取标记\n\t\tString header = AppContext.getContext().getFlag();\n\n\t\tString messageStr = "Hello to ";\n\n\t\tMessageProperties messageProperties = new MessageProperties();\n\t\t// 判断标记\n\t\tif (header != null &amp;&amp; "7DGroup".equals(header)) {\n\t\t\tmessageProperties.setHeader("test",true);\n\t\t\tMessage message = new Message(messageStr.getBytes(), messageProperties);\n\t\t\ttemplate.convertAndSend(exchangeName, key, message);\n\t\t\tlog.info(" [x] Sent \'{}\'", message);\n\t\t} else {\n\t\t\ttemplate.convertAndSend(exchangeName, key, messageStr);\n\t\t\tlog.info(" [x] Sent \'{}\'", messageStr);\n\t\t}\n\n\t}\n\n}\n</code></pre><p><strong>第三步，在 Controller 中添加测试接口。</strong></p><p>在 Controller 中添加测试接口，调用该接口，发布消息。</p><pre><code class="language-java">package com.dunshan.controller;\n\nimport cn.hutool.core.thread.ThreadUtil;\nimport com.dunshan.common.api.CommonResult;\nimport com.dunshan.direct.DirectSender;\nimport io.swagger.annotations.Api;\nimport io.swagger.annotations.ApiOperation;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestMethod;\nimport org.springframework.web.bind.annotation.ResponseBody;\n\n/**\n *\n * @author dunshan\n * @date 2021/10/1\n */\n\n@Api(tags = "RabbitController", description = "RabbitMQ 隔离测试")\n@Controller\n@RequestMapping("/rabbit")\npublic class RabbitController {\n\n    @Autowired\n    private DirectSender directSender;\n\n    @ApiOperation("数据偏移模式")\n    @RequestMapping(value = "/header", method = RequestMethod.GET)\n    @ResponseBody\n    public CommonResult headerTest() throws InterruptedException {\n\n        for(int i=0;i&lt;10;i++){\n            directHeaderSender.send(i);\n\n            ThreadUtil.sleep(1000);\n        }\n        return CommonResult.success(null);\n    }\n\n}\n</code></pre><p><strong>第四步，接口测试验证。</strong></p><p>运行后，可以通过 Postman 做接口测试，往队列中发送包含不同路由键的消息。</p><ul>\n<li>发送正常请求（不带压测标记）<br>\n<img src="https://static001.geekbang.org/resource/image/c4/20/c46dac88e60476dfdcd0404f4a657520.png?wh=1920x1227" alt="图片"></li>\n</ul><p>服务端的控制台日志如下：</p><p><img src="https://static001.geekbang.org/resource/image/53/f3/5330a85903fd3807542cbcde3b7228f3.png?wh=1516x600" alt="图片"></p><ul>\n<li>发送压测请求（带压测标记）<br>\n<img src="https://static001.geekbang.org/resource/image/66/91/66549483007e9cee6bfc95a53byyba91.png?wh=1920x1363" alt="图片"></li>\n</ul><p>服务端的控制台日志如下：</p><p><img src="https://static001.geekbang.org/resource/image/5f/95/5f12f1f4f589f53127f43b7f69fd8a95.png?wh=1535x635" alt="图片"></p><p>我们可以看到，instance 1 第一次获取到了正常消息，第二次获取到了压测标记的消息，这说明测试成功了。</p><p>刚才我们介绍了影子队列和数据偏移两种数据隔离方案并分别进行了 demo 预演。预演成功，接下来，我们就要将 demo 移植到真实系统进行改造了。</p><h2>真实系统改造</h2><p>在进行改造之前，先来看下我们项目的实际情况。</p><p>我们项目的消息队列主要是用于解决用户下单以后，订单超时想要取消订单的问题的。</p><p><img src="https://static001.geekbang.org/resource/image/7c/a8/7cc80114df44be83b9c2743e80b0a3a8.jpg?wh=1920x742" alt="图片"></p><p>项目主要的业务流程是：</p><ul>\n<li>用户进行下单操作（会有锁定商品库存、使用优惠券、积分一系列的操作）；</li>\n<li>生成订单，获取订单的 id；</li>\n<li>获取到设置的订单超时时间（假设设置的为 60 分钟不支付取消订单）；</li>\n<li>按订单超时时间发送一个延迟消息给 RabbitMQ，让它在订单超时后触发取消订单的操作；</li>\n<li>如果用户没有支付，进行取消订单操作（释放锁定商品库存、返还优惠券、返回积分一系列操作）。</li>\n</ul><p>RabbitMQ 实现延迟消息的方式有两种，一种是用死信队列实现，另一种是用延迟插件实现，我们这个项目使用的是死信队列的方式。</p><p>查看添加消息队列的枚举配置类 QueueEnum。</p><pre><code class="language-java">import lombok.Getter;\n\n/**\n * 消息队列枚举配置\n * Created by dunshan on 2018/9/14.\n */\n@Getter\npublic enum QueueEnum {\n    /**\n     * 消息通知队列\n     */\n    QUEUE_ORDER_CANCEL("mall.order.direct", "mall.order.cancel", "mall.order.cancel"),\n    \n    /**\n     * 消息通知 ttl 队列\n     */\n    QUEUE_TTL_ORDER_CANCEL("mall.order.direct.ttl", "mall.order.cancel.ttl", "mall.order.cancel.ttl");\n\n    /**\n     * 交换名称\n     */\n    private String exchange;\n   \n     /**\n     * 队列名称\n     */\n    private String name;\n    \n    /**\n     * 路由键\n     */\n    private String routeKey;\n\n    QueueEnum(String exchange, String name, String routeKey) {\n        this.exchange = exchange;\n        this.name = name;\n        this.routeKey = routeKey;\n    }\n}\n</code></pre><p>这里，我们定义了两个消息队列，即 mall.order.cancel 和 mall.order.cancel.ttl。</p><ul>\n<li><strong>mall.order.direct（取消订单消息队列所绑定的交换机）：</strong>绑定的队列为 mall.order.cancel，一旦有消息以 mall.order.cancel 为路由键发过来，就会发送到这个队列。</li>\n<li><strong>mall.order.direct.ttl（订单延迟消息队列所绑定的交换机）：</strong>绑定的队列为 mall.order.cancel.ttl，一旦有消息以 mall.order.cancel.ttl 为路由键发送过来，就会转发到此队列，并在此队列保存一定时间，等到超时后会自动将消息发送到 mall.order.cancel（取消订单消息消费队列）。</li>\n</ul><p>启动服务后，我们去 RabbitMQ 控制台确认一下。</p><p><img src="https://static001.geekbang.org/resource/image/ed/a4/ed80a1f0867b62ca2923d21a605488a4.png?wh=1920x600" alt="图片"></p><p>这里我们选择的改造方案是使用<strong>数据偏移</strong>，因为相对影子队列的方式来说，数据偏移要改造的代码更少。</p><p><strong>第一步，移植获取标记类。</strong></p><p>快速移植 demo 中的全局 Filter 过滤器和数据上下文类：</p><ul>\n<li>AppContext；</li>\n<li>ContextFilter。</li>\n</ul><p>这个操作前面说过很多次了，这里就不再多赘述了。</p><p>​<strong>第二步，改造 CancelOrderSender 类。</strong></p><p>改造 CancelOrderSender 类，发送时判断是否为压测标记，使用 MessageProperties 构建自定义 Header。</p><pre><code class="language-java">package com.dunshan.mall.order.component;\n\nimport com.dunshan.mall.order.domain.QueueEnum;\nimport com.dunshan.mall.order.config.AppContext;\nimport lombok.extern.slf4j.Slf4j;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.amqp.AmqpException;\nimport org.springframework.amqp.core.AmqpTemplate;\nimport org.springframework.amqp.core.Message;\nimport org.springframework.amqp.core.MessagePostProcessor;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.context.request.RequestContextHolder;\nimport org.springframework.web.context.request.ServletRequestAttributes;\n\nimport javax.servlet.http.HttpServletRequest;\n\n/**\n * 取消订单消息的发出者\n * Created by dunshan on 2018/9/14.\n */\n@Component\n@Slf4j\npublic class CancelOrderSender {\n    private static Logger LOGGER =LoggerFactory.getLogger(CancelOrderSender.class);\n    \n    @Autowired\n    private AmqpTemplate amqpTemplate;\n\n    public void sendMessage(Long orderId,final long delayTimes){\n    \n        // 获取标记\n        String header = AppContext.getContext().getFlag();\n\n        LOGGER.info("RabbitMQ 获取标记 header:{}",header);\n\n        //给延迟队列发送消息\n        amqpTemplate.convertAndSend(QueueEnum.QUEUE_TTL_ORDER_CANCEL.getExchange(), QueueEnum.QUEUE_TTL_ORDER_CANCEL.getRouteKey(), orderId, new MessagePostProcessor() {\n\n            @Override\n            public Message postProcessMessage(Message message) throws AmqpException {\n\n                if ("7DGroup".equals(header)) {\n                    message.getMessageProperties().setHeader("test",true);\n                }\n\n                //给消息设置延迟毫秒值\n                message.getMessageProperties().setExpiration(String.valueOf(delayTimes));\n                return message;\n            }\n        });\n        LOGGER.info("send orderId:{}",orderId);\n    }\n}\n</code></pre><p>​<strong>第三步，改造 RabbitMqConfig 类。</strong></p><p>然后，改造 RabbitMqConfig 配置类。配置 1 个 Bean，在 Bean 的方法里面实现拦截的逻辑。</p><p>每次接收消息都会调用拦截器对消息进行拦截处理，有压测消息标记的，就先存到数据上下文中，后续数据库会根据这个上下文切换影子库。</p><pre><code class="language-java">package com.dunshan.mall.order.config;\n\nimport com.dunshan.mall.order.config.AppContext;\nimport com.dunshan.mall.order.domain.QueueEnum;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.amqp.AmqpException;\nimport org.springframework.amqp.core.*;\nimport org.springframework.amqp.rabbit.config.SimpleRabbitListenerContainerFactory;\nimport org.springframework.amqp.rabbit.connection.ConnectionFactory;\nimport org.springframework.boot.autoconfigure.amqp.SimpleRabbitListenerContainerFactoryConfigurer;\nimport org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\nimport java.util.Map;\n\n/**\n * 消息队列配置\n * Created by dunshan on 2018/9/14.\n */\n\n@Configuration\n@Slf4j\npublic class RabbitMqConfig {\n\n    /**\n     * 订单消息实际消费队列所绑定的交换机\n     */\n    @Bean\n    DirectExchange orderDirect() {\n        return (DirectExchange) ExchangeBuilder\n                .directExchange(QueueEnum.QUEUE_ORDER_CANCEL.getExchange())\n                .durable(true)\n                .build();\n    }\n\n    /**\n     * 订单延迟队列队列所绑定的交换机\n     */\n    @Bean\n    DirectExchange orderTtlDirect() {\n        return (DirectExchange) ExchangeBuilder\n                .directExchange(QueueEnum.QUEUE_TTL_ORDER_CANCEL.getExchange())\n                .build();\n    }\n\n    /**\n     * 订单实际消费队列\n     */\n    @Bean\n    public Queue orderQueue() {\n        return new Queue(QueueEnum.QUEUE_ORDER_CANCEL.getName());\n    }\n\n    /**\n     * 订单延迟队列（死信队列）\n     */\n    @Bean\n    public Queue orderTtlQueue() {\n        return QueueBuilder\n                .durable(QueueEnum.QUEUE_TTL_ORDER_CANCEL.getName())\n                .withArgument("x-dead-letter-exchange", QueueEnum.QUEUE_ORDER_CANCEL.getExchange())//到期后转发的交换机\n                .withArgument("x-dead-letter-routing-key", QueueEnum.QUEUE_ORDER_CANCEL.getRouteKey())//到期后转发的路由键\n                .build();\n    }\n\n    /**\n     * 将订单队列绑定到交换机\n     */\n    @Bean\n    Binding orderBinding(DirectExchange orderDirect,Queue orderQueue){\n        return BindingBuilder\n                .bind(orderQueue)\n                .to(orderDirect)\n                .with(QueueEnum.QUEUE_ORDER_CANCEL.getRouteKey());\n    }\n\n    /**\n     * 将订单延迟队列绑定到交换机\n     */\n    @Bean\n    Binding orderTtlBinding(DirectExchange orderTtlDirect,Queue orderTtlQueue){\n        return BindingBuilder\n                .bind(orderTtlQueue)\n                .to(orderTtlDirect)\n                .with(QueueEnum.QUEUE_TTL_ORDER_CANCEL.getRouteKey());\n    }\n\n    @Bean(name = "rabbitListenerContainerFactory")\n    @ConditionalOnProperty(prefix = "spring.rabbitmq.listener", name = "type", havingValue = "simple", matchIfMissing = true)\n    public SimpleRabbitListenerContainerFactory simpleRabbitListenerContainerFactory(SimpleRabbitListenerContainerFactoryConfigurer configurer,\n                                                                                     ConnectionFactory connectionFactory) {\n        SimpleRabbitListenerContainerFactory factory = new SimpleRabbitListenerContainerFactory();\n        //消息接收之前加拦截处理，每次接收消息都会调用，是有压测消息标记的，先存到副本变量，后续的操作数据库根据这个变量进行切换影子库\n        factory.setAfterReceivePostProcessors(new MessagePostProcessor() {\n            @Override\n            public Message postProcessMessage(Message message) throws AmqpException {\n                Map header = message.getMessageProperties().getHeaders();\n                //判断是压测的消息，动态切换影子库跟后续操作\n                if (header.containsKey("test")){\n                    AppContext.getContext().setFlag("7DGroup");\n                    log.info("RabbitMQ 收到压测标记！");\n                }\n                return message;\n            }\n        });\n        configurer.configure(factory, connectionFactory);\n        return factory;\n    }\n\n}\n</code></pre><p>这样我们的改造就完成了，接着我们来测试验证下。</p><p>​<strong>第四步，接口测试验证。</strong></p><p>使用 curl 命令调用下单接口，这里带上了 Header 压测标记。</p><pre><code class="language-bash"> ~  curl -X POST -H  "Accept:*/*" -H  "Authorization:eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX25hbWUiOiJsaXNpIiwic2NvcGUiOlsiYWxsIl0sImlkIjo1LCJleHAiOjE2MzMzNjA1ODYsImF1dGhvcml0aWVzIjpbIuWJjeWPsOS8muWRmCJdLCJqdGkiOiJiZGU3MWE4Mi0zZmIzLTRkOWMtODBhZC1lZDVkNzQyYjk3YjUiLCJjbGllbnRfaWQiOiJwb3J0YWwtYXBwIn0.QmQ4nH6kGkj-yLRanUGno8ET8Vh10Sku1YUGBmZdlosZwPFAYpJNjfejBoP7OLBryXg6ilgYs6fOuydJXS28PAozNb086lxWlJtDgSIvqNN0vPtr2u5Hw-DUVww4xDX2ER_ZkHHmgj7B2fVHpD0wRdjxg49lVAcA_QoNOoN1R70" -H  "Request-Origion:Knife4j" -H  "Content-Type:application/json" -H "dunshan:7DGroup" -d "{\\"cartIds\\":[12677173],\\"couponId\\":null,\\"memberReceiveAddressId\\":2503350,\\"payType\\":0,\\"useIntegration\\":null}" "http://127.0.0.1:8201/api/order/order/generateOrder"\n{"code":200,"message":"下单成功","data":{"orderItemList":[{"orderId":31813547,"orderSn":"202110050100000004","productId":16,"productPic":"https://perfo7d.oss-cn-beijing.aliyuncs.com/mall/images/20200923/1522738681.jpg","productName":"毛衫测试","productBrand":"小米","productSn":"NO.1098","productPrice":99.00,"productQuantity":2,"productSkuId":16,"productSkuCode":"202008270027016","productCategoryId":19,"promotionName":"无优惠","promotionAmount":0,"giftIntegration":99,"giftGrowth":99,"productAttr":"[{\\"key\\":\\"颜色\\",\\"value\\":\\"银色\\"},{\\"key\\":\\"容量\\",\\"value\\":\\"32G\\"}]"}],"order":{"id":31813547,"memberId":5,"orderSn":"202110050100000004","createTime":"2021-10-05T09:30:30.314+00:00","memberUsername":"lisi","totalAmount":198.00,"payAmount":198.00,"freightAmount":0,"promotionAmount":0,"integrationAmount":0,"couponAmount":0,"discountAmount":0,"payType":0,"sourceType":1,"status":0,"orderType":0,"autoConfirmDay":15,"integration":198,"growth":198,"promotionInfo":"无优惠","receiverName":"test","receiverPhone":"186xxxx8888","receiverPostCode":"123","receiverProvince":"北京","receiverCity":"北京","receiverRegion":"海淀区","receiverDetailAddress":"北京","confirmStatus":0,"deleteStatus":0}}}%\n</code></pre><p>注意：为了方便我们测试验证，这里已经将延迟消息时间调小了。</p><p>查看下 RabbitMQ  控制台。</p><p><img src="https://static001.geekbang.org/resource/image/41/69/417ccb81358184df849201a932433c69.png?wh=1920x652" alt="图片"></p><p>等待一会，查看下 order 服务的控制台日志。</p><pre><code class="language-bash">2021-10-05 17:30:30.705  INFO [mall-order,04190964bd8bce42,249e170cadf982e8,true] 17792 --- [nio-8084-exec-4] c.d.m.order.component.CancelOrderSender  : RabbitMQ 获取标记 header:7DGroup\n2021-10-05 17:30:30.706  INFO [mall-order,04190964bd8bce42,249e170cadf982e8,true] 17792 --- [nio-8084-exec-4] c.d.m.order.component.CancelOrderSender  : send orderId:31813547\n2021-10-05 17:30:30.706  INFO [mall-order,04190964bd8bce42,249e170cadf982e8,true] 17792 --- [nio-8084-exec-4] c.d.m.o.s.impl.PortalOrderServiceImpl    : 结果：{"orderItemList":[{"productSkuCode":"202008270027016","productSkuId":16,"productId":16,"productSn":"NO.1098","orderId":31813547,"orderSn":"202110050100000004","productBrand":"小米","productPic":"https://perfo7d.oss-cn-beijing.aliyuncs.com/mall/images/20200923/1522738681.jpg","giftIntegration":99,"productName":"毛衫测试","productAttr":"[{\\"key\\":\\"颜色\\",\\"value\\":\\"银色\\"},{\\"key\\":\\"容量\\",\\"value\\":\\"32G\\"}]","productQuantity":2,"promotionName":"无优惠","productCategoryId":19,"giftGrowth":99,"promotionAmount":0,"productPrice":99.00}],"order":{"orderType":0,"integrationAmount":0,"orderSn":"202110050100000004","discountAmount":0,"receiverProvince":"北京","receiverCity":"北京","autoConfirmDay":15,"couponAmount":0,"payAmount":198.00,"payType":0,"receiverPhone":"186xxxx8888","receiverPostCode":"123","receiverRegion":"海淀区","deleteStatus":0,"memberUsername":"lisi","confirmStatus":0,"id":31813547,"memberId":5,"freightAmount":0,"receiverName":"test","promotionInfo":"无优惠","receiverDetailAddress":"北京","totalAmount":198.00,"createTime":1633426230314,"sourceType":1,"integration":198,"growth":198,"promotionAmount":0,"status":0}}\n2021-10-05 17:34:30.769  INFO [mall-order,,,] 17792 --- [ntContainer#0-1] c.dunshan.mall.order.context.AppContext  : 打印压测标记：7DGroup\n2021-10-05 17:34:30.772  INFO [mall-order,,,] 17792 --- [ntContainer#0-1] c.d.mall.order.config.RabbitMqConfig     : RabbitMQ 收到压测标记！\n2021-10-05 17:34:30.902  INFO [mall-order,04190964bd8bce42,c0e11235f08391aa,true] 17792 --- [ntContainer#0-1] c.d.m.o.s.impl.PortalOrderServiceImpl    : 收到取消订单！31813547\n2021-10-05 17:34:30.991  INFO [mall-order,04190964bd8bce42,c0e11235f08391aa,true] 17792 --- [ntContainer#0-1] c.d.m.o.component.CancelOrderReceiver    : process orderId:31813547\n</code></pre><p>可以看到，压测标记已经被透传下去了。<br>\n再次确认下 RabbitMQ  控制台。</p><p><img src="https://static001.geekbang.org/resource/image/8d/72/8d90ddd05c96369dffcba44133cd7572.png?wh=1920x745" alt="图片"></p><p>截图显示，延迟的消息已经被成功消费。</p><p>接下来，我们测试一下正常请求，使用 web API 文档调用下单接口。</p><p><img src="https://static001.geekbang.org/resource/image/f1/83/f1d089aeae99ac532455665570025a83.png?wh=1920x1106" alt="图片"></p><p>等待一会，查看下 Order 服务的控制台日志。</p><pre><code class="language-bash"> 18:15:58.108  INFO [mall-order,682d3874c34b8dca,f8f6916562572089,true] 18776 --- [nio-8084-exec-9] c.d.m.order.component.CancelOrderSender  : RabbitMQ 获取标记 header:null\n2021-10-05 18:15:58.114  INFO [mall-order,682d3874c34b8dca,f8f6916562572089,true] 18776 --- [nio-8084-exec-9] c.d.m.order.component.CancelOrderSender  : send orderId:31813549\n2021-10-05 18:15:58.117  INFO [mall-order,682d3874c34b8dca,f8f6916562572089,true] 18776 --- [nio-8084-exec-9] c.d.m.o.s.impl.PortalOrderServiceImpl    : 结果：{"orderItemList":[{"productSkuCode":"202008270027016","productSkuId":16,"productId":16,"productSn":"NO.1098","orderId":31813549,"orderSn":"202110050100000006","productBrand":"小米","productPic":"https://perfo7d.oss-cn-beijing.aliyuncs.com/mall/images/20200923/1522738681.jpg","giftIntegration":99,"productName":"毛衫测试","productAttr":"[{\\"key\\":\\"颜色\\",\\"value\\":\\"银色\\"},{\\"key\\":\\"容量\\",\\"value\\":\\"32G\\"}]","productQuantity":2,"promotionName":"无优惠","productCategoryId":19,"giftGrowth":99,"promotionAmount":0,"productPrice":99.00}],"order":{"orderType":0,"integrationAmount":0,"orderSn":"202110050100000006","discountAmount":0,"receiverProvince":"北京","receiverCity":"北京","autoConfirmDay":15,"couponAmount":0,"payAmount":198.00,"payType":0,"receiverPhone":"186xxxx8888","receiverPostCode":"123","receiverRegion":"海淀区","deleteStatus":0,"memberUsername":"lisi","confirmStatus":0,"id":31813549,"memberId":5,"freightAmount":0,"receiverName":"test","promotionInfo":"无优惠","receiverDetailAddress":"北京","totalAmount":198.00,"createTime":1633428957701,"sourceType":1,"integration":198,"growth":198,"promotionAmount":0,"status":0}}\n2021-10-05 18:19:58.562  INFO [mall-order,682d3874c34b8dca,f1942f9153ec7729,true] 18776 --- [ntContainer#0-1] c.d.m.o.s.impl.PortalOrderServiceImpl    : 收到取消订单！31813549\n2021-10-05 18:19:58.650  INFO [mall-order,682d3874c34b8dca,f1942f9153ec7729,true] 18776 --- [ntContainer#0-1] c.d.m.o.component.CancelOrderReceiver    : process orderId:31813549\n</code></pre><p>可以看到，消息是正常处理的，这说明我们的消息隔离改造成功了。</p><h2>总结</h2><p>好了，这节课就讲到这里。刚才，我们完整演示了 RabbitMQ 消息隔离的技术预演和真实系统改造过程。这节课有几个要点，我希望你能够记住：</p><ol>\n<li>RabbitMQ 消息隔离主要解决的是异步任务之间标记透传的需求；</li>\n<li>引入数据上下文对象能够兼容不同的应用协议，同时能够支持跨线程间及MQ异步任务的透传，最重要的是能支持随时存取；</li>\n<li>相比数据库隔离，消息隔离的方案较少，主要为<strong>影子队列</strong>和<strong>数据偏移</strong>。</li>\n</ol><p>我们这里只是介绍了 RabbitMQ 消息隔离方案。从技术实现来看，其它 MQ 产品的隔离逻辑是一致的。你也可以根据自己项目的实际情况和需求灵活选择，也欢迎有问题来和我讨论。</p><h2>课后题</h2><p>学完这节课，我想请你思考两个问题：</p><ol>\n<li>除了 RabbitMQ，你在工作中还接触过哪些 MQ 产品，它们有什么区别？</li>\n<li>相比数据库隔离，你觉得 MQ 消息隔离的难点在哪里？</li>\n</ol><p>欢迎你在留言区与我交流讨论。我们下节课见！</p>',
        article_title: "18 | 流量隔离：RabbitMQ 消息隔离是怎么做的？",
      },
      {
        title: "19｜日志隔离：如何落地日志隔离？",
        id: 461201,
        content:
          '<p>你好，我是高楼。</p><p>这节课，我们详细来讲讲如何基于微服务技术落地日志隔离。</p><p>在全链路压测的隔离中，有一个很重要的部分就是日志隔离，即区分对应的日志。之所以要将正常日志和压测日志都进行隔离，是为了不影响正常日志存储和方便线上问题排查。</p><p>在当前的技术市场上，主流的日志框架是Log4j、Log4j2（Log4j的升级版）和Logback。</p><p>Logback 是Log4j的改良版，拥有更多的特性，性能也更好，天然支持 SLF4J（ Simple Logging Facade for Java，日志输出门面，非日志框架）。Slf4j可以支持不同的日志框架，实现对日志框架的一层抽象，可以整理不同的日志框架的输出。我们这个电商项目也是使用的 Logback 的方式。</p><p>在具体的方案选择上，我们选择改写 Logback 的 Appender，然后根据压测标记将日志写入对应目录中。</p><p>既然已经选定了Logback日志框架做日志隔离改造，那么我们就可以直接上手进行技术预演了。</p><h2>技术预演</h2><p>首先我们还是先在本地开发环境做一下日志隔离的技术预演。预演通过之后，再把相关代码移植到demo系统中去验证。</p><p>在第 14-18 讲，我们已经通过 Sleuth 把压测标记透传到了每个服务中，下面只需要从对应服务的数据上下文获取标记，然后完成逻辑判断就能实现日志的分离了。</p><!-- [[[read_end]]] --><p>考虑到日志打印一般是无序的，在一个用户的使用过程中，可能有多个不同的线程来进行处理。典型的例子是当用户访问某个页面时，应用服务可能会创建一个新的线程来处理该请求，也可能从线程池中复用已有的线程。在一个用户 session 存续期间，可能有多个线程处理过该用户的请求。当需要追踪某个用户在系统中的相关日志记录时，就会变得很麻烦。</p><p>解决办法就是引入 MDC（Mapped Diagnostic Context，映射调试上下文）。而 <a href="http://www.slf4j.org/">SLF4J </a>提供了<a href="http://www.slf4j.org/api/org/slf4j/MDC.html"> MDC</a> ( Mapped Diagnostic Contexts )功能，它的实现也是利用了 ThreadLocal 机制。改造代码只需要将指定的值 put 到线程上下文的 Map 中，然后在对应的地方使用 get 方法获取对应的值，从而达到自定义和修改日志输出格式内容的目的。</p><p>考虑到压测流量比较大，产生的日志也非常多，压测流量的日志很容易覆盖正常流量的日志。我们选择改写 Logback&nbsp;的 Appender，根据流量标记将日志写入对应的影子目录中，并且配置默认的删除策略，日志只保存三天。</p><p>整体方案设计可以参考下面这张图：</p><p><img src="https://static001.geekbang.org/resource/image/ca/32/ca406e7313aeb885b55acfcbbb2db632.jpg?wh=1764x1920" alt="图片"></p><p>下面我们来看一下详细的操作步骤。</p><h3>通过 MDC 实现日志分离</h3><p>​<strong>第一步，创建测试接口。</strong></p><p>在 Controller 中添加下面的代码，请求资源路径为 <a href="http://localhost:8080/mdc/1">/mdc/1</a>  。其中，我们通过 {id} 来判断要不要把 MDC 信息写入日志中。</p><pre><code class="language-java">    @GetMapping("/mdc/{id}")\n    @ResponseBody\n    public Object MdcGet(@PathVariable Integer id) {\n        HashMap&lt;String, Object&gt; response = new HashMap&lt;&gt;();\n        response.put("mdc", "调试");\n        response.put("time", new Date());\n         if (id == 1) {\n            MDC.put("dunshan", "flag");\n            log.info("日志写入 " + id);\n            MDC.remove("dunshan");\n        } else {\n            log.info("不是 mdc 日志 " + id);\n        }\n        return response;\n    }\n</code></pre><p>​<strong>第二步，修改配置文件。</strong></p><p>代码写好后，需要在 logback-spring.xml 中添加下面的配置：</p><pre><code class="language-xml"> &lt;pattern&gt;[dunshan :%X{dunshan}]  %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger - %msg%n&lt;/pattern&gt;\n</code></pre><p>这个配置主要目的是格式化日志。</p><p>​<strong>第三步，接口测试验证。</strong></p><p>在浏览器中发送一次请求，后台日志就会根据规则输出对应内容。</p><p><img src="https://static001.geekbang.org/resource/image/a3/92/a38f125cdbca92516ec327340b3a8892.png?wh=770x268" alt="图片"></p><p>再来看一下服务端日志的显示情况，我们看到 MDC 信息已经成功加入到控制台日志了。</p><p><img src="https://static001.geekbang.org/resource/image/b1/57/b13b120fd154eeb08a5yyf3e638ae757.png?wh=1920x410" alt="图片"></p><h3></h3><p>​<strong>第四步，创建 GenericFilterBean 全局过滤类。</strong></p><p>我们通过实现全局 Filter 类 GenericFilterBean 获取标记并加入 MDC 上下文中，这样，后续我们就可以根据 MDC 将压测流量和正常流量的日志分别保存到不同文件目录中去了。</p><p>参考代码如下：</p><pre><code class="language-java">@Component\npublic class MyFilter extends GenericFilterBean {\n\n    @Override\n    public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException {\n        HttpServletRequest request = (HttpServletRequest) servletRequest;\n        String flag = request.getHeader("flag");\n        if (StringUtils.isNotBlank(flag)) {\n            MDC.put("dunshan", "shadow");\n            logger.info("flag：" + flag);\n        } else {\n            MDC.put("dunshan", "produce");\n        }\n        filterChain.doFilter(servletRequest, servletResponse);\n    }\n\n    @Override\n    public void destroy() {\n        MDC.clear();\n    }\n}\n</code></pre><p><strong>​第五步，创建 Logback 配置文件。</strong></p><p>标记增加成功后，再在 logback-spring-xml 中添加 Log 输出格式、路径、级别、输出位置以及MDC的配置等信息：</p><pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;\n&lt;configuration&gt;\n    &lt;!-- https://github.com/spring-projects/spring-boot/blob/v1.5.13.RELEASE/spring-boot/src/main/resources/org/springframework/boot/logging/logback/defaults.xml --&gt;\n    &lt;include resource="org/springframework/boot/logging/logback/defaults.xml" /&gt;\n\n    &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt;\n    &lt;property name="FILE_LOG_PATTERN"\n              value="%d{yyyy-MM-dd HH:mm:ss.SSS} ${LOG_LEVEL_PATTERN:-%5p} [%t] [dunshan:%X{dunshan}] %-40.40logger{39} : %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}"/&gt;\n    &lt;property name="CONSOLE_LOG_PATTERN"\n              value="%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}"/&gt;/&gt;\n\n    &lt;property name="APP_NAME" value="dunshan-log" /&gt;\n    &lt;!--    文件保存路径注意--&gt;\n    &lt;property name="LOG_PATH" value="logs/${APP_NAME}/" /&gt;\n    &lt;property name="LOG_FILE" value="${LOG_PATH}/application.log" /&gt;\n    \n    &lt;appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender"&gt;\n        &lt;encoder&gt;\n            &lt;pattern&gt;${CONSOLE_LOG_PATTERN}&lt;/pattern&gt;\n            &lt;charset&gt;utf8&lt;/charset&gt;\n        &lt;/encoder&gt;\n    &lt;/appender&gt;\n    \n    &lt;appender name="MDC"\n              class="ch.qos.logback.classic.sift.SiftingAppender"&gt;\n        &lt;discriminator&gt;\n            &lt;key&gt;dunshan&lt;/key&gt;\n            &lt;defaultValue&gt;produce&lt;/defaultValue&gt;\n        &lt;/discriminator&gt;\n        &lt;sift&gt;\n            &lt;appender name="FILE-${logFileName}" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt;\n                &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt;\n                    &lt;!-- 按天回滚 daily --&gt;\n                    &lt;fileNamePattern&gt;${LOG_PATH}/${dunshan}/log-info-${dunshan}-%d{yyyy-MM-dd}-%i.log&lt;/fileNamePattern&gt;\n                    &lt;!-- 日志最大的历史 30天 --&gt;\n                    &lt;maxHistory&gt;7&lt;/maxHistory&gt;\n                    &lt;timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"&gt;\n                        &lt;!-- maxFileSize:这是活动文件的大小，默认值是10MB，这里设置为500MB --&gt;\n                        &lt;maxFileSize&gt;50MB&lt;/maxFileSize&gt;\n                        &lt;totalSizeCap&gt;20GB&lt;/totalSizeCap&gt;\n                    &lt;/timeBasedFileNamingAndTriggeringPolicy&gt;\n                &lt;/rollingPolicy&gt;\n                &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt;\n                    &lt;pattern&gt;${FILE_LOG_PATTERN}&lt;/pattern&gt;\n                &lt;/encoder&gt;\n                &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt;\n                    &lt;!-- 只打印info日志 --&gt;\n                    &lt;level&gt;INFO&lt;/level&gt;\n                    &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;\n                    &lt;onMismatch&gt;DENY&lt;/onMismatch&gt;\n                &lt;/filter&gt;\n            &lt;/appender&gt;\n        &lt;/sift&gt;\n    &lt;/appender&gt;\n\n\n\n    &lt;root level="INFO"&gt;\n        &lt;appender-ref ref="CONSOLE" /&gt;\n        &lt;appender-ref ref="MDC" /&gt;\n    &lt;/root&gt;\n&lt;/configuration&gt;\n</code></pre><p><strong>​第六步，接口测试验证。</strong></p><p>代码添加成功之后，我们还是使用 JMeter 模拟正常流量和压测流量的请求验证一下。</p><p>首先，我们使用 JMeter 创建两个请求。</p><ul>\n<li>\n<p>正常流量（不带 Header 标记）<br>\n<img src="https://static001.geekbang.org/resource/image/a6/b4/a615dd6af2a6dda1db5ffc19b3a486b4.png?wh=1920x609" alt="图片"></p>\n</li>\n<li>\n<p>压测流量（带Header 标记）<br>\n<img src="https://static001.geekbang.org/resource/image/3a/f7/3aa7607580f03b7a295a5ca7186509f7.png?wh=1920x372" alt="图片"></p>\n</li>\n</ul><p>场景设置并发100次，循环100次，执行后，查看控制台。</p><p><img src="https://static001.geekbang.org/resource/image/4c/2d/4ce7e61190ea54a8d2e6ca3580001f2d.png?wh=1882x572" alt="图片"></p><p>在控制台中，我们可以看到正式流量和压测流量的日志已经被分离。</p><p>打开配置的日志文件路径，查看日志分离情况。</p><p><img src="https://static001.geekbang.org/resource/image/d4/e3/d47acfa63eb7ede3cyy516154b1100e3.png?wh=830x256" alt="图片"></p><p>我们再打开日志看看是否显示正常。</p><ul>\n<li>正常日志<br>\n先打开正常日志文件，可以看到日志生产正常。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/d7/17/d7fc0f51f40ee8a4028a4575a26fee17.png?wh=1920x370" alt="图片"></p><ul>\n<li>压测日志<br>\n再打开影子目录下的压测日志，查看压测日志也生成正常。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/23/42/236619194918664b0bbd265198b99c42.png?wh=1920x380" alt="图片"></p><p>这说明单个服务的日志分离成功了。</p><p>刚才我们介绍了日志隔离方案并分别进行了技术预演。预演成功，接下来，我们就要将相关代码移植到demo系统进行验证了。</p><h3>demo 改造验证</h3><p>首先，我们写个 demo 尝试做一下技术改造验证。</p><p>这里使用的 demo 工程还是 14 讲的示例，主要包括网关、会员系统、购物车系统和订单系统 4 个服务：</p><p><img src="https://static001.geekbang.org/resource/image/ba/27/ba3d569dc9e4f2b98f21b60c46b7dc27.jpg?wh=1920x378" alt="图片"></p><p>接下来我们主要改造会员系统，实现日志隔离技术预演。</p><p><strong>第一步，移植获取标记类。</strong></p><p>快速移植全局 Filter 过滤器和数据上下文类：</p><ul>\n<li>AppContext；</li>\n<li>ContextFilter。</li>\n</ul><p>这个操作前面说过很多次了，这里就不再多赘述了。</p><p>不过在日志隔离改造环节，网关服务需要额外通过实现全局 Filter 类获取标记，然后加入 MDC 上下文中。</p><pre><code class="language-java">@Log4j2\n@Component\npublic class ContextFilter implements Filter {\n\n    @Override\n    public void init(FilterConfig filterConfig) throws ServletException {\n        Filter.super.init(filterConfig);\n    }\n\n    @Override\n    public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException {\n        BaggageField dunshan = BaggageField.getByName("dunshan");\n        String value = dunshan.getValue();\n        AppContext appContext = new AppContext();\n        if (StringUtils.isNotBlank(value)) {\n            log.info("压测流量: " + value);\n            appContext.setFlag(value);\n            AppContext.setContext(appContext);\n            MDC.put("dunshan","shadow");\n        } else {\n            log.info("正常流量: " + value);\n            MDC.put("dunshan","produce");\n            AppContext.setContext(appContext);\n        }\n        filterChain.doFilter(servletRequest, servletResponse);\n    }\n\n    @Override\n    public void destroy() {\n        AppContext.removeContext();\n        Filter.super.destroy();\n\n    }\n}\n</code></pre><p><strong>第二步，移植​ L<strong><strong>ogback 配置文件</strong></strong>。</strong></p><p>这里我们把上面 Logback-spring-xml 配置文件快速移植到 member 服务，其他服务也可以按上面的配置做相应改造。为了演示的完整性，我们又继续改造了 demo 中的 cart 和 order 服务。</p><p>改造完成后，启动网关、memeber、cart 、order服务：</p><p><img src="https://static001.geekbang.org/resource/image/67/5f/67028dd784a126c696yy5e30cef5035f.png?wh=1260x220" alt="图片"></p><p><strong>第三步，接口测试验证。</strong></p><p>打开 JMeter 添加请求接口，注意，这个请求也要区分压测流量（带标记的请求）和正常流量（不带标记的请求）。</p><p>请求完成后，日志目录显示的是下面的样子。</p><p><img src="https://static001.geekbang.org/resource/image/97/ab/9761e3db136de7fda7ccfe9f056572ab.png?wh=1596x886" alt="图片"></p><p>我们看到，正常日志与压测日志目录是分开的。</p><p>但是我们还发现，日志打印是混乱的，日志当中有压测日志，也有正常日志，如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/09/d2/0957aa532b706c6d4ed0dda5852be9d2.png?wh=1920x585" alt="图片"></p><p>也就是说，虽然流量已经识别了，日志也分离了，但日志输出是无序的，这显然不是我们想要的结果。</p><p>我们知道 MDC 底层使用的是 ThreadLocal，天然就存在这个缺陷。那么，我们这么办呢？所以在这里，我们考虑引入<strong>数据上下文</strong>对象来解决这个问题。</p><p><strong>第四步，改造数据上下文类。</strong></p><p>修改数据上下文类，我们可以直接把标记存储到 MDC 中去，你可以参考下面的代码：</p><pre><code class="language-java">@Log4j2\npublic class AppContext implements Serializable {\n\n    private static final TransmittableThreadLocal&lt;AppContext&gt; contextdunshan = new TransmittableThreadLocal&lt;&gt;();\n\n    private String Flag;\n\n    public static AppContext getContext() {\n        return contextdunshan.get();\n    }\n\n    public static void setContext(AppContext context) {\n        //判断标记\n        if (StringUtils.isNotEmpty(context.getFlag())) {\n            MDC.put("dunshan", "shadow");\n        } else {\n            MDC.put("dunshan", "produce");\n        }\n        contextdunshan.set(context);\n    }\n\n    public static void removeContext() {\n        contextdunshan.remove();\n    }\n\n    public String getFlag() {\n        return Flag;\n    }\n\n    public void setFlag(String Flag) {\n        this.Flag = Flag;\n    }\n\n}\n</code></pre><p><strong>第五步，JMeter 接口测试验证。</strong></p><p>我们再次启动项目，执行第三步的 JMeter 脚本发送请求。</p><p>执行完毕后，我们打开影子目录中的最新日志文件查看一下内容。</p><p><img src="https://static001.geekbang.org/resource/image/bf/f6/bf7cb2d5ce661a8a1dyycb81b511dcf6.png?wh=1920x570" alt="图片"></p><p>可以看到，保存的全部是压测日志，这才是合理的。</p><p>我们再打开正常目录中的日志文件查看下内容。</p><p><img src="https://static001.geekbang.org/resource/image/b6/5f/b65123a7fe1d4279c834ec1623671a5f.png?wh=1920x719" alt="图片"></p><p>可以看到，保存的全部都是正常日志。很好，这就是我们想要的日志隔离效果。</p><p>经过刚才的改造，我们 member、cart、order服务的日志都已经隔离成功了，下面，我们就尝试将这个demo移植到真实系统中去进行改造了。</p><h2>真实系统改造</h2><p>打开 mall 项目，我们还是先对 member 系统进行改造，改造成功后，再移植到其他服务中去。</p><p><strong>第一步，修改 Logback 配置文件。</strong></p><p>把之前 demo 的配置文件放到 mall 中的 logback-spring.xml 中：</p><p><img src="https://static001.geekbang.org/resource/image/7a/03/7a9f2383900859fa67e2349f31b47703.png?wh=1920x1020" alt="图片"></p><p><strong>第二步，修改网关全局过滤器 AuthGlobalFilter。</strong></p><p>在 AuthGlobalFilter 类中加入MDC方法对流量标记进行判断 ，具体判断代码是stringutils.isNotEnpty(dunshan)，如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/81/2c/816386033340df3406714c28c745c02c.png?wh=1662x698" alt="图片"></p><p><strong>第三步，修改业务系统数据上下文类。</strong></p><p>接下来，我们改造对应业务系统 mall-member 、mall-auth 项目的数据上下文类，可以直接把标记存储到 MDC中。</p><p>参考代码如下：</p><p><img src="https://static001.geekbang.org/resource/image/e9/18/e97d3a38784a3b7954e43f37a6030518.png?wh=1920x775" alt="图片"></p><p><strong>第四步，接口测试验证。</strong></p><p>我们启动网关服务、认证服务和会员服务来验证一下改造结果。</p><ul>\n<li><strong>正常流量（不带 Header 标记）</strong></li>\n</ul><p>首先，我们模拟发送一下正式流量。打开接口文档，测试下会员服务的【获取用户信息接口】。</p><p><img src="https://static001.geekbang.org/resource/image/d7/30/d7dbf7a086a6bfac5c11217256ffdb30.png?wh=1920x796" alt="图片"></p><p>我们可以在控制台看到正常的日志。</p><p><img src="https://static001.geekbang.org/resource/image/f8/99/f817dayy366eeed65119b7b68aaf8399.png?wh=1920x1164" alt="图片"></p><p>接下来，我们查看下正常的日志文件内容。</p><p><img src="https://static001.geekbang.org/resource/image/41/1a/4112f2cyy484121927d835d863e9f61a.png?wh=1920x630" alt="图片"></p><p>可以看到，正常日志生成成功。</p><ul>\n<li><strong>压测流量（带 Header 标记）</strong></li>\n</ul><p>接下来，我们再模拟发送一下压测请求。我们通过 curl 命令增加 Header 标记 ，然后发送一个请求，模拟压测流量。具体命令可以参考下面的代码：</p><pre><code class="language-bash">curl -X GET -H  "Accept:*/*" -H  "Authorization:eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX25hbWUiOiI3ZDExMTExMSIsInNjb3BlIjpbImFsbCJdLCJpZCI6MjEsImV4cCI6MTYzMzQ0MDAwMywiYXV0aG9yaXRpZXMiOlsi5YmN5Y-w5Lya5ZGYIl0sImp0aSI6IjcwYzYxZDVhLThkYTgtNDU3MS1iNWFmLTRkMmE1Y2FmYTBhNiIsImNsaWVudF9pZCI6InBvcnRhbC1hcHAifQ.jAt-ex0MVNXBAaCvT-A4xB1rpnWYBCxs6om10ujbTvVSNRZ5WFYxW8ZKNfUuFII5GCnh4XL66W0kxKyouCwphjZcPAvsK5PNJlsoBK0yljb-3g4gr4zAPbxoU6MPc0RDEnnXO3bzt4GZWql2rL8-TwitqRolLkLBrrV6OnWChVo" -H  "Request-Origion:Knife4j" -H  "Content-Type:application/x-www-form-urlencoded" -H "dunshan:7DGroup" "http://ip:8201/api/member/sso/feign/info"\n</code></pre><p>测试完成后，我们找到影子目录下的压测日志文件：</p><p><img src="https://static001.geekbang.org/resource/image/23/b1/23e2d8b3ce0a3cbeffyyd7c4f53067b1.png?wh=1920x793" alt="图片"></p><p>可以看到，压测日志也生成成功了。</p><p>到这里，我们的日志隔离改造就成功了。</p><p>另外，我还想说明一点，你可能已经发现了，正常请求和压测请求的用户是一样的。这一点其实我们在<a href="https://time.geekbang.org/column/article/431720">《铺底数据：真实的压测数据应该做成什么样子？》</a>这篇文章中就讲过，因为压测用户是从线上数据脱敏来的，所以正常用户与压测用户是一致的。</p><p>有了上面成功的改造经验，其他的服务就也可以按照目前的方式改造了。</p><h2>总结</h2><p>好了，这节课就讲到这里。刚才，我们完整演示了日志隔离的技术预演和真实系统改造过程。</p><p>日志系统的混乱是每个定位问题人员的恶梦，我想你可能也经历过在微服务分布式架构的汪洋一样的日志中查找问题的痛苦经历。所以日志聚合系统是微服务分布式架构的必备基础设施。</p><p>而在全链路压测的逻辑中，如果没有把压测流量和正式流量做隔离，那出现问题时，查找问题就会累到跳脚了；做日志隔离还有一个好处，就是在日志隔离之后，我们可以把压测日志输出到单独的存储上去，这样就不会因为压测日志增长过快而导致存储空间很快被耗光了。</p><p>在我看来，日志隔离是全链路压测过程中非常必要的改造步骤。刚才，我们演示了 “MDC+Logback+数据上下文”的日志隔离实现逻辑，这是我认为最为合理的解决方案。</p><h2>课后题</h2><p>学完这节课，我想请你思考两个问题：</p><ol>\n<li>除了本文的日志隔离方案，你还有其他的日志隔离方案吗？都有什么优缺点？</li>\n<li>你知道有哪些海量日志抽取的技术方案？</li>\n</ol><p>欢迎你在留言区与我交流讨论。我们下节课见！</p>',
        article_title: "19｜日志隔离：如何落地日志隔离？",
      },
      {
        title: "20 | Mock：如何屏蔽第三方接口的影响？",
        id: 462212,
        content:
          '<p>你好，我是高楼。这节课，我们聊一聊全链路压测中 Mock 技术的落地。</p><p>曾经也有不少人问我，在压测过程中，第三方接口响应时间慢怎么办呢？其实，响应时间慢，容量达不到要求，这是在依赖第三方系统的全链路压测过程中必然会面对的问题。这个时候，性能测试还要做，开发也不会为了性能测试修改业务代码，第三方我们也驱动不了。为了解决这个问题，我们就不得不用上Mock技术了。</p><p>什么是Mock呢？简单来说就是指使用各种技术手段模拟出各种需要的资源以供测试使用。Mock技术目前按应用场景主要分两大类：</p><ul>\n<li>Mock 一个对象，构造返回预期的数据，主要适用于单元测试；</li>\n<li>Mock 一个 Server ，构造返回预期的服务，主要适用于接口和性能测试。</li>\n</ul><p>而对于我们全链路压测来说，最主要使用的技术还是Mock Server。</p><p>Mock Server的逻辑是非常直观的：</p><p><img src="https://static001.geekbang.org/resource/image/43/ce/43e31e693dbf6dba460acaeedb03cace.jpg?wh=1920x574" alt="图片"></p><p>通过这张图我们可以看到，真实用户和压测用户是走同样的应用服务节点的。但不同的是，真实用户最终会走到真实的第三方服务，而压测用户会走到Mock Server上去。</p><p>请注意，在全链路压测的过程中，我们要Mock的是压测流量。而在整个逻辑中，我们是直接在应用服务中做标记透传、识别、流量隔离的，所以压测流量和正式流量是用<strong>同样的应用服务</strong>。</p><!-- [[[read_end]]] --><p>说到这里，我还想提一下Service Mesh。现在网上我们经常看到的通过 Service Mesh 实现的全链路逻辑，是在入口的POD上添加tag，并通过网络路由实现对不同版本的转发的。这个逻辑其实也是实现了全链路方式的。逻辑图如下：</p><p><img src="https://static001.geekbang.org/resource/image/bf/e9/bf1399bb62d3b986c79f8ac7a9ac6ee9.jpg?wh=1920x544" alt="图片"></p><p>但是，这种方式有两个点要说明：</p><ol>\n<li>Service Mesh是用不同的应用服务容器来实现的。也就是说，压测流量和正常流量走的已经不是同一个POD中的应用服务了。而我们专栏描述的内容，是会走同样的POD中的应用服务的。</li>\n<li>Service Mesh 的方式无法实现对数据库、缓存、队列等的隔离。</li>\n</ol><p>所以从这两点来看，Service Mesh做灰度发布是完全没问题的，但是要想实现全链路压测的初衷它是做不到的。</p><p>也正是因此，我们才需要用 Mock 的技术手段来做第三方的服务隔离。</p><p>虽然Mock技术并不复杂，但它却是全链路压测过程中不可或缺的一个知识点，所以我们还是把它拆开讲一讲。</p><p>市面上有很多种前后端 Mock Server框架，我们只要根据项目需要搭建最简单实用的 Mock Server 框架解决问题就可以了。如果实在没有框架可以借鉴，我们还可以自己写一个 Mock Server，<strong>反正Mock 的目的就是把不可控变成可控</strong>。</p><p>通常，我们把Mock Server分为<strong>前端Mock Serve</strong>r和<strong>后端Mock Server</strong>。</p><p>不过，因为前端Mock Server主要是为了规避在开发过程中前端等待后端的情况。但是在全链路的逻辑中，使用Mock Server主要是为了规避调用第三方时的影响，所以在全链路压测项目中，我们只要实现后端Mock Server就可以了。</p><p>为了逻辑描述的完整性，这里我还是会给出一个前端Mock Server技术的简单示例，你可以稍做了解。</p><p>下面，我们就来看看前端Mock Server和后端Mock Server各有什么技术吧。</p><h2>前端 Mock Server</h2><p>当前端开发有依赖后端接口的数据时，我们可以在前端自己搭建 nodejs 服务，构造返回预期的服务。那么常见的前端 Mock Server有哪些呢？</p><p>这里我给你提供两个常见的Mock Server： <a href="https://www.npmjs.com/package/server-mock">server-mock</a> 和 <a href="https://github.com/typicode/json-server">json-server</a> 。</p><p>我们以 server-mock 为例，介绍一下它怎么使用。</p><p>第一步，安装完nodejs环境之后，执行 npm 快速安装 server-mock，命令如下：</p><pre><code class="language-bash">npm install -g server-Mock\n</code></pre><p>第二步，在当前目录新建一个文件夹（名字随便，这里我用了 7d）。</p><p>再次输入命令Mock init，输入完毕后，目录下自然会创建两个文件：index.html 与 router.js。</p><p>你可以打开这两个文件查看一下代码。这是一个很简单的demo，index.html 就是一个表单与发送请求的 js 代码；router.js 内是Mock规则，当收到某个特定字段后，Mock Server应该返回相应的响应信息。</p><p>第三步，简单修改下index.html文件：</p><p><img src="https://static001.geekbang.org/resource/image/a9/66/a99c7e76537ddd7756145b72b44eda66.png?wh=1394x270" alt="图片"></p><p>第四步，修改 route.js 文件，对调用的接口和参数设计对应的响应内容：</p><p><img src="https://static001.geekbang.org/resource/image/d9/8b/d96f41cb7d2dc69df1c6d0afd28fe48b.png?wh=503x362" alt="图片"></p><p>第五步，在命令行输入 Mock start 启动server-mock， 终端上就会有下面的提示：</p><pre><code class="language-bash">Success: server start success， open the link http://localhost:8080 in browser \n</code></pre><p>我们只要根据提示在浏览器中打开地址就能看到效果了。</p><p><img src="https://static001.geekbang.org/resource/image/8f/84/8f1bd83f878713yy7450ee2ac87bbc84.png?wh=724x276" alt="图片"></p><p>另外，你还可以直接在浏览器输入地址，响应结果就是 JSON格式的自定义内容，如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/3b/27/3bfe22e975b12367375a5137940ff227.png?wh=1062x304" alt="图片"></p><p>有了接口响应数据，接下来就可以做前端开发了。</p><h2>后端 Mock Server</h2><p>好了，刚才，我们简单了解了一下前端Mock Server ，并演示了 server-mock 的使用方法。接下来，就要进入我们这节课的重点也就是后端Mock Server了。</p><p>这类Mock Server有很多，下面我们重点介绍一款常用的：Moco。</p><p>简单介绍一下Moco：</p><blockquote>\n<p>Moco 本身支持 API 和独立运行两种方式。通过 API ，开发人员可以在Junit、TestNg 等测试框架里使用 Moco，这样极大地降低了接口测试的复杂度。<br>\nMoco 可以根据一些配置，启动一个真正的 HTTP 服务（监听本地指定端口）。当发起的请求满足一个条件时，就会收到一个 response 。Moco 底层并没有依赖于像 Servlet 这样的重型框架，而是基于 Netty 的网络应用框架编写的，这样就绕过了复杂的应用服务器，所以它的速度是极快的。</p>\n</blockquote><p>了解了Moco的基本特性，接下来我们就看下怎么实现。</p><p>首先第一步，下载Moco的jar（<a href="https://github.com/dreamhead/moco">https://github.com/dreamhead/moco</a>），启动 Moco HTTP Server。</p><pre><code class="language-shell">java -jar &lt;moco-runner-path&gt; http -p &lt;port&gt; -c &lt;configfile-path&gt;\n</code></pre><p>这个启动命令里，这几个参数的含义你可以了解一下：<strong>moco-runner-path</strong>&nbsp;指jar&nbsp;包路径；<strong>port</strong>&nbsp;指HTTP 服务监听端口；<strong>configfile-path</strong>&nbsp;指配置文件路径。</p><p>第二步，在本地启动一个 HTTP 服务器，其中监听端口是 12306，配置文件是 JSON 文件。只需要本机发起一个 request 就可以了，如：<a href="http://localhost:12306">http://localhost:12306</a> 。</p><p>根据不同的请求类型，我们要设计相应的返回值。我给你画了一个简单的思维导图，方便你理清不同类型的接口。</p><p><img src="https://static001.geekbang.org/resource/image/89/12/8912a1b5a103307811f5516e3c098912.jpg?wh=1920x1161" alt="图片"></p><p>针对这张图中几个常见的接口（比如：Get接口、Post接口、带Headers的Post接口、自定义Cookies的Post接口、指定 JSON 响应、指定Status返回值），我们做一下具体的演示。</p><ul>\n<li><strong>Get接口</strong></li>\n</ul><p>返回值JSON内容定义为：</p><pre><code class="language-json">[\n  {\n    "description":"这是一个请求queries",\n    "request":{\n      "uri":"/7d",\n      "queries":{\n        "name":"7DGroup"\n      }\n    },\n    "response":{\n      "text":"success！"\n    }\n  }\n]\n</code></pre><p>通过 Postman 验证服务，测试 Get 请求：</p><p><img src="https://static001.geekbang.org/resource/image/94/55/9414ffd37358768748f4781bb76c8e55.png?wh=1920x1066" alt="图片"></p><p>Moco 服务日志为：</p><pre><code class="language-bash">09 十一月 2021 11:21:04 [nioEventLoopGroup-3-2] INFO  Request received:\nGET /7d HTTP/1.1\nHost: 127.0.0.1:12306\nUser-Agent: PostmanRuntime/7.4.0\nAccept: */*\nAccept-Encoding: gzip, deflate\nCache-Control: no-cache\nPostman-Token: 2d36e386-e022-4478-8acd-258eff4ff684\nX-Lantern-Version: 5.1.0\nContent-Length: 0\n09 十一月 2021 11:21:04 [nioEventLoopGroup-3-2] INFO  Response return:\nHTTP/1.1 200\nContent-Length: 10\nContent-Type: text/plain; charset=utf-8\nsuccess！\n</code></pre><ul>\n<li><strong>Post 接口</strong></li>\n</ul><p>返回值JSON 内容定义如下：</p><pre><code class="language-json">[\n  {\n    "description":"这是一个post请求",\n    "request":{\n      "uri":"/7d",\n      "method":"post"\n    },\n    "response":{\n      "text":"success！"\n    }\n  }\n]\n</code></pre><p>通过 Postman 验证服务，测试 Post 请求：</p><p><img src="https://static001.geekbang.org/resource/image/7a/46/7aa6b89e1b937ff1e2210a717e6b3746.png?wh=1920x844" alt="图片"></p><p>Moco 服务日志为：</p><pre><code class="language-bash">09 十一月 2021 11:29:30 [nioEventLoopGroup-3-2] INFO  Request received:\nPOST /7d HTTP/1.1\nHost: 127.0.0.1:12306\nUser-Agent: PostmanRuntime/7.4.0\nContent-Length: 0\nAccept: */*\nAccept-Encoding: gzip, deflate\nCache-Control: no-cache\nPostman-Token: 73f38af1-4efb-473a-b9d2-de0392c65bbe\nX-Lantern-Version: 5.1.0\n09 十一月 2021 11:29:30 [nioEventLoopGroup-3-2] INFO  Response return:\nHTTP/1.1 200\nContent-Length: 10\nContent-Type: text/plain; charset=utf-8\nsuccess！\n</code></pre><ul>\n<li><strong>带Headers的Post接口</strong></li>\n</ul><p>返回值JSON内容定义如下：</p><pre><code class="language-json">[\n  {\n    "description":"这是一个带headers的post请求",\n    "request":{\n      "uri":"/7d",\n      "method":"post",\n      "headers":{\n        "content-type":"application/json"\n      }\n    },\n    "response":{\n      "text":"success！"\n      }\n    }\n]\n</code></pre><p>通过 Postman 验证服务，测试带<strong>Headers</strong>的 Post 请求：</p><p><img src="https://static001.geekbang.org/resource/image/aa/0a/aae7297e712ee72a7210998f8c78d80a.png?wh=1920x883" alt="图片"></p><p>Moco 服务日志为：</p><pre><code class="language-bash">09 十一月 2021 11:34:43 [nioEventLoopGroup-3-2] INFO  Request received:\nPOST /7d HTTP/1.1\nHost: 127.0.0.1:12306\nUser-Agent: PostmanRuntime/7.4.0\nContent-Length: 0\nAccept: */*\nAccept-Encoding: gzip, deflate\nCache-Control: no-cache\nContent-Type: application/json\nPostman-Token: 0a82d74b-303f-42a3-9da0-32fd6c604166\nX-Lantern-Version: 5.1.0\n09 十一月 2021 11:34:43 [nioEventLoopGroup-3-2] INFO  Response return:\nHTTP/1.1 200\nContent-Length: 10\nContent-Type: text/plain; charset=utf-8\nsuccess！\n</code></pre><ul>\n<li><strong>自定义Cookies的Post接口</strong></li>\n</ul><p>返回值 JSON 内容定义如下：</p><pre><code class="language-json">[\n  {\n    "description":"这是一个带cookies的post请求",\n    "request":{\n      "uri":"/7d",\n      "method":"post",\n      "cookies":{\n        "login":"7dgroup"\n      }\n    },\n    "response":{\n      "text":"success！"\n      }\n    }\n]\n</code></pre><p>通过 Postman 验证服务，发送带自定义 Cookie的 Post 请求，Post请求中的 Cookie 配置如下：</p><p><img src="https://static001.geekbang.org/resource/image/38/37/38e72dfe57744ec13a4c7716287fe537.png?wh=1730x1032" alt="图片"></p><p>Postman接口配置如下：</p><p><img src="https://static001.geekbang.org/resource/image/d8/43/d8d93baffbc1c7b402121b2e8cfcd643.png?wh=1920x818" alt="图片"></p><p>Moco 服务日志如下：</p><pre><code class="language-bash">09 十一月 2021 12:26:46 [nioEventLoopGroup-3-3] INFO  Request received:\n\nPOST /7d HTTP/1.1\nHost: 127.0.0.1:12306\nUser-Agent: PostmanRuntime/7.4.0\nContent-Length: 0\nAccept: */*\nAccept-Encoding: gzip, deflate\nCache-Control: no-cache\nCookie: login=7dgroup\nPostman-Token: 36a12412-6eb1-44a4-a2d8-ea222eba8968\nX-Lantern-Version: 5.1.0\n\n09 十一月 2021 12:26:46 [nioEventLoopGroup-3-3] INFO  Response return:\n\nHTTP/1.1 200\nContent-Length: 10\nContent-Type: text/plain; charset=utf-8\n\nsuccess！\n</code></pre><ul>\n<li><strong>指定 JSON响应</strong></li>\n</ul><p>返回值JSON内容定义如下：</p><pre><code class="language-json">[\n  {\n    "description":"这是一个指定Json响应的post请求",\n    "request":{\n      "uri":"/7d",\n      "method":"post"\n    },\n    "response":{\n      "json":{\n        "name":"success",\n        "code":"1"\n      }\n    }\n  }\n]\n</code></pre><p>通过 Postman 验证服务，测试 Post 请求：</p><p><img src="https://static001.geekbang.org/resource/image/10/e1/1041c719e1809e1f9761e1828c2008e1.png?wh=1920x828" alt="图片"></p><p>Moco 服务日志如下：</p><pre><code class="language-bash">09 十一月 2021 13:25:19 [nioEventLoopGroup-3-2] INFO  Request received:\n\nPOST /7d HTTP/1.1\nHost: 127.0.0.1:12306\nUser-Agent: PostmanRuntime/7.4.0\nContent-Length: 0\nAccept: */*\nAccept-Encoding: gzip, deflate\nCache-Control: no-cache\nContent-Type: multipart/form-data; boundary=--------------------------703341725381001692596870\nPostman-Token: e5686919-85b9-44d0-8a73-61bf804b6377\nX-Lantern-Version: 5.1.0\n\n09 十一月 2021 13:25:19 [nioEventLoopGroup-3-2] INFO  Response return:\n\nHTTP/1.1 200\nContent-Length: 29\nContent-Type: application/json; charset=utf-8\n\n{"name":"success","code":"1"}\n</code></pre><ul>\n<li><strong>指定Status返回值</strong></li>\n</ul><p>返回值JSON内容定义如下：</p><pre><code class="language-json">[\n  {\n    "description":"这是指定响应status的get请求",\n    "request":{\n      "uri":"/7d",\n      "method":"get"\n    },\n    "response":{\n      "status":200\n    }\n  }\n]\n</code></pre><p>通过 Postman 验证服务，测试 Get 请求：</p><p><img src="https://static001.geekbang.org/resource/image/19/11/196212ff1d0c8e4d3362277a1d71ae11.png?wh=1920x837" alt="图片"></p><p>Moco 服务日志如下：</p><pre><code class="language-bash">09 十一月 2021 13:29:07 [nioEventLoopGroup-3-2] INFO  Request received:\n\nGET /7d HTTP/1.1\nHost: 127.0.0.1:12306\nUser-Agent: PostmanRuntime/7.4.0\nAccept: */*\nAccept-Encoding: gzip, deflate\nCache-Control: no-cache\nContent-Type: multipart/form-data; boundary=--------------------------465777039297587100709267\nPostman-Token: 791fa21c-386f-4389-aaa9-ba06d9e53aff\nX-Lantern-Version: 5.1.0\nContent-Length: 0\n\n09 十一月 2021 13:29:07 [nioEventLoopGroup-3-2] INFO  Response return:\n\nHTTP/1.1 200\n</code></pre><p>刚才，我们对常见的接口类型进行了Mock自定义，你可以根据自己的接口类型进行相应的定义。</p><p>不过讲到这里，你可能还会有一个问题，这样的Mock Server可以支持大容量的压力吗？其实是完全可以的，你只要在前面加上一个Nginx，后面的节点数是可以随便添加的，能一直加到满足你的需求为止。</p><h2>系统改造</h2><p>好了，知道了怎么搭建Mock Server之后，现在我们要来解决另一个关键问题，那就是如何基于压测标记实现流量的区分。还记得我们在这节课开头的这张图吧。我们希望，在同一个应用服务中实现正式流量和压测流量的请求（如果你是用Service Mesh做的不同版本的发布，不在这个逻辑之内）。</p><p><img src="https://static001.geekbang.org/resource/image/43/ce/43e31e693dbf6dba460acaeedb03cace.jpg?wh=1920x574" alt="图片"></p><p>注意哦，重点来了！ 请看上面的应用服务C，现在网上我们能看到的几乎所有Mock相关的文章，都是直接把这个应用服务C的后端调用接到了Mock Server上。其实这是有问题的，也不符合全链路压测的逻辑。</p><p>全链路压测的前提是不影响真实的生产服务的同时，又可以在同一应用服务中实现将压测流量转发到Mock Server上去。所以这时，就必须在应用服务C上做逻辑判断。也就是让正式的流量走真实的第三方服务，让压测流量走到Mock Server中去。</p><p>这就是我们需要改造的部分了。因为改造比较简单，这次我们不做demo了，直接在真实系统中改造。</p><p>先来看下我们的整体逻辑设计图：</p><p><img src="https://static001.geekbang.org/resource/image/5d/01/5d50c5ef71be0efc7574d44631c3d501.jpg?wh=1770x1902" alt=""></p><p>在这张图里，HTTP Header透传到Service里后，业务方通过数据上下文获取到压测标记，这时我们要进行判断：</p><ul>\n<li>如果是正式流量，就走Feign Client调用真实的第三方接口；</li>\n<li>如果是压测流量，就走RestTemplate调用Mock Server。</li>\n</ul><p>我们用order服务中典型的需要做第三方Mock的pay接口来详细演示一下实现步骤。</p><p>第一步，在应用服务项目的配置文件application.yaml中添加Mock相关的配置。</p><p><img src="https://static001.geekbang.org/resource/image/1f/91/1fb80f32f92dfb29768ed68e80388c91.png?wh=1258x866" alt="图片"></p><p>第二步，编写Mock配置读取类。</p><pre><code class="language-java">@Log4j2\n@Component\npublic class MockConfig {\n\n    @Value("${spring.mock.host}")\n    private String host;\n    \n    @Value("${spring.mock.port}")\n    private String port;\n    \n    public String getHost() {\n        return host;\n    }\n    \n    public String getPort() {\n        return port;\n    }\n    \n}\n</code></pre><p>第三步，编写RestTemplate 配置类，用于后面的HTTP调用。</p><pre><code class="language-java">import org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.http.client.ClientHttpRequestFactory;\nimport org.springframework.http.client.SimpleClientHttpRequestFactory;\nimport org.springframework.web.client.RestTemplate;\n\n/**\n * @description:  RestTemplate 配置类\n * @author: dunshan\n * @create: 2021-11-28 16:23\n **/\n \n@Configuration\npublic class RestTemplateConfig {\n\n    @Bean\n    public RestTemplate restTemplate(ClientHttpRequestFactory factory) {\n        return new RestTemplate(factory);\n    }\n    \n    @Bean\n    public ClientHttpRequestFactory simpleClientHttpRequestFactory() {\n        SimpleClientHttpRequestFactory factory = new SimpleClientHttpRequestFactory();\n        factory.setReadTimeout(5000);\n        factory.setConnectTimeout(15000);\n        return factory;\n    }\n    \n}\n</code></pre><p>第四步，在调用pay接口时实现Mock判断。</p><pre><code class="language-java">\nprivate SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");\n\n@Autowired\nRestTemplate restTemplate;\n@Autowired\nMockConfig mockConfig;\n\n@GetMapping("/pay")\npublic Object payOrder() {\n\n    HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;();\n    String name = "";\n    \n    //获取流量标识\n    String flag = AppContext.getContext().getFlag();\n    \n    if (StringUtils.isNoneEmpty(flag)) {\n         //请求 mock 服务\n        String url = String.format("http://%s:%s/7d", mockConfig.getHost(), mockConfig.getPort());\n        ResponseEntity&lt;String&gt; response = restTemplate.getForEntity(url, String.class, name);\n        String body = response.getBody();\n        map.put("ok", JSON.parse(body));\n        map.put("time", sdf.format(new Date()));\n        return map;\n    }\n    \n    //走正常服务\n    map.put("ok", "正式数据");\n    return map;\n    \n}\n</code></pre><p>第五步，Moco 服务响应JSON配置。</p><pre><code class="language-json">[\n  {\n    "description": "这是一个get请求",\n    "request": {\n      "uri": "/7d",\n      "method": "get"\n    },\n    "response": {\n      "json":{\n        "name":"Mock Success",\n        "code":"100"\n      }\n    }\n  }\n]\n\n</code></pre><p>第六步，验证 Mock 服务。</p><p>我们启动JMeter，模拟发送正常流量和压测流量。</p><ul>\n<li>正常流量（不带压测标记）<br>\n<img src="https://static001.geekbang.org/resource/image/f2/69/f2cdb47fcda838601ea7d4a1f79e1f69.png?wh=1920x465" alt="图片"></li>\n</ul><p>返回结果如下：</p><p><img src="https://static001.geekbang.org/resource/image/36/f0/365966e827ac88a22a6a116382ff64f0.png?wh=1336x346" alt="图片"></p><ul>\n<li>压测流量（带压测标记）<br>\n<img src="https://static001.geekbang.org/resource/image/9a/12/9a0c68815aefb15e388b080ee814d412.png?wh=1920x536" alt="图片"></li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/ee/00/eed6e95e575bc5a0f047098f34523400.png?wh=1496x372" alt="图片"></p><p>返回结果如下：</p><p><img src="https://static001.geekbang.org/resource/image/25/63/25a043b6bf42dda429bde72e6206b963.png?wh=1534x574" alt="图片"></p><p>从上面的结果可以看到，压测流量请求会走到Mock Server上；而正常的流量请求会走到真正的第三方服务上。</p><p>到这里，全链路压测的Mock改造就成功了。</p><h2>总结</h2><p>好，这节课就讲到这里。</p><p>刚才，我们介绍了常见的前端和后端Mock Server，并做了详细的演示。我们还演示了系统改造的部分。主要的逻辑，就是业务方从数据上下文中获取压测标记，然后通过判断让它走不同的调用方向。<strong>这里的关键是让压测流量和正常流量在同一个服务中去实现</strong>。这样在全链路线上压测时，才能真正地把所有生产上用到的服务和方法都覆盖到了。</p><p>现在市场上的 Mock Server已经有很多了。至于用哪一种工具其实并不重要，只要能满足需求就可以。如果没有 Mock Server，我们还可以自己写一个服务。</p><p>Mock是一种比较简单易懂的逻辑，但它却能解决全链路压测中对第三方强依赖的问题，这可解决了一个大问题，希望你能够用好它。</p><h2>思考题</h2><p>​学完这节课，我想请你思考两个问题：</p><ol>\n<li>你还知道哪些可以作为Mock Server的应用？各有什么优缺点？</li>\n<li>你觉得Mock Server的配置中，还有哪些难点？</li>\n</ol><p>欢迎你在留言区与我交流讨论。我们下节课见！</p>',
        article_title: "20 | Mock：如何屏蔽第三方接口的影响？",
      },
    ],
  },
  {
    chapterTitle: "实践环境",
    children: [
      {
        title: "21 | 压测平台：高效搭建 GoReplay 压测平台",
        id: 463560,
        content:
          '<p>你好，我是高楼。</p><p>从这里开始，我们就进入全链路压测的第三部分“实践环境”了。</p><p>在接下来的三讲，我会详细讲讲如何搭建压测平台。要搭建压测平台，首先，我们就要选择一款适合自己项目的流量工具。在第 6 讲，我们已经详细了解了流量工具的选型。综合对比评估后，最后我们这个项目选择了老牌流量回放工具 GoReplay，因为它简单、轻量、热度够，而且完全能满足我们目前项目的要求。</p><p>所以这节课，我们就来聊一聊怎样搭建 GoReplay 压测平台。如果你对GoReplay非常熟悉，那可以跳过下面的概念讲解，如果你之前没有接触过 GoReplay，可以跟着我一起先大概了解下这个工具。</p><h2>GoReplay 是什么？</h2><p><a href="https://github.com/buger/goreplay">GoReplay</a> 的简称是 Gor，它是一个简单的 TCP/HTTP 流量录制和回放的工具，主要用 Golang 语言编写。</p><p>GoReplay 的<a href="https://github.com/buger/goreplay">工作原理图</a>可以参考下面这张：</p><p><img src="https://static001.geekbang.org/resource/image/3e/55/3e135a37319e7e926f1ac2d410ea1b55.jpg?wh=1082x663" alt="图片"></p><p>GoReplay 可以在服务器上启动一个 Gor 进程，它负责的工作包括<strong>监听、过滤、转发、回放</strong>等。通过监听网卡，它可以直接录制请求。后续它还支持实现<strong>流量回放、压力测试、性能监控</strong>等功能。</p><p>如果我们简化一下核心流程图，它会是下面这个样子：</p><p><img src="https://static001.geekbang.org/resource/image/c4/99/c4eb7cb7c07de1f45212e45bf4897099.jpg?wh=1920x773" alt="图片"></p><p>GoReplay 从数据流中抽象出了两个概念，即<strong>输入（Input）<strong>和</strong>输出（Output ）</strong>，它用这两个概念来表示数据来源与去向，统称为 Plugin。</p><!-- [[[read_end]]] --><p>GoReplay 用介于输入和输出模块之间的 Middleware（中间件）实现拓展机制，而 Emitter 则是核心处理器，实现对于 Input 输入流的读取，并判断是否需要进行 Middleware 的处理、请求修改等。完成这一步后，Emitter会异步复制流量到所有 Output，同时将所有 Output  中有 Response 的数据复制到所有 Outputs 中。</p><h2>Golang 环境安装</h2><p>好了，理清 GoReplay 的工作原理之后，我们来看下Golang的环境安装。只有安装好Golang才能运行 GoReplay。</p><p>第一步，安装 Golang 及相关依赖环境，你可以在<a href="https://golang.org/dl">官网</a>或者 Go 的一些<a href="https://studygolang.com/dl">中文网站</a>上下载安装包。</p><p>我这里下载的是: go1.15.5.linux-amd64.tar.gz。</p><p>第二步，解压到服务器 /usr/local 目录下。</p><pre><code class="language-bash">rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.15.5.linux-amd64.tar.gz\n</code></pre><p>第三步，配置环境变量。</p><pre><code class="language-bash"># 打开\nvim /etc/profile\n\n# 添加\nexport GOROOT=/usr/local/go\nexport PATH=$PATH:$GOROOT/bin\n\n# 编译生效\nsource /etc/profile\n</code></pre><p>最后，我们验证 Golang 环境是否生效。</p><pre><code class="language-bash">[root@vm ~]# go version\ngo version go1.15.5 linux/amd64\n</code></pre><p>我们看到，版本已经显示成功了，这说明我们的环境已经安装成功了。</p><h2>GoReplay 安装</h2><p>接下来，我们安装GoReplay。在我们这个项目中，因为要在网关服务上录制流量，所以需要在网关容器内安装GoReplay。</p><p>我们从 <a href="https://github.com/buger/gor/releases">https://github.com/buger/gor/releases</a> 下载 Gor 二进制文件（提供 Windows、Linux x64 和 macOS 的预编译二进制文件），也可以自己源码<a href="https://github.com/buger/goreplay/wiki/Compilation">编译</a>安装。</p><p><img src="https://static001.geekbang.org/resource/image/06/76/069e2b52cae0e28f2d7140f2636b5a76.png?wh=1258x701" alt="图片"></p><p>这里，我简单演示一下如何通过 curl 下载二进制包。</p><pre><code class="language-bash">$ curl -L -O https://github.com/buger/goreplay/releases/download/v1.3.1/gor_1.3.1_x64.tar.gz\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   626  100   626    0     0    741      0 --:--:-- --:--:-- --:--:--   741\n100 10.5M  100 10.5M    0     0  3258k      0  0:00:03  0:00:03 --:--:-- 5951k\n\n$ tar xvzf gor_1.3.1_x64.tar.gz \ngor\n</code></pre><p>解压缩包后，我们可以直接从当前目录运行 Gor 二进制文件，或者更方便的是将二进制文件复制到系统的 PATH（对于 Linux 和 macOS，它可以是 /usr/local/bin）中。</p><p>同时我们也要记得安装抓包工具 libpcap：</p><pre><code class="language-bash">yum install libpcap libpcap-devel\n</code></pre><p>这里我演示的是 yum 安装方式。</p><p>这样，我们就可以使用 Gor 命令进行各种操作了。</p><p>在正式介绍流量录制和流量回放的逻辑之前，我还想带你熟悉一下 GoReplay 常用的参数。市面上对这些常用参数的介绍，比较齐全的不多，所以我在这里做个总结。</p><p>这里我按照使用类型划分，整理了常用的参数：</p><p><img src="https://static001.geekbang.org/resource/image/db/03/db66558ace720aa61c42ccfa0c41a103.jpg?wh=1920x906" alt="图片"></p><p>其中标星的参数是压测中的必选项。</p><p>好了，知道了常用的参数，特别是在压测中必然会用到的参数，接下来我们就看下如何在具体项目中使用。</p><h2>流量录制</h2><p>我们知道流量录制工具的核心原理就是将线上集群环境的流量复制多份发送到指定的仓库，然后使用指定压力机器进行放大回放以达到压力测试的目的。</p><p>那么 GoReplay 是如何实现流量复制的呢？</p><ul>\n<li>核心原理</li>\n</ul><p>你可以先来看下这张 GoReplay 的录制原理图：</p><p><img src="https://static001.geekbang.org/resource/image/06/ac/069f66babeb0614f7dfa6f8e99b35bac.jpg?wh=1696x1353" alt="图片"></p><p>GoReplay 通过调用 <a href="https://link.zhihu.com/?target=https%3A//github.com/google/gopacket">Google/gopacket</a> 来实现抓包，这里的 gopacket 是基于 libpcap 库的。GoReplay 可以做到捕捉指定端口的网卡流量，它既可以实现 TCP 协议（RAW_SOCKET）的抓包，也可以实现 HTTP 的录制、回放，同时还支持多实例之间的级联。</p><p>录制的时候，每个 TCP 包被抽象为 packet。当数据量较大、数据需要被分拆成多个包发送时，接收端需要把这些包按顺序组合起来，同时还要处理乱序、重复等问题，保证向下传递的是一个完整无误的数据包。这些逻辑统封装在了 tcp_message 中，tcp_message 与 packet 是一对多的关系。后续会将 tcp_message 中的数据取出，打上标记，传递给中间件（如果有）或者是 Output 插件。</p><p>要做到这些，我们来看下GoReplay 基于 libpcap 库的核心函数。</p><p>首先，GoReplay 的 <a href="https://github.com/buger/goreplay/blob/master/capture/capture.go">capture.go </a>类中定义了 pcap 处理引擎函数：</p><pre><code class="language-go">func (l *Listener) activatePcap() error {\n\tvar e error\n\tvar msg string\n\tfor _, ifi := range l.Interfaces {\n\t\tvar handle *pcap.Handle\n\t\thandle, e = l.PcapHandle(ifi)\n\t\tif e != nil {\n\t\t\tmsg += ("\\n" + e.Error())\n\t\t\tcontinue\n\t\t}\n\t\tl.Handles[ifi.Name] = packetHandle{\n\t\t\thandler: handle,\n\t\t\tips:     interfaceIPs(ifi),\n\t\t}\n\t}\n\tif len(l.Handles) == 0 {\n\t\treturn fmt.Errorf("pcap handles error:%s", msg)\n\t}\n\treturn nil\n}\n</code></pre><p><a href="https://github.com/buger/goreplay/blob/master/capture/capture.go">capture.go </a>类描述的几种引擎包括 libpcap、pcap_file、raw_socket 等：</p><pre><code class="language-go">// Set is here so that EngineType can implement flag.Var\nfunc (eng *EngineType) Set(v string) error {\n\tswitch v {\n\tcase "", "libpcap":\n\t\t*eng = EnginePcap\n\tcase "pcap_file":\n\t\t*eng = EnginePcapFile\n\tcase "raw_socket":\n\t\t*eng = EngineRawSocket\n\tcase "af_packet":\n\t\t*eng = EngineAFPacket\n\tdefault:\n\t\treturn fmt.Errorf("invalid engine %s", v)\n\t}\n\treturn nil\n}\n</code></pre><p>好了，知道了录制的原理，接下来我们看下如何在具体项目中做录制。</p><ul>\n<li>录制实例</li>\n</ul><p>这里我们演示的例子是除健康检查（/actuator/health）之外，录制网关服务的所有接口请求。</p><p>录制逻辑图：</p><p><img src="https://static001.geekbang.org/resource/image/4b/e6/4b2edec5d0116eb2b365bf8a9cbd56e6.jpg?wh=1888x344" alt="图片"></p><p>我们主要会使用下面这些命令：</p><pre><code class="language-bash"># --input-raw：捕获指定端口 HTTP 流量\n# --output-file：将获取的流量记录保存到文件\n# -output-file-append：单独文件 Append 存储\n# --http-set-header：为请求添加 header 信息\n# --input-raw-track-response：收集返回结果\n# --prettify-http：自动解码 Content-Encoding:gzip 和 Transfer-Encoding:chunked 的请求和响应 \n# --http-disallow-url：不允许正则 URL\nsudo nohup ./gor --input-raw :8201 \\ \n\t--output-file=request-mall-all.gor \\\n\t-output-file-append \\\n\t--http-set-header "dunshan:7DGroup" \\\n\t--http-set-header "User-Agent:Replayed-by-Gor" \\\n\t--input-raw-track-response \\\n\t--prettify-http \\\n\t--http-disallow-url /actuator/health &gt; /dev/null 2&gt;&amp;1 &amp;\n</code></pre><p>这些命令的意思是我们要监听网关实例的 8201 端口，录制除 /actuator/health 之外的所有请求，然后捕获响应报文并把请求追加到唯一文件中，像这里生成的流量文件名叫“request-mall-all.gor”。同时，我们还要对 Header 信息重写并打上压测标记 “dunshan:7DGroup”。</p><p>这里有一些需要说明的是：</p><ul>\n<li>sudo：以 root 身份运行，这是监听网络流量所必需的 <a href="https://github.com/buger/goreplay/wiki/Running-as-non-root-user">（除非另行配置）</a> ；</li>\n<li>nohup：允许该命令后台运行；</li>\n<li>/dev/null：让 GoReplay 进程监听 /dev/null。/dev/null 表示的是一个黑洞，用于丢弃不需要的输出。如果日志文件太大，并且你对自己的设置有足够的信心，可以选择重定向到 /dev/null。</li>\n<li>2&gt;&amp;1：将 stderr（标准错误） 输出重定向到 stdout（标准输出），允许我们在上面指定的同一日志文件中收集错误输出。</li>\n<li>&amp;：在后台运行进程。这样我们就可以在运行 GoReplay 时继续使用 Shell了。</li>\n</ul><p>命令执行后，输出的是下面的结果：</p><pre><code class="language-bash">[root@s12 ~]# sudo nohup ./gor --input-raw :8201 \\ \n\t--output-file=request-mall-all.gor \\\n\t-output-file-append \\\n\t--http-set-header "dunshan:7DGroup" \\\n\t--http-set-header "User-Agent:Replayed-by-Gor" \\\n\t--input-raw-track-response \\\n\t--prettify-http \\\n\t--http-disallow-url /actuator/health &gt; /dev/null 2&gt;&amp;1 &amp;\n[1] 26251\n</code></pre><p>这里显示的数字 26251 是 Gor 程序的进程 PID，在我们录制完成后，可以使用这个 PID 终止 Gor 的录制进程。</p><p>现在， Gor 已经开始流量录制了，这段时间网关转发的所有请求会被录制。</p><p>在录制了一段时间的流量后，我们可以执行下面的命令终止 Gor 的录制。录制终止之后，可以看到执行录制指令的目录下有了一份文件名为 “request-mall-all.gor ”的流量文件。</p><pre><code class="language-bash"># 手动输入 PID 方式\nsudo kill -9 ${gor 进程 PID}\n\n# 自动化获取 PID 方式\nfor i in `ps -ef|grep gor|grep -v grep |awk \'{print $2}\'`; do &nbsp;kill -9 $i; done\n</code></pre><p>其实如果我们考虑限时录制的话，也可以参考下面这段命令：</p><pre><code class="language-bash"># timeout 60 表示只录制 60 秒后自动停止\nsudo nohup timeout 60 ./gor --input-raw :8201 \\ \n\t--output-file=request-mall-all.gor \\\n\t-output-file-append \\\n\t--http-set-header "dunshan:7DGroup" \\\n\t--http-set-header "User-Agent:Replayed-by-Gor" \\\n\t--input-raw-track-response \\\n\t--prettify-http \\\n\t--http-disallow-url /actuator/health &gt; /dev/null 2&gt;&amp;1 &amp;\n \n# 按小时切割日志文件，并且开启日志追加模式，不会进行日志默认小分片\nsudo nohup timeout 60 ./gor --input-raw :8201 \\ \n\t--output-file=request-mall-all-%Y-%m-%d-%H.gor \\\n\t-output-file-append \\\n\t--http-set-header "dunshan:7DGroup" \\\n\t--http-set-header "User-Agent:Replayed-by-Gor" \\\n\t--input-raw-track-response \\\n\t--prettify-http \\\n\t--http-disallow-url /actuator/health &gt; /dev/null 2&gt;&amp;1 &amp;\n \n# 按小时切割日志文件，并且开启日志追加模式，不会进行日志默认小分片,.gz 压缩文件格式\nsudo nohup timeout 60 ./gor --input-raw :8201 \\ \n\t--output-file=request-mall-all-%Y-%m-%d-%H.gz \\\n\t-output-file-append \\\n\t--http-set-header "dunshan:7DGroup" \\\n\t--http-set-header "User-Agent:Replayed-by-Gor" \\\n\t--input-raw-track-response \\\n\t--prettify-http \\\n\t--http-disallow-url /actuator/health &gt; /dev/null 2&gt;&amp;1 &amp;\n</code></pre><p>接下来，我们尝试打开“request-mall-all.gor ”的流量文件，分析一下里面的结构。</p><p>因为GoReplay 用三个猴头 🐵🙈🙉  作为请求分隔符，所以我们的流量文件是下面这个样子：</p><pre><code class="language-bash"># 请求类型，请求 ID 为 80a820090a6423c03994b9f3，时间戳\n1 80a820090a6423c03994b9f3 1635694104268073719 0\n# 请求的类型及 URL 地址\nPOST /api/member/sso/login HTTP/1.1\nConnection: keep-alive\n# 压测标记\ndunshan: 7DGroup\nContent-Length: 34\nContent-Type: application/x-www-form-urlencoded; charset=UTF-8\nHost: 172.31.184.225:30001\n# header 重写\nUser-Agent: Replayed-by-Gor\n\n# 请求参数\nusername=7dTest006&amp;password=123456\n\n🐵🙈🙉 # 请求分隔符\n\n# 响应类型，请求 ID 为 80a820090a6423c03994b9f3，时间戳\n2 80a820090a6423c03994b9f3 1635694104291403073 392247\nHTTP/1.1 200 OK\nContent-Length: 1053\nVary: Origin\nVary: Access-Control-Request-Method\nVary: Access-Control-Request-Headers\nContent-Type: application/json\nDate: Sun, 31 Oct 2021 15:28:24 GMT\nCache-Control: no-cache, no-store, max-age=0, must-revalidate\nPragma: no-cache\nExpires: 0\nX-Content-Type-Options: nosniff\nX-Frame-Options: DENY\nX-XSS-Protection: 1 ; mode=block\nReferrer-Policy: no-referrer\n\n# 响应体\n{"code":200,"message":"操作成功","data":{"token":"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX25hbWUiOiI3ZFRlc3QwMDYiLCJzY29wZSI6WyJhbGwiXSwiaWQiOjI0MzQ0MjUsImV4cCI6MTYzNTc4MDUwNCwiYXV0aG9yaXRpZXMiOlsi5YmN5Y-w5Lya5ZGYIl0sImp0aSI6IjBhYjQ3OWU5LWYwOWQtNDI1YS04YWQyLTgwNTE4MDg3MmU4ZiIsImNsaWVudF9pZCI6InBvcnRhbC1hcHAifQ.WzOKDhWEu00nXBMghYwxTW6xW8M1CLdqEt63xeS6MwJME2QJg1rIxAsUKRo4KxgbFPoybax3O36xnpMDLAiRskQj6VRil-WaqYFBvTeC3iGfI2whKW8FsySUQh6WU2Vf5SsvA9HhMKLl3S4PS-aIY9bElxFjajeNoVGn4KB1Rwg","refreshToken":"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX25hbWUiOiI3ZFRlc3QwMDYiLCJzY29wZSI6WyJhbGwiXSwiYXRpIjoiMGFiNDc5ZTktZjA5ZC00MjVhLThhZDItODA1MTgwODcyZThmIiwiaWQiOjI0MzQ0MjUsImV4cCI6MTYzNjI5ODkwNCwiYXV0aG9yaXRpZXMiOlsi5YmN5Y-w5Lya5ZGYIl0sImp0aSI6IjAwYTIxYTFiLTJjZDMtNDk0Yi1hNzkxLWE3MTlhMDViN2YxYyIsImNsaWVudF9pZCI6InBvcnRhbC1hcHAifQ.IRSQvTOQpyRAjwp6owpdcYyjLLV8oZ_J39FNv7J4sZVGGnl3o5GEv51cnP89msTa7MXIC9E0k7mUqjiHJKJ-RFPDlHne1k-hP_ZJSCg9GS35cMiVb7jYQV-rVnG3j-yqiJb9g2gsCLVcsF-KnCMwWyV46C8pH2Tiajeee-Io9F8","tokenHead":"Bearer ","expiresIn":86399}}\n</code></pre><p>从流量文件中可以看出，对于每个请求，应该收到 3 个有效 Payload(1-Request, 2-Response, 3-Replayed Response)，它们具有相同的请求 ID（Request和 Response 具有相同的)。</p><p>到此，流量录制就结束了。</p><p>还有一点需要注意的是，在录制大流量的的时候，有可能会出现丢失部分请求的情况。这是因为 Gor 是基于 pcap 和操作系统 BufferSize 进行录制的，当 BufferSize 溢出的时候，请求就会丢失。</p><p>这个时候我们必须设置参数：–input-raw-buffer-size。</p><pre><code class="language-bash">  -input-raw-buffer-size value\n        Controls size of the OS buffer which holds packets until they dispatched. Default value depends by system: in Linux around 2MB. If you see big package drop, increase this value\n</code></pre><p>我们可以在 <a href="https://github.com/buger/goreplay/blob/master/capture/capture.go">capture.go</a> 中找到 BufferSize 相关的这段代码：</p><pre><code class="language-go">\tif l.BufferSize &gt; 0 {\n\t\terr = inactive.SetBufferSize(int(l.BufferSize))\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf("handle buffer size error: %q, interface: %q", err, ifi.Name)\n\t\t}\n\t}\n\tif l.BufferTimeout == 0 {\n\t\tl.BufferTimeout = 2000 * time.Millisecond\n\t}\n\terr = inactive.SetTimeout(l.BufferTimeout)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf("handle buffer timeout error: %q, interface: %q", err, ifi.Name)\n\t}\n\thandle, err = inactive.Activate()\n</code></pre><p>要注意的是，操作系统缓冲区（ BufferSize ）的默认值是用来保存数据包的，它在不同的操作系统上有所不同。例如，在 Linux 上这个值是 2 MB，在 Windows 是 1 MB。如果设置更大的缓冲区或减少 MTU（接口层的最大数据包数据大小） 不能减少丢失的请求数据，那么很有可能是 GoReplay Bug。</p><h2>流量回放</h2><p>接下来，我们再看下流量回放。流量回放的核心原理是通过 Limiter 实现变速回放，通过 Output 插件输出到目标机器。</p><p>这里我们主要使用的是HTTP输出的插件<a href="https://github.com/buger/goreplay/blob/master/output_http.go">output_http.go</a>&nbsp;，它通过实现 HTTP 协议， 进而实现 io.Writer 接口，最后根据配置注册到 Plugin.outputs 队列里。</p><p>GoReplay 回放原理图：</p><p><img src="https://static001.geekbang.org/resource/image/fe/2b/fe26839250a99a3b2001a95a6cc7902b.jpg?wh=1656x1313" alt="图片"></p><p>流量回放主要通过 Limiter 类执行各种变速。默认情况下，Gor 会创建一个动态协程池。初始协程数是 10 ，因为协程数量（N）等于该协程池的队列长度，所以当 HTTP 输出队列长度大于 10 时，Gor 会创建更多的协程数。</p><p>在产生 N 个协程的请求得到满足之前，不会再有协程创建。如果动态协程数当时不能处理消息，它将会睡眠一会。如果动态协程数无法处理消息，它就会死亡。我们可以用 --output-http-workers=20 选项指定并发协程数。</p><p>这里我们演示的例子是，倍数回放网关服务的所有接口请求。</p><p>回放逻辑图：</p><p><img src="https://static001.geekbang.org/resource/image/c9/01/c98c32493c7ca010a06944c55f8f4801.jpg?wh=1873x331" alt="图片"></p><p>我们主要会使用下面这些命令：</p><pre><code class="language-bash"># --input-file：从文件中获取请求数据，重放的时候 100x 倍速\n# --input-file-loop：无限循环，而不是读完这个文件就停止\n# --output-http：发送请求到网关\n# --output-http-workers：并发 100 协程发送请求\n# --stats --output-http-stats：每 5 秒输出一次 TPS 数据\n# --output-http-track-response：获取响应报文\n$ sudo ./gor --input-file \'request-mall-all.gor|10000%\' \\  \n--input-file-loop \\ \n--output-http \'http://10.96.136.36:8201\' \\ \n--output-http-workers 100 \\ \n--stats \\ \n--output-http-stats \\  \n--output-http-track-response \n</code></pre><p>因为我们是性能压测，可以要求无限循环。可以看到，我们向网关服务回放了 100 倍的流量。</p><h2>总结</h2><p>好了，这节课就讲到这里。刚才，我们一起梳理了 GoReplay 的基本概念、核心原理、常用参数。我们还一起认识了GoReplay 两个主要的功能：流量录制和流量回放。下面几个知识点希望你能记住。</p><ol>\n<li>GoReplay 主要依赖Golang环境运行，它通过命令行组织各个参数进行具体的操作；</li>\n<li>GoReplay 底层依赖的是 libpcap 库， Gor 进程可以负责所有的工作，包括监听、过滤、重写、回放等；</li>\n<li>通过参数：-stats 和 --out-http-stats，GoReplay 原生只支持由控制台统计发送请求队列信息，如果想要更复杂的测试统计还需要我们自己去埋点丰富监控指标；</li>\n<li>使用 Middleware 可以让我们在输入输出间做一些复杂的请求增强工作。</li>\n</ol><p>下一节课，我们将进入具体实践环节，我会通过案例演示如何做动态数据关联改造工作。</p><h2>思考题</h2><p>学完这节课，请你思考两个问题：</p><ol>\n<li>你有没有使用过 GoReplay，谈谈你使用这个工具的一些心得。</li>\n<li>相比 Java 语言的压测工具，你觉得 Golang 的压测工具有什么优势？</li>\n</ol><p>欢迎你在留言区与我交流讨论。当然了，你也可以把这节课分享给你身边的朋友，他们的一些想法或许会让你有更大的收获。我们下节课见！</p>',
        article_title: "21 | 压测平台：高效搭建 GoReplay 压测平台",
      },
      {
        title: "22 | 压测平台：如何解决 GoReplay 动态数据关联？",
        id: 464659,
        content:
          '<p>你好，我是高楼。</p><p>在第 6 讲，我们说过目前主流的流量回放工具都无法轻易解决 session 的问题，所以从系统安全的角度来说，工具需要做对应的改造。</p><p>这节课，我们来聊一下 GoReplay 如何通过改造解决回放过程中动态数据关联的问题。</p><h2>关联是什么？</h2><p>我们可以把关联简单地理解为把服务端返回的某个值，传递给后续的调用使用。我们可以在很多场景用到它。举个例子，我们常见的“Session ID”就是一个典型的需要关联的数据。它需要在交互过程中标识一个客户端身份，这个身份要在后续的调用中一直存在，否则服务端就不认识这个客户端了。</p><p>对每一个性能测试工具来说，关联是应该具备的基本功能，GoReplay 也不例外。</p><p>但是有很多新手同学对关联的逻辑并不是十分理解，甚至有人觉得关联和参数化（流量数据）是一样的，因为它们用的都是动态的数据，并且关联过来的数据也可以用到参数化（流量数据）中。其实，这二者还是有所不同的，因为关联的数据后续脚本中会用到，但参数化就不会。</p><p>现在有很多全链路压测都是由单接口基准创建的，这样一来，关联就用得比较少。因为接口级的基准场景都是一发一收就结束了，不需要将数据保存下来再发送出去。</p><p>那么正常情况下，什么样的场景需要关联呢？一般情况下， 它们需要满足下面几个条件：</p><!-- [[[read_end]]] --><ol>\n<li>数据是由服务器端生成的；</li>\n<li>数据在每一次请求时都是动态变化的；</li>\n<li>数据在后续的请求中需要再发送出去。</li>\n</ol><p>你可以通过这张示意图加深一下理解：</p><p><img src="https://static001.geekbang.org/resource/image/6f/62/6f28747510866b140821bfd83ea9bb62.jpg?wh=1264x802" alt="图片"></p><p>好了，我们知道了关联的基本概念和适用场景，那么在 GoReplay 中又如何改造呢？</p><p>作为一款流量回放工具，我们知道GoReplay的核心原理就是基于流量文件去倍数回放请求。很显然，这个流量文件是个死的东西，是不能动态参数数据的，那么，我们又该怎么办呢？</p><p>这时候，我们就需要搬出 GoReplay 的中间件了。</p><h2>中间件是什么？</h2><p><a href="https://github.com/buger/goreplay/wiki/middleware">中间件</a>（ <a href="https://github.com/buger/goreplay/wiki/middleware">Middleware</a> ）是一个在 STDIN（标准输入） 接收请求、响应 payload （有效请求负载）并在 STDOUT（标准输出） 发出修改请求的程序。你可以在中间件上实现任何自定义逻辑，比如认证、复杂的重写和筛选请求等。</p><p>通过传入 Middleware 参数，我们可以发送命令给 GoReplay，GoReplay 会拉起一个进程执行这个命令。在录制过程中，GoReplay 通过获取进程的 STDIN 和 STDOUT 与输入输出插件进程进行通信，中间件内部逻辑为 STDERR，数据流向大致如下：</p><pre><code class="language-bash">                   Original request      +--------------+\n+-------------+----------STDIN----------&gt;+              |\n|  Gor input  |                          |  Middleware  |\n+-------------+----------STDIN----------&gt;+              |\n                   Original response     +------+---+---+\n                                                |   ^\n+-------------+    Modified request             v   |\n| Gor output  +&lt;---------STDOUT-----------------+   |\n+-----+-------+                                     |\n      |                                             |\n      |            Replayed response                |\n      +------------------STDIN-----------------&gt;----+\n</code></pre><p>需要注意的是，如果希望记录原始响应和回放响应，不要忘记添加 <strong>– output-http-track-response</strong> 和 <strong>– input-raw-track-response</strong> 参数。</p><p>GoReplay 支持用任何语言编写中间件的协议，同时中间件程序还需要格外注意一点，就是中间件和 Gor 的所有通信都是异步，因此，我们不能保证原始请求和响应消息会一个接一个地出现。如果业务逻辑依赖于原始响应或回放响应，那么中间件应用程序就应该处理好状态，也就是要做好动态数据的处理动作。</p><p>为了简化中间件的功能实现，官方为 <a href="https://github.com/buger/goreplay/tree/master/middleware">node.js </a>和 Go (即将推出)提供了包。</p><h2>如何使用中间件？</h2><p>那么，应该怎样使用中间件呢？</p><p>下面就是一个简单的使用 bash echo 中间件的示例，我们用它来打印对应的 payload 类型：</p><pre><code>#!/usr/bin/env bash\n#\n# `xxd` utility included into vim-common package\n# It allow hex decoding/encoding\n# \n# This example may broke if you request contains `null` string, you may consider using pipes instead.\n# See: https://github.com/buger/gor/issues/309\n# \n\nfunction log {\n    # Logging to stderr, because stdout/stdin used for data transfer\n    # 记录到 stderr，因 为 stdout/stdin 用于数据传输\n    &gt;&amp;2 echo &quot;[DEBUG][ECHO] $1&quot;\n}\n\nwhile read line; do\n    decoded=$(echo -e &quot;$line&quot; | xxd -r -p)\n\n    header=$(echo -e &quot;$decoded&quot; | head -n +1)\n    payload=$(echo -e &quot;$decoded&quot; | tail -n +2)\n\n    encoded=$(echo -e &quot;$header\\n$payload&quot; | xxd -p | tr -d &quot;\\\\n&quot;)\n\n    log &quot;&quot;\n    log &quot;===================================&quot;\n\n    case ${header:0:1} in\n    &quot;1&quot;)\n        log &quot;Request type: Request&quot;\n        ;;\n    &quot;2&quot;)\n        log &quot;Request type: Original Response&quot;\n        ;;\n    &quot;3&quot;)\n        log &quot;Request type: Replayed Response&quot;\n        ;;\n    *)\n        log &quot;Unknown request type $header&quot;\n    esac\n    echo &quot;$encoded&quot;\n\n    log &quot;===================================&quot;\n\n    log &quot;Original data: $line&quot;\n    log &quot;Decoded request: $decoded&quot;\n    log &quot;Encoded data: $encoded&quot;\ndone;\n</code></pre><p>这里我们使用【会员登录接口】来做演示。</p><p><img src="https://static001.geekbang.org/resource/image/yy/18/yy0d0a5afb7e1a295c0e10b94b989518.png?wh=1282x941" alt="图片"></p><p>首先，通过指定 Middleware 可执行文件的命令，也就是使用 Middleware 参数在 GoReplay 启用中间件功能：</p><pre><code class="language-bash"> sudo ./goreplay --input-raw :8081 --middleware "./echo.sh" --output-http "http://staging.server"\n</code></pre><p>接下来，我们通过 Postman 对【会员登录】接口做一次测试。</p><p><img src="https://static001.geekbang.org/resource/image/0c/c4/0cd47ef4bfd1b47513bc36d312db33c4.png?wh=633x479" alt="图片"></p><p>通过控制台我们看到，中间件程序已经成功把经过的流量信息全部打印出来了。</p><pre><code class="language-bash">Interface: en0 . BPF Filter: ((tcp dst port 8081) and (dst host fe80::8f6:ee40:ebd1:bec or dst host 192.168.3.58))\nInterface: awdl0 . BPF Filter: ((tcp dst port 8081) and (dst host fe80::50a5:ceff:feeb:47e3))\nInterface: llw0 . BPF Filter: ((tcp dst port 8081) and (dst host fe80::50a5:ceff:feeb:47e3))\nInterface: utun0 . BPF Filter: ((tcp dst port 8081) and (dst host fe80::d9a3:ab1b:f8e4:4de))\nInterface: utun1 . BPF Filter: ((tcp dst port 8081) and (dst host fe80::c2a0:19a0:9d9d:6699))\nInterface: utun2 . BPF Filter: ((tcp dst port 8081) and (dst host fe80::771:4985:8642:7857))\nInterface: utun3 . BPF Filter: ((tcp dst port 8081) and (dst host fe80::4a93:e598:6e37:37b3))\nInterface: lo0 . BPF Filter: ((tcp dst port 8081) and (dst host 127.0.0.1 or dst host ::1 or dst host fe80::1))\n2021/11/14 17:16:34 [PPID 8021 and PID 8022] Version:1.3.0\n[DEBUG][ECHO] \n[DEBUG][ECHO] ===================================\n[DEBUG][ECHO] Request type: Request\n[DEBUG][ECHO] ===================================\n[DEBUG][ECHO] Original data: 3120636239663166393130303030303030313535353939393337203136333638383133393937363036333730303020300a504f5354202f61646d696e2f6c6f67696e20485454502f312e310d0a436f6e74656e742d547970653a206170706c69636174696f6e2f6a736f6e0d0a417574686f72697a6174696f6e3a20747275650d0a557365722d4167656e743a20506f73746d616e52756e74696d652f372e32382e340d0a4163636570743a202a2f2a0d0a506f73746d616e2d546f6b656e3a2034666132356536622d666434362d346539362d386166362d6636633562613066303033660d0a486f73743a206c6f63616c686f73743a383038310d0a4163636570742d456e636f64696e673a20677a69702c206465666c6174652c2062720d0a436f6e6e656374696f6e3a206b6565702d616c6976650d0a436f6e74656e742d4c656e6774683a2035320d0a0d0a7b0a202020202270617373776f7264223a2022313233343536222c0a2020202022757365726e616d65223a202274657374220a7d\n[DEBUG][ECHO] Decoded request: 1 cb9f1f910000000155599937 1636881399760637000 0\nPOST /admin/login HTTP/1.1\nContent-Type: application/json\nAuthorization: true\nUser-Agent: PostmanRuntime/7.28.4\nAccept: */*\nPostman-Token: 4fa25e6b-fd46-4e96-8af6-f6c5ba0f003f\nHost: localhost:8081\nAccept-Encoding: gzip, deflate, br\nConnection: keep-alive\nContent-Length: 52\n\n{\n    "password": "123456",\n    "username": "test"\n}\n[DEBUG][ECHO] Encoded data: 3120636239663166393130303030303030313535353939393337203136333638383133393937363036333730303020300a504f5354202f61646d696e2f6c6f67696e20485454502f312e310d0a436f6e74656e742d547970653a206170706c69636174696f6e2f6a736f6e0d0a417574686f72697a6174696f6e3a20747275650d0a557365722d4167656e743a20506f73746d616e52756e74696d652f372e32382e340d0a4163636570743a202a2f2a0d0a506f73746d616e2d546f6b656e3a2034666132356536622d666434362d346539362d386166362d6636633562613066303033660d0a486f73743a206c6f63616c686f73743a383038310d0a4163636570742d456e636f64696e673a20677a69702c206465666c6174652c2062720d0a436f6e6e656374696f6e3a206b6565702d616c6976650d0a436f6e74656e742d4c656e6774683a2035320d0a0d0a7b0a202020202270617373776f7264223a2022313233343536222c0a2020202022757365726e616d65223a202274657374220a7d0a\n</code></pre><p>到这里，我们已经了解了中间件的基本功能和使用方法，接下来我们回到这节课的主题，如何实现关联操作？</p><h2>如何实现回放关联？</h2><p>这里我们引入“会员登录”和“查询所有后台资源分类”两个接口为例。</p><p>你可以先看看这张整体的请求交互示意图：</p><p><img src="https://static001.geekbang.org/resource/image/8d/ce/8da2fa743c61455yyd80f63ebe1e18ce.jpg?wh=1424x1953" alt="图片"></p><ul>\n<li>会员登录</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/yy/18/yy0d0a5afb7e1a295c0e10b94b989518.png?wh=1282x941" alt="图片"></p><ul>\n<li>查询所有后台资源分类</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/1a/d8/1a1dbcd1f3eb0566308bfed7fa03efd8.png?wh=1296x754" alt="图片"></p><p>我们知道 token 是有时效的，如果失效，那么二次请求服务端校验就会失败。如下图：</p><p><img src="https://static001.geekbang.org/resource/image/b2/96/b20756aca3d9886966984da4096ee596.png?wh=1244x795" alt="图片"></p><p>下面我们具体来演示下如何解决token关联的问题。</p><p>第一步，创建一个流量录制的命令：</p><pre><code class="language-shell">#!/bin/bash\n\nPORT="8081"\nOUT_FILE="request.gor"\n\nsudo ./goreplay --input-raw :$PORT --output-file=$OUT_FILE  -output-file-append --input-raw-track-response --prettify-http\n</code></pre><p>录制下的流量文件如下：</p><pre><code class="language-shell">1 d1ae1f9100000001e404ee86 1635588156669182000 0\nPOST /admin/login HTTP/1.1\nContent-Type: application/json\nAuthorization: true\nUser-Agent: PostmanRuntime/7.28.4\nAccept: */*\nPostman-Token: 480f15ca-53df-44fd-8980-5e9118b2107e\nHost: localhost:8081\nAccept-Encoding: gzip, deflate, br\nConnection: keep-alive\nContent-Length: 52\n\n{\n    "password": "123456",\n    "username": "test"\n}\n\n🐵🙈🙉\n2 d1ae1f9100000001e404ee86 1635588156828973000 458000\nHTTP/1.1 200 \nContent-Length: 254\nVary: Origin\nVary: Access-Control-Request-Method\nVary: Access-Control-Request-Headers\nX-Content-Type-Options: nosniff\nX-XSS-Protection: 1; mode=block\nCache-Control: no-cache, no-store, max-age=0, must-revalidate\nPragma: no-cache\nExpires: 0\nX-Frame-Options: DENY\nContent-Type: application/json\nDate: Sat, 30 Oct 2021 10:02:36 GMT\nKeep-Alive: timeout=60\nConnection: keep-alive\n\n{"code":200,"message":"操作成功","data":{"tokenHead":"","token":"eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJ0ZXN0IiwiY3JlYXRlZCI6MTYzNTU4ODE1Njc5NCwiZXhwIjoxNjM1NTg4MjE2fQ.-wsZa0gijz2KfCF-eAYK1Tt-pd_vw2_LShShlIDCQOsHjOZZlGl8yX2MncZlO9St_oPj1JdBaERjfEU6iu12qw"}}\n\n\n\n🐵🙈🙉\n1 d1ae1f9100000001e405029a 1635588192031592000 0\nGET /resource/listAll HTTP/1.1\ntoken: eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJ0ZXN0IiwiY3JlYXRlZCI6MTYzNTU4ODE1Njc5NCwiZXhwIjoxNjM1NTg4MjE2fQ.-wsZa0gijz2KfCF-eAYK1Tt-pd_vw2_LShShlIDCQOsHjOZZlGl8yX2MncZlO9St_oPj1JdBaERjfEU6iu12qw\nUser-Agent: PostmanRuntime/7.28.4\nAccept: */*\nPostman-Token: 4bc2152e-dbd4-4b2a-b880-72ed5ee4303a\nHost: localhost:8081\nAccept-Encoding: gzip, deflate, br\nConnection: keep-alive\n\n🐵🙈🙉\n2 d1ae1f9100000001e405029a 1635588192064563000 1084000\nHTTP/1.1 200 \nContent-Length: 3997\nVary: Origin\nVary: Access-Control-Request-Method\nVary: Access-Control-Request-Headers\nX-Content-Type-Options: nosniff\nX-XSS-Protection: 1; mode=block\nCache-Control: no-cache, no-store, max-age=0, must-revalidate\nPragma: no-cache\nExpires: 0\nX-Frame-Options: DENY\nContent-Type: application/json\nDate: Sat, 30 Oct 2021 10:03:12 GMT\nKeep-Alive: timeout=60\nConnection: keep-alive\n\n{"code":200,"message":"操作成功","data":[{"id":1,"createTime":"2020-02-04T09:04:55.000+00:00","name":"商品品牌管理","url":"/brand/**","description":null,"categoryId":1},{"id":2,"createTime":"2020-02-04T09:05:35.000+00:00","name":"商品属性分类管理","url":"/productAttribute/**","description":null,"categoryId":1},{"id":3,"createTime":"2020-02-04T09:06:13.000+00:00","name":"商品属性管理","url":"/productAttribute/**","description":null,"categoryId":1},{"id":4,"createTime":"2020-02-04T09:07:15.000+00:00","name":"商品分类管理","url":"/productCategory/**","description":null,"categoryId":1},{"id":5,"createTime":"2020-02-04T09:09:16.000+00:00","name":"商品管理","url":"/product/**","description":null,"categoryId":1},{"id":6,"createTime":"2020-02-04T09:09:53.000+00:00","name":"商品库存管理","url":"/sku/**","description":null,"categoryId":1},{"id":8,"createTime":"2020-02-05T06:43:37.000+00:00","name":"订单管理","url":"/order/**","description":"","categoryId":2},{"id":9,"createTime":"2020-02-05T06:44:22.000+00:00","name":" 订单退货申请管理","url":"/returnApply/**","description":"","categoryId":2},{"id":10,"createTime":"2020-02-05T06:45:08.000+00:00","name":"退货原因管理","url":"/returnReason/**","description":"","categoryId":2},{"id":11,"createTime":"2020-02-05T06:45:43.000+00:00","name":"订单设置管理","url":"/orderSetting/**","description":"","categoryId":2},{"id":12,"createTime":"2020-02-05T06:46:23.000+00:00","name":"收货地址管理","url":"/companyAddress/**","description":"","categoryId":2},{"id":13,"createTime":"2020-02-07T08:37:22.000+00:00","name":"优惠券管理","url":"/coupon/**","description":"","categoryId":3},{"id":14,"createTime":"2020-02-07T08:37:59.000+00:00","name":"优惠券领取记录管理","url":"/couponHistory/**","description":"","categoryId":3},{"id":15,"createTime":"2020-02-07T08:38:28.000+00:00","name":"限时购活动管理","url":"/flash/**","description":"","categoryId":3},{"id":16,"createTime":"2020-02-07T08:38:59.000+00:00","name":"限时购商品关系管理","url":"/flashProductRelation/**","description":"","categoryId":3},{"id":17,"createTime":"2020-02-07T08:39:22.000+00:00","name":"限时购场次管理","url":"/flashSession/**","description":"","categoryId":3},{"id":18,"createTime":"2020-02-07T08:40:07.000+00:00","name":"首页轮播广告管理","url":"/home/advertise/**","description":"","categoryId":3},{"id":19,"createTime":"2020-02-07T08:40:34.000+00:00","name":"首页品牌管理","url":"/home/brand/**","description":"","categoryId":3},{"id":20,"createTime":"2020-02-07T08:41:06.000+00:00","name":"首页新品管理","url":"/home/newProduct/**","description":"","categoryId":3},{"id":21,"createTime":"2020-02-07T08:42:16.000+00:00","name":"首页人气推荐管理","url":"/home/recommendProduct/**","description":"","categoryId":3},{"id":22,"createTime":"2020-02-07T08:42:48.000+00:00","name":"首页专题推荐管理","url":"/home/recommendSubject/**","description":"","categoryId":3},{"id":23,"createTime":"2020-02-07T08:44:56.000+00:00","name":" 商品优选管理","url":"/prefrenceArea/**","description":"","categoryId":5},{"id":24,"createTime":"2020-02-07T08:45:39.000+00:00","name":"商品专题管理","url":"/subject/**","description":"","categoryId":5},{"id":25,"createTime":"2020-02-07T08:47:34.000+00:00","name":"后台用户管理","url":"/admin/**","description":"","categoryId":4},{"id":26,"createTime":"2020-02-07T08:48:24.000+00:00","name":"后台用户角色管理","url":"/role/**","description":"","categoryId":4},{"id":27,"createTime":"2020-02-07T08:48:48.000+00:00","name":"后台菜单管理","url":"/menu/**","description":"","categoryId":4},{"id":28,"createTime":"2020-02-07T08:49:18.000+00:00","name":"后台资源分类管理","url":"/resourceCategory/**","description":"","categoryId":4},{"id":29,"createTime":"2020-02-07T08:49:45.000+00:00","name":"后台资源管理","url":"/resource/**","description":"","categoryId":4}]}\n🐵🙈🙉\n</code></pre><p>第二步，创建一个流量回放 Shell 脚本。</p><pre><code class="language-shell">#!/bin/bash\n## Usage: ./replay.sh\n\nOUTPUT="http://127.0.0.1:8081"\nINPUT_FILE="requests.gor"\n\nsudo ./goreplay --input-file $INPUT_FILE --input-file-loop --output-http=$OUTPUT --prettify-http --output-http-track-response --output-stdout\n</code></pre><p>第三步，我们尝试进行一次回放操作。</p><p><img src="https://static001.geekbang.org/resource/image/c8/e4/c8e10a5c7b6621cc54d4971b217959e4.png?wh=750x355" alt="图片"></p><p>等待一会，我们看到回放的【查询所有后台资源分类】接口已经失败了，提示 token 失效了。</p><p>要怎么解决这个问题呢？</p><p>在这种情况下，我们需要实时将来自录制的 token 关联到来自回放响应的 token 上 ，然后使用关联的 token 修改回放的请求。我们使用 GoReplay 存储库中的这个方便的<a href="https://github.com/buger/goreplay/blob/master/examples/middleware/token_modifier.go">示例</a>进行扩展。</p><p>所涉及的基本算法你可以看看下面这张图片。<br>\n<img src="https://static001.geekbang.org/resource/image/c0/00/c0b9bd3a17b2d34f0ec2d4e1ea5f0e00.jpg?wh=1920x1040" alt=""></p><p>因为原始服务器没有预定义的token，而回放服务器有自己的token，它不能与原始服务器同步。所以不使用中间件或者中间件只使用请求有效 payload，都会使得token失效。</p><p>为了解决这个问题，我们的中间件应该考虑回放和源服务器的响应，存储’ originalToken -&gt; replayedToken \'别名，并使用此 token 重写所有请求以使用回放别名。</p><p>顺着这个思路，我们看下第四步，创建 token 关联中间件程序。</p><pre><code class="language-go">package main\n\nimport (\n\t"bufio"\n\t"bytes"\n\t"encoding/hex"\n\t"fmt"\n\t"github.com/bitly/go-simplejson"\n\t"github.com/buger/goreplay/proto"\n\t"os"\n)\n\n// requestID -&gt; originalToken\n// 请求 ID -&gt; 原始 Token\nvar originalTokens map[string][]byte\n\n// originalToken -&gt; replayedToken\n// 原始 Token -&gt; 回放 Token\nvar tokenAliases map[string][]byte\n\nvar json_data interface{}\n\n\n\nfunc main() {\n\toriginalTokens = make(map[string][]byte)\n\ttokenAliases = make(map[string][]byte)\n\n\tscanner := bufio.NewScanner(os.Stdin)\n\n\tfor scanner.Scan() {\n\t\tencoded := scanner.Bytes()\n\t\tbuf := make([]byte, len(encoded)/2)\n\t\thex.Decode(buf, encoded)\n\n\t\tprocess(buf)\n\t}\n}\n\nfunc process(buf []byte) {\n\t// First byte indicate payload type, possible values:\n\t//  1 - Request\n\t//  2 - Response\n\t//  3 - ReplayedResponse\n\t// 第一个字节表示有效负载类型，可能的值:\n\t// 1 - 请求\n\t// 2 - 响应\n\t// 3 - 回放响应\n\tpayloadType := buf[0]\n\theaderSize := bytes.IndexByte(buf, \'\\n\') + 1\n\theader := buf[:headerSize-1]\n\n\t// Header contains space separated values of: request type, request id, and request start time (or round-trip time for responses)\n\t// Header 包含空格分隔的值:请求类型，请求 id，请求开始时间(或响应的往返时间)\n\tmeta := bytes.Split(header, []byte(" "))\n\t\n    // For each request you should receive 3 payloads (request, response, replayed response) with same request id\n\t// 对于每个请求，你应该收到 3 个有效负载(request, response, replayed response)，具有相同的请求 id\n\treqID := string(meta[1])\n\tpayload := buf[headerSize:]\n\n\tDebug("Received payload:", string(buf))\n\n\tswitch payloadType {\n\tcase \'1\': // Request\n\t\tif bytes.Equal(proto.Path(payload), []byte("/admin/login")) {\n\t\t\toriginalTokens[reqID] = []byte{}\n\t\t\tDebug("Found token request:", reqID)\n\t\t} else {\n\t\t\t//token, vs, _ := proto.PathParam(payload, []byte("token")) //取到回放响应的 token 值\n\t\t\ttoken := proto.Header(payload, []byte("token")) //取到原始的 token 值\n\n\t\t\tDebug("Received token:", string(token))\n\n\t\t\tif len(token) != 0 { // If there is GET token param\n\t\t\t\tDebug("If there is GET token param")\n\t\t\t\tDebug("tokenAliases", tokenAliases)\n\t\t\t\tif alias, ok := tokenAliases[string(token)]; ok { \t\t//检查要替换的 token 值是否存在\n\t\t\t\t\tDebug("Received alias")\n\t\t\t\t\t// Rewrite original token to alias\n\t\t\t\t\tpayload = proto.SetHeader(payload, []byte("token"), alias)  //将原始的 token 替换成回放的 token\n\n\t\t\t\t\t// Copy modified payload to our buffer\n\t\t\t\t\tbuf = append(buf[:headerSize], payload...)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Emitting data back\n\t\tos.Stdout.Write(encode(buf)) //重写请求准备发往回放服务\n\tcase \'2\': // Original response\n\t\tif _, ok := originalTokens[reqID]; ok {\n\t\t\tjsonObject, err := simplejson.NewJson([]byte(proto.Body(payload)))\n\t\t\tif err != nil {\n\t\t\t\tfmt.Println(err)\n\t\t\t}\n\n\t\t\tresult := jsonObject.Get("data")\n\t\t\ttoken := result.Get("token")\n\t\t\tsecureToken:=token\n\n\t\t\tf ,_:=secureToken.Bytes()\n\n\t\t\toriginalTokens[reqID] = f\n\t\t\tDebug("Remember origial token:", f)\n\n\t\t}\n\tcase \'3\': // Replayed response\n\t\tif originalToken, ok := originalTokens[reqID]; ok {\n\t\t\tdelete(originalTokens, reqID)\n\n\n\n\t\t\tjsonObject, err := simplejson.NewJson([]byte(proto.Body(payload)))\n\t\t\tif err != nil {\n\t\t\t\tfmt.Println(err)\n\t\t\t}\n\t\t\tresult := jsonObject.Get("data")\n\t\t\ttoken := result.Get("token")\n\t\t\tf ,_:=token.Bytes()\n\t\t\ttokenAliases[string(originalToken)] = f //拿到现在的 token 值用来替换掉过去的 token 值\n\n\t\t\tDebug("Create alias for new token token, was:", string(originalToken), "now:", string(f))\n\t\t}\n\t}\n}\n\nfunc encode(buf []byte) []byte {\n\tdst := make([]byte, len(buf)*2+1)\n\thex.Encode(dst, buf)\n\tdst[len(dst)-1] = \'\\n\'\n\n\treturn dst\n}\n\nfunc Debug(args ...interface{}) {\n\tif os.Getenv("GOR_TEST") == "" { // if we are not testing\n\t\tfmt.Fprint(os.Stderr, "[DEBUG][TOKEN-MOD] ")\n\t\tfmt.Fprintln(os.Stderr, args...)\n\t}\n\n}\n</code></pre><p>我们可以使用 process 函数异步处理原始请求或回放响应从而重新设置 token。由于 GoReplay 的每个三元组（请求、响应、回放响应）共享一个请求 ID，因此到达中间件的第一个响应可以将它的 token 关联到请求 ID。当第二个响应到达时，我们就可以访问两个 token了。我们可以将原始 token 关联到回放的 token，并能够一一对应（因为第二个响应类型也可用）。</p><p>好了，这样中间件就写完了，我们一起来测试一下。</p><p>我们先创建一个运行中间件的 Shell 脚本 middleware_wrapper.sh。</p><pre><code class="language-bash">#!/bin/bash\ngo run token_modifier.go\n</code></pre><p>第二步，修改启动回放的 Shell 脚本 replay.sh。</p><pre><code class="language-bash">#!/bin/bash\n## Usage: ./replay.sh\n\nOUTPUT="http://127.0.0.1:8081"\nMIDDLEWARE="./middleware_wrapper.sh"\nINPUT_FILE="requests.gor"\n\nsudo ./goreplay --input-file $INPUT_FILE --input-file-loop --output-http=$OUTPUT --middleware $MIDDLEWARE --prettify-http --output-http-track-response --output-stdout\n</code></pre><p>最后一步，我们就要紧盯运行控制台了。</p><ul>\n<li>登录接口实时返回的 token。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/c4/90/c4a97041689054c34c2b721a48ecd190.png?wh=1881x695" alt="图片"></p><ul>\n<li>【查询所有后台资源分类】接口，可以看到已经成功替换到回放响应的 token了。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/de/98/de4c63d1d179f2b5b4804e5febac1898.png?wh=750x386" alt="图片"></p><ul>\n<li>【查询所有后台资源分类】接口，回放响应的数据也是正常的。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/55/24/55b26162d5dacaa39a0a161d5f83f124.png?wh=750x321" alt="图片"></p><ul>\n<li>服务端日志显示正常。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/71/yy/71e8fd0b45f03d5861c1debe0cd504yy.png?wh=599x348" alt="图片"></p><p>好了，到这里，我们的动态数据关联功能就已经实现了。</p><h2>总结</h2><p>好了，这节课就讲到这里。刚才，我们一起梳理了关联的基本概念、 GoReplay 中间件（ Middleware ）原理、常用的用法。我们还通过例子演示了GoReplay 如何通过扩展 Middleware 做到关联功能。实际上，我们可以在中间件上实现任何自定义逻辑，比如认证、复杂的重写和筛选请求等。</p><p>下一节课，我们将进入具体的分布式改造环节，我会通过案例演示如何做分布式平台改造工作。</p><h2>课后题</h2><p>学完这节课，请你思考两个问题：</p><ol>\n<li>你有没有使用过 Middleware，谈谈你对 Middleware 应用的一些心得吧！</li>\n<li>相比 JMeter，你觉得 GoReplay 关联的难度在什么地方？</li>\n</ol><p>欢迎你在留言区与我交流讨论。当然了，你也可以把这节课分享给你身边的朋友，他们的一些想法或许会让你有更大的收获。我们下节课见！</p>',
        article_title: "22 | 压测平台：如何解决 GoReplay 动态数据关联？",
      },
      {
        title: "23 | 压测平台：如何改造对象存储和性能监控？",
        id: 465623,
        content:
          '<p>你好，我是高楼。</p><p>这节课我们来聊聊如何改造分布式压测平台。</p><p>在第 6 讲，我们已经详细了解了流量工具的选型，我们一起来回顾下全链路流量平台必须具备的能力：</p><ol>\n<li>能录制线上真实流量；</li>\n<li>能实现海量数据的并发请求，并覆盖地域性的 CDN 边缘节点；</li>\n<li>能支持常见协议的请求；</li>\n<li>对线上尽量应用透明，也就是说无侵入性；</li>\n<li>避免写请求的脏数据，压测流量能够被识别，方便压测后清理；</li>\n<li>工具使用简单，能够满足压测实时监控，服务安全保护（过载熔断）。</li>\n</ol><p>按照上面这几条能力需求，我还画了一个比较典型的全链路流量平台架构设计图。</p><p><img src="https://static001.geekbang.org/resource/image/71/82/713cb37c39316091a6b6e987cb83c382.jpg?wh=1920x1715" alt="图片"></p><p>在这张架构图中，我把压测平台分为压测 web 管理端、调度服务、压测引擎、监控服务、对象存储、录制服务六大模块。这样的一个全链路流量平台基本上就可以覆盖大部分企业的需求了。</p><p>接下来，我会就这里面部分的技术细节进行拆解。因为内容比较多，我会分成两节课讲解。这节课，我们主要看一下对象存储和性能监控模块如何落地。</p><h2>对象存储</h2><p>对象存储简单来说就是一种海量、安全、低成本、高可靠的云存储服务，适合存放任意类型的文件。它还支持快速查询、上传下载等功能。通俗来说就是一个文件仓库。</p><p>现在大部分的公有云厂商，都提供了自己的对象存储服务，比如阿里云的OSS、华为云的OBS，腾讯云的COS等，我们只需集成云厂商提供的SDK即可访问。而开源产品方面，比较常见的有 Ceph 和 <a href="http://www.minio.org.cn/overview.shtml#">MinIO</a>。</p><!-- [[[read_end]]] --><p>其中，Ceph是一个比较强大的分布式存储系统，但是它整个系统非常复杂，比较重量级，需要花费大力气去维护，很显然与我们的目标不是很符合，所以暂时不考虑。</p><p>而MinIO是一个基于Apache License V2.0开源协议的高性能、分布式的对象存储系统，而且兼容亚马逊S3云存储服务，非常适合存储大容量非结构化的数据，比如图片、视频、日志文件等。而且MinIO系统较为轻量级，可以很简单地和其它的应用结合，很显然，气质和我们的流量平台比较符合。</p><p>总而言之，如果想自建对象存储服务，而且有能力、规模又比较大的话，采用 Ceph 更好一点。但是我们只是想要一个对象存储，要求没有那么多，所以我们才选择了 MinIO。它的整体结构图如下：</p><p><img src="https://static001.geekbang.org/resource/image/e5/3c/e5500bd1ed32efe787f09d7c51d05d3c.jpg?wh=843x912" alt=""></p><p>我们设计的上传文件的大致流程你可以看看下面这张图：</p><p><img src="https://static001.geekbang.org/resource/image/4a/46/4a304423ee40dbc8110646679e4c9f46.jpg?wh=1347x1564" alt="图片"></p><p>知道了大致的流程，接下来看看我们如何落地。</p><ul>\n<li>搭建 MinIO Server</li>\n</ul><p>这里，为了方便调试，我们使用Docker快速搭建 MinIO Server，并设置端口号、容器名。</p><pre><code class="language-bash">docker run -d --name minio-server \\\n-p 9000:9000 \\\n-p 9001:9001 \\\nminio/minio server /data \\\n--console-address ":9001"\n</code></pre><p>如果你希望集成到k8s，还可以使用Operator方式搭建MinIO Server，具体方法可以参考<a href="http://docs.minio.org.cn/minio/k8s/deployment/deploy-minio-operator.html">官网文档</a>。</p><p>启动成功后，访问MinIO的IP地址，因为我的 MinIO Server 安装在本机，所以是 <a href="http://localhost:9001">http://localhost:9001</a>，输入默认的账号密码是 minioadmin/minioadmin。</p><p><img src="https://static001.geekbang.org/resource/image/6a/85/6a99f51224acf674c7464463efc70385.png?wh=1918x1034" alt="图片"></p><p>登录后，进入控制台看板页。</p><p><img src="https://static001.geekbang.org/resource/image/11/74/11feff640cc83593f32de5f0bf3c8074.png?wh=1500x769" alt="图片"></p><p>好了，搭建完 MinIO Server 之后，我们就需要实现自己的对象存储 HTTP服务了。</p><ul>\n<li>通过 SpringBoot 实现 HTTP 服务</li>\n</ul><p>因为项目使用的是 SpringBoot 应用，所以这里主要演示通过 SpringBoot 实现的 HTTP 服务。</p><p>首先， 在 pom.xml中 添加 MinIO 的相关依赖：</p><pre><code class="language-xml">&lt;!--MinIO JAVA SDK--&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;io.minio&lt;/groupId&gt;\n    &lt;artifactId&gt;minio&lt;/artifactId&gt;\n    &lt;version&gt;3.0.10&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre><p>第二步，在 SpringBoot 中开启文件上传功能，在 application.yml 中添加如下配置：</p><pre><code class="language-yaml">spring:\n  servlet:\n    multipart:\n      enabled: true #开启文件上传\n      max-file-size: 10MB #限制文件上传大小为10M\n</code></pre><p>然后，添加一个 MinioController 控制器用于实现文件的上传、下载、删除操作：</p><pre><code class="language-java">package com.dunshan.controller;\n\nimport com.google.api.client.util.IOUtils;\nimport com.dunshan.common.api.CommonResult;\nimport com.dunshan.dto.MinioUploadDto;\nimport io.minio.MinioClient;\nimport io.minio.policy.PolicyType;\nimport io.swagger.annotations.Api;\nimport io.swagger.annotations.ApiOperation;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestMethod;\nimport org.springframework.web.bind.annotation.RequestParam;\nimport org.springframework.web.bind.annotation.ResponseBody;\nimport org.springframework.web.multipart.MultipartFile;\n\nimport javax.servlet.http.HttpServletResponse;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.net.URLEncoder;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * Created by dunshan on 2019/12/25.\n */\n\n@Api(tags = "MinioController", description = "MinIO对象存储管理")\n@Controller\n@RequestMapping("/minio")\npublic class MinioController {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(MinioController.class);\n    @Value("${minio.endpoint}")\n    private String ENDPOINT;\n    @Value("${minio.bucketName}")\n    private String BUCKET_NAME;\n    @Value("${minio.accessKey}")\n    private String ACCESS_KEY;\n    @Value("${minio.secretKey}")\n    private String SECRET_KEY;\n\n    @ApiOperation("文件上传")\n    @RequestMapping(value = "/upload", method = RequestMethod.POST)\n    @ResponseBody\n    public CommonResult upload(@RequestParam("file") MultipartFile file) {\n        try {\n            //创建一个MinIO的Java客户端\n            MinioClient minioClient = new MinioClient(ENDPOINT, ACCESS_KEY, SECRET_KEY);\n            boolean isExist = minioClient.bucketExists(BUCKET_NAME);\n            if (isExist) {\n                LOGGER.info("存储桶已经存在！");\n            } else {\n                //创建存储桶并设置只读权限\n                minioClient.makeBucket(BUCKET_NAME);\n                minioClient.setBucketPolicy(BUCKET_NAME, "*.*", PolicyType.READ_ONLY);\n            }\n            String filename = file.getOriginalFilename();\n            SimpleDateFormat sdf = new SimpleDateFormat("yyyyMMdd");\n            // 设置存储对象名称\n            String objectName = sdf.format(new Date()) + "/" + filename;\n            // 使用putObject上传一个文件到存储桶中\n            minioClient.putObject(BUCKET_NAME, objectName, file.getInputStream(), file.getContentType());\n            LOGGER.info("文件上传成功!");\n            MinioUploadDto minioUploadDto = new MinioUploadDto();\n            minioUploadDto.setName(filename);\n            minioUploadDto.setUrl(ENDPOINT + "/" + BUCKET_NAME + "/" + objectName);\n            return CommonResult.success(minioUploadDto);\n        } catch (Exception e) {\n            LOGGER.info("上传发生错误: {}！", e.getMessage());\n        }\n        return CommonResult.failed();\n    }\n\n    @ApiOperation("文件删除")\n    @RequestMapping(value = "/delete", method = RequestMethod.POST)\n    @ResponseBody\n    public CommonResult delete(@RequestParam("objectName") String objectName) {\n        try {\n            MinioClient minioClient = new MinioClient(ENDPOINT, ACCESS_KEY, SECRET_KEY);\n            minioClient.removeObject(BUCKET_NAME, objectName);\n            return CommonResult.success(null);\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n        return CommonResult.failed();\n    }\n    \n    @ApiOperation("文件下载")\n    @RequestMapping(value = "/download", method = RequestMethod.GET)\n    @ResponseBody\n    public CommonResult download(@RequestParam("filename") String filename, HttpServletResponse httpResponse) {\n        try {\n            MinioClient minioClient = new MinioClient(ENDPOINT, ACCESS_KEY, SECRET_KEY);\n            InputStream inputStream= minioClient.getObject(BUCKET_NAME, filename);\n            httpResponse.reset();\n            httpResponse.setHeader("Content-Disposition", "attachment;filename=" + URLEncoder.encode(filename, "UTF-8"));\n            httpResponse.setContentType("application/octet-stream");\n            httpResponse.setCharacterEncoding("utf-8");\n            OutputStream outputStream = httpResponse.getOutputStream();\n            IOUtils.copy(inputStream, outputStream);\n            outputStream.close();\n\n        } catch (Exception e) {\n            LOGGER.info("导出失败：", e.getMessage());\n            e.printStackTrace();\n        }\n        return null;\n    }\n}\n</code></pre><p>最后，在 application.yml 中对 MinIO 客户端进行配置：</p><pre><code class="language-yaml"># MinIO对象存储相关配置\nminio:\n  endpoint: http://127.0.0.1:9000 #MinIO服务所在地址\n  bucketName: goreplay #存储桶名称\n  accessKey: minioadmin #访问的key\n  secretKey: minioadmin #访问的秘钥\n</code></pre><ul>\n<li>接口测试</li>\n</ul><p>接下来我们启动 SpringBoot 应用，使用 Postman 来测试验证一下功能。</p><p>首先，访问上传接口，进行文件上传。上传接口的地址是：<a href="http://localhost:8080/minio/upload">http://localhost:8080/minio/upload</a>。</p><p><img src="https://static001.geekbang.org/resource/image/97/91/97d28bf2379590a77357f5513d681d91.png?wh=1500x897" alt="图片"></p><p>上传完成后，我们打开 MinIO 的管理界面，可以看到上传后的文件。</p><p><img src="https://static001.geekbang.org/resource/image/4b/e7/4b4eca50dd7546ee1c2f9653f16419e7.png?wh=1500x831" alt="图片"></p><p>我们还可以调用删除接口来删除其中某文件，需要注意的是，objectName 参数值是存储桶（Buckets）中的文件相对路径，删除文件接口地址：<a href="http://localhost:8080/minio/delete">http://localhost:8080/minio/delete</a>。</p><p><img src="https://static001.geekbang.org/resource/image/57/3d/574acaf12f603b89a23e88a8a542593d.png?wh=1500x924" alt="图片"></p><p>最后，我们还可以调用下载接口来下载文件，下载文件接口地址：<a href="http://localhost:8080/minio/download">http://localhost:8080/minio/download</a>。</p><p><img src="https://static001.geekbang.org/resource/image/32/yy/32c6a4a957d88e00254f4a1c71f3dcyy.png?wh=1500x731" alt="图片"></p><p>导出文件时填好文件名称，选择 <strong>send and download</strong>按钮就可以在 Postman 中下载文件了。</p><p>好了，到这里，我们的对象存储服务就改造完成了，这样，我们流量仓库的功能也就差不多实现了。</p><h2>性能监控</h2><p>接下来，我们要对 GoReplay 性能监控进行改造。</p><ul>\n<li>GoReplay 统计请求队列信息</li>\n</ul><p>前面我们讲过， GoReplay 通过参数：-stats --out-http-stats 在控制台将统计的发送请求队列信息默认每 5 秒输出到控制台。</p><p>下面是参数 --stats --output-http-stats 的说明：</p><pre><code class="language-bash">--stats&nbsp; //打开输出队列的状态\n&nbsp; &nbsp; &nbsp; &nbsp; Turn on queue stats output\n-output-http-stats  //每5秒钟输出一次输出队列的状态\n        Report http output queue stats to console every N milliseconds. See output-http-stats-ms\n-output-http-stats-ms int\n        Report http output queue stats to console every N milliseconds. default: 5000 (default 5000)\n</code></pre><p>统计并发送请求队列信息的核心代码主要会用到下面两个函数。</p><p>我们用 <a href="https://github.com/buger/goreplay/blob/master/output_http.go">output_http.go</a> 实现统计信息收集：</p><pre><code class="language-go">// PluginWrite writes message to this plugin\n// 统计信息收集\nfunc (o *HTTPOutput) PluginWrite(msg *Message) (n int, err error) {\n\tif !isRequestPayload(msg.Meta) {\n\t\treturn len(msg.Data), nil\n\t}\n\n\tselect {\n\tcase &lt;-o.stop:\n\t\treturn 0, ErrorStopped\n\tcase o.queue &lt;- msg:\n\t}\n\n\tif o.config.Stats {\n\t\to.queueStats.Write(len(o.queue))\n\t}\n\tif len(o.queue) &gt; 0 {\n\t\t// try to start a new worker to serve\n\t\tif atomic.LoadInt32(&amp;o.activeWorkers) &lt; int32(o.config.WorkersMax) {\n\t\t\tgo o.startWorker()\n\t\t\tatomic.AddInt32(&amp;o.activeWorkers, 1)\n\t\t}\n\t}\n\treturn len(msg.Data) + len(msg.Meta), nil\n}\n</code></pre><p><a href="https://github.com/buger/goreplay/blob/master/gor_stat.go">gor_stat.go</a> 类中用 NewGorStat 函数构造统计：</p><pre><code class="language-go">// NewGorStat统计函数\nfunc NewGorStat(statName string, rateMs int) (s *GorStat) {\n\ts = new(GorStat)\n\ts.statName = statName\n\ts.rateMs = rateMs\n\ts.latest = 0\n\ts.mean = 0\n\ts.max = 0\n\ts.count = 0\n\n\tif Settings.Stats {\n\t\tgo s.reportStats()\n\t}\n\treturn\n}\n</code></pre><p>控制台输出的发送请求队列信息是这样的：</p><p><img src="https://static001.geekbang.org/resource/image/fa/f2/fab6fb7d04705d0c47cfb639e34769f2.png?wh=547x357" alt="图片"></p><p>其中，倒数第二列等同于当前的TPS，但是这就是仅有的统计项了。如果想要更复杂的测试统计结果，就需要我们自己去埋点丰富监控指标了。</p><ul>\n<li>GoReplay 埋点思路</li>\n</ul><p>在前面的课程中，我们已经选用了Exporter+Prometheus+Grafana作为我们全局的监控解决方案了，这里我们能不能把 GoReplay 的 Metrics 实时接入进来呢？</p><p>事实上是可以做到的。</p><p>我们先来看看 node_exporter+Prometheus+Grafana套件的运行效果：</p><p><img src="https://static001.geekbang.org/resource/image/60/68/6048c0f6a90cf1ae1b0f4d67f1425168.png?wh=1842x873" alt="图片"></p><p>Prometheus 提供了 <a href="https://github.com/prometheus/client_golang">官方版 Golang 库</a> ，用于采集并暴露监控数据。我们只需要埋点，让 GoReplay 支持实时 Metrics 统计就可以了。这样，在 GoReplay 执行流量回放时，我们就可以实时采集TPS和响应时间等各项压测指标了，另外，结合 Grafana 看板还可以做到图形可视化展示。</p><p>既然我们要去 GoReplay 埋点，那么就得知道去哪里埋，对吧？所以我们得先分析源码，找出 GoReplay 发出请求的代码，理清 GoReplay 埋点的思路。</p><p>在用 GoReplay 进行流量回放时，我们主要使用的是HTTP输出的插件<a href="https://github.com/buger/goreplay/blob/master/output_http.go">output_http.go</a> 。它通过实现 HTTP 协议， 进而实现 io.Writer 接口，最后根据配置注册到 Plugin.outputs 队列里。</p><p>在<a href="https://github.com/buger/goreplay/blob/master/output_http.go">output_http.go</a> 中， NewHTTPOutput 是默认初始化函数：</p><pre><code class="language-go">// NewHTTPOutput constructor for HTTPOutput\n// Initialize workers\nfunc NewHTTPOutput(address string, config *HTTPOutputConfig) PluginReadWriter {\n\to := new(HTTPOutput)\n\tvar err error\n\tconfig.url, err = url.Parse(address)\n\tif err != nil {\n\t\tlog.Fatal(fmt.Sprintf("[OUTPUT-HTTP] parse HTTP output URL error[%q]", err))\n\t}\n\tif config.url.Scheme == "" {\n\t\tconfig.url.Scheme = "http"\n\t}\n\tconfig.rawURL = config.url.String()\n\tif config.Timeout &lt; time.Millisecond*100 {\n\t\tconfig.Timeout = time.Second\n\t}\n\tif config.BufferSize &lt;= 0 {\n\t\tconfig.BufferSize = 100 * 1024 // 100kb\n\t}\n\tif config.WorkersMin &lt;= 0 {\n\t\tconfig.WorkersMin = 1\n\t}\n\tif config.WorkersMin &gt; 1000 {\n\t\tconfig.WorkersMin = 1000\n\t}\n\tif config.WorkersMax &lt;= 0 {\n\t\tconfig.WorkersMax = math.MaxInt32 // idealy so large\n\t}\n\tif config.WorkersMax &lt; config.WorkersMin {\n\t\tconfig.WorkersMax = config.WorkersMin\n\t}\n\tif config.QueueLen &lt;= 0 {\n\t\tconfig.QueueLen = 1000\n\t}\n\tif config.RedirectLimit &lt; 0 {\n\t\tconfig.RedirectLimit = 0\n\t}\n\tif config.WorkerTimeout &lt;= 0 {\n\t\tconfig.WorkerTimeout = time.Second * 2\n\t}\n\to.config = config\n\to.stop = make(chan bool)\n\t//是否收集统计信息，统计输出间隔是多少\n\tif o.config.Stats {\n\t\to.queueStats = NewGorStat("output_http", o.config.StatsMs)\n\t}\n\n\to.queue = make(chan *Message, o.config.QueueLen)\n\tif o.config.TrackResponses {\n\t\to.responses = make(chan *response, o.config.QueueLen)\n\t}\n\t// it should not be buffered to avoid races\n\to.stopWorker = make(chan struct{})\n\n\tif o.config.ElasticSearch != "" {\n\t\to.elasticSearch = new(ESPlugin)\n\t\to.elasticSearch.Init(o.config.ElasticSearch)\n\t}\n\to.client = NewHTTPClient(o.config)\n\to.activeWorkers += int32(o.config.WorkersMin)\n\tfor i := 0; i &lt; o.config.WorkersMin; i++ {\n\t\tgo o.startWorker()\n\t}\n\tgo o.workerMaster()\n\treturn o\n}\n</code></pre><p>初始化配置后，启动 HttpClient 网络库：</p><p><img src="https://static001.geekbang.org/resource/image/dc/34/dc34079b07eece4b32370b950a687f34.png?wh=620x438" alt="图片"></p><p>紧接着，HttpClient 会启动多个发送请求协程：</p><p><img src="https://static001.geekbang.org/resource/image/e8/38/e81f619ff63648f6b4aa34f1fee52038.png?wh=616x767" alt="图片"></p><p>HttpClient 执行请求发送：</p><p><img src="https://static001.geekbang.org/resource/image/81/c3/817fb12cae9c908672dca0a340c754c3.png?wh=1173x849" alt="图片"></p><p>我们可以看看HttpClient 发送请求的细节，下面这张图中，我圈出的内容是各种配置的生效点：</p><p><img src="https://static001.geekbang.org/resource/image/24/e6/2453ee30abf8f331d086d156c84afde6.png?wh=1165x950" alt="图片"></p><p>为了对<a href="https://github.com/buger/goreplay/blob/master/output_http.go">output_http.go</a> 有一个更清晰的理解，你可以看看下面这张核心代码逻辑调用图：</p><p><img src="https://static001.geekbang.org/resource/image/c6/c5/c66dcecb6e3148274b13b1810d2d5cc5.png?wh=1419x964" alt="图片"></p><p>好了，搞懂了 GoReplay 发出请求的逻辑，接下来，我们就需要具体埋点了。</p><ul>\n<li>GoReplay 实现埋点</li>\n</ul><p>我们之所以要埋点，主要是为了实现获取请求的状态码以及对应的 TPS，然后在获取请求返回值的位置插入对应的  Metric。</p><p>在此之前，我们先要了解 Prometheus 中常见的四大指标类型：</p><ul>\n<li>Counter（计数器）：一个递增的计数器，只增不减，但是它可以被重置为0（例如重启服务）。常用于需要记录请求次数、错误数量的场景；</li>\n<li>Gauge（仪表盘）：可以用它来表示一个可以任意变化的浮动值，可增可减。常用于反馈当前情况的场景；</li>\n<li>Histogram（累积直方图）：多用于需要统计一些数据分布的情况。它可以计算在一定范围内的分布情况，同时还提供了度量指标值的总和。常用于记录请求延迟、响应大小等统计场景；</li>\n<li>Summary（摘要）：在一段时间范围内对数据进行采样，和 Histogram 累积直方图比较类似，主要用于计算在一定时间窗口范围内，度量指标对象的总数以及所有度量指标值的总和。</li>\n</ul><p>更多内容你可以参考 <a href="https://prometheus.io/docs/instrumenting/writing_clientlibs/#counter">Prometheus官网</a>。</p><p>了解了基础概念，我们就来看看要怎么在 GoReplay 中进行改造。</p><p>第一步，通过 go get 命令来安装相关依赖库，示例如下：</p><pre><code class="language-bash">go get github.com/prometheus/client_golang/prometheus/promhttp\n</code></pre><p>有关全面的 API 文档，你可以参考 Prometheus 的各种 Go 库的 <a href="https://godoc.org/github.com/prometheus/client_golang">GoDoc</a> 文档。</p><p>第二步，创建 prometheus_family.go 类，初始化监控 Metric。</p><pre><code class="language-go">package metrics\n\nimport "github.com/prometheus/client_golang/prometheus"\n\n// 创建 Prometheus 数据Metric, 就相当于SQL 数据库 声明table\nvar (\n\t//Counter（计数器）\n\ttotalRequestsCounter = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: "goreplay_total_requests",\n\t\t\tHelp: "total income requests",\n\t\t},\n\t\t[]string{"location", "code"},\n\t)\n\t//Counter (计数器)\n\tsubRequestsCounter = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: "test_sub_requests",\n\t\t\tHelp: "sub requests",\n\t\t},\n\t\t[]string{"test"},\n\t)\n\t//Gauge（仪表盘）\n\tcircuitBreakerRateGauge = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: "goreplay_circuit_breaker_rate",\n\t\t\tHelp: "rate of circuit breaker",\n\t\t},\n\t\t[]string{"location", "code"},\n\t)\n\n\tbuckets = []float64{0, 100, 200}\n\t// Histogram(累积直方图)\n\ttotalRequestsTimeHistogram = prometheus.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    "goreplay_total_requests_time",\n\t\t\tHelp:    "income requests time",\n\t\t\tBuckets: buckets,\n\t\t},\n\t\t[]string{"location"},\n\t)\n\n)\n\n// 注册定义好的Metric 相当于执行SQL create table 语句\nfunc init() {\n\tprometheus.MustRegister(totalRequestsCounter)\n\tprometheus.MustRegister(subRequestsCounter)\n\tprometheus.MustRegister(circuitBreakerRateGauge)\n\tprometheus.MustRegister(totalRequestsTimeHistogram)\n}\n\nfunc IncreaseTotalRequests(location,code string) {\n\ttotalRequestsCounter.With(prometheus.Labels{"location": location, "code": code}).Add(1)\n}\n\nfunc IncreaseSubRequests() {\n\tsubRequestsCounter.With(prometheus.Labels{}).Add(1)\n}\n\n\n\nfunc ObserveTotalRequestsTimeHistogram(location string, d float64) {\n\ttotalRequestsTimeHistogram.With(prometheus.Labels{"location": location}).Observe(d)\n}\n</code></pre><p>这里，我在项目中创建了 package metrics，初始化了 API 请求相关的 Metric，并且注册到了github.com/prometheus/client_golang/prometheus 。</p><p>第三步，改造 output_http.go 类，在业务代码中采集数据。</p><p><img src="https://static001.geekbang.org/resource/image/fb/cb/fb287c1e977bb01f2c111897bf36f4cb.png?wh=1500x1734" alt="图片"></p><p>从这张截图可以看到，我们为了获取请求状态码以及对应的TPS，在获取请求返回值的位置插入了counter metric；为了获取不同请求的响应时间等指标，我们还需要在发出请求前记录开始时间，待请求返回后记录结束时间，同时还要记录时间消耗的 Histogram Metric。</p><p>这里大家可以举一反三，扩展其它的 Metric。</p><p>第四步，改造 main.go ，进行 Metric 注册，提供/metric 接口给 Prometheus TSDB 时序数据库收集数据。</p><p><img src="https://static001.geekbang.org/resource/image/cc/b4/cc4c813da3f554a6cd84290a94ed19b4.png?wh=1500x1427" alt="图片"></p><p>第五步，重新编译 GoReplay 程序。</p><p>在代码所在目录（./src/goreplay）下使用 go build 命令。</p><pre><code class="language-bash">go build\n</code></pre><p>第六步，在启动 GoReplay 进行流量回放时，查看监听端口，可以看到我们注册的端口 28081，访问接口：<a href="http://localhost:28081/metrics">http://localhost:28081/metrics</a>。</p><pre><code class="language-bash">。......\n\n# HELP goreplay_total_requests total income requests\n# TYPE goreplay_total_requests counter\ngoreplay_total_requests{code="200 ",location=""} 22\n# HELP goreplay_total_requests_time income requests time\n# TYPE goreplay_total_requests_time histogram\ngoreplay_total_requests_time_bucket{location="",le="0"} 0\ngoreplay_total_requests_time_bucket{location="",le="100"} 22\ngoreplay_total_requests_time_bucket{location="",le="200"} 22\ngoreplay_total_requests_time_bucket{location="",le="+Inf"} 22\ngoreplay_total_requests_time_sum{location=""} 2.1513923769999996\ngoreplay_total_requests_time_count{location=""} 22\n# HELP promhttp_metric_handler_requests_in_flight Current number of scrapes being served.\n# TYPE promhttp_metric_handler_requests_in_flight gauge\npromhttp_metric_handler_requests_in_flight 1\n# HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code.\n# TYPE promhttp_metric_handler_requests_total counter\npromhttp_metric_handler_requests_total{code="200"} 20\npromhttp_metric_handler_requests_total{code="500"} 0\npromhttp_metric_handler_requests_total{code="503"} 0\n</code></pre><p>第七步，在Prometheus 主程序中拉取采集数据。</p><p>在Prometheus主程序的配置文件中填写第四步的API接口信息，这样，Prometheus TSDB 时序数据库就可以开始定时收集 Metric 数据了。</p><pre><code class="language-yaml">###################### GoReolay ######################\n- job_name: "GoReolays5"\n  static_configs:\n  - targets: [\'172.31.184.225:28081\']\n    labels:\n      instance: s5\n</code></pre><p>第八步，在 Grafana 做图形数据展示。</p><p>Prometheus 提供了一种功能表达式语言 PromQL，这种语言可以允许用户实时选择和汇聚时间序列数据。另外，表达式的结果可以在结合 Grafana 的控件中显示为图形，也可以显示为表格数据，或者由外部系统通过 HTTP API 调用，因为篇幅有限，网上关于这部分的资料又很多，这里我就不多说了。</p><p>到这里，我们对 GoReplay 的埋点改造工作就做完了，通过 Metric 实现了从客户端统计压测过程中的各项指标。</p><h2>总结</h2><p>好，这节课就讲到这里。</p><p>刚才，我们介绍了流量平台的对象存储和性能监控功能的选型及改造，我还对这两个部分做了详细的演示。通过结合 MinIO Server 和 HTTP 服务，我们可以实现程序二进制包、执行器 jar 包、流量文件等文件管理，并能够使用通用的 HTTP 上传下载功能。另外，通过 Prometheus 在GoReplay 埋点，我们还进一步丰富了性能监控指标。</p><p>在当前的全链路压测的市场中，对全链路压测工具的分布式改造总是讳忌莫深的部分，而压力工具对全链路压测来说，目标是能够实现足够的流量即可。从本节课的内容可以看到，相比传统的压力工具，全链路压测工具在改造上的技术成本还算是高的，但值得欣慰的是开源的工具也是完全可以实现的。</p><p>下一节课，我们将继续讲解分布式改造的各环节，我会通过案例给你演示怎样进行分布式调度改造工作。</p><h2>课后题</h2><p>学完这节课，请你思考两个问题：</p><ol>\n<li>你有没有做过 Prometheus 埋点，谈谈你对业务埋点的一些心得吧！</li>\n<li>相比文件系统，你觉得对象存储的优势在什么地方？</li>\n</ol><p>欢迎你在留言区与我交流讨论。当然了，你也可以把这节课分享给你身边的朋友，他们的一些想法或许会让你有更大的收获。我们下节课见！</p>',
        article_title: "23 | 压测平台：如何改造对象存储和性能监控？",
      },
      {
        title: "24 | 压测平台：如何改造分布式调度平台？",
        id: 466663,
        content:
          '<p>你好，我是高楼。</p><p>这节课，我们继续来讲分布式压测平台的改造。在上一讲，我们已经知道了对象存储和性能监控的实现步骤，这节课，我们来看看分布式调度的改造。</p><h2>分布式调度平台的选型</h2><p>在 Java 技术栈中，传统的任务调度实现方案，比如 Quartz、Timer，都存在一些问题，例如：</p><ul>\n<li>仅单点，不支持集群任务分发；</li>\n<li>无法统计任务执行数据；</li>\n<li>没有监控任务执行状态；</li>\n<li>没有任务异常告警；</li>\n<li>没有 web 后台管理。</li>\n</ul><p>但在如今的全链路压测场景中，大部分的压测平台都是分布式架构，它对任务调度的需求也更高，例如：</p><ul>\n<li>当同一个压力机或多个压力机的下发任务互斥时，需要做到统一的调度；</li>\n<li>下发的任务需要支持高可用、实时监控、异常告警等功能；</li>\n<li>压力机集群需要支持动态、协同进行压测，能应付高并发海量流量的场景；</li>\n<li>需要统一的管理并能够追踪各压力机任务执行结果，需要快速记录各种任务的属性数据。</li>\n</ul><p>也就是说，传统的任务调度实现方案已经不能满足全链路压测平台的需求了。这时候，我们就需要一个分布式任务调度平台。目前业界比较主流的中间件有哪些呢？<br>\n<img src="https://static001.geekbang.org/resource/image/7e/f2/7e6d4yy6334c6c5e02a21144056934f2.jpg?wh=1920x2462" alt=""><br>\n目前业界比较主流的中间件主要有<a href="https://link.segmentfault.com/?url=http%3A%2F%2Felasticjob.io%2F">Elastic-Job</a>、<a href="https://link.segmentfault.com/?url=https%3A%2F%2Fwww.xuxueli.com%2Fxxl-job%2F">XXL-JOB</a>和Quartz三种。由于 XXL-JOB 的热度足够（目前关于 XXL-JOB 的 GitHub Stars 有 20.1k 个，登记在用的公司有 446 个），另外它的开发迅速、学习简单、轻量级、易扩展，这些特性都挺适合我们这个全链路压测项目的。综合考虑后，最后我们选择了<strong>XXL-JOB</strong>。</p><!-- [[[read_end]]] --><h2>XXL-JOB 基本概念</h2><p>既然选定了 XXL-JOB，就要先了解一下 XXL-JOB 的基本结构。</p><p>下面是一张 XXL-JOB （V2.1.0）的官网架构图。</p><p><img src="https://static001.geekbang.org/resource/image/c8/a0/c897f1feea1a144b751f456ea916efa0.png?wh=1500x822" alt="图片"></p><p>从图中我们也可以很直观地看到，XXL-JOB 主要包括两大模块：</p><ul>\n<li><strong>调度模块（调度中心）</strong>：负责管理调度信息，按照调度配置发出调度请求，它自身不承担业务代码。这种调度系统与任务解耦的方式，提高了系统可用性和稳定性，也让调度系统性能不再受限于任务模块。另外，调度中心支持可视化、简单且动态地管理调度信息，例如任务新建、更新、删除、GLUE 开发和任务报警等，所有上述操作都会实时生效。调度中心还支持监控调度结果以及执行日志，支持执行器 Failover。</li>\n<li><strong>执行模块（执行器）</strong>：负责接收调度请求并执行任务逻辑。它可以让任务模块专注于任务的执行等操作，让开发和维护更加简单高效。</li>\n</ul><p>为了方便理解，我这里画了一个更加精简的逻辑图：</p><p><img src="https://static001.geekbang.org/resource/image/f6/7a/f689a85fc158522b8fa93e20a8a1f17a.jpg?wh=1920x1878" alt="图片"></p><p>可以看到，调度器和执行器是独立的。调度器决定任务的调度，并且通过 HTTP 的方式调用执行器接口，执行相应任务。</p><h2>分布式调度平台的落地思路</h2><p>那么，我们要怎么通过分布式调度平台来满足我们的需求呢？要清楚这一点，我们就要先理清执行器在流量平台的主要用途，它可以：</p><ol>\n<li>管理 GoReplay 的生命周期 ，可以调起和终止 GoReplay 程序；</li>\n<li>回传状态（在 GoReplay 启动、结束、或其他标志性事件结束后都会向调度服务回传状态）；</li>\n<li>对录制和回放的相关文件进行下载、处理和回传；</li>\n<li>实时 Rolling 日志，记录 GoReplay 输出的状态数据，便于实时监控。</li>\n</ol><p>对于我们分布式流量平台来说，主要的需求就是流量录制和流量回放操作的调度。我们先来熟悉一下这两种调度方式的落地思路，再来进行具体的实战演示。</p><p>流量录制主要过程：</p><ol>\n<li>用户设置录制参数，定义录制命令，创建录制任务；</li>\n<li>调度中心生成录制任务，并下发到对应网关的执行器上；</li>\n<li>执行器收到录制命令，回传状态给调度中心，随即执行对应录制命令，拉起  GoReplay 程序；</li>\n<li>录制结束后，GoReplay 程序退出，执行器回传状态给调度中心；</li>\n<li>调度中心判定录制任务结束后，下发上传文件子任务到相关执行器上，执行器执行上传命令将对应的流量文件通过 HTTP 上传到对象存储中；</li>\n<li>流量录制任务结束。</li>\n</ol><p>流量回放的主要过程：</p><ol>\n<li>用户设置回放参数，定义回放命令，创建回放任务；</li>\n<li>调度中心生成回放任务，并下发到对应压力机的执行器上；</li>\n<li>执行器收到回放命令，回传状态给调度服务，随即拉起  GoReplay 程序；</li>\n<li>回放结束，GoReplay 退出，执行器回传状态给调度中心；</li>\n<li>流量回放任务结束。</li>\n</ol><p>好了，了解了流量平台的调度使用过程，我们就来一起看看具体怎样去落地。</p><h2>分布式调度平台的落地实践</h2><p>首先，我们需要搭建 XXL-JOB 调度平台，具体的搭建和系统初始化步骤你可以参考<a href="https://www.xuxueli.com/index.html">官方文档</a>。</p><p>搭建成功后，登录调度中心，运行界面如下：</p><p><img src="https://static001.geekbang.org/resource/image/ce/6e/ceb0eb9ba6ed756ebd2f7be9060ce76e.png?wh=1500x783" alt="图片"></p><p>调度中心支持集群部署，这可以提升调度系统的容灾和可用性。不过，在调度中心集群部署时我有几点建议：</p><ul>\n<li>DB 配置保持一致；</li>\n<li>集群机器时钟保持一致（单机集群忽视）；</li>\n<li>通过 Nginx 为调度中心集群做负载均衡，分配域名（调度中心访问、执行器回调配置、调用 API 服务等操作均通过该域名进行）。</li>\n</ul><p>有了调度平台，我们就可以进行具体的实践了：</p><ul>\n<li>创建执行器</li>\n</ul><p>执行器相当于是包工头，它可以是一个单独项目，也可以集成在其它业务项目里面。要注意的是，执行器首先需要注册到调度中心，这样调度中心才能发现执行器。执行器的执行结果，也需要通过回调的方式反馈给调度中心。</p><p>在源码的 xxl-job-executor-samples 里，有 2 个执行的 demo 教我们如何去创建执行器。</p><p><img src="https://static001.geekbang.org/resource/image/c0/f5/c01677c778cfb355cb12abacf0c8baf5.png?wh=818x1370" alt="图片"></p><p>这里我用 SpringBoot 项目用来举个例子。首先，要单独拷一个项目出来，如果你想在自己的项目里集成的话，同样也是参考这个 demo，只要在项目 pom 里加上 xxl-job-core 的依赖即可。</p><p><img src="https://static001.geekbang.org/resource/image/54/f0/544998255f82118017e48b1615b7aef0.png?wh=736x1192" alt="图片"></p><p>然后我们需要修改配置文件 application.properties：</p><pre><code class="language-yaml"># web port\nserver.port=8089\n# no web\n#spring.main.web-environment=false\n\n# log config\nlogging.config=classpath:logback.xml\n\n### 注册到调度中心\n### xxl-job admin address list, such as "http://address" or "http://address01,http://address02"\nxxl.job.admin.addresses=http://127.0.0.1:8080/xxl-job-admin\n\n### xxl-job, access token\nxxl.job.accessToken=\n\n### 集群部署，这两项配置要一致\n### xxl-job executor appname\nxxl.job.executor.appname=xxl-job-executor-sample\n### xxl-job executor registry-address: default use address to registry , otherwise use ip:port if address is null\nxxl.job.executor.address=\n\n### xxl-job executor server-info\nxxl.job.executor.ip=127.0.0.1\n### 如果要模拟执行器集群部署，打包后单击运行多次，为服务设置随机端口，${random.int[10000,19999]}\nxxl.job.executor.port=9999\n### 日志存放路径\n### xxl-job executor log-path\nxxl.job.executor.logpath=./data/applogs/xxl-job/jobhandler\n### xxl-job executor log-retention-days\nxxl.job.executor.logretentiondays=30\n</code></pre><p>接着，我们在 IDEA 启动项目。</p><p><img src="https://static001.geekbang.org/resource/image/a3/7e/a3a7254c614f0afa8761c330da2b2e7e.png?wh=1500x842" alt="图片"></p><p>最后，我们在调度中心的【执行器管理】模块里新增一个执行器。</p><p><img src="https://static001.geekbang.org/resource/image/e2/14/e264f40c313d9c22e4ffd7bd0e34f314.png?wh=899x764" alt="图片"></p><p>刷新之后就可以看到我们注册好的的执行器了。</p><p><img src="https://static001.geekbang.org/resource/image/5e/7d/5ed834a4c0f7d993caebd3703eaf007d.png?wh=1500x547" alt="图片"></p><p>到这里，我们的执行器就已经成功被调度中心注册和发现了，下面我们需要创建对应的任务。</p><ul>\n<li>创建流量录制任务</li>\n</ul><p>我们在调度中心新建一个调度任务。这里的运行模式我们选择 GLUE 模式(Shell)。</p><p><img src="https://static001.geekbang.org/resource/image/e2/14/e264f40c313d9c22e4ffd7bd0e34f314.png?wh=899x764" alt="图片"></p><p>简单说明一下，运行模式主要分为两种，一种是 BEAN，一种是 GLUE：</p><ul>\n<li>BEAN 模式：在项目中写 Java 类，然后在 JobHandler 里填上 @XxlJob 里面的名字，它是在执行器端代码编写的；</li>\n<li>GLUE 模式：支持 Java、Shell、Python、PHP、Nodejs、PowerShell，以源码方式直接维护在调度中心，该任务实际上是一段对应脚本的执行。</li>\n</ul><p>我们知道 GoReplay 的所有操作都是通过命令行组织的，所以 GLUE模式的Shell脚本是满足我们的需求的，且能做到集中式管理。</p><p>首先创建任务，点击该任务右侧的“GLUE”按钮，前往 GLUE 任务的 Web IDE 界面。这个界面支持对任务代码进行开发（也可以在 IDE 中开发完成后，复制粘贴到 Web IDE 中）。</p><p><img src="https://static001.geekbang.org/resource/image/5f/2d/5fccd03d6438b93b15c4cdc07a1eb02d.png?wh=1905x676" alt="图片"></p><p>你可以看看我们在 IDE 中开发的流量录制命令。</p><pre><code class="language-bash">#!/bin/bash\necho "xxl-job: hello shell"\n\necho "脚本位置：$0"\necho "任务参数：$1"\necho "分片序号 = $2"\necho "分片总数 = $3"\n\ncd /Users/GoProjects/src/goreplay/examples/middleware\n./goreplay --input-raw :8201 --output-file=request-mall-all.gor -output-file-append --http-set-header "dunshan:7DGroup" --http-set-header "User-Agent:Replayed-by-Gor" --input-raw-track-response --prettify-http --http-disallow-url /actuator/health\n\necho "Good bye!"\nexit 0\n</code></pre><p>刚才我们说过，因为GLUE这个模式的任务实际上是一段 “Shell” 脚本，所以任务建好后会存在一个安全问题：我们没有做鉴权。要解决这个问题方法也很简单，只需要在调度中心和执行器的 application.properties 里加上相同的 token 即可。</p><p>任务建好后，在执行时，我们可以传入任务参数，这里我们传入的是监听端口号8081。</p><p><img src="https://static001.geekbang.org/resource/image/f3/dc/f31020b359cff249081a882b56f3b2dc.png?wh=601x317" alt="图片"></p><p>好了，开发完成后，我们点击启动：</p><p><img src="https://static001.geekbang.org/resource/image/0f/c8/0fb3a118815427f20792798fbae5e0c8.png?wh=1500x474" alt="图片"></p><p>点击执行日志右侧的 “执行日志” 按钮，可跳转至执行日志界面查看业务代码中打印的完整日志：</p><p><img src="https://static001.geekbang.org/resource/image/5f/09/5f646acef4d995397952189a0aaae409.png?wh=1500x547" alt="图片"></p><p>点击“日志”按钮，可以查看任务历史调度日志，每次任务调度的调度结果、执行结果等。点击“执行日志”按钮可查看执行器完整日志。</p><p><img src="https://static001.geekbang.org/resource/image/bc/04/bc3b848848135b61f163bb8bd65ebc04.png?wh=1916x513" alt="图片"></p><p>好了，到这里我们的流量录制任务就已经定义完了。至于流量回放、文件上传等其它任务的套路和这个类似。你可以按照差不多的步骤进行操作，遇到问题的话也可以来跟我讨论。</p><ul>\n<li>使用子任务</li>\n</ul><p>当我们需要做流量录制时，首先我们需要执行程序初始化任务，然后再执行流量录制任务，最后，我们需要将产生的流量文件上传到对象存储（流量仓库）中。那么，我们要怎么保证数据在三个任务之间顺利传递呢？</p><p><img src="https://static001.geekbang.org/resource/image/fc/6e/fc57cd3d513e89b80d69342b6b373e6e.jpg?wh=1436x229" alt=""></p><p>解决这种问题的思路一般有两种：</p><ul>\n<li>第一种就是写成一个大 job，让任务串行化执行；</li>\n<li>第二种就是使用多个子任务，在一个上游任务末尾触发另一个子任务。</li>\n</ul><p>如果我们想要在本任务成功执行后触发另外一个任务，可以把另外的任务作为本任务的子任务执行，它的操作也很简单，只需要在子任务一栏填上任务 id 即可：</p><h3><img src="https://static001.geekbang.org/resource/image/23/a5/23e099333f90b8d67b5f1b0f3910bea5.png?wh=1500x1277" alt="图片"></h3><ul>\n<li>压测任务分片</li>\n</ul><p>全链路压测中一个显著的特点就是能够实现海量数据的并发请求，但这对于单线程的普通的任务来说是存在瓶颈的。这个时候如果你想提高并发，那么自然就要多线程跑嘛。但是用多线程跑数据也会存在问题，比如我开 3 个执行器，但你会发现只有个别执行器在拼命工作，其它的都处在休息状态。这是一个典型的任务分配不均的问题。</p><p><img src="https://static001.geekbang.org/resource/image/40/ca/40203d86a2322aa9e91975a405f1dcca.jpg?wh=1920x1600" alt="图片"></p><p>这时候就要用到我们的分片任务了，也就是要多台执行器共同完成任务。</p><p><img src="https://static001.geekbang.org/resource/image/9c/5c/9cc041a7a910759e9d77e192b354db5c.jpg?wh=1920x1619" alt="图片"></p><p>但这里又有一个问题。比如流量回放的时候，多台压力机都执行同一个流量文件，相同的请求三台压力机都发一遍，那岂不是乱套了。</p><p>这里的解决思路也很简单，我们让每台执行器处理1/3的流量，大家把需要回放的流量文件平均切分了嘛，我干 1/3，你干 1/3，它干 1/3，这样就不会产生冲突了。</p><p>分片任务在运行的时候，调度器会给每个执行器发送一个不同的分片序号，分片的最大序号跟执行器的总数量是一样的，这样可以确保每个执行器都会执行到这个任务。</p><p>比如上图中第一个执行器拿到分片序号 0，第二台执行器拿到分片序号 1，第三台执行器拿到分片序号 2。那现在就好办了，我们只需要把处理的流量文件进行模 3 取余，余数为 0 的文件就由执行器 1 跑，余数为 1 的文件就由执行器 2 跑，余数为 2 的文件就由执行器 3 跑。</p><p>具体的解决方案，我们可以使用路由策略「分片广播」来调度任务：</p><ol>\n<li>通过“任务参数”传入执行任务压力机数量；</li>\n<li>在定时任务逻辑中，根据获取到的分片参数、执行任务压力机数量，决策当前节点是否需要执行，切分流量文件并处理：\n<ul>\n<li>如果 分片序号 &gt; (执行压力机数量 - 1)，则当前节点不执行任务，直接返回；</li>\n<li>否则，取「分片序号」和 「压力机数量」 作为分片参数，流量文件并处理。</li>\n</ul>\n</li>\n</ol><p>这样，我们可以实现灵活调度 [1, N] 个节点并行执行压测任务。</p><p>我们切分流量文件的代码可以这样写。</p><pre><code class="language-java">package com.xxl.job.executor.until;\n\nimport cn.hutool.core.io.file.FileReader;\nimport cn.hutool.core.io.file.FileWriter;\nimport org.junit.jupiter.api.Test;\nimport org.junit.platform.commons.util.StringUtils;\n\nimport java.util.*;\n\n/**\n * @author dunshan\n * @description: 分割流量文件\n * @date 2021-12-04 11:11:11\n */\npublic class NcountGor {\n\n\n\n    @Test\n    public void countTxt() {\n        //分组\n        int n = 3;\n\n        LinkedHashSet&lt;String&gt; total = new LinkedHashSet&lt;&gt;();\n        //读取文件\n        FileReader fileReader = new FileReader("/Users/2021/10/request-mall-all.gor");\n        String result = countTotal(total, fileReader);\n        System.out.println("total:" + total.size());\n        //放入 list 中进行分组使用\n        List&lt;String&gt; list = new ArrayList&lt;&gt;();\n        //组装分割文件\n        for (String str : total) {\n            list.add(str);\n        }\n        saveFile(n, result, list);\n\n    }\n\n    /**\n     * 根据分割数据把文件切割文件\n     *\n     * @param n      切割数据\n     * @param result 文件内容\n     * @param list   分组数据\n     */\n    private void saveFile(int n, String result, List&lt;String&gt; list) {\n        int number = (list.size() / n) + 1;\n        List&lt;List&lt;String&gt;&gt; lists = groupList(list, number);\n        for (int i = 0; i &lt; lists.size(); i++) {\n            //保存文件\n            String path = String.format("/Users/2021/10/11/request_%s.gor", i);\n            FileWriter writer = new FileWriter(path);\n            for (int j = 0; j &lt; lists.get(i).size(); j++) {\n                //分割数据\n                String[] content = result.split("\\uD83D\\uDC35\\uD83D\\uDE48\\uD83D\\uDE49");\n                for (String s : content) {\n                    if (s.contains(lists.get(i).get(j))) {\n                        writer.append(s);\n                        //拼接切割文件\n                        writer.append("\\uD83D\\uDC35\\uD83D\\uDE48\\uD83D\\uDE49");\n                    }\n                }\n            }\n        }\n    }\n\n    /**\n     * 统计流量文件总数\n     *\n     * @param total\n     * @param fileReader\n     * @return\n     */\n    private String countTotal(LinkedHashSet&lt;String&gt; total, FileReader fileReader) {\n        String result = fileReader.readString();\n        String[] twoLine = result.split("\\r\\n");\n        for (String str : twoLine) {\n            if (StringUtils.isNotBlank(str)) {\n                //转换字符数组\n                char[] chars = str.toCharArray();\n                //判断第一个字母是否是一\n                if (String.valueOf(chars[0]).equals("1")) {\n                    total.add(str.substring(2, 26));\n                }\n            }\n        }\n        return result;\n    }\n\n    /**\n     * 流量文件分组\n     *\n     * @param list 流量文件\n     * @param n    每个数组多少\n     * @return 分组\n     */\n    private static List&lt;List&lt;String&gt;&gt; groupList(List&lt;String&gt; list, Integer n) {\n        // 求余数\n        int remainder = list.size() % n;\n        // 求分组数\n        int count0 = list.size() / n;\n        boolean flag = false;\n        if (remainder &gt;= n / 1) {\n            flag = true;\n        }\n        List&lt;List&lt;String&gt;&gt; data = new ArrayList&lt;&gt;();\n        // 获取多余 list\n        List&lt;String&gt; sub = new ArrayList&lt;&gt;(list.subList(list.size() - remainder, list.size()));\n        // 遍历 list 到余数前\n        for (int i = 0; i &lt; count0 + 1; i++) {\n            List&lt;String&gt; ls;\n            if (flag) {\n                if (i == count0) {\n                    break;\n                }\n                ls = new ArrayList&lt;&gt;(list.subList(i * n, i * n + n));\n                // 将余出的数据加入分组\n                if (i &lt; sub.size()) {\n                    ls.add(sub.get(i));\n                }\n            } else {\n                int endex = i * n + n;\n                if (endex &gt;= list.size()) {\n                    endex = list.size();\n                }\n                ls = new ArrayList&lt;&gt;(list.subList(i * n, endex));\n            }\n            if (ls.size() != 0) {\n                data.add(ls);\n            }\n        }\n        return data;\n    }\n\n}\n</code></pre><p>切分的文件效果如下：</p><p><img src="https://static001.geekbang.org/resource/image/56/fe/5631878728d5a595270be192dc16a2fe.png?wh=481x149" alt="图片"></p><p>JobHandler 中的实现可以参考下面这种写法：</p><pre><code class="language-java">\t/**\n     * 2、分片广播任务\n     */\n    @XxlJob("shardingJobHandler")\n    public void multiMachineMultiTasks() throws Exception {\n        String param = XxlJobHelper.getJobParam();\n        if (StringUtils.isBlank(param)) {\n            XxlJobHelper.log("任务参数为空");\n            XxlJobHelper.handleFail();\n            return;\n        }\n\n        //读取文件\n        FileReader fileReader = new FileReader(param);\n        //总分片数\n        String shardTotal = countTotal(total, fileReader);\n        //分片参数\n        int shardIndex = XxlJobHelper.getShardIndex();\n\n        XxlJobHelper.log("分片参数：当前分片序号 = {}, 总分片数 = {}", shardIndex, shardTotal);\n\n        LinkedHashSet&lt;String&gt; total = new LinkedHashSet&lt;&gt;();\n\n        //放入 list 中进行分组使用\n        List&lt;String&gt; list = new ArrayList&lt;&gt;();\n        //组装分割文件\n        for (String str : total) {\n            list.add(str);\n        }\n        saveFile(n, result, list);\n\n        // 业务逻辑\n        for (int i = 0; i &lt; shardTotal; i++) {\n            if (i == shardIndex) {\n                XxlJobHelper.log("第 {} 片, 命中分片开始处理", i);\n                //分片流量回放\n                process(shardIndex, shardTotal);\n            } else {\n                XxlJobHelper.log("第 {} 片, 忽略", i);\n            }\n        }\n\n    }\n</code></pre><p>新建任务的时候，我们选择分片任务，填上对应的 JobHandler 即可。</p><p><img src="https://static001.geekbang.org/resource/image/e9/37/e9ec293b698424a858d85bc2cae1fa37.png?wh=1500x1273" alt="图片"></p><p>最后需要说明一下，分片的数据量不一定是完全均等的，上面的取模只是一个例子、一个思路。我们也可以把 0、1、2 替换成其他条件去从所有数据中获取部分数据。比如分片序号是 0 的压力机回放网关 1 的流量文件，分片序号是 1 的压力机回放网关 2 的流量文件，分片序号是 2 的压力机我回放网关 3 的流量文件，具体怎么分我们可以自由选择。</p><h2>通用 RESTful API 支持</h2><p>我们知道，线上压测是一项对风险管控要求更高的活动，压测平台需要有更及时的异常预警机制，也就是说，必须要有压测实时监控和异常熔断能力。这就要求我们的调度任务能实时对接第三方的监控系统。</p><p>而XXL-JOB 目标就是一种跨平台、跨语言的任务调度规范和协议。针对第三方监控系统，比如Prometheus，我们可以借助 XXL-JOB 的标准 RESTful API 方便地实现任务回调支持。</p><p>标准 RESTful API 主要包括两部分：</p><ul>\n<li>调度中心 RESTful API：调度中心提供给执行器使用的 API；不局限于官方执行器使用，第三方可使用该 API 来实现执行器。主要包括：执行器注册、任务结果回调等API；</li>\n<li>执行器 RESTful API：执行器提供给调度中心使用的 API；官方执行器默认已实现，第三方执行器需要实现并对接提供给调度中心，任务触发、任务终止、任务日志查询等API。</li>\n</ul><p>除此之外，如果我们有需要通过 API 操作调度中心，可以个性化扩展 “调度中心 RESTful API” 并使用。</p><h2>总结</h2><p>流量平台是一个全链路压测项目的引擎。但对于一个大企业来说，分布式的流量平台是一个必要的条件。</p><p>在全链路分布式流量平台改造中，我强调了几个特性：高可靠、高性能、低成本、高效率。</p><p>我们在做技术改造的时候，一定要牢记下面几项关键原则：</p><ul>\n<li>基于开源，拥抱开源，尽量使用成熟的第三方框架，不重复造轮子；</li>\n<li>技术栈和组件要适合研发团队技术能力及主流技术方向；</li>\n<li>KISS 原则(Keep It Simple and Stupid)；</li>\n<li>尽量考虑 ROI（投入产出比）；</li>\n<li>不断演进迎合公司业务发展。大公司方案不一定是适合的，要避免为技术而技术；</li>\n<li>任何方案不必追求大而全，可以在实践中不断完善和演进；</li>\n<li>需求导向，借力开源，科学选型，快速集成，重视扩展，演进改善。</li>\n</ul><p>刚才，我还把一个分布式流量平台的需求以及需要改造到什么地步，都给你梳理了一遍。希望能给你一些借鉴。</p><h2>思考题</h2><p>学完这节课，请你思考两个问题：</p><ol>\n<li>你有没有接触过分布式流量平台的改造工作，请谈谈你的心得。</li>\n<li>关于分布式调度框架，你有没有接触过其它的项目？</li>\n</ol><p>欢迎你在留言区与我交流讨论。当然了，你也可以把这节课分享给你身边的朋友，他们的一些想法或许会让你有更大的收获。我们下节课见！</p>',
        article_title: "24 | 压测平台：如何改造分布式调度平台？",
      },
      {
        title: "25 | 环境搭建：我们的系统是怎么搭建起来的？",
        id: 467606,
        content:
          '<p>你好，我是高楼。</p><p>到现在，我们的专栏已经更新了20多讲了，在前面的课程中，你应该也已经看到了，为了真正地把全链路压测拉到地面上来，同时也让你更好地理解我的全链路压测理念，我们专门搭建了一个完整的系统，这个专栏所有的内容都是基于这个系统展开的。这节课，我们就来看看，这个系统是怎么搭建起来的。</p><p>因为整个系统相对比较复杂，有很多需要考虑、部署的细节，所以这节课的内容会比较长。下面这张图是这节课的目录，你可以先整体了解一下这节课的结构，然后对应这张目录图来学习具体的搭建步骤，以免迷失方向。</p><p><img src="https://static001.geekbang.org/resource/image/06/3c/06b108yy64a35bd74469c86b1694893c.jpg?wh=1920x943" alt="图片"></p><p>我会依照云计算的架构特点，自下向上从云虚拟机到微服务应用，介绍整个搭建的过程。</p><h2>一. 云虚拟机</h2><p>云虚拟机负责管理虚拟机的生命周期，包括创建、修改、备份、启停、销毁等。云虚拟机主要是面向资源分配，虚拟机创建出来了云产商就基本没有责任了。至于服务高可用、自动伸缩、监控等这类的功能完全由应用方来处理，云平台不提供支持。云虚拟机适合传统的部署模式，对应用而言和物理机时代没有区别。</p><h3>1. 主机规划</h3><p>在这个系统中，我们主要用到了8 台云虚拟机，下面是具体的硬件配置：</p><p><img src="https://static001.geekbang.org/resource/image/4a/9e/4a0e230dfbd54e88877868f41924369e.png?wh=1103x570" alt="图片"></p><p>我们可以看到，当前云主机总共使用在应用中的资源是：38 C的 CPU 资源，88 G的内存资源。NFS 网络存储服务器不会用在应用中。</p><!-- [[[read_end]]] --><p>关于云虚拟机类型，云产商一般会提供这几个类型：</p><ul>\n<li>通用均衡型（通常比例1:4）；</li>\n<li>计算密集型（通常比例1:2）；</li>\n<li>内存优化型（通常比例1:8以上）；</li>\n<li>图形计算型（带有GPU能力）。</li>\n</ul><p>我们这里选择的云主机类型主要为计算密集型，如下图。</p><p><img src="https://static001.geekbang.org/resource/image/f8/dc/f802782dee88466d9c60ed92d526b3dc.png?wh=1634x719" alt="图片"></p><p>在存储方面，云产商一般会提供高效云盘和SSD两种类型，这里我们选择的是SSD：</p><p><img src="https://static001.geekbang.org/resource/image/63/16/6395f495c81ac7ee390d7677a20f7616.png?wh=1008x247" alt="图片"></p><p>你可能会问，这么一套搞下来，成本要多少呢？我算了一下，我们这个项目，所有云主机加在一起一年大概需要六万左右的费用，这其中还包括了云硬盘、云网络等杂七杂八的费用。</p><p>我们接着来看下云主机规划。这里我把云主机类型按照节点差异分为两类：</p><ul>\n<li>普通节点：</li>\n</ul><p>普通节点主要供非被测系统使用，比如压力机、管理平台等。我们可以采用 Docker、二进制等方式来部署。</p><ul>\n<li>Kubernetes节点：</li>\n</ul><p>Kubernetes节点用于部署项目的应用服务，包括 mall-admin、mall-portal、mall-gateway、mall-member、mall-cart 等。另外 Zipkin、Nacos 等基础组件也都采用 Kubernetes的方式来部署。</p><p>具体的节点规划，你可以参考这张表：</p><p><img src="https://static001.geekbang.org/resource/image/a0/7c/a09c92479c27f915f565ca6f4879757c.jpg?wh=1920x1080" alt="图片"></p><p>在这里，我们规划了一个Kubernetes控制节点。如果你计划搭建高可用 Master 集群，至少需要规划三个Kubernetes控制节点。至于Kubernetes计算节点，结合前面的节点规划，我们在这里配置 7 个 worker 节点，其他的节点会根据需求灵活扩展。</p><p>这里我还想提醒一下，如果你所在的项目需要购买云主机的话，最好提早规划。因为我们之前购买的Kubernetes集群云主机都是包年包月机型，后来想省钱换成按量付费类型（支持停机后不收费功能）的主机，结果发现网络不互通。这可都是血淋淋的教训。</p><h3>2. 操作系统搭建</h3><p>下面，我们再来看下操作系统的搭建。</p><p>目前，行业内主流的基于 x86 架构的 Linux 系统，无非是 CentOS 和 Ubuntu 两种。我们选择了 CentOS 系列来搭建 Linux 系统，主要是考虑到了系统的稳定性。CentOS 来自 RedHat 商业版本的重新编译，它在稳定性、系统优化以及兼容性方面，具有比较完善的测试和发版流程。</p><p>在 CentOS 7 之后的版本中，CentOS 的内核换成了 Linux 3.x，因此，我们这个课程的分析都是基于 Linux 3.x 这个内核版本展开的。</p><p>而大部分的云产商一般会提供下面两种类型的操作系统镜像：</p><ul>\n<li>流行成熟的Linux发行版</li>\n<li>云厂商自有的Linux版本（更适配该产商的硬件）</li>\n</ul><p>因为我们课程的技术栈不想与云产商绑定，所以我们给每台服务器都安装了流行成熟的 CentOS  7.8 的操作系统。云主机安装操作系统非常简单，创建实例的时候选择对应的系统镜像就可以了：</p><p><img src="https://static001.geekbang.org/resource/image/fd/yy/fd93bbd0392993229250405d758yy6yy.png?wh=1005x124" alt="图片"></p><h2>二. Kubernetes 集群</h2><h3>1. 计算资源</h3><p>关于集群计算资源，你可以参考这张表：</p><p><img src="https://static001.geekbang.org/resource/image/56/35/56c111c74bdfb2d79ea2ce72dd36f835.jpg?wh=1920x865" alt="图片"></p><p>我们在做计算资源规划的时候，通常需要考虑不同的应用场景：</p><ul>\n<li>\n<p>传统虚拟化技术的 I/O 损耗较大，对于 I/O 密集型应用，性能敏感的应用，裸金属服务器相比云虚拟机有更好的性能表现；</p>\n</li>\n<li>\n<p>在裸金属服务器上部署应用，有更少的额外资源开销（如虚拟化管理、虚拟机操作系统等），并且可以有更高的部署密度，这可以降低基础设施成本；</p>\n</li>\n<li>\n<p>在裸金属服务器上可以更加灵活地选择网络、存储等设备和软件应用生态。<br>\n如果从实际生产环境考虑，我给你的建议是：</p>\n</li>\n<li>\n<p>对性能极其敏感的应用，如高性能计算，裸金属服务器是较好的选择；</p>\n</li>\n<li>\n<p>云主机支持热迁移，可以有效降低运维成本；</p>\n</li>\n<li>\n<p>在工作实践中，可以为 Kubernetes 集群划分静态资源池和弹性资源池，我们就是这样做的。通常而言，静态资源池可以根据需要选择裸金属服务器或者云主机实例；弹性资源池则可以根据应用负载，使用合适规格的云主机实例来优化成本，避免资源浪费，同时提升弹性供给保障。</p>\n</li>\n</ul><p>由于我们这个系统只是课程的示例项目，为了尽可能压榨服务器资源，节省服务器成本，我们选择了云虚拟机的方案。</p><h3>2. 集群搭建</h3><p>关于集群搭建，我们的节点规划如下：</p><pre><code class="language-bash">[root@s5 k8s]# kubectl get nodes -o wide\nNAME&nbsp; &nbsp;STATUS&nbsp; &nbsp;ROLES&nbsp; &nbsp; AGE&nbsp; &nbsp; VERSION&nbsp; &nbsp;INTERNAL-IP&nbsp; &nbsp; &nbsp; EXTERNAL-IP&nbsp; &nbsp;OS-IMAGE&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; KERNEL-VERSION&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; CONTAINER-RUNTIME\ns10&nbsp; &nbsp; Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp;5d1h&nbsp; &nbsp;v1.19.5&nbsp; &nbsp;172.31.184.227&nbsp; &nbsp;&lt;none&gt;&nbsp; &nbsp; &nbsp; &nbsp; CentOS Linux 7 (Core)&nbsp; &nbsp;3.10.0-1127.19.1.el7.x86_64&nbsp; &nbsp;docker://19.3.11\ns11&nbsp; &nbsp; Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp;5d1h&nbsp; &nbsp;v1.19.5&nbsp; &nbsp;172.31.184.230&nbsp; &nbsp;&lt;none&gt;&nbsp; &nbsp; &nbsp; &nbsp; CentOS Linux 7 (Core)&nbsp; &nbsp;3.10.0-1127.19.1.el7.x86_64&nbsp; &nbsp;docker://19.3.11\ns12&nbsp; &nbsp; Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp;5d1h&nbsp; &nbsp;v1.19.5&nbsp; &nbsp;172.31.184.233&nbsp; &nbsp;&lt;none&gt;&nbsp; &nbsp; &nbsp; &nbsp; CentOS Linux 7 (Core)&nbsp; &nbsp;3.10.0-1127.19.1.el7.x86_64&nbsp; &nbsp;docker://19.3.11\ns5&nbsp; &nbsp; &nbsp;Ready&nbsp; &nbsp; master&nbsp; &nbsp;5d2h&nbsp; &nbsp;v1.19.5&nbsp; &nbsp;172.31.184.225&nbsp; &nbsp;&lt;none&gt;&nbsp; &nbsp; &nbsp; &nbsp; CentOS Linux 7 (Core)&nbsp; &nbsp;3.10.0-1160.31.1.el7.x86_64&nbsp; &nbsp;docker://19.3.11\ns6&nbsp; &nbsp; &nbsp;Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp;5d1h&nbsp; &nbsp;v1.19.5&nbsp; &nbsp;172.31.184.226&nbsp; &nbsp;&lt;none&gt;&nbsp; &nbsp; &nbsp; &nbsp; CentOS Linux 7 (Core)&nbsp; &nbsp;3.10.0-1127.19.1.el7.x86_64&nbsp; &nbsp;docker://19.3.11\ns7&nbsp; &nbsp; &nbsp;Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp;5d1h&nbsp; &nbsp;v1.19.5&nbsp; &nbsp;172.31.184.224&nbsp; &nbsp;&lt;none&gt;&nbsp; &nbsp; &nbsp; &nbsp; CentOS Linux 7 (Core)&nbsp; &nbsp;3.10.0-1127.18.2.el7.x86_64&nbsp; &nbsp;docker://19.3.11\ns8&nbsp; &nbsp; &nbsp;Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp;5d1h&nbsp; &nbsp;v1.19.5&nbsp; &nbsp;172.31.184.229&nbsp; &nbsp;&lt;none&gt;&nbsp; &nbsp; &nbsp; &nbsp; CentOS Linux 7 (Core)&nbsp; &nbsp;3.10.0-1127.19.1.el7.x86_64&nbsp; &nbsp;docker://19.3.11\ns9&nbsp; &nbsp; &nbsp;Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp;5d1h&nbsp; &nbsp;v1.19.5&nbsp; &nbsp;172.31.184.228&nbsp; &nbsp;&lt;none&gt;&nbsp; &nbsp; &nbsp; &nbsp; CentOS Linux 7 (Core)&nbsp; &nbsp;3.10.0-1127.19.1.el7.x86_64&nbsp; &nbsp;docker://19.3.11\n</code></pre><p>集群搭建的具体步骤，你可以参考下面这两个文档进行部署：</p><ul>\n<li>单 Master 集群：<a href="https://mp.weixin.qq.com/s/eQOMt3dz1P2JwezDaoZGzg">使用 kubeadm 安装单master kubernetes 集群（脚本版）</a></li>\n<li>高可用方案： <a href="https://mp.weixin.qq.com/s/bdq4GySQWjcIjJmn0ZD80g">Kubernetes 高可用集群落地二三事</a></li>\n</ul><p>安装的负载均衡组件如下：</p><p><img src="https://static001.geekbang.org/resource/image/37/16/37b9dfc3054964f73df1076121ffd016.jpg?wh=1920x732" alt="图片"></p><p>关于Traefik 搭建的具体步骤，你可以按照下面这个文档进行部署：</p><ul>\n<li><a href="https://mp.weixin.qq.com/s/YGfndOfsbIsSs3ahmOgWCw">Kubernetes 集群部署 Ingress Traefik</a></li>\n</ul><p>如果你没有Kubernetes的使用基础，那么我建议学习一下这几篇入门文章：</p><ul>\n<li><a href="https://mp.weixin.qq.com/s/X2Z_a_eYq12O6yTQmRRxfw">Kubernetes 集群基本概念</a></li>\n<li><a href="https://mp.weixin.qq.com/s/dSbCK_ms4YMoEvqp0afNZg">k8s入门篇-Kubernetes的基本概念和术语</a></li>\n<li><a href="https://mp.weixin.qq.com/s/EPdOEwFaoc-hI27C0v3urA">k8s命令篇-Kubernetes工作实用命令集结号</a></li>\n<li><a href="https://mp.weixin.qq.com/s/xYjR5_WZNhny_0RdjM1tpA">Kubernetes 集群常用操作总结</a></li>\n</ul><h3>3. 插件安装</h3><p>我们需要安装的插件主要有三种：网络插件、存储插件和组件。</p><p>对于网络插件，我们选用的是目前主流的网络插件 Flannel。如果你的系统有其它选型需求，那你可以参考下面这篇文章，这里我就不做赘述了。</p><ul>\n<li><a href="https://mp.weixin.qq.com/s/H4ncgG0AqsmpPSg3ICBP3g">Kubernetes 网络插件（CNI）超过 10Gbit/s 的基准测试结果</a></li>\n</ul><p>如果你在阿里云上安装 k8s，建议使用 Flannel，因为有多个案例表明 Calico 与阿里云存在兼容性问题。</p><p>安装 Flannel 插件的具体步骤，你可以参考下文的第11章。</p><ul>\n<li><a href="https://blog.csdn.net/zuozewei/article/details/108182174">使用 kubeadm 安装单master kubernetes 集群（手动版）</a></li>\n</ul><p>关于存储插件，我们选用的是 NFS 网络存储。因为 NFS 相对简单，上手快，我们只需要部署一个NFS服务，再由Kubernetes提供一个自动配置卷程序，然后通过 StoageClass 动态配置 Kubernetes 持久就可以了。而且在性能上，NFS 也能满足我们这个系统的需求。</p><p>只不过，NFS 并不是高可用方案。如果你是在生产环境中使用，可以考虑把 Ceph 作为存储选型方案。Ceph 是一个统一的分布式存储系统，也是高可用存储方案，并且可以提供比较好的性能、可靠性和可扩展性。但是，Ceph 部署起来更复杂些，同时维护也比 NFS 困难。</p><p>我把 NFS 和 Ceph 的详细安装步骤放在这里，你如果有需要，可以参考学习。</p><ul>\n<li>NFS： <a href="https://mp.weixin.qq.com/s/-jLNloENj7za4N91GcSGGQ">Kubernetes 如何安装 NFS-Subdir-External-Provisioner存储插件？</a></li>\n<li>Ceph： <a href="https://mp.weixin.qq.com/s/tww0dPNeFNR_WaEWUh97ZQ">Kubernetes 集群分布式存储插件 Rook Ceph部署</a></li>\n</ul><p>另外，不要忘了，NFS 配置中还需要这两个组件：</p><p><img src="https://static001.geekbang.org/resource/image/dc/38/dca27690dda1fbca0d36a72424de8d38.jpg?wh=1920x648" alt="图片"></p><p>NFS-Subdir-External-Provisioner 是对 <a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">nfs-client-provisioner</a> 的扩展，但是 nfs-client-provisioner 已经不再提供更新，而且 nfs-client-provisioner 的 GitHub 仓库已经迁移到<a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner"> NFS-Subdir-External-Provisioner</a> 的仓库了。</p><h3>4. Kubernetes管理平台</h3><p>安装组件：</p><p><img src="https://static001.geekbang.org/resource/image/1a/83/1a73728efd188e44137240a9aec44d83.jpg?wh=1920x523" alt="图片"></p><p>Kuboard 采用可视化UI的方式来管理应用和组件，降低了Kubernetes集群的使用门槛。下面我们看看怎么部署 Kuboard 组件。</p><p>第一步，k8s 集群执行资源文件：</p><pre><code class="language-shell">kubectl apply -f https://kuboard.cn/install-script/kuboard.yaml\nkubectl apply -f https://addons.kuboard.cn/metrics-server/0.3.7/metrics-server.yaml\n</code></pre><pre><code class="language-bash">[root@s5 ~]# kubectl apply -f https://kuboard.cn/install-script/kuboard.yaml\ndeployment.apps/kuboard created\nservice/kuboard created\nserviceaccount/kuboard-user created\nclusterrolebinding.rbac.authorization.k8s.io/kuboard-user created\nserviceaccount/kuboard-viewer created\nclusterrolebinding.rbac.authorization.k8s.io/kuboard-viewer created\n[root@s5 ~]#&nbsp;\n[root@s5 ~]# kubectl apply -f https://addons.kuboard.cn/metrics-server/0.3.7/metrics-server.yaml\nclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\nclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\nrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\nclusterrole.rbac.authorization.k8s.io/system:metrics-server created\nclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\napiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\nserviceaccount/metrics-server created\ndeployment.apps/metrics-server created\nservice/metrics-server created\n[root@s5 ~]#&nbsp;\n</code></pre><p>第二步，把 Kuboard 安装好后，我们看一下 Kuboard 的运行状态：</p><pre><code class="language-shell">kubectl get pods -l k8s.kuboard.cn/name=kuboard -n kube-system\n</code></pre><p>输出结果：</p><pre><code class="language-shell">[root@s5 ~]# kubectl get pods -l k8s.kuboard.cn/name=kuboard -n kube-system\nNAME&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;READY&nbsp; &nbsp;STATUS&nbsp; &nbsp; RESTARTS&nbsp; &nbsp;AGE\nkuboard-74c645f5df-bsj4k&nbsp; &nbsp;1/1&nbsp; &nbsp; &nbsp;Running&nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 98s\n[root@s5 ~]#&nbsp;\n</code></pre><p>这个结果表明 kuboard 已经部署成功了。<br>\n接着，我们获取管理员 Token 。这一步是为了登录访问 Kuboard，检查组件是否成功运行。</p><pre><code class="language-shell"># 可在第一个 Master 节点上执行此命令\necho $(kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-user | awk \'{print $1}\') -o go-template=\'{{.data.token}}\' | base64 -d)\n</code></pre><pre><code class="language-bash">[root@s5 k8s]# echo $(kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-user | awk \'{print $1}\') -o go-template=\'{{.data.token}}\' | base64 -d)\neyJhbGciOiJSUzI1NiIsImtpZCI6IldTYlJYVXIzbWZNUnBSczF1ZFc5cmRQbEp2UlFtbXREZDVCSEtzd09kU2MifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJvYXJkLXVzZXItdG9rZW4tZGNwMmYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoia3Vib2FyZC11c2VyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiODY2MWU1NzAtYWMyNC00YzA0LTgxNmYtMTM3YWMwOWU0MTcxIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmt1Ym9hcmQtdXNlciJ9.lHhr261HMdqGpMWhGVt9mOty2QpWAcuKaGYZ2FwlJPof4iddGSi6O5_SgZHi8wj4FrOiH3xwUMGGqiIcxhp6VuTFch7tUeBQ3x40dvt3t1u6kogZBMz-k2K39fnBtvcFJV9hghIXoxMsH3E09wjKrJmq0bXUfPicdCMbbwpRNXt97pqGIPBrhNtqX_-agmLSlaiqG3rTZIwGAgsBmcJVHeGhgme2PAMwuiSXUoi0W22BeH-c91_rLJ4LrB9svrhaWf_dltQ-mLT4lfnr_Imk-Dvo9Zhmm8Y9_OOe9VkWRJVtJpnVYWs9WKjMGLeYNL_h8jgNN7cq-0vxddAT31rdYQ\n</code></pre><p>通过检查部署我们了解到，Kuboard Service 使用了 NodePort 的方式暴露服务，NodePort 为 32567。因此，我们可以按照下面这个方式访问 Kuboard：</p><pre><code class="language-bash">http://任意一个Worker节点的IP地址:32567/\n</code></pre><p>然后，在登录界面中输入管理员 Token，这样就可以进入到 Kuboard 集群的概览页了。</p><p><img src="https://static001.geekbang.org/resource/image/3b/5b/3b6e5705851ce8d1c4f25169e1bb765b.png?wh=1915x997" alt="图片"></p><p>注意，如果你使用的是阿里云、腾讯云等云服务，那么你可以在对应的安全组设置里，开放 worker 节点 32567 端口的入站访问，你也可以修改 Kuboard.yaml 文件，使用自己定义的 NodePort 端口号。</p><h2></h2><h2>三. 依赖组件</h2><h3>1. 部署清单</h3><p><img src="https://static001.geekbang.org/resource/image/f9/04/f962c721394ceba314207e729b209804.jpg?wh=1920x1080" alt="图片"></p><h3>2. 安装部署</h3><p>对于上述依赖组件的安装部署，我整理了对应的教程放在这里，如果有兴趣你可以尝试一下。</p><p>MySQL 的二进制安装方式，在网上的教程多如牛毛，我在这里就不介绍了，如果你想知道怎么在Kubernetes下部署 MySQL，你可以参考这个链接中的详细步骤：</p><ul>\n<li><a href="https://mp.weixin.qq.com/s/-DEgzyqZKOApjynsrdLPmw">Kubernetes 集群部署 MySQL+ mysqld_exporter (单节点)</a></li>\n<li><a href="https://mp.weixin.qq.com/s/4YSx4a2oqjrp1f6wNQgR-g">如何在 Kubernetes 集群中搭建一个复杂的 MySQL 数据库</a></li>\n</ul><p>Elasticsearch 集群的部署可以参考：</p><ul>\n<li><a href="https://mp.weixin.qq.com/s/VAlbWoSf_16-Cf5nIwEShg">Kubernetes Helm3 部署 Elasticsearch &amp; Kibana 7 集群</a></li>\n</ul><p>Weave Scope的部署可以参考：</p><ul>\n<li><a href="https://mp.weixin.qq.com/s/QEB2UkJC14BXcZKfxOsrxA">Kubernetes 集群可视化监控之 Weave Scope 入门​</a></li>\n</ul><p>镜像仓库 Harbor 的部署可以参考：</p><ul>\n<li><a href="https://mp.weixin.qq.com/s/_0hEWz7TUvYGASg9I3az4g">Kubernetes 集群仓库 harbor Helm3 部署</a></li>\n</ul><p>Nacos 的部署可以参考：</p><ul>\n<li>Docker 单机模式： <a href="https://nacos.io/zh-cn/docs/quick-start-docker.html">Nacos Docker 快速开始</a></li>\n<li>Kubernetes：<a href="https://mp.weixin.qq.com/s/gA-MtApZXTSF1TtR9Jj4OA">Kubernetes 集群部署nacos 1.4（集群版）</a></li>\n</ul><p>Redis、RabbitMQ、MongoDB 单机部署的部署可以参考：</p><ul>\n<li><a href="https://mp.weixin.qq.com/s/SFCeHlChDQY3zgulnk1D3g">Kubernetes 集群部署 Redis + redis_exporter (单点)</a></li>\n<li><a href="https://mp.weixin.qq.com/s/QGCvYvWz60o_qBX6YbZCFw">Kubernetes 集群部署 RabbitMQ + exporter (单节点)</a></li>\n<li><a href="https://mp.weixin.qq.com/s/lCeWAC4tbh_624f7oDUs2g">Kubernetes 集群部署 MongoDB+exporter (单节点)</a></li>\n</ul><h2>四. 监控组件</h2><h3>1. 全局监控</h3><p>不知道你还记不记得我们这个系统的架构：</p><p><img src="https://static001.geekbang.org/resource/image/2f/4b/2f032901efyy2d9f9e3df8f123e8534b.png?wh=1673x926" alt="图片"></p><p>根据这个系统的架构，我们选择的工具需要监控到这几个层面：</p><ul>\n<li>第一层，云虚拟机；</li>\n<li>第二层，Kubernetes套件；</li>\n<li>第三层，各种应用所需要的技术组件。</li>\n</ul><p>其实，有了上面的系统架构，监控设计大体的轮廓就有了。对于我们这个课程所用的系统，全局监控如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/71/b5/71e39cd615bb0ac4b322fe638e1918b5.jpg?wh=1960x826" alt=""><br>\n从上图来看，我们使用 Prometheus/Grafana/Spring Boot Admin/Zipkin/Weave Scope/ELFK/Sentinel 就可以实现具有全局视角的第一层监控。对于工具中没有覆盖的第一层计数器，我们只能在执行场景时再执行命令来补充了。</p><h3>2. 部署清单</h3><p><img src="https://static001.geekbang.org/resource/image/97/yy/97d7de1b543c21019d39ae610ca278yy.jpg?wh=1788x997" alt="图片"></p><h3>3. 安装部署</h3><p>对于上面这些监控工具的部署，我也把相应的安装教程放在这里供你参考学习。</p><p>Kubernetes集群资源监控的部署：</p><ul>\n<li><a href="https://mp.weixin.qq.com/s/AOl_z0zMuPzRlf36J6OLMg">Kubernetes 集群监控 kube-prometheus 部署</a></li>\n<li><a href="https://mp.weixin.qq.com/s/T1Yhbw2efw-wjKu1PZyudg">Kubernetes 集群监控 controller-manager &amp; scheduler 组件</a></li>\n<li><a href="https://mp.weixin.qq.com/s/chvukvjmCDvrYqYapRCEkA">Kubernetes 集群监控 ETCD 组件</a></li>\n</ul><p>日志聚合部署的部署：</p><ul>\n<li><a href="https://mp.weixin.qq.com/s/7shBCfpKRBBQDsutSpClhQ">Kubernetes 集群日志监控 EFK 安装</a></li>\n</ul><p>依赖组件的部署：</p><ul>\n<li><a href="https://mp.weixin.qq.com/s/yuAU-qWeTr4svaSI497sbQ">Kubernetes 集群监控 kube-prometheus 自动发现</a></li>\n</ul><p>APM 链路跟踪的部署：</p><ul>\n<li><a href="https://mp.weixin.qq.com/s/xTr3IBBUggXdCGL047RF0Q">Kubnernetes 集群部署 Zipkin+Kafka+ElasticSearch 实现链路追踪</a></li>\n</ul><h2>五. 微服务</h2><h3>1. 项目介绍</h3><p>在搭建这个课程所用的系统时，我采用了微服务的架构，这也是当前主流的技术架构。</p><p>如果你有兴趣了解详细的项目介绍，可以参考这篇文章：<a href="https://mp.weixin.qq.com/s/a8nDBbkuvSjreaCxuM2PuQ">《微服务电商项目技术全解析》</a>。这里面主要介绍了该项目的一些预备知识、系统结构、主要技术栈以及核心组件。此外，还有相关的运行效果截图。</p><h3>2. 拉取源代码</h3><p>我们把 Git clone 项目源代码下载到本地，来部署我们的被测系统：</p><pre><code class="language-shell">git clone https://github.com/xncssj/7d-mall-microservice.git\n</code></pre><h3>3. 修改 Nacos 配置</h3><p>我们先将项目 config 目录下的配置包导入到 Nacos 中，然后根据自己的实际需要修改相关配置。</p><p>接着，我们将配置信息导入到 Nacos 中后，会显示这样的信息：</p><p><img src="https://static001.geekbang.org/resource/image/92/11/92b7f9bf9ed9f7786e7b99ffece13111.png?wh=1907x953" alt="图片"></p><p>请注意，我们修改的配置文件主要是每个单体服务下的 application-prod.yml 和 bootstrap-prod.yml。因为这两个全局配置文件，都是服务容器内加载的配置文件。</p><h3>4. 镜像打包及推送</h3><p>我们使用 Java 语言的 IDE （推荐 IDEA ）打开项目工程。</p><p>首先，修改项目根目录下的 pom.xml 文件：</p><pre><code class="language-xml">&lt;properties&gt;\n    &lt;!--改为你自己的 Docker 服务远程访问地址--&gt;\n    &lt;docker.host&gt;http://172.16.106.237:2375&lt;/docker.host&gt;\n&lt;/properties&gt;\n</code></pre><p>在 IDEA 的右边 Maven 标签页，我们可以找到 root 工程下的 package 按钮，选中并执行：</p><p><img src="https://static001.geekbang.org/resource/image/e5/42/e5abee1a643b6cfab36620a9b5496842.png?wh=250x702" alt="图片"></p><p>然后，在编译的远程 Docker 主机上，我们修改所有服务的镜像标签名称。之后，再推送镜像到 Docker 仓库。</p><h3>5. 导入数据库</h3><p>这一步需要将项目 document/sql 目录下的 SQL 脚本导入到 MySQL 数据库中。</p><h3>6. 初始化依赖组件</h3><h4>6.1 RabbitMQ</h4><p>第一步，进入 RabbitMQ 容器并开启管理功能：</p><pre><code class="language-shell">#登录容器的时候需要注意到容器支持的 shell 是什么。\nkubectl exec -it &lt;pod-name&gt; -n &lt;ns-name&gt; bash\nkubectl exec -it &lt;pod-name&gt; -n &lt;ns-name&gt; sh\n\nroot@cloud-rabbitmq-5b49d784c-gbr8m:/# rabbitmq-plugins enable rabbitmq_management\nEnabling plugins on node rabbit@cloud-rabbitmq-5b49d784c-gbr8m:\nrabbitmq_management\nThe following plugins have been configured:\n  rabbitmq_management\n  rabbitmq_management_agent\n  rabbitmq_web_dispatch\nApplying plugin configuration to rabbit@cloud-rabbitmq-5b49d784c-gbr8m...\nPlugin configuration unchanged.\n</code></pre><p>因为 RabbitMQ Service 使用 NodePort 的方式暴露控制台地址，比如 NodePort 为 15672。所以，第二步，我们访问地址 <a href="http://xn--IP-rm4eu79a9miiio:15672/">http://计算节点IP:15672/</a> 地址，查看是否安装成功：</p><p><img src="https://static001.geekbang.org/resource/image/85/cd/85f0c673661e98de473841a238a8a7cd.png?wh=440x171" alt="图片"></p><p>第三步，输入账号密码并登录guest/guest。</p><p>第四步，创建帐号并设置其角色为管理员 mall/mall。</p><p><img src="https://static001.geekbang.org/resource/image/aa/27/aab6efeb759a35d60fe1eacc32a16727.png?wh=1056x652" alt="图片"></p><p>第五步，创建一个新的虚拟 host 为 /mall。</p><p><img src="https://static001.geekbang.org/resource/image/e2/76/e27be6f50d9674c2eecdaf72ba7cf876.png?wh=1051x502" alt="图片"></p><p>第六步，点击 mall 用户进入用户配置页面，给 mall 用户配置该虚拟 host 的权限。</p><p><img src="https://static001.geekbang.org/resource/image/8c/ed/8c2d86835779258a786be6f2a72937ed.png?wh=675x753" alt="图片"></p><p>到这里，RabbitMQ 的初始化就完成了。</p><h4>6.2 Elasticsearch</h4><p>安装中文分词器 IKAnalyzer，并重新启动：</p><pre><code class="language-shell">#此命令需要在容器中运行\nelasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.6.2/elasticsearch-analysis-ik-7.6.2.zip\n</code></pre><h3>7. 使用 yaml 资源文件部署应用</h3><p>将项目 document/k8s 目录下的 yaml 资源文件中的 Dokcer 镜像，修改为自己的 Tag 并上传到 k8s 集群中执行：</p><pre><code class="language-shell">kubectl apply -f k8s/\n</code></pre><h2>六. 运行效果展示</h2><p>前面的环境搭建完成了，最后我们一起来看下运行效果。</p><h3>1. Kubernetes 集群</h3><p>Kubernetes 集群：</p><pre><code class="language-shell">&nbsp;~  kubectl get nodes\nNAME&nbsp; &nbsp;STATUS&nbsp; &nbsp;ROLES&nbsp; &nbsp; AGE&nbsp; &nbsp; VERSION\ns10&nbsp; &nbsp; Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp;5d8h&nbsp; &nbsp;v1.19.5\ns11&nbsp; &nbsp; Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp;5d8h&nbsp; &nbsp;v1.19.5\ns12&nbsp; &nbsp; Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp;5d8h&nbsp; &nbsp;v1.19.5\ns5&nbsp; &nbsp; &nbsp;Ready&nbsp; &nbsp; master&nbsp; &nbsp;5d8h&nbsp; &nbsp;v1.19.5\ns6&nbsp; &nbsp; &nbsp;Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp;5d8h&nbsp; &nbsp;v1.19.5\ns7&nbsp; &nbsp; &nbsp;Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp;5d8h&nbsp; &nbsp;v1.19.5\ns8&nbsp; &nbsp; &nbsp;Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp;5d8h&nbsp; &nbsp;v1.19.5\ns9&nbsp; &nbsp; &nbsp;Ready&nbsp; &nbsp; &lt;none&gt;&nbsp; &nbsp;5d8h&nbsp; &nbsp;v1.19.5\n&nbsp;~ \n</code></pre><p>微服务管理：</p><p><img src="https://static001.geekbang.org/resource/image/3a/a3/3a2721f42318ca30d970a0bcc809d9a3.png?wh=1917x996" alt="图片"></p><h3>2. 微服务</h3><p>部署架构图：</p><p><img src="https://static001.geekbang.org/resource/image/2f/4b/2f032901efyy2d9f9e3df8f123e8534b.png?wh=1673x926" alt="图片"></p><p>API 文档：</p><p><img src="https://static001.geekbang.org/resource/image/e4/fb/e43b2826b17305a2f759bdf3697564fb.png?wh=1917x1002" alt="图片"></p><p>调用链监控：</p><p><img src="https://static001.geekbang.org/resource/image/25/c8/2589ff807a90609f80b08209e1b85ac8.png?wh=1913x997" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/2c/af/2ce6b93d67eaa781e45772b7a245b8af.png?wh=1917x998" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/cd/44/cd94913bb15891d8811eec8ca70f2b44.png?wh=1486x602" alt="图片"></p><p>服务注册：</p><p><img src="https://static001.geekbang.org/resource/image/92/11/92b7f9bf9ed9f7786e7b99ffece13111.png?wh=1907x953" alt="图片"></p><p>服务监控:</p><p><img src="https://static001.geekbang.org/resource/image/19/1e/1943ef1fe1af579813a50623305d4d1e.png?wh=1900x965" alt="图片"></p><p>日志聚合：</p><p><img src="https://static001.geekbang.org/resource/image/dc/82/dc30e55e731729fefbc46b4270cd2382.png?wh=1918x966" alt="图片"></p><p>配置管理：</p><p><img src="https://static001.geekbang.org/resource/image/89/78/89c301c7d716d390d658fa9107266378.png?wh=1917x966" alt="图片"></p><p>系统保护：</p><p><img src="https://static001.geekbang.org/resource/image/f1/df/f1d46b00991361b7d880ce23cd74cfdf.jpg?wh=1920x969" alt="图片"></p><p>容器仓库：</p><p><img src="https://static001.geekbang.org/resource/image/81/4b/81ec549df56997408e96953453a48a4b.png?wh=1896x929" alt="图片"></p><h3>3. 资源监控</h3><p>Kubernetes集群资源监控：</p><p><img src="https://static001.geekbang.org/resource/image/cf/bd/cf6a0965ea9a71fa17ee954784bde3bd.png?wh=1913x995" alt="图片"></p><p>Linux 资源监控：</p><p><img src="https://static001.geekbang.org/resource/image/14/11/1491a3f54d2665da46f4988a1232d211.png?wh=1914x996" alt="图片"></p><p>MySQL 资源监控：</p><p><img src="https://static001.geekbang.org/resource/image/45/20/45e0b30e91f415ef260b30df56906120.png?wh=1853x917" alt="图片"></p><p>RabbitMQ 资源监控：</p><p><img src="https://static001.geekbang.org/resource/image/71/a2/71114d5422e1def11ca64f616c00efa2.png?wh=1861x963" alt="图片"></p><p>MongoDB 数据库资源监控：</p><p><img src="https://static001.geekbang.org/resource/image/15/27/152247bdabe28da0d42476ede1c66f27.png?wh=1857x966" alt="图片"></p><p>Kubernetes etcd 资源监控：</p><p><img src="https://static001.geekbang.org/resource/image/df/f4/df7b8d87598971c7575bce39e99e6af4.png?wh=1915x996" alt="图片"></p><p>Kubernetes API Server 资源监控：</p><p><img src="https://static001.geekbang.org/resource/image/3e/04/3e3e12ce0e1ca4f5d84379c3c9e15104.png?wh=1909x963" alt="图片"></p><p>Kubernetes 服务拓扑：</p><p><img src="https://static001.geekbang.org/resource/image/fb/be/fbd7534d614295ba3e3c33a3670d16be.png?wh=1919x969" alt="图片"></p><h2>总结</h2><p>这节课的内容包括了云环境的说明、技术组件的具体搭建过程、示例系统的搭建过程以及运行效果。经过上面所有的步骤，我们就把整个课程涉及的所有技术组件、示例系统完全搭建起来了。</p><p>而我之所以选择这样的技术栈，主要有三方面的考虑：</p><ol>\n<li><strong>核心优势</strong></li>\n</ol><ul>\n<li>任务调度：为集群系统中的任务提供调度服务，自动将服务按资源需求分配到资源限制的计算节点；</li>\n<li>资源隔离：为产品提供管控与服务节点隔离能力，保证研发应用和管控服务不产生相互的影响；</li>\n<li>高可用能力：自动监控服务运行，根据运行情况对失效的服务进行自动重启恢复；</li>\n<li>网络互联互通能力：提供统一的IP地址分配和网络互通能力；</li>\n<li>统一编排管理能力：结合 Gitlab 和 k8s ，对输出的产品进行统一的编排管理；</li>\n<li>公共产品组件可以为团队提供统一部署、验证、授权、调度和管控能力，为私有云服务提供基础性的支撑。</li>\n</ul><p><strong>2. 核心设施平台（ IaaS 云）</strong></p><ul>\n<li>提供计算、网络、存储等核心资源设备的虚拟化；</li>\n<li>支持不同操作系统，包括主流的 Win 和 Linux 系统；</li>\n<li>提供主要的三种服务：云主机、云网络、云硬盘；</li>\n<li>提供可视化 Web UI；</li>\n<li>提供 k8s 集群（容器云）规划、部署和运营；</li>\n<li>支持多种计算、存储和网络方案。</li>\n</ul><p><strong>3. 基础服务平台（ PaaS 云）</strong></p><ul>\n<li>提供数据存储：支持常见 NFS 、Ceph RBD、Local Volume 等；</li>\n<li>提供应用服务：支持自愈和自动伸缩、调度和发布、负载均衡等；</li>\n<li>提供运维管理：支持日志监控、资源监控、消息告警等。</li>\n</ul><p>我们这个系统采用的技术栈，是当前技术市场中流行的主流技术栈，这样的环境具有很高的借鉴价值。而且，从我们要表达的 RESAR 全链路压测和逻辑来说，也说明 RESAR 全链路压测理念是足以支撑当前的技术栈的。</p><p>有了主流技术栈的技术能力，再有我们的性能分析逻辑思维的落地，不能说天下无双，也是够你横霸一方，独挡一面的了。</p><p>这节课的内容非常多，希望你能够好好消化吸收。我在这里只是给你提供了我们项目的选择和理由，我们项目的选择可能不是最适合你的，如果你有其他想法，也欢迎在留言区和我交流讨论。另外在文稿的最后，我还给你汇总了你可能想要参考的文章链接，希望能让你有所收获。</p><h2>课后题</h2><p>学完这节课，请你思考两个问题：</p><ol>\n<li>你有没有搭建过云原生系统，谈谈你对云原生系统搭建的心得！</li>\n<li>引入 k8s 后，你觉得企业内的基础设施最大的变化是什么？</li>\n</ol><p>欢迎你在留言区与我交流讨论。当然了，你也可以把这节课分享给你身边的朋友，他们的一些想法或许会让你有更大的收获。我们下节课见！</p><h2>参考链接汇总</h2><ol>\n<li>Kubernetes 集群搭建：</li>\n</ol><ul>\n<li>单 Master 集群：<a href="https://mp.weixin.qq.com/s/eQOMt3dz1P2JwezDaoZGzg">使用 kubeadm 安装单master kubernetes 集群（脚本版）</a></li>\n<li>高可用方案： <a href="https://mp.weixin.qq.com/s/bdq4GySQWjcIjJmn0ZD80g">Kubernetes 高可用集群落地二三事</a></li>\n</ul><ol start="2">\n<li>Kubernetes的使用基础：</li>\n</ol><ul>\n<li><a href="https://mp.weixin.qq.com/s/X2Z_a_eYq12O6yTQmRRxfw">Kubernetes 集群基本概念</a></li>\n<li><a href="https://mp.weixin.qq.com/s/dSbCK_ms4YMoEvqp0afNZg">k8s入门篇-Kubernetes的基本概念和术语</a></li>\n<li><a href="https://mp.weixin.qq.com/s/EPdOEwFaoc-hI27C0v3urA">k8s命令篇-Kubernetes工作实用命令集结号</a></li>\n<li><a href="https://mp.weixin.qq.com/s/xYjR5_WZNhny_0RdjM1tpA">Kubernetes 集群常用操作总结</a></li>\n</ul><ol start="3">\n<li>\n<p>Kubernetes网络插件选型：<a href="https://mp.weixin.qq.com/s/H4ncgG0AqsmpPSg3ICBP3g">Kubernetes 网络插件（CNI）超过 10Gbit/s 的基准测试结果</a></p>\n</li>\n<li>\n<p>Kubernetes存储插件：</p>\n</li>\n</ol><ul>\n<li>NFS部署：  <a href="https://mp.weixin.qq.com/s/-jLNloENj7za4N91GcSGGQ">Kubernetes 如何安装 NFS-Subdir-External-Provisioner存储插件？</a></li>\n<li>Ceph部署： <a href="https://mp.weixin.qq.com/s/tww0dPNeFNR_WaEWUh97ZQ">Kubernetes 集群分布式存储插件 Rook Ceph部署</a></li>\n</ul><ol start="5">\n<li>Kubernetes下的MySQL部署：<a href="https://mp.weixin.qq.com/s/4YSx4a2oqjrp1f6wNQgR-g">如何在 Kubernetes 集群中搭建一个复杂的 MySQL 数据库</a></li>\n<li>Elasticsearch 集群的部署：<a href="https://mp.weixin.qq.com/s/VAlbWoSf_16-Cf5nIwEShg">Kubernetes Helm3 部署 Elasticsearch &amp; Kibana 7 集群</a></li>\n<li>镜像仓库 Harbor 的部署：<a href="https://mp.weixin.qq.com/s/_0hEWz7TUvYGASg9I3az4g">Kubernetes 集群仓库 harbor Helm3 部署</a></li>\n<li>Nacos 的部署：</li>\n</ol><ul>\n<li>Docker 单机模式： <a href="https://nacos.io/zh-cn/docs/quick-start-docker.html">Nacos Docker 快速开始</a></li>\n<li>Kubernetes：<a href="https://mp.weixin.qq.com/s/gA-MtApZXTSF1TtR9Jj4OA">Kubernetes 集群部署 nacos 1.4（集群版）</a></li>\n</ul><ol start="9">\n<li>Redis、RabbitMQ、MongoDB 单机部署的部署：</li>\n</ol><ul>\n<li><a href="https://mp.weixin.qq.com/s/SFCeHlChDQY3zgulnk1D3g">Kubernetes 集群部署 Redis + redis_exporter (单点)</a></li>\n<li><a href="https://mp.weixin.qq.com/s/QGCvYvWz60o_qBX6YbZCFw">Kubernetes 集群部署 RabbitMQ + exporter (单节点)</a></li>\n<li><a href="https://mp.weixin.qq.com/s/lCeWAC4tbh_624f7oDUs2g">Kubernetes 集群部署 MongoDB+exporter (单节点)</a></li>\n</ul><ol start="10">\n<li>Logstash 的部署：<a href="https://mp.weixin.qq.com/s/D7Xuw_8nsituhR33BpjU2w">整合ELK实现日志收集</a></li>\n<li>Kubernetes集群资源监控的部署：</li>\n</ol><ul>\n<li><a href="https://mp.weixin.qq.com/s/AOl_z0zMuPzRlf36J6OLMg">Kubernetes 集群监控 kube-prometheus 部署</a></li>\n<li><a href="https://mp.weixin.qq.com/s/T1Yhbw2efw-wjKu1PZyudg">Kubernetes 集群监控 controller-manager &amp; scheduler 组件</a></li>\n<li><a href="https://mp.weixin.qq.com/s/chvukvjmCDvrYqYapRCEkA">Kubernetes 集群监控 ETCD 组件</a></li>\n</ul><ol start="12">\n<li>日志聚合部署的部署：<a href="https://mp.weixin.qq.com/s/7shBCfpKRBBQDsutSpClhQ">Kubernetes 集群日志监控 EFK 安装</a></li>\n<li>依赖组件的部署：</li>\n</ol><ul>\n<li><a href="https://mp.weixin.qq.com/s/yuAU-qWeTr4svaSI497sbQ">Kubernetes 集群监控 kube-prometheus 自动发现</a></li>\n<li>APM 链路跟踪的部署：<a href="https://mp.weixin.qq.com/s/xTr3IBBUggXdCGL047RF0Q">Kubnernetes 集群部署 Zipkin+Kafka+ElasticSearch 实现链路追踪</a></li>\n</ul><ol start="14">\n<li>微服务项目介绍：<a href="https://mp.weixin.qq.com/s/a8nDBbkuvSjreaCxuM2PuQ">《高楼的性能工程实战课》微服务电商项目技术全解析</a></li>\n</ol>',
        article_title: "25 | 环境搭建：我们的系统是怎么搭建起来的？",
      },
      {
        title: "26 | 全局监控（上）：如何快速落地全局监控？",
        id: 468591,
        content:
          '<p>你好，我是高楼。</p><p>在第7讲中，我们已经讲解了如何设计全链路压测的全局监控。其中的主要逻辑是：<br>\n<img src="https://static001.geekbang.org/resource/image/4e/40/4ebf722499dd638784758aba167dc440.jpg?wh=1824x1624" alt=""></p><p>根据这个逻辑呢，我们可以得出下面这样的性能分析决策树：</p><p><img src="https://static001.geekbang.org/resource/image/e4/7b/e47eea44ce87fcb1d94b38fe14fdd07b.jpg?wh=1732x923" alt="图片"></p><p>为了实现对这个性能分析决策树中所有计数器的监控，我们要将需要监控的组件，一一对应监控工具。整体视图如下：</p><p><img src="https://static001.geekbang.org/resource/image/71/b5/71e39cd615bb0ac4b322fe638e1918b5.jpg?wh=1960x826" alt=""></p><p>这张图其实我们前面也已经见过好几次了。从图里我们可以看出，这个项目中需要用到哪些监控工具。</p><p>这里你就要注意了，虽然我们选择了这些监控工具，但这只是因为这个工具相比其他工具，更多地覆盖了前面列出的计数器。但！并不是说，这些工具就覆盖了<strong>全部</strong>的计数器。</p><p>下面，我们就来看看这些工具到底能做哪些内容。因为全局监控的内容比较多，所以我会分两讲，挑选最为常见并且重要的几个组件为你拆解。这节课呢，我们的重点是拆解k8s+docker监控、操作系统监控和数据库监控。</p><h2>k8s+docker监控</h2><p>对于k8s+docker的监控，主要的全局监控计数器如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/8f/d5/8fb9e0855482e16523f41fd9d1a776d5.jpg?wh=1192x1144" alt="图片"></p><p>为了实现对k8s+docker的全局监控，我希望能有工具可以把这些关键的性能计数器都展示出来，于是这里我选择了cAdvisor+Prometheus+Grafana的组合。如果你对安装部分感兴趣，可以参考上一讲<a href="https://time.geekbang.org/column/article/467606">《环境搭建：我们的系统是怎么搭建起来的？》</a>。</p><!-- [[[read_end]]] --><p>针对我们列出的全局监控计数器，这个监控套件都可以满足，并且它以不同的视角给出了不同的图形。</p><h3>节点资源总览</h3><p>节点资源总览的上半部分是所有节点的总体统计数据，下半部分是每个节点的明细数据。这都是以节点为维度来展示的。</p><p><img src="https://static001.geekbang.org/resource/image/13/69/131e711de159190yy10987ce51yy4e69.png?wh=1864x824" alt="图片"></p><p>通过这个视图，我们可以看到每个Worker的当前使用状态，并判断出资源是否被过度请求。</p><p>我们再来看看节点信息的明细数据。</p><p><img src="https://static001.geekbang.org/resource/image/80/99/80e78cda6d7620331cbc37cfd18a6599.png?wh=1888x376" alt="图片"></p><p>这个图主要分为三个部分：CPU、内存、磁盘。我们可以在图里看到各资源总数、使用率、请求比例、限制等信息。</p><p>看到图里那两条红色的CPU使用率%没有？它为什么是红色的呢？拿上面那一条来说，由于CPU限制在配置时达到了3.10，而CPU总数只有2个，所以就是$3.1\\div2=155%$，这就意味着，在这个节点上，如果所有的POD都满负荷运行起来的话，就会出现抢资源的情况。你可以到这个节点上去看一下，k8s自动调度了哪些POD在这台机器上。</p><p><img src="https://static001.geekbang.org/resource/image/00/f1/0074b7b753d8d0b63d1b6a032269b9f1.png?wh=1856x765" alt="图片"></p><p>从这张图可以看到，其中的Kafka、Nacos、Flannel 几个节点配置的CPU限制加在一起就达到3.1了，并且这里面还有些POD是没有配置CPU和内存限制的。这就意味着，在大压力的场景下，如果这些POD都满负荷运转起来，那必然会出现抢资源的情况。</p><p>但上面的数据都是表格的形式，想要用表格来判断一段时间的趋势是不行的。而我们做性能分析的时候，必然要看的就是，一个场景在它的执行时间段里的资源使用率的趋势图。所以这个监控界面又很贴心地给出了几个曲线图。</p><p><img src="https://static001.geekbang.org/resource/image/1f/66/1f1979694f21ae469d8255d073024b66.png?wh=601x370" alt="图片"></p><p>在这样的曲线图中，你一旦选择了场景执行时间段，就可以非常容易地判断出是哪个节点的CPU使用率高了。这样，你就可以直接到这台机器上去进一步定位资源使用率高的进程了。</p><p>内存部分的计算和CPU的逻辑也是一致的。</p><p>而对磁盘监控来说呢，cAdvisor+Prometheus+Grafana的组合在这个视角上除了能看到磁盘使用率，其他的倒没有特别有用的信息。而磁盘使用率其实对我们做性能分析的来说，只要不用完，通常是不用怎么关心的。我们更需要关心的是磁盘计数器的读写，而不是使用率。</p><p>对于网络来说呢，这部分给出了两个图，一个是所有的节点网络总览图，另一个是每个节点的进出字节数。</p><p><img src="https://static001.geekbang.org/resource/image/95/32/954e86247a26e6d7651a14aae71b5032.png?wh=597x369" alt="图片"></p><p>这也可以让我们对网络有个基础的判断。</p><h3>微服务资源总览</h3><p>在微服务资源总览部分，我们也可以看到和节点资源总览同样的结构。它的上半部分是表格，用来展示每个微服务的CPU、内存、磁盘、容器数的信息。下半部分呢是曲线，用来展示每个微服务的CPU、内存、网络信息。</p><p><img src="https://static001.geekbang.org/resource/image/45/06/45c57f0d7d6f1eb2306fe8f8940dfe06.png?wh=1812x847" alt="图片"></p><p>我一直在强调要使用曲线来观察性能的趋势，意思就是在每个视角上都应该用这样的曲线图来展示性能的趋势。</p><p>针对微服务来说，如果配置了CPU和内存的限制，也就是限制了整个微服务中的所有副本。为什么能得到这样的结论呢？我们一起来解析一下。</p><p>比如说，上图中我们看到Zookeeper的总CPU限制是1.50，我们可以先去查一下总共有几个Zookeeper副本。</p><pre><code class="language-java">[root@s5 ~]# kubectl get pods -n default -o wide | grep zookeeper\nzookeeper-0                               1/1     Running   5          3d1h    10.100.7.34      s11    &lt;none&gt;           &lt;none&gt;\nzookeeper-1                               1/1     Running   0          3d1h    10.100.2.8       s9     &lt;none&gt;           &lt;none&gt;\nzookeeper-2                               1/1     Running   0          3d1h    10.100.1.4       s6     &lt;none&gt;           &lt;none&gt;\n[root@s5 ~]#\n</code></pre><p>从查询结果来看，总共是3个ZooKeeper副本分别分布在不同的Worker节点上。那我们再来查一ZooKeeper的配置文件：</p><pre><code class="language-java">[root@s5 ~]# kubectl describe pod zookeeper -n default\nName:         zookeeper-0\nNamespace:    default\nPriority:     0\nNode:         s11/172.31.184.230\n.....................\n    Restart Count:  5\n    Limits:\n      cpu:     500m\n      memory:  512Mi\n    Requests:\n      cpu:      250m\n      memory:   256Mi\n.....................\n\nName:         zookeeper-1\nNamespace:    default\nPriority:     0\nNode:         s9/172.31.184.228\n.....................\n    Limits:\n      cpu:     500m\n      memory:  512Mi\n    Requests:\n      cpu:      250m\n      memory:   256Mi\n.....................\nName:         zookeeper-2\nNamespace:    default\nPriority:     0\nNode:         s6/172.31.184.226\n.....................\n    Limits:\n      cpu:     500m\n      memory:  512Mi\n    Requests:\n      cpu:      250m\n      memory:   256Mi\n.....................\n</code></pre><p>看到这些配置内容了吧，上面的1.50就是从三个 ZooKeeper 副本上的CPU限制（500m）相加得来的。</p><p>内存的计算逻辑和CPU一致。磁盘的部分在微服务的视角上那就更没有太多要说的了。因为这里只有一个总磁盘使用大小，只要微服务不会无限地使用磁盘，通常都不用太过关注。</p><p>微服务的网络部分是从微服务的视角出发，来计算进出两个方向的带宽的。</p><p><img src="https://static001.geekbang.org/resource/image/29/9b/299bd4dfb9a565b43dd75198e76d849b.png?wh=610x334" alt="图片"></p><p>请注意，这里也同样是计算了所有副本的带宽的。</p><h3>POD资源总览</h3><p>在POD资源总览部分，我们可以看到它的上半部分是表格，展示的是每个POD的CPU、内存、磁盘、容器数的信息。而下半部分呢是曲线，展示的是每个POD的CPU、内存、网络信息。</p><p><img src="https://static001.geekbang.org/resource/image/27/50/274a0ded0de1d19yy9600166ecb4d650.png?wh=1824x856" alt="图片"></p><p>这里你就要注意一下了，这里是每个微服务下所有的POD都会一一分开展示。而CPU和内存的限制逻辑也和微服务一致，只是这里统计到了每一个POD，而微服务部分是把所有POD副本都加到一起了。</p><p>内存的部分，这里分为了WSS（Working Set Size，指一个应用正常运行必须的内存）和RSS（Resident Set Size，实际使用的物理内存，包含与其他进程共享占用的内存）两个视角。</p><p>磁盘也只有一个总使用大小，这里不多说它。网络部分呢，这里也从每个POD的视角给出了进出两个方向的带宽大小。</p><p><img src="https://static001.geekbang.org/resource/image/d1/99/d119cc5fafde57bea0bc3e56763d8e99.png?wh=607x337" alt="图片"></p><p>从以上三个资源总览视图我们可以看到，这三个资源视角在逻辑上是合理递进的。对于微服务分布式架构来说，微服务是包括多个POD的，而节点上是运行多个POD的，微服务是逻辑划分，而节点和POD是具体的承载。</p><p>所以，只要从这三个视角来分析，就可以让你快速找到是哪个节点或哪个微服务消耗的资源多。</p><p>关于k8s，我在这个专栏里主要描述和应用相关的部分。如果想对它有更系统的了解的话，你可以去找相关内容有针对性地学习。</p><h2>操作系统监控</h2><p>对操作系统的监控，我在之前的专栏中已经讲过很多次了。在这里呢，我再带着大家往深处多走几步。</p><p>我还是先把操作系统的全局监控计数器列出来给你看一下。</p><p><img src="https://static001.geekbang.org/resource/image/37/85/37f2a00024073bc1e448d2bc952c4885.jpg?wh=1301x1871" alt="图片"></p><p>在操作系统层面，对于性能分析来说，我经常看的计数器就是上图中的红色计数器部分。这里我们选择Prometheus+Grafana+node_exporter来实现监控。你可以参考一下下面这几张监控图：</p><p><img src="https://static001.geekbang.org/resource/image/51/a1/51d433fd9a050ddb4bb37cf87ea87ba1.png?wh=1824x418" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/77/cb/77d728854a81714f0f1ef102fed4b2cb.png?wh=1824x709" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/f0/da/f0335a129341f1f66f71bbbbcf0cedda.png?wh=1820x809" alt="图片"><img src="https://static001.geekbang.org/resource/image/e5/23/e573c061a0b0c719332254ced120af23.png?wh=1815x383" alt="图片"></p><p>从监控图中可以看得出，我们的计数器已经非常丰富了。</p><p>第一张图是节点资源的总览视图，它同样也是以表格来展现的，这一点和k8s部分的节点信息明细视图虽然有点像，但还是有区别的。在k8s部分的节点信息明细视图中，资源是以请求及限制的逻辑来计算的，而在这个视图中是以实际使用来计算的。比如说吧，CPU使用率在k8s部分的节点信息明细视图中是以CPU核数来计算的，这种算法比较粗犷；而在上面这个视图中就比较精细了，它是以CPU时钟来计算的，所以具体的数值在这个视图中会更为精准。</p><p>那这样丰富的视图是不是就覆盖了前面所列出的所有全局计数器的需求了呢？其实并没有。在CPU、内存、网络和系统部分，其实都是存在一定缺陷的。</p><h3></h3><h3>CPU部分的缺陷</h3><p>比如说CPU部分。我在思维导图中列出的是下面这些计数器：</p><p><img src="https://static001.geekbang.org/resource/image/fb/a7/fb2c7d266b7f60c0075ef8db955f4ba7.jpg?wh=675x613" alt="图片"></p><p>这里一共有9个计数器。但是在这个监控视图中，我们只看到了4个。</p><p><img src="https://static001.geekbang.org/resource/image/f2/57/f20745bb1b02faeb4d32df88yy90df57.png?wh=605x369" alt="图片"></p><p>虽然这已经是比较常用的CPU计数器了，但是对我们做性能分析的人来说，仍然是不够的。我们需要看所有的计数器才可以，同时我们还要看<strong>每个CPU的每个计数器</strong>的值才可以。而这一点在这个监控套件中是实现不了的。</p><h3></h3><h3>内存部分的缺陷</h3><p>再来说说内存部分的缺陷。我在思维导图中列出的内存计数器如下：</p><p><img src="https://static001.geekbang.org/resource/image/55/32/55721c3882cd229d5e3e29fc00dc0e32.jpg?wh=624x763" alt="图片"></p><p>而在这个监控套件中只有已用内存、可用内存和内存使用率。这显然也是不够的，通常我们判断内存够不够用是要靠page faults的，而这个关键的计数器在这个监控套件中居然没有。</p><p>其实你去node_exporter（Grafana展现的数据是通过Prometheus到node_exporter中取出的）中去看也是没有和page faults相关计数器的，不得不说这个是大麻烦。</p><p>那在做具体内存相关的分析的时候，你就要登录到主机中，执行sar、pidstat等相关的命令去查看系统级和进程级的page faults数据了。</p><h3></h3><h3>网络部分的缺陷</h3><p>再来说一下网络部分的缺陷。我在思维导图中列出的网络计数器如下：</p><p><img src="https://static001.geekbang.org/resource/image/d3/67/d379f7f549cd61282797d110f182ed67.jpg?wh=722x664" alt="图片"></p><p>在我选择的监控套件中，网络流量是可以看到的，也区分了不同的网卡，也区分了进出带宽。</p><p>但是这里我列的队列却是没有的。全连接队列和半连接队列是用来判断TCP连接能不能建立的重要计数器，同时，对于网络到底堵在了哪个节点上，Recv_Q（接收队列）和Send_Q（发送队列）也是非常重要的判断依据。这里都缺失了。</p><p>这就导致我在做网络性能瓶颈判断时，通常都不看这个监控套件中的数据，而是直接到主机上执行netstat、iftop等命令来判断网络问题。</p><h3>系统部分的缺陷</h3><p>再来说一下系统部分的缺陷。我在思维导图中列出的系统计数器如下：</p><p><img src="https://static001.geekbang.org/resource/image/11/5a/11ecf48e9ea55840260dd91af65ca25a.jpg?wh=682x295" alt="图片"></p><p>注意：上图中的Load Average放在这里，只是为了方便我自己在做这个全链路压测项目时，来理解系统相关的内容，这个计数器应该放在CPU部分。</p><p>在我选择的这个监控套件中，是缺少interrupt（中断）相关的计数器的。在CPU的部分有hi/si两个计数器来标明硬中断和软中断消耗的CPU。但是在做性能分析时，我们还需要软硬中断的具体次数。而这个具体次数是存放在Linux操作系统的proc目录中的interrupts和softirqs两个文本中的。所以当你需要的时候，就要去查看这两个文本了。</p><p>不过，对于磁盘部分，这个监控套件倒是够用的。</p><p>好了，刚才我们细数了操作系统监控套件的这么多缺陷。那既然问题这么多，为什么我们还要选择它呢？<strong>因为在现在的技术市场上，这已经是最好用的监控工具了</strong>。它可以原生支持k8s，并且也确实提供了相当多有用的计数器数据。</p><p>只不过，我们还是要知道这些缺陷，具体分析的时候呢，先从监控视图判断有没有瓶颈点，再根据判断到相应的节点中进一步查看更详细的计数器，那就是定向监控的部分了。幸好这部分需要弥补的计数器，可以通过自行扩展node_exporter去实现。</p><h2>数据库监控</h2><p>我们下面再来看看数据库的性能监控分析。这里我拿MySQL举个例子。</p><p>虽然MySQL在被Oracle收购之后，前景变得越来越不明朗，很多企业为了寻求心理上的安全感还停留在5.x的版本，也有企业已经转向了MariaDB。同时呢，国内还有很多厂商使用MySQL的开源代码改造自己的数据库，并且也有企业付之商用。</p><p>但是，在现在开源的市场上，MySQL数据库仍然是大部分企业选择的关系型数据库，所以我还是拿它来做个说明。</p><h3>MySQL性能监控</h3><p>先来看一下我经常使用的MySQL性能分析决策树的思维导图。</p><p><img src="https://static001.geekbang.org/resource/image/2d/72/2dyyd6584c617bc4b2187233bc712b72.jpg?wh=1482x1591" alt="图片"></p><p>在MySQL的分析中，我还是遵从自己的习惯，着重先看图上红色的计数器。从我的经验中来看，这些计数器已经可以覆盖大部分的性能问题了。</p><p>我们知道对于关系型数据库来说，它的功能就是执行增删改查的SQL。我们只要能让SQL执行得足够快就好了。为了直观地看到SQL执行得快不快，针对罗列的计数器，我们选择通过Prometheus+Grafana+mysql_exporter套件来实现对MySQL的全局监控。下面是它具体的展示效果。</p><p><img src="https://static001.geekbang.org/resource/image/da/21/da38866c39af5ed2a3b6fd3e37aa0a21.png?wh=1836x838" alt="图片"><img src="https://static001.geekbang.org/resource/image/24/26/247a355472ec13d63b0a29dff6a81026.png?wh=1826x663" alt="图片"><img src="https://static001.geekbang.org/resource/image/76/b8/76029d96af003cb58c20a16c9692f4b8.png?wh=1820x665" alt="图片"><img src="https://static001.geekbang.org/resource/image/6e/e6/6efeb2b0ca038df4cyya29c5034e61e6.png?wh=1826x666" alt="图片"><img src="https://static001.geekbang.org/resource/image/f8/0c/f8c747a4d2ec295c09fd2bafa2cf980c.png?wh=1833x879" alt="图片"><img src="https://static001.geekbang.org/resource/image/68/ee/68b1939e8164e5408dc5ae95fced07ee.png?wh=1838x941" alt="图片"><img src="https://static001.geekbang.org/resource/image/97/92/97354a2a839696fa47ea4f7a92fb0692.png?wh=1839x331" alt="图片"></p><p>这些图中的数据和思维导图中的数据基本上是可以对应上的，所以这是我经常使用的一个MySQL监控模板，对我来说，这个监控套件没有缺陷。因为它不仅包括了我想要的计数器，还都是有趋势图的，历史数据也可以保存，perfect!</p><p>不过，我们总会遇到没有Grafana+Prometheus的场景。这时，我建议你使用mysqlreport，mysqlreport是对mysql状态的一个快照。这个快照是包括了启动以来的所有状态信息的，所以我建议你在执行性能场景之前先刷新一遍状态值，甚至可以考虑重启一下MySQL数据库（当然这个动作过大，你也要考量一下之后再做），争取取到的状态和执行的性能场景有着直接的关系。</p><p>当然，在MySQL中即使有了上面的全局监控思路，还是缺少了重要的一个环节，那就是慢日志。慢日志是分析MySQL不得不提的一个重要功能模块。在慢日志的分析中，我常用的工具是两个pt-query-digest和mysqldumpslow。因为我需要用它们统计慢日志的执行次数和执行时间（总时间和平均时间），然后对慢日志进行排序。</p><p>mysqldumpslow的执行语句如下：</p><pre><code class="language-java">mysqldumpslow -s t -t 10 TENCENT64-slow.log.last\n</code></pre><p>执行的结果如下：<br>\n<img src="https://static001.geekbang.org/resource/image/c1/c4/c1d473d0535a714f02e556447f8585c4.png?wh=1757x908" alt="图片"></p><p>pt-query-digest的执行语句：</p><pre><code class="language-java">pt-query-digest TENCENT64-slow.log.last\n</code></pre><p>结果如下：<br>\n<img src="https://static001.geekbang.org/resource/image/a8/2a/a819880023b44f0578a9831d4e25912a.png?wh=1440x776" alt="图片"></p><p>在上面的结果中，可以看到调用次数（mysqldumpslow中是Count，pt-query-digest中是Calls）和时间（mysqldumpslow中是Time，pt-query-digest中是Response Time）两个关键的排序参数，而且结果中也给出了具体的SQL。当我们看到某个SQL占比较高时，就需要进一步定向分析这个SQL的执行计划或者Profile信息了。</p><h2>总结</h2><p>好，这节课就讲到这里啦。</p><p>在微服务分布式架构盛行的技术市场中，k8s和docker作为容器编排和容器中的绝对的翘楚，是不能不说的环节。作为性能测试和性能分析的人，如果不理解它们的原理和逻辑，那就算你看到监控工具炫丽的界面也会毫无知觉。所以在这节课中，我们描述了这一部分的资源监控结构。</p><p>同样，我也描述了操作系统和数据库相关的监控逻辑，把我工作中最常用的判断逻辑和相应的计数器展示给你了。</p><p>对于监控工具，我一直在强调，就是不仅要知道有哪些计数器，还要知道每个计数器的含义和作用。我这不是要你完全都记在脑子里，而希望当你发现一个问题时，能知道下一步要做什么才能进一步做判断。</p><p>我们一直都在说云架构，而云架构对于上层应用来说，还是要看用哪个具体的计数器来判断性能瓶颈，这是需要逻辑的。并不是说提到了云架构，我们就要换一套全新的计数器了。还有人一提到“信创”这样的新词就是蒙的，搞得好像完全不是一回事似的。这就像小学数学题里问：“一排4棵树，有4排，共有几棵树？”他知道是16棵，但是把树换成电线杆子，他就不会了。</p><p>其实，如果你不是做底层架构的、不是做信创的具体实现的，对应用来说，该看什么还是看什么。底层的知识都是需要学习的，这是基础的IT知识体系中必不可少的内容。</p><h2>课后题</h2><p>好了，最后我还是给你留两道思考题：</p><ol>\n<li>你觉得在k8s+docker的监控中，还需要哪些这节课没有提到的计数器？</li>\n<li>你觉得操作系统作为一个跳不过去的技术组件，在性能瓶颈分析的逻辑中，最大的价值是什么？</li>\n</ol><p>欢迎你在留言区和我交流讨论，我们下节课再见！</p>',
        article_title: "26 | 全局监控（上）：如何快速落地全局监控？",
      },
      {
        title: "27 | 全局监控（下）：如何快速落地全局监控？",
        id: 469464,
        content:
          '<p>你好，我是高楼。</p><p>上节课，我们就全局监控中的k8s+docker监控、操作系统监控和数据库监控进行了详细的讲解。</p><p><img src="https://static001.geekbang.org/resource/image/f6/11/f6f14d05d4ab266412af27401feefb11.jpg?wh=1920x790" alt="图片"></p><p>这节课呢，我们继续全局监控这一部分的内容。我们一起来看看应用监控、链路监控、缓存监控和日志监控这四部分都是怎样的。</p><h2>应用监控</h2><p>在应用监控中，是要先考虑开发语言的。我们这个专栏是用Java语言开发的应用，所以这里我描述一下Java应用的性能分析决策树。</p><p><img src="https://static001.geekbang.org/resource/image/3c/ac/3c5750c20e52be88966bf5f2b21072ac.jpg?wh=1188x1131" alt="图片"></p><p>在Java应用的性能分析决策树中，我会主要关注两个方面：堆和栈。</p><p>具体的计数器我在上面这张图里用红色标记出来了。但是要说Java的监控工具，那就像汪洋大海了。不管是开源的还是商用的，可以说都是多如牛毛。我不建议在Java应用的监控工具选型上花太多功夫，我们只需要选择易用、成本低的工具就可以了。当前开源的Java监控工具已经完全可以实现所有的功能了。</p><p>之前我在专栏中提到过JVisualVM，这是一个我非常常用的工具之一。不过我想强调一点，在全链路压测过程中，如果我们用基于k8s+docker的微服务分布式架构，那使用JVisualVM会比较麻烦。因为它的每个Java实例都需要单独连接，并且要把容器中的端口映射出来，操作上比较繁琐。</p><p>我们的这个专栏的项目使用的是Spring Cloud架构，这也是当前市场上最流行的一种架构了。我选择的工具是Spring Boot Admin，在应用中集成了相关依赖之后，你可以打开Spring Boot Admin，查看应用墙。</p><!-- [[[read_end]]] --><p><img src="https://static001.geekbang.org/resource/image/1d/a0/1d39a7ec7bf7de9e8e93a81375e5f4a0.png?wh=1906x972" alt="图片"></p><p>这个应用墙可以看到一个应用的所有实例。要注意的是，这里的绿色只是代表进程端口的启动是正常的，并不表明这个应用性能就是好的。</p><p>如果你想看具体的性能分析决策树中的信息，可以点击实例进入到相应的监控界面中。下面这张图展示的是我经常看的几个监控界面。</p><p><img src="https://static001.geekbang.org/resource/image/42/08/424f19176e7fb0a72a552fea9bdd4308.png?wh=1718x545" alt="图片"></p><p>在上面的这个图里，我们不仅可以看到堆和非堆内存使用的情况，也同时可以判断GC的效率，这就可以对应到我们性能分析决策树的 GC、CPU 和 Memory 三个部分的计数器了。</p><p>再来看一下线程视图：</p><p><img src="https://static001.geekbang.org/resource/image/4e/cb/4e4f2b074c23812a2431915b68cce3cb.png?wh=1920x1042" alt="图片"></p><p>在线程视图中，可以看到自打开此界面后的所有线程状态。我们可以通过这个视图判断线程是否健康。</p><p>像上面这张图就是线程的 blocked 过多的情况。这时我们就要去使用 jstack 之类的命令去打印栈信息，然后打到对的时间点上。</p><p>请注意，这个界面即使全是绿色也并不能说明性能就是好的。比如下面这张图：</p><p><img src="https://static001.geekbang.org/resource/image/da/c5/da0a84370f0a1c4fcde51yycfd3059c5.png?wh=1920x998" alt="图片"></p><p>这种情况下，我们还要进一步判断方法执行时间是不是过长。像上面这张图里，绿色段较长的地方也是不合理的。这个界面要多观察一会，不能刚打开就判断，不然就看不到状态随着时间变化的趋势了。</p><p>不过，要判断Java应用方法的执行时间，只看Java应用监控也是不行的。我们还要结合上节课提到的操作系统的计数器做关联分析。</p><p>关联分析的逻辑就是：当us cpu（注意这里是us cpu，不要看错了，这可是关键的一个起点）使用率过高时，查看相应的进程；当确定了是 Java 进程时，再到线程图（上面两张图）界面查看方法的执行时间。</p><p>当我们点击线程图（上面两张图）中的某段带有红颜色或绿颜色的线时，就可以看到相应的Java栈了：</p><pre><code class="language-java">"http-nio-8401-exec-884" #86813 daemon prio=5 os_prio=0 tid=0x00007f2868073000 nid=0x559e waiting for monitor entry [0x00007f2800c6d000]\n   java.lang.Thread.State: BLOCKED (on object monitor)\n\tat java.security.Provider.getService(Provider.java:1035)\n\t- waiting to lock &lt;0x000000071ab1a5d8&gt; (a sun.security.provider.Sun)\n\tat sun.security.jca.ProviderList.getService(ProviderList.java:332)\n.....................\n\tat com.dunshan.mall.auth.util.MD5Util.toMD5(MD5Util.java:11)\n\tat com.dunshan.mall.auth.config.MyPasswordEncoder.matches(MyPasswordEncoder.java:23)\n.....................\n  at com.dunshan.mall.auth.controller.AuthController.postAccessToken$original$sWMe48t2(AuthController.java:46)\n\tat com.dunshan.mall.auth.controller.AuthController.postAccessToken$original$sWMe48t2$accessor$jl0WbQJB(AuthController.java)\n\tat com.dunshan.mall.auth.controller.AuthController$auxiliary$z8kF9l34.call(Unknown Source)\n.....................\n\tat com.dunshan.mall.auth.controller.AuthController.postAccessToken(AuthController.java)\n.....................\n</code></pre><p>这就已经到了代码层了。但是！是不是就怕看到但是？没有办法，在性能分析中，这一个个但是，就是我们的分析路径。</p><p>但是上面的栈信息中只有代码的调用关系，并没有每个方法的调用时间。怎么办呢？这时候我们就需要进一步去拆分栈中每一个方法的调用时间了。你可以用JVisualVM、Arthas等等的工具，这里我用Arthas截取一段栈的内容看一下。</p><pre><code class="language-java">-- 执行语句\ntrace -E com.dunshan.mall.cart.controller.CartItemController listPromotionnew -n 5 -v --skipJDKMethod false \'1==1\'\n\n-- 局部内容\n`---ts=2021-01-16 15:08:58;thread_name=http-nio-8086-exec-34;id=f8;is_daemon=true;priority=5;TCCL=org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader@56887c8f\n    `---[97.827186ms] com.dunshan.mall.cart.service.imp.CartItemServiceImpl$$EnhancerBySpringCGLIB$$ac8f5a97:listPromotion()\n        `---[97.750962ms] org.springframework.cglib.proxy.MethodInterceptor:intercept() #57\n            `---[97.557484ms] com.dunshan.mall.cart.service.imp.CartItemServiceImpl:listPromotion()\n                +---[72.273747ms] com.dunshan.mall.cart.service.imp.CartItemServiceImpl:list() #166\n                +---[0.003516ms] cn.hutool.core.collection.CollUtil:isNotEmpty() #172\n                +---[0.004207ms] java.util.List:stream() #173\n                +---[0.003893ms] java.util.stream.Stream:filter() #57\n                +---[0.003018ms] java.util.stream.Collectors:toList() #57\n                +---[0.060052ms] java.util.stream.Stream:collect() #57\n                +---[0.002017ms] java.util.ArrayList:&lt;init&gt;() #177\n                +---[0.003013ms] org.springframework.util.CollectionUtils:isEmpty() #179\n                `---[25.152532ms] com.dunshan.mall.cart.feign.CartPromotionService:calcCartPromotion() #181\n</code></pre><p>从上面的执行结果中可以看到，Arthas可以把一个栈中每个方法的调用时间都梳理出来。这里还要强调一点，不要只看一次就轻易判断说某个方法的执行时间过长，最好多刷几次。如果确认一直就是这个方法的执行时间较长，你就可以拿着这个证据挺胸抬头去找这个应用的开发讲理去了。</p><p>上面就是我在进行应用监控分析时的思路。主要就是从操作系统出发，判断出进程和线程所消耗的资源大小，然后根据语言特性做判断，找到出问题的代码段就可以了。</p><p>不过这里我还是要强调一点，对于应用监控分析，分析的逻辑是最重要的，无论是使用Java、C/C++、Python 还是 Go，这个逻辑都不会变！不会变！之所以这样强调是因为，我经常看到有些同学因为换了语言就手足无措，其实只是换个监控工具而已。</p><h2>链路监控</h2><p>对于全链路压测来说，不说链路监控是不行的。因为微服务分布式架构中的服务太多，一个个去追着日志查，那是要崩溃的。如果使用链路监控，我们就可以高效、即时地判断时间消耗在哪里了。</p><p>在链路监控工具中，我们主要是看两个内容：</p><ul>\n<li>服务拓扑图：</li>\n<li>服务链路调用耗时树。</li>\n</ul><p>我们这个专栏的项目是选择Sleuth+Zipkin来做链路的全局监控的。</p><p>先来看下服务拓扑图：</p><p><img src="https://static001.geekbang.org/resource/image/cd/44/cd94913bb15891d8811eec8ca70f2b44.png?wh=1486x602" alt="图片"></p><p>在全链路压测的过程中，服务拓扑图的地位可以说是重中之重了。从这个图里，你不仅可以看到链路的调用路径，还能看到每个服务的流量大小。看到上面线上的点点没有？当压力发起时，那些点是流动的。观察这个流动的状态，你就可以判断出哪个服务的调用次数多，进而重点关注了。</p><p>虽然这里我是以Zipkin来演示的，但并不代表只有Zipkin有这样的功能，其实所有的链路监控工具都能做到这一点。所以在链路监控工具的选型上，不用过于执着，只要用着顺手就可以了。</p><p>我们再来看下服务调用链路耗时树：</p><p><img src="https://static001.geekbang.org/resource/image/48/10/48f25bc20c40247eb244be6fc3a50210.png?wh=1909x638" alt="图片"></p><p>在服务调用链路耗时树中，我们可以看到一个接口从网关开始直到一个具体的SQL执行的整个过程。这里不仅可以显示数据库，还可以显示Redis缓存、RabbitMQ队列各种调用所耗的时间。当然，你需要加相应的依赖包，这一点我们在标记透传那两节课已经详细讲过了。</p><p>像上面这张图，我们就可以判断出是由于Portal服务响应慢，导致了Cart服务响应慢，再进一步导致了Order服务响应慢。这层级关系清晰而优雅。</p><p>有了这个判断之后，我们就可以用刚才讲的应用监控部分的分析逻辑，进一步定位具体是哪段代码了。</p><h2>缓存监控</h2><p>缓存对大容量系统的作用不容忽视，这一点你可以在很多地方看到相应的案例。而开源缓存中，使用较多的那就非Redis莫属了，它用C语言开发，效率高，稳定性好。我们这个专栏的项目同样也用到了Redis。</p><p>记得在上一个专栏，我就在对登录的功能做优化时加了一层缓存，TPS直接上升了一倍，而对于一些系统的热点数据来说，如果使用了缓存，效率会更为明显。</p><p>同样，我也画了一下Redis的性能分析决策树：</p><p><img src="https://static001.geekbang.org/resource/image/a0/d6/a0b9a2efeecddee531ba01c10944b2d6.jpg?wh=1141x1201" alt="图片"></p><p>虽然Redis的计数器看起来也不少，但在我的经验里，要看的其实并不多。总结下来其实就三点：</p><ol>\n<li>有没有慢命令（我看有的地方叫慢查询，其实是不合理的，应该是慢命令）。</li>\n<li>内存够不够用。</li>\n<li>持久化对性能的影响。</li>\n</ol><p>这次监控呢，我们采用Prometheus+Grafana+redis_exporter来实现对Redis的全局监控。展示效果如下：</p><p><img src="https://static001.geekbang.org/resource/image/aa/64/aa570295bfa8aee98172a8114a8cb164.png?wh=1842x914" alt="图片"><img src="https://static001.geekbang.org/resource/image/46/5a/4673a8f0b8878c9fb25681446091d55a.png?wh=1844x922" alt="图片"></p><p>这里我截了几个我认为重要的计数器。并且把原模板中的QPS改为了TPS。</p><p>在这个图上我们需要关心的几点是：单节点平均TPS、总TPS和慢命令数量。你可以看到这个图里有两段数据。第一段数据所在的时间段是10：55 - 11：08；第二段数据所在的时间段是11：13 - 11：24。</p><p>对比一下你就会发现，虽然连接数有10倍的差别，但是TPS在两个时间段里并没有什么区别，可见已经有瓶颈了。至于瓶颈在哪个环节，只看这些图还不能确定。</p><p>我们再来看一下同一段数据的前后两个部分。</p><p><img src="https://static001.geekbang.org/resource/image/68/f8/68289b69567073531fbb50782d8138f8.png?wh=1348x1210" alt="图片"></p><p>在第一段数据10：55 - 11：08这个时间范围里，从10：55 - 11：01是属于操作的数据较小的阶段，TPS很高，但网络流量不大；而从11：01 - 11：08是属于操作的数据较大的阶段，TPS很低，但网络流量是比较大的，一度达到了300M。第二段数据也类似。</p><p>在这种情况下，如果你想进一步定位问题，就需要再结合操作系统的计数器做关联分析了。</p><p>如果你觉得上面的监控图表不够用，还可以直接登录到Redis中执行info命令，那里可以看到更多数据。只是info命令和mysqlreport一样，统计的是累加数据。如果你需要做判断，就需要判断数据是否在你执行的场景时间范围内有效，不让数据累加到场景之外的值。</p><p>这里我也给出我对Redis的分析思路：当TPS不再增加时，我建议你先去看操作系统的全局监控。对于C语言编写的Redis来说，在操作系统上可以直接看到它的CPU、内存、IO、网络等的资源消耗（而对于Java语言编写的应用来说，还是先去看JVM里面的计数器为好）。</p><p>举例来说，如果Redis的内存不足，那在大流量下，必然会出现page faults增加的情况。这里我们需要重点关注major page faults的增加，它代表着页面硬错误，这里只能产生IO了；所以我们还要去查看当前的IO状态。对于主要用内存的缓存服务来说，出现大量的读写IO，那是会严重拖慢系统的。</p><p>还有一点需要强调，对于缓存服务来说，因为是以使用内存为主，所以数据的丢失就必须要提到台面上来讨论。我们必须得知道，如果内存中的数据丢失的话会产生什么样的后果。为了保证内存数据丢失不至于产生严重的生产事故，Redis给出了AOF和RDB两种持久化策略，你可以根据自己的项目做相应的选择。但是你得知道，无论是哪种持久化策略，它对性能的影响都是有的；而且，如果Redis因为故障而死掉，那也必然会有数据的丢失，就是多少的区别而已。</p><p>通常我们在生产环境中会选择AOF策略，因为这种策略会让我们丢失的数据少一点。这是一个平衡上的哲学问题，持久化频繁了性能差，持久化不频繁就要承担风险。所以这个持久化策略，我建议你针对项目做严格的验证（对于一些不关心性能的企业，倒也无所谓，反正我的资源多得很，就可以用最严苛的策略，上最多的资源）。</p><p>另外对Redis来说，慢命令日志也是极为重要的参考数据。你可以通过slowlog get来获取有哪些慢的命令：</p><p><img src="https://static001.geekbang.org/resource/image/c3/37/c396cf9b0442cd1b279132de1a967837.png?wh=414x874" alt="图片"></p><p>解释一下每个命令的前四条：</p><ol>\n<li>日志标识符：具有唯一性(unique)；</li>\n<li>执行时间点：以 Unix 时间戳格式表示，你可以转为当前时间；</li>\n<li>执行时间：以微秒为单位，也就是这个命令慢到什么程度；</li>\n<li>具体命令：以数组的形式排列。比如第4个就是：LRANGE mylist 0 599。</li>\n</ol><p>到这里，你就可以判断到Redis为什么慢，慢在哪里了。但是对性能分析来说，只知道问题出在哪还是不够的，最好你还能提出解决方案。</p><p>Redis作为一个非常成熟且稳定的缓存服务器，要想找到代码层面的问题还是比较难的。所以大部分对于Redis的优化都是参数配置或使用方法方面的优化。比如使用Pipelining、设置一下内存分配器、持久化配置得合理一些、日志级别调低一些。上面这些都是通过调整Redis来调优，如果还想再调优的话，基本上就是在操作系统上做动作了，比如配置网络参数（进出队列）、优化CPU（NUMA和绑定配置）等等操作。</p><p>这样，我就基本把对Redis这样的缓存服务进行优化的思路描述清楚了，总的来说，Redis的可优化的点并不是非常多，是容易掌握的。</p><p>说完了缓存的部分，下面再来说说日志监控。</p><h2>日志监控</h2><p>可能会有人说，日志监控，最多不就是查查日志吗？但其实，对于性能来说，通过日志你是看不出来快慢的，除非你把请求时间、响应时间打印到日志中并提取出来。</p><p>如果你想说的是找问题查日志，那通常也是在报错的时候才需要的。而我们要通过日志得到的不止是某个具体的日志，而是通过日志得到业务模型，所以日志聚合监控是非常有必要的。</p><p>对于微服务分布式系统来说，因为没有日志聚合分析系统，所以要想找到某些依赖日志的问题，会浪费至少一半以上的时间。这里我推荐使用ELFKK(Elasticsearch/Logstash/Filebeat/Kibana/Kafka)套件，这一套下来，说覆盖80%的日志分析场景应该不过分。</p><p>在日志全局监控的逻辑中，我最经常看的是下面这个图：</p><p><img src="https://static001.geekbang.org/resource/image/0b/21/0b0fa80c1cb4f1aa8fd7f07352caa121.png?wh=1233x263" alt="图片"></p><p>这个图是我用来判断某个时间段内的请求量级的。同时在这个界面里，也可以看到每个接口的请求趋势。记住哦，这个请求趋势可不是看一眼热闹就可以了的。我们应该通过它来判断应该设置多少个业务场景。简单来说，就是如果所有接口的趋势都是一样的，都是在同样的时间点请求量高，那只需要设置一个场景就可以了；如果接口的高峰请求量不在同一时间点，那就需要判断是否需要增加业务场景了。</p><p>另一个我常看的视图是这样的：</p><p><img src="https://static001.geekbang.org/resource/image/fa/b9/fa9716d8514b597995ba008cc3ebbeb9.png?wh=252x340" alt="图片"></p><p>从这个视图中可以看到接口的请求比例，这可以方便我们做压测时配置业务比例。不过，这里涉及到一个用什么压测工具的问题。如果你用的是录制回放工具，这个图是不用看的，因为录制时的业务比例已经固定了，只放大就行了。这个时候，你直接回放就已经是按这个业务比例发出的请求了。</p><h2>总结</h2><p>好了，这节课呢，就聊到这里了。在这节课中，我们把应用监控、链路监控、缓存监控、日志监控等在全链路压测中非常常用的几个部分拆解了一下。如果这两节课还有没涉及到的其他组件，你可以根据我所描述的逻辑自己思考一下，落地到你的全局监控思路中。</p><p>关于全局监控有几个点需要提醒一下：</p><ol>\n<li>全局监控计数器要尽量覆盖范围大一些，不能遗漏大的模块；</li>\n<li>一定要先画性能分析决策树，再找监控工具，如果工具不能全部覆盖你想看的计数器，要记得在具体的分析过程中做相应的补充；</li>\n<li>全局监控是定向监控的起点，定向监控是找证据链的过程，而证据链是可以让我们找出问题的根本原因的。所以全局监控想不清楚，计数器不能理解，那后面的步骤都会卡住。性能分析从来都没有只看某个计数器、或只看某个监控工具就可以完全定位出问题的证据链的，只有层层分析才能找到证据链。可惜现在市面上的监控工具，没有一个是以这个思路来设计的，这就没有办法只能靠性能分析人员的逻辑思维能力了。</li>\n</ol><p>希望通过这节课，你可以对全局监控有个更深刻的认识。在我们后面的章节中，我会直接使用这些思路来分析具体的问题。</p><h2>问题</h2><p>学完这节课，请你思考两个问题：</p><ol>\n<li>你在自己的项目中有没有画过全局性能决策树？有没有分析过每个性能监控工具的局限性？</li>\n<li>如何快速理解每个技术组件的每个计数器？</li>\n</ol><p>欢迎你在留言区与我交流讨论。我们下节课见！</p>',
        article_title: "27 | 全局监控（下）：如何快速落地全局监控？",
      },
      {
        title: "28 | 定向监控：怎样快速发现业务异常？",
        id: 470494,
        content:
          '<p>你好，我是高楼。这节课，我们来讲一讲定向监控。</p><p>其实全局监控和定向监控，我在三个专栏里都反复提过。你可能会问了，既然之前都写过了，为什么还要在这个专栏中专门再写一篇呢？毕竟凑字也不是我的风格，我还真是要写点不一样的东西。</p><p>在这一节课，我会主要介绍业务级的定向监控，也就是要实现业务代码级的监控。如果你想学习更多定向监控的知识，也可以看看我以前讲过的内容： <a href="https://time.geekbang.org/column/article/190132">《性能测试场景：如何进行监控设计？》</a> 和 <a href="https://time.geekbang.org/column/article/361138">《如何设计全局和定向监控策略？》</a> 。</p><p>话说回来，为什么要做业务级的定向监控呢？因为线上压测是有风险的，这个大家都知道。而做到业务级的定向监控可以降低压测风险。</p><p>业务级的定向监控对于刚入行的性能测试小白来说确实是一个新话题，但对于从事多年性能测试老鸟来说并不奇怪。那我们怎么具体落地呢，下面我们就来仔细看一看。</p><h2>本地环境准备</h2><p>在之前的课程里，我已经介绍了市场上常见的监控工具，这里我就结合 Prometheus SDK 来说说如何改造代码并落地业务 Metrics 埋点 。</p><blockquote>\n<p>Prometheus 提供度量的四种基本类型包括：Counter，Gauge，Histogram，Summary。</p>\n</blockquote><p>这里，我主要演示电商项目下单业务的定向监控。我们会主要采集<strong>订单成功次数、订单失败次数、订单金额总数</strong>等业务的 Metrics，如果你还想扩展其它业务，也可以参考这一方法进行改造。</p><!-- [[[read_end]]] --><p>依照惯例，我还是先在本地搭建 demo 做技术预演，预演成功后再做真实系统改造。</p><p>首先，我们要搭建 Prometheus 集成 Grafana 的本地环境。具体的操作你可以参考下面三篇文章：</p><ul>\n<li><a href="https://mp.weixin.qq.com/s/C6E0Ak-dokBtuTzTczZAaA">《性能监控之初识 Prometheus》</a></li>\n<li><a href="https://mp.weixin.qq.com/s/IakvrGQBzTSq_FPUF-pAYw">《性能监控之 node_exporter+Prometheus+Grafana 实现主机监控》</a></li>\n<li><a href="https://time.geekbang.org/column/article/190684">《性能监控工具之Grafana+Prometheus+Exporters》</a></li>\n</ul><p>环境搭建好之后，运行效果如下：</p><p>Prometheus 监控系统：</p><p><img src="https://static001.geekbang.org/resource/image/70/72/707389253947f6528d96b34a6d809272.png?wh=1740x880" alt="图片"></p><p>Grafana 可视化看板：</p><p><img src="https://static001.geekbang.org/resource/image/c4/b0/c456f5e15124a0b2de374d592df3fdb0.png?wh=752x492" alt="图片"></p><p>这样，我们用 Prometheus 集成的 Grafana 的本地环境就准备好了。</p><h2>demo 技术预演</h2><p>下一步，我们就要搭建业务级的定向监控 demo了，你可以先看下这张逻辑图：</p><p><img src="https://static001.geekbang.org/resource/image/61/f3/61805753c8f81f09a652b550546693f3.jpg?wh=1461x379" alt="图片"></p><p>一般情况下，使用 Prometheus SDK 做业务埋点主要有这几个步骤：</p><ol>\n<li>创建 Prometheus Metric 数据项，可以创建一个自定义类；</li>\n<li>注册定义好的 Metric；</li>\n<li>在业务代码中埋点，对 TSDB 数据写入操作；</li>\n<li>提供 HTTP API 接口，让 Prometheus 定时来收集 Metric 数据。</li>\n</ol><p>这是一个通用的埋点套路，无论是 Golang 还是 Java 应用都是适应的。下面我们来看下它在demo项目中的具体实现：</p><ul>\n<li>新建 SpringBoot 工程</li>\n</ul><p>使用 SpringBoot（项目使用的框架） 集成 Prometheus SDK。这里我们要导入相关依赖：</p><pre><code class="language-xml">    &lt;dependency&gt;\n        &lt;groupId&gt;io.micrometer&lt;/groupId&gt;\n        &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt;\n        &lt;version&gt;1.7.4&lt;/version&gt;\n    &lt;/dependency&gt;\n</code></pre><ul>\n<li>修改 SpringBoot 全局配置文件</li>\n</ul><p>在 application.yml 文件中输入如下配置：</p><pre><code class="language-yaml">spring:\n  application:\n    name: dunshan-prometh\n\nserver:\n  port: 8086\n\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: \'*\'\n  endpoint:\n    health:\n      show-details: always\n    prometheus:\n      enabled: true\n  metrics:\n    tags:\n      application: ${spring.application.name}\n</code></pre><p>改好配置文件之后，启动应用服务器，可以看到 Metric 的访问路径和端口号：</p><p><img src="https://static001.geekbang.org/resource/image/ca/1d/caa0b2e053010219ba1640723e05691d.png?wh=752x248" alt="图片"></p><p>我们通过路径和端口号可以得到下面这个地址： <a href="http://localhost:8086/actuator/prometheus">http://localhost:8086/actuator/prometheus</a> 。</p><p>启动工程，在浏览器中输入这个地址，结果显示如下：</p><p><img src="https://static001.geekbang.org/resource/image/0c/e2/0cb7a0b475b38187729fb84d5b0ff3e2.png?wh=752x203" alt="图片"></p><p>可以看到， Actuator Metrics 数据已经正常显示出来了。</p><ul>\n<li>配置 Prometheus 拉取数据</li>\n</ul><p>接下来，在 Prometheus 主程序的配置文件中填写上面的 Metrics 接口信息。</p><p>在 prometheus.yml 写入的配置你可以参考下面的代码：</p><pre><code class="language-yaml">global:\n  scrape_interval: 15s\n\nscrape_configs:\n  - job_name: "prometheus"\n    static_configs:\n    - targets: ["localhost:9090"]\n\n  - job_name: "dunshan-prometh"\n    metrics_path: "/actuator/prometheus"\n    static_configs:\n     - targets: ["localhost:8086"]\n</code></pre><p>配置完之后，我们重启 Prometheus 服务，在浏览器中输入地址： <a href="http://ip:9090">http://ip:9090</a> ，在控制台的 Status 菜单栏中选择 Targets：</p><p><img src="https://static001.geekbang.org/resource/image/da/ed/da2dd20b76883b8177228a84b01955ed.png?wh=1106x610" alt="图片"></p><p>这样就能看到目前 Prometheus 收集的 Metrics 的路径了：</p><p><img src="https://static001.geekbang.org/resource/image/db/f8/dbc97b4ce6e252092e0b3a2d70a961f8.png?wh=752x274" alt="图片"></p><p>再次点击 <a href="http://localhost:8086/actuator/prometheus">http://localhost:8086/actuator/prometheus</a> ，显示的信息如下：</p><p><img src="https://static001.geekbang.org/resource/image/3c/7b/3cc9f80a95f8fe2f31a8e8d7b2fbc27b.png?wh=712x466" alt="图片"></p><p>这部分信息告诉我们，应用服务 Metrics 已经被 Prometheus 成功定时收集起来了。</p><p>接下来就要在应用服务中埋点收集数据了。在我们的业务服务里面，需要统计的 Metrics 有订单成功次数、订单失败次数和订单金额总数。我们可以通过对订单类中生成的订单号进行埋点来实现。</p><p>在项目中新增 PrometheusCustomMonitor 类，初始化业务 Metric。具体的代码参考如下：</p><pre><code class="language-java">package com.dunshan.prometh.controller;\n\nimport io.micrometer.core.instrument.Counter;\nimport io.micrometer.core.instrument.DistributionSummary;\nimport io.micrometer.core.instrument.MeterRegistry;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Component;\n\nimport javax.annotation.PostConstruct;\n\n/**\n * @author dunshan\n * @program: dunshan-pef\n * @description: 自定义下单接口采集类\n * @date 2021-09-25 10:13:46\n */\n@Component\npublic class PrometheusCustomMonitor {\n\n    /**\n     * 下单次数\n     */\n    private Counter orderCount;\n    /**\n     * 下单失败数\n     */\n    private Counter failureCount;\n    /**\n     * 金额统计\n     */\n    private DistributionSummary amountSum;\n\n\n\n    private final MeterRegistry registry;\n\n    @Autowired\n    public PrometheusCustomMonitor(MeterRegistry registry) {\n        this.registry = registry;\n    }\n\n    @PostConstruct\n    private void init() {\n        //统计下单数\n        orderCount = registry.counter("order_request_count"， "order"， "mall-order");\n        //统计失败数\n        failureCount = registry.counter("order_failure_count"， "order"， "mall-order");\n        //统计金额数据\n        amountSum = registry.summary("order_amount_sum"， "orderAmount"， "mall-order");\n\n    }\n    public Counter getOrderCount() {\n        return orderCount;\n    }\n\n    public Counter getFailureCount() {\n        return failureCount;\n    }\n\n    public DistributionSummary getAmountSum() {\n        return amountSum;\n    }\n\n}\n</code></pre><p>声明了 Prometheus Metric 类之后，只要在业务方直接调用就可以了。这里我们直接在 Controller 层中增加请求进行模拟测试，从而实现对业务的埋点，收集我们想要的 Metrics 数据。</p><ul>\n<li>模拟业务请求</li>\n</ul><p>这里我们模拟调用请求，增加订单成功次数、订单失败次数和订单金额总数。</p><p>具体代码参考如下：</p><pre><code class="language-java">package com.dunshan.prometh.controller;\n\nimport com.dunshan.prometh.popj.Result;\nimport lombok.extern.log4j.Log4j2;\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RestController;\n\nimport javax.annotation.Resource;\nimport java.util.HashMap;\nimport java.util.Random;\n\n/**\n * @author dunshan\n * @program: dunshan-pef\n * @description: 自定义监控采集控制层\n * @date 2021-09-25 21:33:59\n */\n@Log4j2\n@Controller\n@RestController\npublic class IndexPromenthController {\n\n\n\n    @Resource\n    private PrometheusCustomMonitor customMonitor;\n\n\n\n    @GetMapping("/")\n    public Result IndexPage() {\n        HashMap&lt;Object，Object&gt; map = new HashMap&lt;&gt;();\n        map.put("data"， "全链路压测业务监控");\n        return Result.buildSuccess(map);\n    }\n\n\n\n    /**\n     * 模拟订单数据\n     *\n     * @param num\n     * @return\n     */\n    @RequestMapping("/order/{num}")\n    public Result orderTotalNumber(@PathVariable int num) {\n        // 统计下单次数\n        if (num == 1) {\n            customMonitor.getOrderCount().increment();\n        } else {\n            //失败数据\n            customMonitor.getFailureCount().increment();\n        }\n        Random random = new Random();\n        int amount = random.nextInt(108);\n        // 统计金额\n        customMonitor.getAmountSum().record(amount);\n        HashMap&lt;String，Object&gt; map = new HashMap&lt;&gt;();\n        map.put("total"，amount);\n        map.put("time"，System.currentTimeMillis());\n        return Result.buildSuccess(map);\n    }\n\n}\n</code></pre><p>请求完成后，我们在 Prometheus 中查看 Metric 数据，如果数据显示正常，Prometheus TSDB 时序数据库就可以开始定时收集 Metric 数据了。</p><p><img src="https://static001.geekbang.org/resource/image/80/6a/8006a7ddeeb8084b59355f3da9528b6a.png?wh=752x573" alt="图片"></p><p>最后，我们只要在 Prometheus 控制台输入表达式语言 PromQL，就能查询 Metric 相关的数据了。</p><p><img src="https://static001.geekbang.org/resource/image/21/5f/211874f96dcf5f4b8a630028c3a6085f.png?wh=1764x822" alt="图片"></p><ul>\n<li>集成 Grafana 数据展示</li>\n</ul><p>表达式的结果可以通过多种形式展示，我们可以结合 Grafana 的控件将数据显示为图形或者表格，也可由外部系统通过 HTTP API 调用。这里我主要介绍一下通过集成 Grafana 做可视化数据展示的方法。</p><p>第一步，创建数据源：</p><p><img src="https://static001.geekbang.org/resource/image/b7/94/b7a74ceee499717ed32b4cedb00ac094.png?wh=702x858" alt="图片"></p><p>第二步，选择 Prometheus 数据源：</p><p><img src="https://static001.geekbang.org/resource/image/60/13/6026237ce43f4e5e18f00793df3f7313.png?wh=1920x412" alt="图片"></p><p>第三步，配置数据源地址：</p><p><img src="https://static001.geekbang.org/resource/image/38/c7/38b973fa0c059b2f47d5066763d102c7.png?wh=1280x678" alt="图片"></p><p>第四步，测试数据源是否配置成功：</p><p><img src="https://static001.geekbang.org/resource/image/b0/1c/b07fcdae910bdf78d5f14e5e5a54bb1c.png?wh=1012x442" alt="图片"></p><p>第五步，为了添加 Metrics 的图表，选择“创建第一个 dashboard”：</p><p><img src="https://static001.geekbang.org/resource/image/ae/1c/aec7782d843ac42d19e2aa1ca99f071c.png?wh=752x251" alt="图片"></p><p>第六步，添加一个空面板：</p><p><img src="https://static001.geekbang.org/resource/image/6c/7e/6c7bdd194462f41bf2e3e4730e4e1e7e.png?wh=851x376" alt="图片"></p><p>第七步，在 Data source 中选择 Prometheus 数据源，在 Metrics browser 中输入 Metric PromQL 表达式：</p><p><img src="https://static001.geekbang.org/resource/image/fb/54/fbcdeae7cdf12af72cd5bc6117871654.png?wh=1530x658" alt="图片"></p><p>第八步，在右侧控件类型中选择图表控件：</p><p><img src="https://static001.geekbang.org/resource/image/b6/7b/b69b42ba88a3b9f9d9b80e7ddd84bd7b.png?wh=680x752" alt="图片"></p><p>图表创建好之后，我们就可以看到 Metric 数据了。</p><p><img src="https://static001.geekbang.org/resource/image/ff/21/ffc9f0fbc3a110a14ba74bfcc3a1f221.png?wh=1808x936" alt="图片"></p><p>因为需要展示不同的 Metrics，所以我们要重复上面的步骤，增加订单成功次数、订单失败次数、订单金额总数等 Metrics 绑定的图表。</p><p>创建好的可视化看板你可以参考下面这张图：</p><p><img src="https://static001.geekbang.org/resource/image/09/5c/09549d4e834eeb48a478c51854acb65c.png?wh=752x275" alt="图片"></p><p>因为在添加可视化看板之前已经有过几次成功的订单，所以我们可以看到，上面已经有数据显示出来了。为了验证订单失败情况下 Metrics 数据是否能正常显示，我们还是使用 JMeter\b模拟一下不同的业务情况（区分正确订单与失败订单），这样才能验证业务埋点是否成功。</p><p>我们使用 JMeter Throughput Controller 组件分配失败订单与成功订单的比例，脚本配置如下：</p><p><img src="https://static001.geekbang.org/resource/image/e0/2e/e0640db455a88c3dec4ed74bf1d52a2e.png?wh=1740x714" alt="图片"></p><p>执行 JMeter 后，我们通过 Grafana 看板中显示的正常和失败订单的数据，可以知道 demo 的埋点已经成功了：</p><p><img src="https://static001.geekbang.org/resource/image/e7/f1/e7d93ccee8dccd23c26a80d0064f79f1.png?wh=752x275" alt="图片"></p><p>好了，刚才我们在 demo 技术预演项目中完成了业务模块的埋点工作，接下来就要开始真实系统的改造了。针对订单业务的数据，我们还是要用流量标记区分正常流量和压测流量，这样才能更精准地监控业务情况。当然啦，这也方便我们把这些 Metrics 数据提取出来，后续提供给管理层做决策。</p><h2>真实系统改造</h2><p>在对真实系统进行改造之前，我们先梳理一下订单类的代码逻辑，知道在哪里埋点是很重要的。</p><ul>\n<li>创建  Prometheus 数据的 Metric</li>\n</ul><p>首先还是在项目中新增 Prometheus Metric 类：</p><pre><code class="language-java"> /**\n     * 下单次数\n     */\n    private Counter orderCount;\n    /**\n     * 下单失败数\n     */\n    private Counter failureCount;\n    /**\n     * 金额统计\n     */\n    private DistributionSummary amountSum;\n\n\n\n    /**\n     * 影子表订单数据\n     */\n    private Counter shadowOrderCount;\n\n    /**\n     * 影子表下单失败数\n     */\n    private Counter shadowFailureCount;\n    /**\n     * 影子表金额统计\n     */\n    private DistributionSummary shadowAmountSum;\n\n\n\n    /**\n     * Micrometer 是一个抽象类\n     */\n    private final MeterRegistry registry;\n\n\n\n\n\n    @Autowired\n    public PrometheusCustomMonitor(MeterRegistry registry) {\n        this.registry = registry;\n    }\n\n    @PostConstruct\n    private void init() {\n        //统计下单数\n        orderCount = registry.counter("order_request_count"， "order"， "mall-order");\n        //统计失败数\n        failureCount = registry.counter("order_failure_count"， "order"， "mall-order");\n        //统计金额数据\n        amountSum = registry.summary("order_amount_sum"， "orderAmount"， "mall-order");\n\n        //影子表统计下单数\n        shadowOrderCount = registry.counter("shadow_order_request_count"， "order"， "mall-order");\n        //影子表统计失败数\n        shadowFailureCount = registry.counter("shadow_order_failure_count"， "order"， "mall-order");\n        //影子表统计金额数据\n        shadowAmountSum = registry.summary("shadow_order_amount_sum"， "orderAmount"， "mall-order");\n\n\n\n    }\n    public Counter getOrderCount() {\n        return orderCount;\n    }\n\n    public Counter getFailureCount() {\n        return failureCount;\n    }\n\n    public DistributionSummary getAmountSum() {\n        return amountSum;\n    }\n\n    public Counter getShadowOrderCount() {\n        return shadowOrderCount;\n    }\n\n    public Counter getShadowFailureCount() {\n        return shadowFailureCount;\n    }\n\n    public DistributionSummary getShadowAmountSum() {\n        return shadowAmountSum;\n    }\n</code></pre><ul>\n<li>集成 Prometheus SDK</li>\n</ul><p>选择订单系统项目，在项目的 pom 文件中添加相关依赖：</p><pre><code class="language-xml">&lt;dependency&gt;\n  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n  &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n  &lt;groupId&gt;io.micrometer&lt;/groupId&gt;\n  &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre><ul>\n<li>注册定义好的业务 Metric</li>\n</ul><p>在代码中增加如下配置：</p><p><img src="https://static001.geekbang.org/resource/image/f0/2d/f0119f47efc4bd559b2d751517e8942d.png?wh=545x464" alt="图片"></p><ul>\n<li>埋点到业务代码</li>\n</ul><p>接下来，我们开始梳理 mall-order 服务中生成订单类代码的逻辑。我们可以找到控制器接口代码：</p><pre><code class="language-java">@ApiOperation("根据购物车信息生成订单")\n@RequestMapping(value  = "/generateOrder"，method = RequestMethod.POST) \n@ResponseBody  \npublic CommonResult generateOrder(@RequestBody  OrderParam orderParam) { \n    Map&lt;String，Object&gt; result = portalOrderService.generateOrder(orderParam);\n    return CommonResult.success(result， "下单成功");\n}\n</code></pre><p><strong>通过代码调用关系，我们可以很快找到 GenerateOrder 方法。业务 Metric 需要在这里埋点，以便收集 API 请求数据。</strong></p><pre><code class="language-java">/**\n * 根据提交信息生成订单\n */\n@Override\npublic Map&lt;String，Object&gt; generateOrder(OrderParam orderParam) {\n    List&lt;OmsOrderItem&gt; orderItemList = new ArrayList&lt;&gt;();\n    //获取用户名\n    UmsMember currentMember = memberService.getCurrentMember();\n      //获取包含促销活动信息的购物车列表\n    List&lt;CartPromotionItem&gt; cartPromotionItemList = cartItemService.listPromotionnew(currentMember.getId()，orderParam.getCartIds());\n\n   .......\n   ......\n   ......\n\n    //发送延迟消息取消订单\n    sendDelayMessageCancelOrder(order.getId());\n    Map&lt;String，Object&gt; result = new HashMap&lt;&gt;();\n    result.put("order"，order);\n    result.put("orderItemList"，orderItemList);\n    logger.info("结果：" + JSON.toJSON(result));\n    return result;\n}\n</code></pre><p>因为前面我们针对流量隔离进行系统改造的时候，已经区分了正常流量与压测流量，所以在具体进行业务埋点的时候，我们就需要根据 Header 标记区分不同流量来统计对应的 Metric。</p><p>这里我演示一下如何通过 HttpServletRequest 来获取 Header 标记来区分不同的流量（也可以使用 AOP 的方式）。参考代码如下：</p><p><img src="https://static001.geekbang.org/resource/image/0a/25/0a8d24ba801b931727ae73243d597325.png?wh=1374x846" alt="图片"></p><p>好了，完成业务代码埋点之后，我们需要提供 Metrics 接口给 Prometheus TSDB 时序数据库收集数据。</p><ul>\n<li>公布业务 Metrics 访问路径</li>\n</ul><p>修改项目的全局配置文件，公布 Metrics 访问路径。具体配置如下：</p><p><img src="https://static001.geekbang.org/resource/image/39/5a/396c716fde4740557e038cfcccd4205a.png?wh=828x622" alt="图片"></p><p>然后启动项目观察日志，发现已经有 actuator 对应的 URL 路径了。</p><p><img src="https://static001.geekbang.org/resource/image/a7/96/a7cf934993570bb76f2d3eac068ed796.png?wh=1176x600" alt="图片"></p><p>在浏览器中访问地址，可以看到，Metrics 数据就都已经显示出来了。</p><p><img src="https://static001.geekbang.org/resource/image/5b/46/5bf27bc93cf9f226aec415b82yy6dd46.png?wh=752x406" alt="图片"></p><ul>\n<li>配置 Prometheus 拉取数据</li>\n</ul><p>配置 prometheus.yml 配置文件：</p><pre><code class="language-yaml">  - job_name: "dunshan-mall-order"\n    metrics_path: "/actuator/prometheus"\n    static_configs:\n     - targets: ["localhost:8084"]\n</code></pre><p>文件配置好之后，我们重启 Prometheus，参考命令如下：</p><pre><code class="language-bash">$  brew services restart prometheus\nStopping `prometheus`... (might take a while)\n==&gt; Successfully stopped `prometheus` (label: homebrew.mxcl.prometheus)\n==&gt; Successfully started `prometheus` (label: homebrew.mxcl.prometheus)\n</code></pre><p>打开 Prometheus 控制台，可以看到，显示的 metrics_path 和我们预期的一致。</p><p><img src="https://static001.geekbang.org/resource/image/0f/40/0f92e080b768f97ef77353ccdba24840.png?wh=1570x780" alt="图片"></p><p>到这里，我们就把订单系统、业务 Metrics 和 Prometheus 监控系统打通了。做完这些我们的任务就完成 80%了，最后一步就是集成 Grafana ，这样才能做到酷炫的可视化数据展示。</p><ul>\n<li>集成 Grafana 可视化看板</li>\n</ul><p>现在只要打开 Grafana，添加自定义图表绑定 Metrics 即可。</p><p>你可以参考下面的运行效果图和最终效果图：</p><p><img src="https://static001.geekbang.org/resource/image/d4/0f/d434aeb1f688cc1e63956ebb0143e30f.png?wh=1920x1128" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/0b/e3/0bb51c30dca3fbac51801c4098edaee3.png?wh=1920x985" alt="图片"></p><p>好了，到这里我们的真实系统改造就完成了。</p><h2>总结</h2><p>这节课，我们介绍了结合  Prometheus SDK 对 Java 应用的业务埋点改造，我还对这部分细节做了详细的演示。希望可以让你在选择业务级定向监控解决方案时有一些参考。</p><p>通过这节课的学习，我们知道了使用 Prometheus 可以快速搭建监控服务，它还能够非常方便地在应用程序中进行集成。Prometheus允许我们通过多种语言的 SDK 进行应用程序的数据埋点。基于这些 SDK ，我们可以让应用程序快速纳入到 Prometheus 的监控或者开发者自己的监控数据收集程序当中。同时，这些客户端收集的监控数据，不仅仅支持 Prometheus，还能支持其他的监控工具。从实现的难度上来说，这节课不算太复杂，只要有一点代码功底都可以实现，但这却是非常重要的部分。</p><p>我们做性能工作的人经常不擅长用好理解的方式表达业务上的数据，当我们去说 TPS、响应时间、吞吐量、CPU 使用率等测试指标或技术指标时，对非技术行业的业务人员和领导们来说，他们很难理解，也不会有直观的感受。但是如果你说，我们一小时可以处理 2000 万笔下单操作，他们马上就能明白了。这也是我希望借助这节课的内容达到的效果。</p><p>做技术工作的时候，你可以非常细致，但是在和不同团队、不同职位的人沟通的时候，我建议你一定要从对方的角度来思考问题，说对方听得懂的话，这样才能让性能工作的价值体现出来。</p><h2>思考题</h2><p>学完今天的课程，请你思考下面两个问题：</p><ol>\n<li>业务级的定向监控一般会监控哪些 Metrics？你们公司有没有相应的方法来进行监控？</li>\n<li>前面，我们还提到了可以用 AOP 埋点收集数据，但是我并没有给出具体的示例。你知道怎么实现吗？</li>\n</ol><p>欢迎你在留言区与我交流讨论。当然了，你也可以把这节课分享给你身边的朋友，他们的一些想法或许会让你有更大的收获。我们下节课见！</p>',
        article_title: "28 | 定向监控：怎样快速发现业务异常？",
      },
    ],
  },
  {
    chapterTitle: "实践性能场景执行",
    children: [
      {
        title: "29 | 基准场景：一个案例，带你搞懂基准场景的关键要点",
        id: 471368,
        content:
          '<p>你好，我是高楼。</p><p>学到这里，我们可以小小地庆祝一下了。因为在前面的28讲里，我已经把全链路压测所需要的前提条件全部都梳理了一遍。从这节课开始，我们就正式进入场景执行阶段了。</p><p>在我的RESAR性能工程理念中，我一直强调只有四种性能场景类型。分别是：基准场景、容量场景、稳定性场景和异常场景。这四种场景是递进的关系，每一类场景都是有清晰的目标的。这一节课，我们就是要先整体地了解一下这四种性能场景的目标，然后进行具体的演示。</p><h2>一个场景设计的问题</h2><p>在说每个场景的目标之前，我要先说一个问题，这是一个同行问我的。我觉得这是性能行业中非常典型的疑问，也是现在性能行业中经常有人做错的一个地方。</p><blockquote>\n<p>你好，我想请教一个关于TPS压测指标的问题。我们的生产环境，一个场景的系统TPS在高峰期达到了4000/s，但是就持续了几秒，按小时统计最大也不到30w业务量。业务人员给我们的业务指标是75w/h，那在实际压测的时候，如何兼顾两者的指标呢？如果全部按峰值TPS去跑一个小时，感觉太高了，不切实际。这种情况该如何评估压测呢？</p>\n</blockquote><p>我们可以来分析一下这几句话中的关键词：</p><ol>\n<li>4000TPS持续了几秒（这里我把问题中的4000直接当成业务级的事务来看了，因为他并没有描述清楚这个4000是哪一级的事务）；</li>\n<li>真实环境下，30w笔业务每小时；</li>\n<li>业务部门给的需求是，系统可以支持每小时75w笔的业务量。</li>\n</ol><!-- [[[read_end]]] --><p>第2、3点的区别是，一个是真实的每小时业务量，另一个是新提的需求。因为新提的需求要高于真实的每小时业务量，所以我们就以这个更高的要求作为场景目标。</p><p>那这时我们容易出现的疑问是：如果每个小时的业务量增加了一倍多，生产峰值TPS会不会增加呢？这个问题，我们从上面的描述中还看不出来，所以也没办法判断。因为75w/h的需求如果平均算过来，每秒大约只有208，这个值远远小于生产上真实达到过的峰值。</p><p>所以我们还是拿之前的4000TPS来要求TPS峰值，但是这里要记录一下，有需要的话再去和业务部门沟通。</p><p>那我们再整理一下需求就是：</p><ol>\n<li>4000TPS：这是一个明确的需求。但是也要注意，这是真实的业务模型所要求的TPS，这里会包括多少业务接口，我们还不清楚。</li>\n<li>75w/h：这也是一个明确的需求。但是，我们还不确定是否会因此导致峰值TPS的增加，既然不知道，那我们就先设计场景满足这个75w/h的容量需求。至于峰值TPS，我们先按4000TPS来计算。</li>\n</ol><p>明确了这个需求之后，我们还不知道要设计什么样的场景来覆盖这个需求对不对？要想知道这点，我们先来看看每个场景的具体要求，再对应上述问题，看看如何设计可用的场景。</p><h2>四类性能场景解析</h2><h3>基准场景</h3><p>基准场景的目标是把每个接口都调优到最好的状态。这里我们要把每个接口都压测一遍，以便将基础的问题呈现出来。很显然，基准场景是不考虑真实的业务模型的，只是为了排除基础问题。</p><p>说明一下，这里提到的接口是原子类的接口，是业务流程中涉及到的一个个单独的接口。</p><h3>容量场景</h3><p>容量场景的目标是模拟真实的业务峰值。容量场景是性能项目中最重要的场景之一，要给出线上环境是否能满足需求的<strong>明确的答案</strong>。</p><p>在设计容量场景的压力过程时，要注意最核心的两点，那就是<strong>场景的连续递增设计和业务模型。</strong></p><p>你也许会问，那容量场景的持续时间应该设置为多长呢？通常情况下，我们对容量场景的持续时间是不做特别限制的，具体的时间长度要根据实际的情况做出判断。比如说，场景TPS上升到最高点之后持续了几分钟，这时你看到TPS有下降的趋势，那就必然需要再多等一会，看一下性能衰减有多快。</p><p>那话说回来，对应上面那个问题的第一个需求（4000TPS），显然这个容量场景是一定要做的，对应的目标就是4000TPS。</p><h3>稳定性场景</h3><p>稳定性场景的核心是能够覆盖业务积累量，它的<strong>两个要点</strong>就是<strong>时长和业务积累量。</strong></p><p>这两个要点之间是有关系的，用固定TPS值来执行稳定性的场景时，业务积累量和时长是线性的关系。对应上面的问题中的需求，如果容量场景中的4000TPS能够稳定达到的话，那显然75w的业务积累量只需要3.125分钟（750000笔业务 ÷ 4000TPS = 187.5s）就可以完成了。</p><p>那可不可以用这样的逻辑来计算TPS呢？750000笔业务 ÷ 3600s（即一小时）≈ 208.3TPS，也就是用208.3TPS运行一个小时来达到业务积累量。</p><p>从业务积累量上来说，显然，这两者是没有区别的，但区别在于TPS的量级和时长。这个区别在技术层面会产生什么影响呢？</p><p>我们先假设在这两个场景中用到的参数化数据都是一样的。那4000TPS和208TPS的区别就在于，在单位时间内发送了多少请求到服务端。当请求较为集中的时候（前面我们已经说了4000TPS是可以达到的），服务器上需要开启的工作线程资源、内存资源、CPU资源、网络资源等显然都需要得更多。</p><p>不过这里我还想提一下，其实CPU、网络、工作线程类的资源和容量场景中的要求是一样的，但是内存会有不同。在容量场景中，因为跑得时间较短，服务器端或数据库层不需要那么多的内存资源。但在稳定性场景中呢，就需要更多的内存来保存临时数据了，因为参数化数据的量级在稳定性场景中是更多的。</p><h3>异常场景</h3><p>异常场景的目标是在出现故障（操作系统、数据库、中间件、进程等等故障）时，验证业务受到的影响。异常场景就是为了模拟生产故障出现时的场景，以此来验证业务系统能不能自动调整。很多企业中把它归结为“非功能测试”的范畴。这一点如果你想了解更多，可以参考一下我上一个专栏的<a href="https://time.geekbang.org/column/article/377229">第28讲</a>。</p><p>所以针对前面我们提到的需求，显然容量场景和稳定性场景都是必须要做的。那基准场景就不用考虑了吗？直接上容量场景和稳定性场景就够了吗？</p><p>其实没有基准场景是不行的。但是，因为上面的需求中只描述了容量场景和稳定性场景的需求，所以有些人会觉得可以跳过基准场景。但是，基准场景作为可以发现通用性能问题的阶段，在性能问题的定位上起着不可忽视的作用。所以，当有了容量场景的需求的时候，你就应该默认基准场景是必须要做的了。</p><p>描述了这么多场景设计的内容，针对我们的这个项目，后面我们自然是要把四类场景都执行起来的。</p><p>在这节课中，我们一起先来看一下基准场景是怎么跑起来的。</p><p>要想把场景跑起来，自然先要有压力数据。在全链路压测的逻辑中，经常会有人提到一种先进炫酷的技术那就是用流量录制回放，那这里我们就来用它产生基准场景的压力。</p><p>不过这里有个小的难点。因为我们这个是自己搭建的项目，并没有真实的流量，而GoReplay 可以做的又只是录制回放，那GoReplay要录制的真实流量怎么产生呢。我的逻辑是：</p><ol>\n<li>先用 JMeter 模拟真实用户的请求。</li>\n<li>在 Gateway 上用 GoReplay 录制请求。</li>\n<li>将录制的请求放大回放。</li>\n</ol><p>我们就按照这个步骤来做一下。</p><h2>准备工作</h2><h3>使用Go录制回放</h3><p>首先，我们要安装一下 Go 语言环境和 GoReplay ：</p><pre><code class="language-java">下载go语言包。解压。\n# tar -C /usr/local -zxvf go1.14.4.linux-amd64.tar.gz\n\n# 打开环境变量文件\nvim /etc/profile\n# 添加\nexport GOROOT=/usr/local/go\nexport PATH=$PATH:$GOROOT/bin\n# 生效\nsource /etc/profile\n</code></pre><p>下载GoReplay包。</p><pre><code class="language-java">[root@vm ~]# curl -L -O https://github.com/buger/goreplay/releases/download/v1.3.1/gor_1.3.1_x64.tar.gz\n</code></pre><p>更详细的安装步骤可以参考：<a href="https://mp.weixin.qq.com/s/O66d9hwIw4L7Gmgvmm_8UA">《性能工具之 Goreplay 安装及入门使用》</a>。<br>\n然后，我们用JMeter 把100个线程运行起来。JMeter配置如下：</p><p><img src="https://static001.geekbang.org/resource/image/f2/30/f291fd4f7829277373e1dc670f772630.png?wh=1910x966" alt="图片"></p><p>这里，我把 Ramp-up period 配置为300秒，这是为了实现递增的过程，从而模拟生产环境中的递增趋势。请注意，在真实环境中是不需要这样做的。在真实环境中，我们直接在要录制的网关上执行GoReplay录制的动作就可以了。</p><p>接下来，我们使用GoReplay录制回放请求，这里又分了几个小步骤。</p><ol>\n<li>创建录制脚本。</li>\n</ol><pre><code class="language-java">[root@gateway-mall-gateway-764bdbcc94-qdp5k /]# cat record.sh \n\n#!/bin/bash\n\nPORT="8201"\nOUT_FILE="request.gor"\nHEADER="dunshan:7DGroup"\n\n./gor --input-raw :$PORT --output-file=$OUT_FILE  -output-file-append --http-set-header $HEADER\n\n[root@gateway-mall-gateway-764bdbcc94-qdp5k /]#\n</code></pre><p>在上面的脚本中，“PORT”是我们要录制的端口，“OUT_FILE”是保存的文件名。“HEADER”是在录制的过程中要添加到请求中的HTTP Header，这样在回放的时候，我们就可以直接根据这个Header进行判断了。</p><ol start="2">\n<li>执行./record.sh启动录制，这样在同目录下会生成request.gor文件。</li>\n<li>将request.gor复制到压力机上。</li>\n<li>创建GoReplay压力脚本run.sh，脚本内容如下：</li>\n</ol><pre><code class="language-java">#!/bin/bash\n\nOUTPUT="http://10.96.136.36:8201"\nMIDDLEWARE="./middleware_wrapper.sh"\n#MIDDLEWARE="./middleware_echo.py"\nINPUT_FILE="request.gor"\n\nsudo ./gor --input-file $INPUT_FILE --input-file-loop  --output-http=$OUTPUT --prettify-http --output-http-track-response\n</code></pre><p>在上面这段脚本当中，“OUTPUT”是目标服务器的URL路径；“MIDDLEWARE”是用来扩展GoReplay功能的中间件，我们可以根据需要自己编写，像关联这样重要的功能就要在这一步实现（详见本专栏第21-22讲）；“INPUT_FILE”是在录制过程中生成的文件。</p><ol start="5">\n<li>执行run.sh。</li>\n</ol><p>我们先只执行一遍，查看下调用的内容是不是正常。</p><p>先看订单有没有生成，影子库的截图是这样的：<br>\n<img src="https://static001.geekbang.org/resource/image/fa/86/faa79a847a519170af569021a0001586.png?wh=892x271" alt="图片"><br>\n从影子库的最近一条订单时间来看，确实是产生了新的订单，说明执行是成功的。</p><p>下面再来看看日志，确定一下执行的过程也是没问题的。Auth服务上的日志：</p><pre><code class="language-java">2021-10-24 11:22:56.464  INFO [mall-auth,ddcce4f1b50707d1,1551de5039a85bf9,false] 1 --- [nio-8401-exec-1] c.d.mall.auth.aop.db.DataSourceAspect    : ==============redis=mall.auth==请求内容===============\n2021-10-24 11:22:56.464  INFO [mall-auth,ddcce4f1b50707d1,1551de5039a85bf9,false] 1 --- [nio-8401-exec-1] c.d.mall.auth.aop.db.DataSourceAspect    : 请求地址:http://10.100.220.76:8401/oauth/token\n2021-10-24 11:22:56.464  INFO [mall-auth,ddcce4f1b50707d1,1551de5039a85bf9,false] 1 --- [nio-8401-exec-1] c.d.mall.auth.aop.db.DataSourceAspect    : 请求方式:postAccessToken\n2021-10-24 11:22:56.465  INFO [mall-auth,ddcce4f1b50707d1,1551de5039a85bf9,false] 1 --- [nio-8401-exec-1] c.d.mall.auth.aop.db.DataSourceAspect    : 请求类方法头信息:7DGroup\n2021-10-24 11:22:56.465  INFO [mall-auth,ddcce4f1b50707d1,1551de5039a85bf9,false] 1 --- [nio-8401-exec-1] c.d.mall.auth.aop.db.DataSourceAspect    : redis 压测流量：5\n2021-10-24 11:22:56.465  INFO [mall-auth,ddcce4f1b50707d1,1551de5039a85bf9,false] 1 --- [nio-8401-exec-1] c.d.mall.auth.aop.db.DataSourceAspect    : ============My===请求内容===============\n2021-10-24 11:22:56.465  INFO [mall-auth,ddcce4f1b50707d1,1551de5039a85bf9,false] 1 --- [nio-8401-exec-1] c.d.mall.auth.aop.db.DataSourceAspect    : 请求地址:http://10.100.220.76:8401/oauth/token\n2021-10-24 11:22:56.465  INFO [mall-auth,ddcce4f1b50707d1,1551de5039a85bf9,false] 1 --- [nio-8401-exec-1] c.d.mall.auth.aop.db.DataSourceAspect    : 请求方式:POST\n2021-10-24 11:22:56.465  INFO [mall-auth,ddcce4f1b50707d1,1551de5039a85bf9,false] 1 --- [nio-8401-exec-1] c.d.mall.auth.aop.db.DataSourceAspect    : 请求类方法:CommonResult com.dunshan.mall.auth.controller.AuthController.postAccessToken(Principal,Map)\n2021-10-24 11:22:56.465  INFO [mall-auth,ddcce4f1b50707d1,1551de5039a85bf9,false] 1 --- [nio-8401-exec-1] c.d.mall.auth.aop.db.DataSourceAspect    : 请求类方法参数:[org.springframework.security.authentication.UsernamePasswordAuthenticationToken@f00b9e84: Principal: org.springframework.security.core.userdetails.User@ff46140: Username: portal-app; Password: [PROTECTED]; Enabled: true; AccountNonExpired: true; credentialsNonExpired: true; AccountNonLocked: true; Not granted any authorities; Credentials: [PROTECTED]; Authenticated: true; Details: null; Not granted any authorities, {password=123456, grant_type=password, client_secret=123456, client_id=portal-app, username=7dTest005}]\n2021-10-24 11:22:56.465  INFO [mall-auth,ddcce4f1b50707d1,1551de5039a85bf9,false] 1 --- [nio-8401-exec-1] c.d.mall.auth.aop.db.DataSourceAspect    : 请求类方法头信息:7DGroup\n2021-10-24 11:22:56.465  INFO [mall-auth,ddcce4f1b50707d1,1551de5039a85bf9,false] 1 --- [nio-8401-exec-1] c.d.mall.auth.aop.db.DataSourceAspect    : Mysql 压测流量\n2021-10-24 11:22:56.469 DEBUG [mall-auth,ddcce4f1b50707d1,1551de5039a85bf9,false] 1 --- [nio-8401-exec-1] c.d.mall.auth.aop.db.DataSourceAspect    : redis switch 0 to 5\n</code></pre><p>Portal日志：</p><pre><code class="language-java">2021-10-24 11:22:56.403  INFO [mall-portal,1bd396aebbbcc2b7,98336b0b0e15d0ba,false] 1 --- [nio-8085-exec-9] c.d.mall.portal.aop.db.DataSourceAspect  : ===============请求内容===============\n2021-10-24 11:22:56.403  INFO [mall-portal,1bd396aebbbcc2b7,98336b0b0e15d0ba,false] 1 --- [nio-8085-exec-9] c.d.mall.portal.aop.db.DataSourceAspect  : 请求地址:http://10.96.136.36:8201/home/content\n2021-10-24 11:22:56.403  INFO [mall-portal,1bd396aebbbcc2b7,98336b0b0e15d0ba,false] 1 --- [nio-8085-exec-9] c.d.mall.portal.aop.db.DataSourceAspect  : 请求方式:GET\n2021-10-24 11:22:56.403  INFO [mall-portal,1bd396aebbbcc2b7,98336b0b0e15d0ba,false] 1 --- [nio-8085-exec-9] c.d.mall.portal.aop.db.DataSourceAspect  : 请求类方法:CommonResult com.dunshan.mall.portal.controller.HomeController.content(Integer,Integer)\n2021-10-24 11:22:56.403  INFO [mall-portal,1bd396aebbbcc2b7,98336b0b0e15d0ba,false] 1 --- [nio-8085-exec-9] c.d.mall.portal.aop.db.DataSourceAspect  : 请求类方法参数:[4, 1]\n2021-10-24 11:22:56.403  INFO [mall-portal,1bd396aebbbcc2b7,98336b0b0e15d0ba,false] 1 --- [nio-8085-exec-9] c.d.mall.portal.aop.db.DataSourceAspect  : 请求类方法头信息:7DGroup\n........................\n</code></pre><p>Order日志：</p><pre><code class="language-java">2021-10-24 11:22:57.015  INFO [mall-order,8f7b6e9d44c60348,85b18558eb463fc5,false] 1 --- [nio-8084-exec-5] c.d.mall.order.aop.db.DataSourceAspect   : ===============请求内容===============\n2021-10-24 11:22:57.015  INFO [mall-order,8f7b6e9d44c60348,85b18558eb463fc5,false] 1 --- [nio-8084-exec-5] c.d.mall.order.aop.db.DataSourceAspect   : 请求地址:http://10.96.136.36:8201/order/generateConfirmOrder\n2021-10-24 11:22:57.015  INFO [mall-order,8f7b6e9d44c60348,85b18558eb463fc5,false] 1 --- [nio-8084-exec-5] c.d.mall.order.aop.db.DataSourceAspect   : 请求方式:POST\n........................\n</code></pre><p>从上面的日志内容来看，每个服务中的日志都显示正确。</p><p>下面我们再来看一下Zipkin中的调用树，确认一下是不是已经把设置的Header加到标签中去了。</p><p><img src="https://static001.geekbang.org/resource/image/48/10/48f25bc20c40247eb244be6fc3a50210.png?wh=1909x638" alt="图片"></p><p>从Zipkin来看，请求确实调到了不同的脚本上，结果也是正确的。</p><ol start="6">\n<li>循环多遍执行run.sh，查看网关上的回放日志。</li>\n</ol><pre><code class="language-java">2021-10-24 11:10:36.710  INFO [mall-gateway,8313cfa404164710,8313cfa404164710,false] 1 --- [undedElastic-13] c.dunshan.mall.filter.AuthGlobalFilter   : dunshan-流量标识：7DGroup\n2021-10-24 11:10:36.710  INFO [mall-gateway,8313cfa404164710,8313cfa404164710,false] 1 --- [undedElastic-13] c.dunshan.mall.filter.AuthGlobalFilter   : AuthGlobalFilter.filter() user:{"user_name":"7dTest005","scope":["all"],"id":2434424,"exp":1635129516,"authorities":["前台会员"],"jti":"8368728f-79db-4bff-9afc-c63d15247257","client_id":"portal-app"}\n2021-10-24 11:10:36.837  INFO [mall-gateway,c6efe4825748f2d4,c6efe4825748f2d4,false] 1 --- [undedElastic-13] c.dunshan.mall.filter.AuthGlobalFilter   : dunshan-流量标识：7DGroup\n2021-10-24 11:10:36.837  INFO [mall-gateway,c6efe4825748f2d4,c6efe4825748f2d4,false] 1 --- [undedElastic-13] c.dunshan.mall.filter.AuthGlobalFilter   : AuthGlobalFilter.filter() user:{"user_name":"7dTest005","scope":["all"],"id":2434424,"exp":1635129516,"authorities":["前台会员"],"jti":"8368728f-79db-4bff-9afc-c63d15247257","client_id":"portal-app"}\n........................\n</code></pre><p>从日志上看，系统已经按照预期执行了相应的请求，压测标签在日志中也都是对的。</p><h2>执行基准场景</h2><p>上面的准备工作都完成了之后，我们就可以放大更多倍数进行回放，执行基准场景了。还记得我们基准场景的目标吧？主要是为了找到系统中的各种单接口的问题。</p><h3>压力场景数据</h3><p>我们先来模拟访问首页这个基准场景。首页是最为常用的，所以也非常重要。我们先来看看首页的访问路径。</p><p><img src="https://static001.geekbang.org/resource/image/39/4f/398931392d57696834c3f5294101154f.png?wh=1920x284" alt="图片"></p><p>图中这个结构就是访问首页时的路径了，你也可以把它看成是逻辑图。因为这个架构并不复杂，所以RESAR性能分析七步法中的第二步“分析架构图”就可以省了。</p><p>下面，我们根据GoReplay录制回放的逻辑，准备好相关的脚本，使用Goreplay放大100倍执行首页基准场景。脚本如下：</p><pre><code class="language-java">[root@s5 ~]# cat run.sh\n#!/bin/bash\n\nOUTPUT="http://10.96.136.36:8201"\n#MIDDLEWARE="./middleware_wrapper.sh"\nINPUT_FILE="request-mall-home.gor|10000%"\nsudo ./gor_ldd --input-file $INPUT_FILE --input-file-loop --output-http=$OUTPUT --prettify-http --output-http-track-response --stats --output-http-stats  -output-http-compatibility-mode true\n</code></pre><p>可以看到，上面的脚本中，“10000%”就是放大的倍数，这里是指100倍。</p><p>接着我们执行run.sh发起压力，监控结果如下：</p><p><img src="https://static001.geekbang.org/resource/image/1c/fa/1c9f5fba57413539c64db91b99fc39fa.png?wh=1833x643" alt="图片"></p><p>从这张监控截图中我们可以看到，一开始响应时间较高，达到了500ms，这时候TPS只有50；随着压力的持续，响应时间降到了200ms左右，TPS也增加到了200左右。</p><p>但是200TPS算是好还是坏呢？根据我在上一个专栏的经验，我们对这个系统优化之后，这个首页的TPS在单个容器上达到过1200，而现在只有200，这显然是不能接受的。</p><p>这里我们可以先停一下，不要着急去猜问题的答案。回想我们的基准场景的目标，就是要找到单接口的性能问题，而现在我们已经从压力数据判断出这个接口的性能是有问题的了。那下一步要做什么事情呢？</p><p>请注意，我说的是“下一步”。<strong>现在你要做的就是静下心来，不要慌张，想清楚“下一步”，这恰恰是最重要的。</strong></p><p>我带领团队做性能分析的时候经常一再强调，做性能分析并不是直接给出性能瓶颈的答案，而是给出性能分析的逻辑。有了逻辑自然会找到性能瓶颈的证据链，也自然会找到性能瓶颈的根本原因。所以在分析性能瓶颈的过程中，知道答案并不是首要的，只要知道“下一步”做什么具体的动作，自然可以做到心中不慌乱。</p><p>好了，现在我们回想一下RESAR性能分析七步法。</p><p><img src="https://static001.geekbang.org/resource/image/08/61/0835e35c31f93266baebe987b57c9c61.jpg?wh=1920x592" alt="图片"></p><p>显然，前两步“压力场景数据”和“分析架构图”我们已经看过了。下一步就是去“拆分响应时间”了。但是鉴于我们这个首页架构非常简单，只有 gateway-portal-mysql 三个环节，而首页的逻辑就是在portal上，至于mysql慢不慢，我们也很容易从portal上做出判断，所以这一步也不用花太多时间。</p><p>接下来，我们可以直接去全局监控数据中看一看有什么不合理的地方。</p><h3>全局监控</h3><p>全局监控数据如下：</p><p><img src="https://static001.geekbang.org/resource/image/e6/7b/e66a5d50fc9a809a0fb7a2743d86d37b.png?wh=1788x352" alt="图片"></p><p>通过这张截图我们发现，Portale服务所在的s7工作节点的CPU已达到97%了。但是因为这个机器是一个只有2C的虚拟机，配置过低，稍微有一点压力就会达到很高的CPU消耗，何况我们也达到了200TPS，所以暂时不分析原因，而是把服务移到配置高一点的机器上再接着观察。</p><p>下面我们把Portal服务移到一个8C16G的虚拟机上。再次查看压力数据。</p><p><img src="https://static001.geekbang.org/resource/image/e1/62/e1b292e1bf832a8e5b375c3d7e599162.png?wh=1838x647" alt="图片"></p><p>从这张趋势图中我们看到，TPS可以达到550左右。因为这个接口只需要300多的TPS，而且根据经验，压力越大的时候TPS增加得越慢。所以我们可以限制资源到4C8G，再来看一下能不能达到300TPS。限制资源配置如下：</p><p><img src="https://static001.geekbang.org/resource/image/0c/5f/0c3c763c92e5d118db3e22f04d8a735f.png?wh=708x184" alt="图片"></p><p>这里出现一个了插曲，在保存资源限制时出现了报错：节点没有足够的CPU可用。报错信息是下面这个样子：</p><p><img src="https://static001.geekbang.org/resource/image/71/ea/71e96dd123e2bdd67d6fcaafee6644ea.png?wh=329x320" alt="图片"></p><p>为了搞清楚这个问题的原因，我们去查看了这个 Portal 所在的 Worker 上的服务，发现这个Worker上还有其他服务。这个Worker上的资源分配情况是，虽然资源使用率都很低，但是CPU已经被分配出去了。所以当我们想分配4C8G的资源给Portal服务时，显示了上面的错误。</p><p>为了避免服务之间的资源争用，我们把这个Portal服务移到一个没有任何服务的Worker上去了。这时，我们添加了一个8C16G的新Worker节点，单独运行Portal服务。再次启动压力：</p><p><img src="https://static001.geekbang.org/resource/image/22/5d/220a8a14854b6dbce570717dyy950d5d.png?wh=1920x863" alt="图片"></p><p>看起来已经很美好了：TPS能达到700左右，响应时间在15毫秒，并且抖动也不大。我们再查看一下全局监控：</p><p><img src="https://static001.geekbang.org/resource/image/76/49/764617ac7b4ebca74bfc57c0f26a3e49.png?wh=1920x405" alt="图片"></p><p>从图上可以看出，s6消耗的资源多。s6是一个2C的机器，我们先进去看一下是哪个进程导致的CPU消耗过高。</p><pre><code class="language-java">[root@s6 ~]# top\ntop - 18:05:53 up 75 days, 23:20,  2 users,  load average: 9.16, 7.58, 4.12\nTasks: 198 total,   2 running, 196 sleeping,   0 stopped,   0 zombie\n%Cpu(s): 69.7 us, 12.1 sy,  0.0 ni, 12.1 id,  0.0 wa,  0.0 hi,  6.1 si,  0.0 st\nKiB Mem :  8008972 total,   192056 free,  5909516 used,  1907400 buff/cache\nKiB Swap:        0 total,        0 free,        0 used.  1797212 avail Mem\n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n31214 root      20   0 4681832 612756   7152 S 125.0  7.7 365:49.67 java\n  956 root      20   0 1058124  90796   6636 S  25.0  1.1   6274:30 dockerd\n  954 root      20   0 2465836 152556  20364 S   6.2  1.9   5786:27 kubelet\n 4310 root      20   0 4617612 546260   6820 S   6.2  6.8 201:44.12 java\n 9241 root      20   0  162152   2268   1544 R   6.2  0.0   0:00.01 top\n</code></pre><p>通过PID 31214对应的容器，我们知道问题出在 Gateway：</p><pre><code class="language-java">[root@s6 ~]# docker ps |grep gateway\n8a4bf818dfab        registry.cn-beijing.aliyuncs.com/7d-mall/mall-gateway   "java -Dcom.sun.mana饪?"   13 days ago         Up 13 days                              k8s_mall-gateway_gateway-mall-gateway-7cf7868798-fc2gr_default_7c55578b-7e1d-4c58-a58e-f347caa3bbdc_0\n25dbe722defd        registry.aliyuncs.com/k8sxio/pause:3.2                  "/pause"                 13 days ago         Up 13 days                              k8s_POD_gateway-mall-gateway-7cf7868798-fc2gr_default_7c55578b-7e1d-4c58-a58e-f347caa3bbdc_0\n[root@s6 ~]# docker inspect --format "{{.State.Pid}}" 8a4bf818dfab\n31214\n[root@s6 ~]#\n</code></pre><p>因为我们希望Gateway能支撑更大的转发能力。而s6只是一个2C的虚拟机，在这个TPS下表现已经很不错了，没有太大的可操作空间了。所以我们选择直接增加一个Gateway的节点，并且把它调度到其他的Worker上去：</p><pre><code class="language-java">[root@s5 ~]# kubectl get pods -n default -o wide|grep gateway\ngateway-mall-gateway-69dc766c9d-gbvbb        1/1     Running     0          31s     10.100.219.253   s11    &lt;none&gt;           &lt;none&gt;\ngateway-mall-gateway-69dc766c9d-n828c        1/1     Running     0          24m     10.100.59.84     s12    &lt;none&gt;           &lt;none&gt;\n[root@s5 ~]#\n</code></pre><p>查看一下压力数据：</p><p><img src="https://static001.geekbang.org/resource/image/6b/e8/6bd30c8d1402bc90cca82de7a1b13ee8.png?wh=1920x959" alt="图片"></p><p>这时候，TPS已经可以达到1100了，响应时间在10ms左右。现在，我们来判断一下瓶颈在哪里，查看全局监控数据：</p><p><img src="https://static001.geekbang.org/resource/image/1d/68/1ddfec2a58acc4a29272e44a3fe39568.png?wh=1920x408" alt="图片"></p><p>现在已经没有明显的系统资源占用太高的情况了。但是只看这一层还不够，我们还要看一下Service级别的资源。我们要根据这个接口的路径，一层层去找瓶颈点在哪里，当查到mysql服务的资源时，我看到了下面的信息：</p><p><img src="https://static001.geekbang.org/resource/image/9a/05/9a70a013251bffd236bb075c4d82bc05.png?wh=1920x509" alt="图片"></p><p>恼火得很，这里只分配了512M的内存，但申请的内存已经超过了660M，这个限制就有点过狠了。</p><p><img src="https://static001.geekbang.org/resource/image/04/96/04f8eeb10ffc1bb4d3yya8644e733196.png?wh=880x232" alt="图片"></p><p>所以我们就得来调整一下资源了。放开mysql的资源限制，让他可以用到8C16G的资源。再压一遍看看：</p><p><img src="https://static001.geekbang.org/resource/image/78/e2/7802e5704e8568b7b841yy60b6e10ee2.png?wh=1920x862" alt="图片"></p><p>压力数据看起来和上一轮一样，还是得再看一下全局监控：</p><p><img src="https://static001.geekbang.org/resource/image/dc/24/dce003c93798a69d1b30deaef7a93a24.png?wh=1825x321" alt="图片"></p><p>从这张全局监控的截图里，我们已经看不出有什么明显的问题了。但我们在<a href="https://time.geekbang.org/column/article/468591">第26讲</a>中说过这个界面是有缺陷的，所以还是要登录到各个主机上查看一下是否有优化点。</p><h3>定向监控</h3><p>下面我们来看下定向监控。我们要到每个机器里去执行一下top（我之所以喜欢喜欢用top是因为top给出的CPU数据是非常明确且完整的，而CPU又是非常综合的计数器，所以我习惯性地会进到系统中执行一下top，看一下每颗CPU的情况）。</p><p>在每个机器里都执行了一遍top之后，我发现了这样的情况。</p><pre><code class="language-java">%Cpu0  : 29.4 us,  5.8 sy,  0.0 ni, 64.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu1  : 21.2 us,  4.4 sy,  0.0 ni, 73.0 id,  0.0 wa,  0.0 hi,  1.4 si,  0.0 st\n%Cpu2  : 31.8 us,  5.5 sy,  0.0 ni, 62.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu3  : 19.6 us,  5.1 sy,  0.0 ni, 74.3 id,  0.0 wa,  0.0 hi,  1.0 si,  0.0 st\n%Cpu4  : 26.2 us,  5.5 sy,  0.0 ni, 68.2 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu5  : 21.3 us,  3.8 sy,  0.0 ni, 73.9 id,  0.0 wa,  0.0 hi,  1.0 si,  0.0 st\n%Cpu6  : 27.6 us,  5.1 sy,  0.0 ni, 53.1 id,  0.0 wa,  0.0 hi, 54.8 si,  0.0 st\n%Cpu7  : 28.6 us,  3.4 sy,  0.0 ni, 67.0 id,  0.0 wa,  0.0 hi,  1.0 si,  0.0 st\n</code></pre><p>一看到这种情况我就怀疑是软中断，这是我最近经常会遇到的问题（如果你不了解这个分析逻辑，可以查看一下上一个专栏的<a href="https://time.geekbang.org/column/article/365426">第14讲</a>。）我去查了一下这个机器所用的网络队列：</p><p><img src="https://static001.geekbang.org/resource/image/14/b7/1490f8fc7a8cb2040ddbe06c23e8c4b7.png?wh=602x114" alt="图片"></p><p>显然，是单队列网卡了。但是这是一个虚拟网卡。</p><p>当前用的是Calico的IPIP模式，查看Calico源码之后，我发现IPIP模式有局限，只能使用单队列网卡，那就没招只能换网络插件了。于是我把网络插件换成了Flannel。接着压起来，看一下效果：</p><p><img src="https://static001.geekbang.org/resource/image/fd/e1/fd3bdec786f86e292e8c2d73e67423e1.png?wh=1835x673" alt="图片"></p><p>看起来有效果哦，同样的场景，已经达到了1250TPS。再来看看软中断：</p><pre><code class="language-java">%Cpu0  : 38.2 us, 14.2 sy,  0.0 ni, 39.2 id,  0.0 wa,  0.0 hi,  8.3 si,  0.0 st\n%Cpu1  : 26.4 us, 13.9 sy,  0.0 ni, 58.3 id,  0.0 wa,  0.0 hi,  1.4 si,  0.0 st\n%Cpu2  : 40.5 us, 13.9 sy,  0.0 ni, 35.0 id,  0.0 wa,  0.0 hi, 10.5 si,  0.0 st\n%Cpu3  : 32.9 us, 12.1 sy,  0.0 ni, 52.6 id,  0.0 wa,  0.0 hi,  2.4 si,  0.0 st\n%Cpu4  : 40.7 us, 14.5 sy,  0.0 ni, 31.6 id,  0.0 wa,  0.0 hi, 13.1 si,  0.0 st\n%Cpu5  : 33.0 us, 11.7 sy,  0.0 ni, 52.9 id,  0.0 wa,  0.0 hi,  2.4 si,  0.0 st\n%Cpu6  : 39.7 us, 14.8 sy,  0.0 ni, 36.2 id,  0.0 wa,  0.0 hi,  9.3 si,  0.0 st\n%Cpu7  : 31.8 us, 11.1 sy,  0.0 ni, 55.4 id,  0.0 wa,  0.0 hi,  1.7 si,  0.0 st\n</code></pre><p>这里我们看到，si已经能用到四颗CPU了。效果还是明显的。</p><p>到这里，定向监控分析也就完成了。</p><h2>总结</h2><p>好了，这节课就讲到这里。这是我们这个专栏中关于场景的第一课。执行场景是压测项目的核心环节，它不仅把前面准备的所有内容都用起来了，还是性能分析的起点。</p><p>在这一节课里，我描述了各类场景的目标，也给了你一个案例，告诉你如何去分析性能需求并设计出合适的场景。</p><p>在这节课所讲的基准场景中，我们的目标就是把所有的接口都调试到最好的状态。这一步在全链路压测中是非常重要的环节，我不建议你跳过。</p><p>刚才，我们只通过对一个接口的基准场景模拟，就发现了Calico IPIP模式单网卡软中断的问题。像这样的问题，如果直接执行容量场景再去做判断，会因为容量场景里的接口过于复杂而导致你需要查更多的计数器，分析成本和判断成本都会高出很多。</p><p>当然，如果艺高人胆大，你也可以直接执行容量场景，那样的话，出现了多个性能问题会交织在一起，就只能凭借技术功底做判断了。我不建议你这样做。</p><h2>课后题</h2><p>学完这节课，我想请你思考两个问题：</p><ol>\n<li>你在执行场景的时候是直接压大容量的场景还是有非常清晰的策略？可以分享一下。</li>\n<li>在k8s的云原生环境中，网络结构有什么特点？</li>\n</ol><p>欢迎你在留言区与我交流讨论。我们下节课见！</p>',
        article_title: "29 | 基准场景：一个案例，带你搞懂基准场景的关键要点",
      },
      {
        title: "30 | 预压测：如何基于小流量快速验证容量压测效果？",
        id: 472197,
        content:
          '<p>你好，我是高楼。</p><p>上一节课呢，我们重点讲解了基准场景、容量场景、稳定性场景和异常场景这四类性能场景。通过对一个接口的基准场景模拟，我们提前发现了Calico IPIP模式单网卡软中断的问题，为后面容量场景的测试减少了一些麻烦。</p><p>这节课，我们就再进一步，把容量场景跑起来。我们先预压测一下，把场景中存在的小问题先找出来。</p><p>因为是预压测，所以一开始我们不用上太多的线程。先添加20个线程，运行一下看看结果就可以了。</p><p>说明一下，这里我换到JMeter上来运行，倒不是因为GoReplay不能满足压测需求，经过前面的改造，GoReplay其实已经可以完成我们的压测需求了。只是我还是更习惯JMeter在功能上的丰富性，所以在预压测这一讲，我们还是用JMeter来执行。</p><h2>第一阶段</h2><h3>压力场景数据</h3><p>怀着激动的心情，我们先来启动压力：</p><p><img src="https://static001.geekbang.org/resource/image/fb/24/fbcd373f5761322dfdb75cee69bd9024.png?wh=1824x528" alt="图片"></p><p>果然是不压不知道，一压心直跳呀。错误率很高不说，响应时间也很长。这就是我们要面对的问题了。</p><p>我们先来分析一下错误在哪。</p><p><img src="https://static001.geekbang.org/resource/image/c2/8d/c2bc56fee700956d51d5c132f9d4b28d.png?wh=1748x284" alt="图片"></p><p>从报错的信息来看，是断言判断不到设定的“操作成功”和“下单成功”字样。这是什么问题呢？</p><p>我们先把脚本拿到本地来跑一下试试。</p><p><img src="https://static001.geekbang.org/resource/image/4d/66/4d6ae4222c39facf1ed55e4aa252b766.png?wh=1157x236" alt="图片"></p><p>我们发现，返回信息里的中文全是乱码。这样一来，断言自然也就失效了。为了解决乱码，我们打开JMeter目录中的/bin/jmeter.properties，添加UTF-8的配置到文件中。</p><!-- [[[read_end]]] --><pre><code class="language-java">sampleresult.default.encoding=utf-8\n</code></pre><p>再次回放，脚本就可以正常显示了。</p><p><img src="https://static001.geekbang.org/resource/image/f8/3f/f8828d213afb1e453ef4fe5434f1703f.png?wh=1141x195" alt="图片"></p><p>我们还是用不大的压力（20个线程）再把压力跑起来，压力数据变成了下面这个样子：</p><p><img src="https://static001.geekbang.org/resource/image/3c/38/3c425ab41e0d447b0e2bd6b7e792d338.png?wh=1817x579" alt="图片"></p><p>看起来好像还不错。虽然也有几个错误，但相比刚才已经少很多了。剩下的这些错误应该就是真正的业务错误了。不过我们现在先不管它。先看看资源使用率的情况。</p><h3>全局监控分析</h3><p><img src="https://static001.geekbang.org/resource/image/14/e9/14a8391b804a4747b830616726969fe9.png?wh=1813x328" alt="图片"></p><p>通过这张全局监控截图，我们比较欣喜地看到，终于有一个机器的CPU已经标红了。在这种情况下，再分析就是要走我们的RESAR性能分析七步法了，保证手到擒来。</p><p>下面我们进到CPU高的s12服务器，执行top。</p><pre><code class="language-java">top - 23:14:44 up 3 days, 22:55,  1 user,  load average: 14.43, 11.99, 8.53\nTasks: 193 total,   1 running, 192 sleeping,   0 stopped,   0 zombie\n%Cpu0  : 76.4 us,  6.7 sy,  0.0 ni, 13.8 id,  0.3 wa,  0.0 hi,  2.7 si,  0.0 st\n%Cpu1  : 71.4 us,  7.7 sy,  0.0 ni, 19.9 id,  0.3 wa,  0.0 hi,  0.7 si,  0.0 st\n%Cpu2  : 76.5 us,  6.4 sy,  0.0 ni, 14.8 id,  1.0 wa,  0.0 hi,  1.3 si,  0.0 st\n%Cpu3  : 79.9 us,  4.7 sy,  0.0 ni, 14.1 id,  0.7 wa,  0.0 hi,  0.7 si,  0.0 st\n%Cpu4  : 85.6 us,  3.7 sy,  0.0 ni,  9.4 id,  0.3 wa,  0.0 hi,  1.0 si,  0.0 st\n%Cpu5  : 81.4 us,  4.1 sy,  0.0 ni, 13.2 id,  1.0 wa,  0.0 hi,  0.3 si,  0.0 st\n%Cpu6  : 87.6 us,  4.4 sy,  0.0 ni,  5.4 id,  0.7 wa,  0.0 hi,  2.0 si,  0.0 st\n%Cpu7  : 85.2 us,  5.4 sy,  0.0 ni,  8.4 id,  0.7 wa,  0.0 hi,  0.3 si,  0.0 st\nKiB Mem : 15731892 total,  1074660 free,  7042760 used,  7614472 buff/cache\nKiB Swap:        0 total,        0 free,        0 used.  8349352 avail Mem\n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n 6515 27        20   0 3916212 325008  13304 S 547.0  2.1 715:56.39 /opt/rh/rh-mysql57/root/usr/libexec/mysqld --defaults-file=/etc/my.cnf\n 3868 root      20   0 9706460 804212  14332 S  61.3  5.1 113:43.58 java -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1100 +\n29812 1000      20   0 1942964 243780   9608 S  25.5  1.5   0:38.93 java -Xms512m -Xmx512m -Duser.timezone=Asia/Shanghai -Djava.io.tmpdir=/tmp -cp classes zipkin2.dependencies.ZipkinDependenciesJob\n25306 nfsnobo+  20   0 2599060 489856  51700 S   7.3  3.1  59:17.13 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --web.enable-lifecycle --storage.tsdb.path=/prometheus --storage.tsdb.retention.time=10d --web.console.libr+\n 1189 root      20   0 1275732 108700  30124 S   6.6  0.7  92:33.30 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\n 1174 root      20   0 2271884 104784  36608 S   4.3  0.7 152:25.24 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --network+\n22095 root      10 -10  200708  33180  15344 S   2.0  0.2   6:42.03 /usr/local/aegis/aegis_client/aegis_11_17/AliYunDun\n24216 1000      20   0 2068796 486356  13508 S   2.0  3.1   5:23.46 java -Xms512m -Xmx512m -Dlogging.level.zipkin=DEBUG -Dlogging.level.zipkin2=DEBUG -Duser.timezone=Asia/Shanghai -cp . -Dlog4j2.disable.jmx=true org.springframework.boot+\n 3850 root      20   0  109096   8148   2652 S   1.7  0.1   3:10.85 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/0be4d32538621f4f99f2837f37f860c939fd9f3bf0faeb7b84c7da4ee0e38f68 -addre+  3773 1000      20   0 7509920   3.4g  22540 S   0.7 22.6  30:48.27 /usr/share/elasticsearch/jdk/bin/java -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.head+\n    1 root      20   0   44252   4672   2656 S   0.3  0.0   2:04.64 /usr/lib/systemd/systemd --switched-root --system --deserialize 22                                                                                                            6 root      20   0       0      0      0 S   0.3  0.0   0:30.63 [ksoftirqd/0]\n  414 root      20   0  149560  77504  77136 S   0.3  0.5   2:19.20 /usr/lib/systemd/systemd-journald\n</code></pre><p>从top的数据来看，问题在于mysql消耗的CPU较高。</p><h3>定向监控分析</h3><p>在我的经验中，这种时候就可以直接到innodb_trx里面蒙一把了，里面大概率是能看到慢日志的。我们在查询innodb_trx表中的数据时要多查几次，如果同一事务ID持续了一段时间，那就是明显慢了。当然，如果你开启了慢日志，可以到慢日志里去查看。</p><p><img src="https://static001.geekbang.org/resource/image/12/7c/12256aacfb1d32d282bb4737c185f97c.png?wh=1424x158" alt="图片"></p><p>果不其然，我在这里确实抓到了可能比较慢的SQL。我们取出它的执行计划看看。</p><p><img src="https://static001.geekbang.org/resource/image/b4/6a/b4a5b962b3bf9e9301fcc6950bb8bb6a.png?wh=1165x100" alt="图片"></p><p>因为ot表上是全表扫描，涉及到10万条数据，这数据太多了，又没走索引。我们执行语句，添加一个索引。</p><pre><code class="language-java">alter table mall.oms_order_item add index index_order_id(order_id) ;\nanalyze table mall.oms_order_item;\n</code></pre><p>再次查看执行计划看看区别：</p><p><img src="https://static001.geekbang.org/resource/image/05/eb/058510c0f93393d0a2b33ccb1e8e70eb.png?wh=1176x83" alt="图片"></p><p>这回，从rows那一列是不是看到了明显的变化？ot表相关的type也变成了我们添加的索引。这种问题，应该说是非常常见了。</p><p>这时候我又刷新了几下innodb_trx表，又找了几个走全表扫描的SQL。各自添加索引之后，我们再回去看一眼TPS：</p><p><img src="https://static001.geekbang.org/resource/image/12/26/12dfb272c6ace0bb936c43640356b526.png?wh=1803x588" alt="图片"></p><p>截图里这种情况就有点奇怪了呀。很显然在刚才改索引的时候，TPS是上去过的，你可以看一下23：24分之前的那一段，TPS曾经达到过800左右。但是中间却断了一会儿，这里应该是和我刚才执行的添加索引、分析表的SQL语句有关。我们再观察一会儿：</p><p><img src="https://static001.geekbang.org/resource/image/08/e4/08dd0ba01741d56b0a2b7b148d1bd2e4.png?wh=1832x585" alt="图片"></p><p>可以看到，报错之后压力并没有变化，但TPS是在逐渐恢复的。这是个好的方向。这时候我们再回来看看资源使用的情况：</p><p><img src="https://static001.geekbang.org/resource/image/06/1a/06c1b5bf5e8b3ce3208e8ed6c121681a.png?wh=1814x338" alt="图片"></p><p>还记得上面我们看到的是s12资源使用率多吗？现在已经转移到了s6身上了吧。这说明上一个问题已经解决，现在是个新问题了。</p><h2>第二阶段</h2><p>接下来我们再运行一段时间场景，看看还有啥问题。</p><p><img src="https://static001.geekbang.org/resource/image/d7/04/d72ee7f484ae9d964a1158a1f5bbdb04.png?wh=1827x583" alt="图片"></p><p>这里还是有一些报错。我查了一下日志，发现是因为库存不足。</p><pre><code class="language-java">2021-12-01 23:39:53.327 ERROR [mall-order,,,] 1 --- [io-8084-exec-18] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is com.dunshan.mall.common.exception.ApiException: 库存不足，无法下单] with root cause\n\ncom.dunshan.mall.common.exception.ApiException: 库存不足，无法下单\n\tat com.dunshan.mall.common.exception.Asserts.fail(Asserts.java:11)\n\tat com.dunshan.mall.order.service.impl.PortalOrderServiceImpl.generateOrder(PortalOrderServiceImpl.java:190)\n\tat com.dunshan.mall.order.service.impl.PortalOrderServiceImpl$$FastClassBySpringCGLIB$$9b833cea.invoke(&lt;generated&gt;)\n......................\n</code></pre><p>业务的问题比较好处理，加上库存就行了。</p><p>从上面这张图我们也可以看到，TPS一直稳稳地在近600左右。</p><p>资源使用率方面呢，根据上面的全局监控，我们可以知道s6上的资源使用率过高。所以我们要登录到这台主机，看看是什么进程导致的。</p><pre><code class="language-java">top - 23:45:54 up 14 days,  8:12,  2 users,  load average: 18.57, 16.12, 11.52\nTasks: 126 total,   2 running, 124 sleeping,   0 stopped,   0 zombie\n%Cpu0  : 70.9 us, 13.6 sy,  0.0 ni,  2.3 id,  0.0 wa,  0.0 hi, 13.2 si,  0.0 st\n%Cpu1  : 71.1 us, 16.5 sy,  0.0 ni,  8.9 id,  0.3 wa,  0.0 hi,  3.1 si,  0.0 st\nKiB Mem :  8008964 total,   173104 free,  2266508 used,  5569352 buff/cache\nKiB Swap:        0 total,        0 free,        0 used.  5433884 avail Mem\n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n31288 root      20   0 4624912 536084  12736 S 100.3  6.7  19:48.18 java -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1100 +\n 7931 root      20   0 4637452 697772  12728 S  60.5  8.7  90:39.72 java -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1100 +\n 1473 root      20   0  675600  94608  26768 S  16.6  1.2 540:03.39 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\n 1894 root      20   0 1729636  90460  35948 S   2.3  1.1 618:52.03 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --network+\n21598 root      10 -10  197132  27972  15480 S   2.3  0.3  10:07.67 /usr/local/aegis/aegis_client/aegis_11_17/AliYunDun\n31271 root      20   0  107688   3380   2556 S   2.3  0.0   0:27.37 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/e15c7ab19edcf9e3e02a3a03f2f8c4dc80232b6af2795eb70f4c12b0dd821f11 -addre+\n 7913 root      20   0  109096   5620   2716 S   1.7  0.1   3:33.94 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/4516faf8c4c2f8545e63224a793f60c17a21eee0ef5a4585cba2842063c78067 -addre+\n    9 root      20   0       0      0      0 R   0.3  0.0  11:05.99 [rcu_sched]\n  609 root      20   0  835660  26056   7852 S   0.3  0.3  58:34.33 CmsGoAgent-Worker start\n 2927 1000      20   0  141496  29604  12708 S   0.3  0.4  13:10.48 /metrics-server --cert-dir=/tmp --secure-port=4443 --kubelet-insecure-tls=true --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,externalDNS\n................\n</code></pre><p>通过top中的process table，我们看到上面有两个Java的进程，由于名字太长看不全，不知道是什么具体的服务。这种情况是有多个服务跑在了一个2C的s6上。接下来，我们查看一下s6上有什么Pod：</p><pre><code class="language-java">[root@s5 runMall]# kubectl get pods -n default -o wide | grep s6\nkafka-1                                   1/1     Running     48         13d     10.100.1.3       s6     &lt;none&gt;           &lt;none&gt;\nnode-exporter-rqgrx                       1/1     Running     0          3d4h    172.31.184.226   s6     &lt;none&gt;           &lt;none&gt;\nsvc-mall-cart-dd5db86d7-dcwdd             1/1     Running     0          3d23h   10.100.1.14      s6     &lt;none&gt;           &lt;none&gt;\nsvc-mall-order-67cd58db5-ht5mb            1/1     Running     0          12m     10.100.1.15      s6     &lt;none&gt;           &lt;none&gt;\n[root@s5 runMall]#\n</code></pre><p>果然，这里有两个服务，还都是重要的服务。对一个2C的虚拟机来说，能跑近600的TPS，我觉得已经挺好的了，不能再指望更高的TPS了。为了让TPS再高一些，我把Order服务移到s8上去，因为s8感觉没用上。</p><p>把Order服务从s6移到s8之后，再执行一会场景，查看压力数据：</p><p><img src="https://static001.geekbang.org/resource/image/0d/f9/0df57e95a309c4ce88yy7317cc3c07f9.png?wh=1828x586" alt="图片"></p><p>看起来不错哦，TPS从600左右上升到了960左右。</p><p>接着我们再回来看一下全局监控数据：</p><p><img src="https://static001.geekbang.org/resource/image/c8/14/c8191e405a1ba097d56d955e2c021d14.png?wh=1817x384" alt="图片"></p><p>资源使用率也比之前高了不少。</p><p>看起来，我们的系统已经要进入到正常的系统状态了，到这里，预压测也就可以结束了。下节课，我们就可以上更大一些的压力接着玩了。</p><h2>总结</h2><p>好了，这节课就讲到这里。刚才，我们对容量场景进行了预压测。</p><p>其实这个过程和正式的容量场景没有什么区别，只是压力线程少一点而已。预压测可以验证场景设置、参数化数据、网络环境、硬件环境等内容。它可以让我们在执行正式的场景的时候，不至于出现耗时的问题。像这节课讲的“索引”这种基础问题，就应该在预压测的时候就解决掉，不应该出现在大压力的场景之中。</p><p>在很多人的性能思路中，基准场景就是拿三五个压力线程运行一下，而预压测的过程则根本不存在。但是在我的RESAR性能工程的理念中，在真正模拟生产峰值的容量场景之前，是有必要加上预压测的部分的。预压测使用的是生产的业务模型，是混合的业务场景。预压测的存在可以给容量场景做好充分的铺垫。</p><h2>课后题</h2><p>学完这节课，请你思考两个问题：</p><ol>\n<li>预压测是不是必须的呢？</li>\n<li>不做预压测会带来什么问题？</li>\n</ol><p>欢迎你在留言区与我交流讨论。当然了，你也可以把这节课分享给你身边的朋友，他们的一些想法或许会让你有更大的收获。我们下节课见！</p>',
        article_title: "30 | 预压测：如何基于小流量快速验证容量压测效果？",
      },
      {
        title: "31 | 容量场景：决定线上容量场景的关键因素是什么？",
        id: 472965,
        content:
          '<p>你好，我是高楼。</p><p>上节课，我们对场景进行了一个预压测，提前解决了一些基础性的问题。这节课呢，我们就可以直接上大的压力来执行容量场景了。容量场景的目的就是验证线上环境能不能满足需求，最后给我们一个明确的答案。</p><p>通常在真实的项目中，我们都会有总TPS的需求，也会有详细的TPS需求（每个接口应该在多长的响应时间内达到多少TPS）。在第3讲的压测方案中，我们已经有了一个指标表，我也把它直接放在下面了。</p><p><img src="https://static001.geekbang.org/resource/image/16/ce/165ed134d7a2ed867a093c18bddcf4ce.jpg?wh=1920x1144" alt="图片"></p><p><strong>这里的目标TPS都加在一起是1350</strong>，后面响应时间也给出了个大概的值（200ms以下），这也算是个比较明确的指标了。下面我们就通过容量场景来看看这个目标能不能达到，要是能达到，我们就可以给出“测试通过”的结论了。</p><p>下面我们先运行一下容量场景，这里用JMeter或GoReplay来运行都是可以的。</p><p>不过在这之前我们得知道，虽然在前面的预压测中，我们有过把TPS调到900左右的时候。但是现在环境也经过了好几轮的折腾，各应用也重启过好几次了，我们不知道各个服务是不是还是在预压测的那个状态。</p><p>这也是我们在性能项目中经常会面对的问题：有些指标在之前执行的场景中一切良好，但在解决其他问题的过程中，突然就出现了性能变差的情况。这种情况经常会随机地出现，所以如果有性能工程师把某一次压力良好的数据直接当成最终的性能结果，就这样把它写进性能报告里，这肯定是应付差事的表现。</p><!-- [[[read_end]]] --><p>果然，在我们的这个容量场景中就出现了性能衰减的问题。我们一起来看看数据。</p><h2>第一阶段</h2><h3>压力场景数据</h3><p>先来看压力场景的数据：</p><p><img src="https://static001.geekbang.org/resource/image/30/bc/3099a425ac118754627c4065bd9d7cbc.png?wh=1830x589" alt="图片"></p><p>在预压测中，同样的20个压力线程，我们达到过900左右的TPS。但是现在只有600左右了。这里要说明一下，在性能项目中，由于环境的变化、版本的变化、数据量级的变化等，TPS有下降也是时有发生的事情，遇到这样的事情，首先是不要慌，先分析下是什么原因，再来看如何解决。</p><p>从响应时间上看，有几个事务的响应时间已经到80ms左右了。这和之前的数据相比差了很多。</p><p>我们还是按照性能分析七步法来分析一下问题在哪。</p><p>因为架构图我在前面也提了很多次了，相信你跟着我这个专栏走下来，这个架构图是可以记得住的。所以在这里我就不再强调了。</p><p>在性能分析七步法中，“拆分时间”和“全局监控”这两步我经常是不分先后的，因为这两步都有可能直接看到性能瓶颈的问题方向。如果我们分析全局监控看不到问题的方向，就可以回去拆分响应时间来判断瓶颈点在哪了。对于性能来说，核心其实就是响应时间的快慢，我们所有的努力都是为了让响应时间更快。当响应时间快了之后，单节点的容量也就高了。然后我们可以再进行扩展性测试，来判断需要多少资源才能支撑整个架构的容量。</p><p>下面我们先来看看全局监控的计数器。</p><h3>全局监控分析</h3><p>前面我们讲过全局监控分析的设计和落地实现，我们按照这个思路查看一下全局监控计数器。在微服务分布式架构中，全局监控主要的视角包括：Node级（系统级）、微服务级和Pod级。从Node级来看：</p><p><img src="https://static001.geekbang.org/resource/image/25/5f/2572b9851c4e352ccd05b7264390985f.png?wh=1813x436" alt="图片"></p><p>s6 CPU资源占用比较高。它的算法是用CPU使用核数/CPU总核数得到的数据，所以可能会让你感觉这个值不精准。比如上面的s6节点的CPU使用率是80.5%，这个数据就跟主机监控视图里的数据不一样：</p><p><img src="https://static001.geekbang.org/resource/image/6d/6a/6d507d51322f2dc589d084d5e596786a.png?wh=1807x332" alt="图片"></p><p>在主机监控视图里，我们看到s6的CPU使用已经超过90%了。这个值有点太高了，应该从它开始往下分析。</p><h3></h3><h3>定向监控分析</h3><p>我们再来看下定向监控。登录s6主机，查看资源。</p><pre><code class="language-java">top - 16:44:01 up 15 days,  1:10,  1 user,  load average: 16.38, 13.87, 7.53\nTasks: 127 total,   2 running, 125 sleeping,   0 stopped,   0 zombie\n%Cpu0  : 67.2 us, 14.4 sy,  0.0 ni,  2.0 id,  0.0 wa,  0.0 hi, 16.4 si,  0.0 st\n%Cpu1  : 66.7 us, 18.4 sy,  0.0 ni, 11.9 id,  0.0 wa,  0.0 hi,  3.1 si,  0.0 st\nKiB Mem :  8008964 total,   267664 free,  2270248 used,  5471052 buff/cache\nKiB Swap:        0 total,        0 free,        0 used.  5430304 avail Mem\n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n13465 root      20   0 4607344 491400  12044 S  94.0  6.1  18:21.24 java -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dc+\n 7931 root      20   0 4636424 713848  11920 S  64.1  8.9 120:26.35 java -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dc+\n 1473 root      20   0  675856  84604  20440 S  14.6  1.1 562:05.09 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\n 1894 root      20   0 1729636  83780  24696 S   3.7  1.0 649:29.87 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconf+\n13446 root      20   0  109096   3724   2768 S   2.3  0.0   0:23.05 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.lin+\n 7913 root      20   0  109096   5684   2716 S   2.0  0.1   4:42.36 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.lin+\n21598 root      10 -10  229756  58396  13980 S   2.0  0.7  32:35.85 /usr/local/aegis/aegis_client/aegis_11_17/AliYunDun\n</code></pre><p>从上面的数据可以看到，有两个Java进程消耗CPU比较多。这个时候我们要再往下分析就需要考虑是从哪里开始了。上面的数据中，us CPU在66%左右，sy在20%以下，这两个部分是挺正常的数据了。但si CPU达到了16%，这就有点高了。我们可以先从这里往下找，查看软中断计数器（注意这个值是累加值）：</p><pre><code class="language-java">[root@s6 ~]# cat /proc/softirqs\n                    CPU0       CPU1\n          HI:          1          0\n       TIMER:  234909974  214047397\n      NET_TX:         25         40\n      NET_RX:  126031087   17310460\n       BLOCK:     904406          0\nBLOCK_IOPOLL:          0          0\n     TASKLET:     175969        485\n       SCHED:  113910312  103576801\n     HRTIMER:          0          0\n         RCU:  169468493  162852369\n</code></pre><p>可以看到，CPU0确实比CPU1中断次数要多很多。</p><p>我们接着查看一下网卡队列。这里需要查看两个网卡，一个是系统网卡，一个是k8s网络插件Flannel的网卡。</p><pre><code class="language-java">[root@s6 ~]# ll /sys/class/net/eth0/queues/\ntotal 0\ndrwxr-xr-x 2 root root 0 Nov 17 23:33 rx-0\ndrwxr-xr-x 2 root root 0 Nov 17 23:33 rx-1\ndrwxr-xr-x 3 root root 0 Nov 17 23:33 tx-0\ndrwxr-xr-x 3 root root 0 Nov 17 23:33 tx-1\n[root@s6 ~]# ll /sys/class/net/flannel.1/queues/\ntotal 0\ndrwxr-xr-x 2 root root 0 Nov 17 15:47 rx-0\ndrwxr-xr-x 3 root root 0 Nov 17 15:47 tx-0\n[root@s6 ~]#\n</code></pre><p>从检查的结果来看，系统的网卡队列是和CPU数一致的，而Flannel的插件只创建了一个网卡接收队列和一个网卡发送队列。<br>\n我们再查一下这上面总共有多少个Pod。</p><pre><code class="language-java">[root@s5 runMall]# kubectl get pods -n default -o wide|grep s6\nkafka-1                                   1/1     Running     48         13d     10.100.1.3       s6     &lt;none&gt;           &lt;none&gt;\nnode-exporter-rqgrx                       1/1     Running     0          3d21h   172.31.184.226   s6     &lt;none&gt;           &lt;none&gt;\nsvc-mall-cart-dd5db86d7-dcwdd             1/1     Running     0          4d16h   10.100.1.14      s6     &lt;none&gt;           &lt;none&gt;\nsvc-mall-member-85668c9bbf-f97kg          1/1     Running     0          16h     10.100.1.16      s6     &lt;none&gt;           &lt;none&gt;\n[root@s5 runMall]#\n</code></pre><p>从查询的结果来看，这里有Cart和Member两个服务，还有Kafka和node_exporter，这是每个机器上都会有的监控容器。</p><p>因为s6只有2C，数据即使能接收下来处理能力也有限，所以我们先把服务从s6移到其他主机上去，然后再执行一下场景，看看效果：</p><p><img src="https://static001.geekbang.org/resource/image/69/20/6955999c0864df1548ae6b0cf13f8e20.png?wh=1818x667" alt="图片"></p><p>这次TPS增加到了700左右，已经不再增加了。主机资源呢，s12的CPU使用率达到了75%左右。 同时，网络带宽也用起来了。</p><p><img src="https://static001.geekbang.org/resource/image/e5/bf/e5a80f6007ca5b38e39df0675308d1bf.png?wh=1811x330" alt="图片"></p><p>不过这个网络带宽还是有点高的，所以我们来查一下是什么消耗了这么大的网络带宽。登录到s11上，执行下iftop命令。</p><p><img src="https://static001.geekbang.org/resource/image/7e/99/7ed873665794fe574f62407657c99799.png?wh=1920x1080" alt="图片"></p><p>发现这是NFS做同步时使用的带宽。172.31.184.233这台主机是s12，而s12上使用最多的是我们的shadow数据库。</p><p>从全局监控的数据来看，s12和s11之间的带宽占得实在是有点高。在同步比较大的时候，居然能达到1.7G以上。就像下面这张图：</p><p><img src="https://static001.geekbang.org/resource/image/7c/79/7cb15bae5yyd8effd759ed3c0b236979.png?wh=1814x369" alt="图片"></p><p>我们到s12和s11上分别执行命令netstat，查看一下网络的队列是否存在。</p><p><img src="https://static001.geekbang.org/resource/image/52/18/5271153e7850cc2bd42192b40b9a7018.png?wh=1395x1040" alt="图片"></p><p>这一步我们要把两边的机器对应着来看。显然，这里是长期有网络队列的，我多次执行了命令来查看这个数据，发现双方还都有send_Q队列值。这就明显是网络带宽不足的问题了。</p><p>因为s12上是数据库，现在的问题就是，数据库作持久化时，由于数据量较大，所以把持久化数据通过NFS写到s11上会占用很大的带宽。要想让带宽降下来，只有一个办法就是把MySQL表数据大小降下来。</p><p>在这里，我先把压力产生的数据库的数据给清理一下，像记录表、订单表、购物车表等等，让NFS的流量先降下来。然后再来看看TPS能不能上升一些。</p><pre><code class="language-java">truncate table mall.oms_order_item ;\ntruncate table mall.ums_member_login_log ;\ntruncate table mall.oms_cart_item ;\nanalyze table mall.oms_order_item ;\nanalyze table mall.ums_member_login_log ;\nanalyze table mall.oms_cart_item ;\n</code></pre><p>一顿迅猛的操作之后，我们再来看看压力数据。</p><p><img src="https://static001.geekbang.org/resource/image/0e/5f/0ecd2f32c2a23dc848e10d4240bb4c5f.png?wh=1812x660" alt="图片"></p><p>哟，这一下可是快到1000TPS了！再来看一下全局监控的数据。</p><p><img src="https://static001.geekbang.org/resource/image/0e/ff/0ee26fd0eb0d5c85814ccddc0b9275ff.png?wh=1807x324" alt="图片"></p><p>网络带宽也已经降下来很多了。 效果看起来还不错！</p><p>不过这个优化动作虽然在我们这个场景中有效，但是不是一个一劳永逸的解决办法。正常的处理方式应该是换个性能好的IO存储设备，我们这个云服务器的本地磁盘做的NFS，确实在读写能力上是比较差的。当然啦，我们这个示例项目还是可以继续通过清理数据的方式先往下分析。</p><h2>第二阶段</h2><p>从上面的监控截图中可以看到，我们的目标TPS还没有达到，响应时间倒是挺低。各服务器的CPU资源使用率也并没有特别高，资源都没有用完，还是心有不甘的。</p><p>接下来应该怎么办呢？我们还是要来分析有没有优化的点。</p><h3>拆分响应时间</h3><p>先来看看响应时间。我们得想想时间消耗到哪里去了，再来判断下有没有优化时间的可能性，从资源没有用完的角度来看，应该是有阻塞点。</p><p>打开Zipkin，搜索一下结果：</p><p><img src="https://static001.geekbang.org/resource/image/fe/8d/fe6a6b96954e80b2e48f5b069799fa8d.png?wh=1232x756" alt="图片"></p><p>这个时间倒是和JMeter上的时间是接近的。我们点第一个接口show看看：</p><p><img src="https://static001.geekbang.org/resource/image/df/07/df6919f93e703ac7262379239620a307.png?wh=1152x395" alt="图片"></p><p>再点第二个接口show，看到的是下面的结果：</p><p><img src="https://static001.geekbang.org/resource/image/97/78/97949c766d51249cda63bd3870741078.png?wh=1155x633" alt="图片"></p><p>从这两个接口的调用树来看，都是在cartpromotion这里比较慢，子调用中的SQL语句看起来也并不慢。</p><h3>定向监控分析</h3><p>接着我们就要看看是哪个具体的方法耗时比较长了。这里我用Arthas跟踪一下cartpromotion接口。</p><p>我们可以先从代码中找到cartpromotion的接口代码：</p><pre><code class="language-java">    @PostMapping("/cart/CartPromotion")\n    @ResponseBody\n    List&lt;CartPromotionItem&gt; calcCartPromotion(@RequestBody List&lt;OmsCartItem&gt; cartItemList) {\n        List&lt;CartPromotionItem&gt; cartPromotionItems = omsPromotionService.calcCartPromotion(cartItemList);\n        return cartPromotionItems;\n    }\n</code></pre><p>这里是calcCartPromotion，是计算优惠券的。又因为这个接口是从Portal里面调的，所以我们进入Portal服务中，下载Arthas并启动跟踪。</p><pre><code class="language-java">[root@svc-mall-portal-75cf5c7989-ccsr4 /]# curl -O https://arthas.aliyun.com/arthas-boot.jar\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  138k  100  138k    0     0   363k      0 --:--:-- --:--:-- --:--:--  364k\n[root@svc-mall-portal-75cf5c7989-ccsr4 /]# java -jar arthas-boot.jar \n\n[arthas@1]$ trace com.dunshan.mall.portal.controller.CartPromotionController calcCartPromotion -v -n 5 --skipJDKMethod false \'1==1\'\nPress Q or Ctrl+C to abort.\nAffect(class count: 1 , method count: 1) cost in 97 ms, listenerId: 6\nCondition express: 1==1 , result: true\n`---ts=2021-12-05 21:50:29;thread_name=http-nio-8085-exec-85;id=74b5;is_daemon=true;priority=5;TCCL=org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader@5ed190be\n    `---[37.443819ms] com.dunshan.mall.portal.controller.CartPromotionController:calcCartPromotion()\n        `---[37.38849ms] com.dunshan.mall.portal.service.OmsPromotionService:calcCartPromotion() #35\n\nCondition express: 1==1 , result: true\n`---ts=2021-12-05 21:50:29;thread_name=http-nio-8085-exec-73;id=7438;is_daemon=true;priority=5;TCCL=org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader@5ed190be\n    `---[34.832809ms] com.dunshan.mall.portal.controller.CartPromotionController:calcCartPromotion()\n        `---[34.811947ms] com.dunshan.mall.portal.service.OmsPromotionService:calcCartPromotion() #35\nCondition express: 1==1 , result: true\n\n`---ts=2021-12-05 21:50:29;thread_name=http-nio-8085-exec-86;id=74b9;is_daemon=true;priority=5;TCCL=org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader@5ed190be\n\nCondition express: 1==1 , result: true\n`---ts=2021-12-05 21:50:29;thread_name=http-nio-8085-exec-81;id=7480;is_daemon=true;priority=5;TCCL=org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader@5ed190beCondition express: 1==1 , result: true\n\n    `---[41.252226ms] com.dunshan.mall.portal.controller.CartPromotionController:calcCartPromotion()\n        `---[40.220118ms] com.dunshan.mall.portal.service.OmsPromotionService:calcCartPromotion() #35\n\n`---ts=2021-12-05 21:50:29;thread_name=http-nio-8085-exec-71;id=7430;is_daemon=true;priority=5;TCCL=org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader@5ed190be\nCommand execution times exceed limit: 5, so command will exit. You can set it with -n option.\n    `---[42.934061ms] com.dunshan.mall.portal.controller.CartPromotionController:calcCartPromotion()\n        `---[42.849213ms] com.dunshan.mall.portal.service.OmsPromotionService:calcCartPromotion() #35\n</code></pre><p>接着跟踪。</p><pre><code class="language-java">[arthas@1]$ trace com.dunshan.mall.portal.service.OmsPromotionService calcCartPromotion -v -n 5 --skipJDKMethod false \'1==1\'\nCondition express: 1==1 , result: true\n`---ts=2021-12-05 21:51:36;thread_name=http-nio-8085-exec-91;id=74db;is_daemon=true;priority=5;TCCL=org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader@5ed190be\n    +---[0.007964ms] com.dunshan.mall.portal.service.impl.OmsPromotionServiceImpl:groupCartItemBySpu() #40\n    +---[19.243656ms] com.dunshan.mall.portal.service.impl.OmsPromotionServiceImpl:getPromotionProductList() #42\n    +---[0.002098ms] java.util.ArrayList:&lt;init&gt;() #44\n    +---[0.001859ms] java.util.Map:entrySet() #45\n    +---[0.002208ms] java.util.Set:iterator() #45\n    +---[min=0.001209ms,max=0.001753ms,total=0.002962ms,count=2] java.util.Iterator:hasNext() #45\n    +---[0.001215ms] java.util.Iterator:next() #45\n    +---[0.001574ms] java.util.Map$Entry:getKey() #46\n    +---[0.003588ms] com.dunshan.mall.portal.service.impl.OmsPromotionServiceImpl:getPromotionProductById() #47\n    +---[0.004646ms] java.util.Map$Entry:getValue() #50\n    +---[0.001664ms] com.dunshan.mall.portal.domain.PromotionProduct:getPromotionType() #51\n    `---[0.038219ms] com.dunshan.mall.portal.service.impl.OmsPromotionServiceImpl:handleNoReduce() #135\n\nCondition express: 1==1 , result: true\n`---ts=2021-12-05 21:51:36;thread_name=http-nio-8085-exec-85;id=74b5;is_daemon=true;priority=5;TCCL=org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader@5ed190be\n    `---[40.440293ms] com.dunshan.mall.portal.service.impl.OmsPromotionServiceImpl:calcCartPromotion()\n        +---[0.010603ms] com.dunshan.mall.portal.service.impl.OmsPromotionServiceImpl:groupCartItemBySpu() #40\n        +---[18.010145ms] com.dunshan.mall.portal.service.impl.OmsPromotionServiceImpl:getPromotionProductList() #42\n        +---[0.002138ms] java.util.ArrayList:&lt;init&gt;() #44\n        +---[0.001818ms] java.util.Map:entrySet() #45\n        +---[0.001696ms] java.util.Set:iterator() #45\n        +---[min=0.00118ms,max=0.001659ms,total=0.002839ms,count=2] java.util.Iterator:hasNext() #45\n        +---[0.0012ms] java.util.Iterator:next() #45\n        +---[0.00145ms] java.util.Map$Entry:getKey() #46\n        +---[0.004004ms] com.dunshan.mall.portal.service.impl.OmsPromotionServiceImpl:getPromotionProductById() #47\n        +---[0.001356ms] java.util.Map$Entry:getValue() #50\n        +---[0.001632ms] com.dunshan.mall.portal.domain.PromotionProduct:getPromotionType() #51\n        `---[0.043778ms] com.dunshan.mall.portal.service.impl.OmsPromotionServiceImpl:handleNoReduce() #135\n</code></pre><p>接着跟踪。</p><pre><code class="language-java">Condition express: 1==1 , result: true\n`---ts=2021-12-05 21:58:38;thread_name=http-nio-8085-exec-94;id=7551;is_daemon=true;priority=5;TCCL=org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader@5ed190be\n    `---[18.520576ms] com.dunshan.mall.portal.service.impl.OmsPromotionServiceImpl:getPromotionProductList()\n        +---[0.001613ms] java.util.ArrayList:&lt;init&gt;() #145\n        +---[0.001367ms] java.util.List:iterator() #146\n        +---[min=0.001099ms,max=0.001269ms,total=0.002368ms,count=2] java.util.Iterator:hasNext() #146\n        +---[0.001267ms] java.util.Iterator:next() #146\n        +---[0.001399ms] com.dunshan.mall.model.OmsCartItem:getProductId() #147\n        +---[0.00125ms] java.util.List:add() #147\n        `---[17.622753ms] com.dunshan.mall.portal.dao.PortalProductDao:getPromotionProductList() #149\n\nCondition express: 1==1 , result: true\n`---ts=2021-12-05 21:58:38;thread_name=http-nio-8085-exec-109;id=7576;is_daemon=true;priority=5;TCCL=org.springframework.boot.web.embedded.tomcat.TomcatEmbeddedWebappClassLoader@5ed190be\n    `---[58.172854ms] com.dunshan.mall.portal.service.impl.OmsPromotionServiceImpl:getPromotionProductList()\n        +---[0.002119ms] java.util.ArrayList:&lt;init&gt;() #145\n        +---[0.001274ms] java.util.List:iterator() #146\n        +---[min=0.001152ms,max=0.001362ms,total=0.002514ms,count=2] java.util.Iterator:hasNext() #146\n        +---[0.001101ms] java.util.Iterator:next() #146\n        +---[0.001395ms] com.dunshan.mall.model.OmsCartItem:getProductId() #147\n        +---[0.001216ms] java.util.List:add() #147\n        `---[58.125958ms] com.dunshan.mall.portal.dao.PortalProductDao:getPromotionProductList() #149\n</code></pre><p>可以看到，跟踪到DAO层的时候，这里就直接对应着一个SQL了。</p><pre><code class="language-java">SELECT \n    p.id,\n    p.`name`,\n    p.promotion_type,\n    p.gift_growth,\n    p.gift_point,\n    sku.id sku_id,\n    sku.price sku_price,\n    sku.sku_code sku_sku_code,\n    sku.promotion_price sku_promotion_price,\n    sku.stock sku_stock,\n    sku.lock_stock sku_lock_stock,\n    ladder.id ladder_id,\n    ladder.count ladder_count,\n    ladder.discount ladder_discount,\n    full_re.id full_id,\n    full_re.full_price full_full_price,\n    full_re.reduce_price full_reduce_price\nFROM\n    mall.pms_product p\n        LEFT JOIN\n    mall.pms_sku_stock sku ON p.id = sku.product_id\n        LEFT JOIN\n    mall.pms_product_ladder ladder ON p.id = ladder.product_id\n        LEFT JOIN\n    mall.pms_product_full_reduction full_re ON p.id = full_re.product_id\nWHERE\n    p.id IN (7);\n</code></pre><p>可是要说SQL执行效果差，也没见数据库所在的节点资源使用率高。这时就不能不来查查JDBC连接池了：<br>\n<img src="https://static001.geekbang.org/resource/image/4a/2f/4a18798fa86261907766b1f17fa0092f.png?wh=1114x364" alt="图片"></p><p>从这张application.yml截图里可以看到，最大JDBC也只有20，有点低了。我们调到40，把压力线程也提高到60，再压起来，看一下压力场景的数据：</p><p><img src="https://static001.geekbang.org/resource/image/20/98/20ac15db84bb86edb64ac7538172dd98.png?wh=1813x659" alt="图片"></p><p>这时候TPS已经能达到1300了！可以达到我们对这个系统的TPS的预期目标了。再来看一下主机的资源：</p><p><img src="https://static001.geekbang.org/resource/image/86/ba/8686cf367c1f6ab279aed5a71f2342ba.png?wh=1824x291" alt="图片"></p><p>挺好！资源也用起来了。</p><p>不过这里还要说明一下，calcCartPromotion方法的实现是比较复杂的。代码你可以看看我给你截的下面这张图：</p><p><img src="https://static001.geekbang.org/resource/image/49/0d/498651e156407b123d2a5f70b3756a0d.png?wh=1067x717" alt="图片"></p><p>这还是折叠起来的。如果把代码都展开，一屏都显示不完。如果想再优化的话，就得考虑一下怎么优化这个方法了。</p><p>因为我们现在要做的是全链路压测，现在我们的主要目标是跑出正常流量和压测流量来，所以这个优化方向就先放一放，先跑出两种流量的场景再说。</p><h2>第三阶段</h2><p>我们再接着来分析还有没有优化的空间。既然刚才我们已经看到了，有些接口的响应时间会随着压力增加而增加，那么我们就直接来判断一下响应时间消耗在哪里去了。</p><p>打开Zipkin，看一下排名前十的请求，这里我截取几个图给你看一下。</p><p><img src="https://static001.geekbang.org/resource/image/56/b2/560c689a2d9bbf0dac2f0c26bea7f1b2.png?wh=1207x771" alt="图片"></p><p>点开一个来看一下。</p><p><img src="https://static001.geekbang.org/resource/image/0b/38/0b6d967a5fee98899eea9a710104ea38.png?wh=1143x643" alt="图片"></p><p>可以看到，这里cartpromotion耗时是最长的，这个逻辑我们前面已经看过了，我就不重复上面的动作了。</p><h3>同时启动正常流量和压测流量</h3><p>现在我们换个逻辑来跑。我们一边用JMeter来跑正常的流量，一边用GoReplay来跑压测的流量。</p><p>还记得我们前面的全链路改造架构图吧。</p><p><img src="https://static001.geekbang.org/resource/image/ab/9e/abd7df412998461cd240fe314104a99e.jpg?wh=1920x1590" alt=""></p><p>我把它简化一下就是下面的样子：</p><p><img src="https://static001.geekbang.org/resource/image/b9/b2/b9e57d8bbdd37cbb7010000e7a0290b2.jpg?wh=1920x554" alt=""></p><p>现在我们就来按全链路压测的逻辑来实现真实流量和压测流量。我们把JMeter运行起来，TPS在100左右，GoReplay放大150倍。因为GoReplay本身没办法实现连续递增加压（可以自己写脚本实现），我们这里就直接加压力了。</p><p>先看看看能达到多高的峰值，我们前面知道，系统的整体TPS大概在1300左右。</p><p><img src="https://static001.geekbang.org/resource/image/f8/9e/f8967077131e70a431e900aa263a749e.png?wh=1842x739" alt="图片"></p><p>从上面的截图可以看出来，左边的GoReplay运行了两个实例，加在一起大概能达到1250左右，右边JMeter大概能达到120TPS左右。两边加在一起在1370左右。运行非常稳定。</p><p>再来看看主机资源使用情况。</p><p><img src="https://static001.geekbang.org/resource/image/7e/0a/7ea558d3d128cfc2a13d893f2093f10a.png?wh=1820x339" alt="图片"></p><p>现在s11这个主机，CPU使用率接近80%，其他的还好，说明硬件资源还有空间。</p><p>再来看一下两个数据库的流量。</p><pre><code class="language-java">[root@s5 ~]# kubectl get pods -n default -o wide | grep mysql\ndb-mysql-produce-7b8fc76c76-45nc7         2/2     Running            5          6d1h    10.100.4.86      s8     &lt;none&gt;           &lt;none&gt;\ndb-mysql-shadow-74ff9bdfb4-kkxbq          2/2     Running            0          5d19h   10.100.2.92      s9     &lt;none&gt;           &lt;none&gt;\n[root@s5 ~]#\n</code></pre><p>这是两个数据库的信息，我们把两边的监控都打开对比看一下。<br>\n<img src="https://static001.geekbang.org/resource/image/27/24/272aebfc5958ea60c4158900b2bb5524.png?wh=1848x823" alt="图片"></p><p>可以看到，影子库的连接（MySQL Connections图）和执行的SQL数量（MySQL Questions图）都比正式库要高出很多。</p><p>作为全链路压测的场景，这样的资源使用率是比较合理的。因为我们不能给生产环境带来风险，所以不能像在线下压测的环境中那样，让资源使用率达到100%，这也是要注意的一个要点。</p><p>因为前面我们也做了大量的工作，所以这里我们要把相应的记录表给清理一下。再次运行JMeter和GoReplay看看结果：</p><p><img src="https://static001.geekbang.org/resource/image/5f/41/5fc46ea75d2846a7d0ac4864a6171541.png?wh=1834x734" alt="图片"></p><p>从这个数据可以看到，GoReplay的TPS达到1500左右，JMeter的TPS达到150左右，加在一起一共1650左右，比之前多了300TPS。可见，数据量大了之后对性能的影响也是比较明显的，所以定时定量清理的策略是非常有必要的。</p><p>同时你也可以看到，在前面的截图中，有很多次都出现了分区使用率较高的情况，在这个过程中，由于我们的磁盘空间不太多（NFS现在只有100G），很快就用完了，所以我们还要不停去清理日志、监控数据清理等。但是为了不影响主线叙述逻辑，这些琐碎的工作我都没有体现在专栏中。不过你在做压测的时候，一定是要考虑到的。</p><h2>总结</h2><p>好了，这节课就讲到这里。</p><p>刚才，我们同时启动了正常流量和压测流量，模拟了全链路压测场景应该有的样子。</p><p>我们全链路压测的目标呢，就是要把两种流量一起跑起来，并且让结果保持稳定。但是不要因为我们把这些场景都跑起来了，也没遇到啥问题，就觉得这个场景非常简单。其实，全链路压测的准备工作是非常多的，像我们之前讲的全链路的改造、数据的准备、两种压力流量脚本的准备、监控的准备等等都是要提前处理好的。我们是在做准备的时候就解决了疑难的技术点，才有了一个比较让人满意的结果。</p><p>那么，影响全链路线上压测容量场景的关键因素有哪些呢？我来画个思维导图说明一下：</p><p><img src="https://static001.geekbang.org/resource/image/0f/f5/0fbd81700d5ca22c2f73687943831df5.jpg?wh=1794x2244" alt="图片"></p><p>其实在全链路线上压测的逻辑中，想同时跑起来两类场景，对企业的各方面支持都要求非常高。上图中的因素都是需要在具体落地时保证细致而精准的。</p><p>全链路线上压测，到这里，我觉得我已经把逻辑描述完整了。如果你一步步跟着学下来，就可以看到全链路压测的所有技术落地细节。全链路压测的实现并不是一个方案就可以描述清楚的，只有通过这些技术的完整落地，才能实现真正的全链路线上压测。</p><h2>课后题</h2><p>学完这节课，请你思考两个问题：</p><ol>\n<li>如果让你实现一个系统的全链路压测，你有明确的方向吗，可以分享一下。</li>\n<li>如果你的企业想要落地全链路压测，你觉得最大的阻碍在哪？</li>\n</ol><p>欢迎你在留言区与我交流讨论。当然了，你也可以把这节课分享给你身边的朋友，他们的一些想法或许会让你有更大的收获。我们下节课见！</p>',
        article_title: "31 | 容量场景：决定线上容量场景的关键因素是什么？",
      },
      {
        title: "32 | 稳定性场景：怎样搞定线上稳定性场景？",
        id: 473704,
        content:
          '<p>你好，我是高楼。</p><p>要让我来说几个对线上系统来说特别重要的质量属性的话，我觉得稳定性绝对排在最靠前的位置。在我看来，它对一个系统的重要性可以堪比性能，甚至更为重要。</p><h2>稳定性的重要性</h2><p>那一个对稳定性要求特别高的系统，靠什么来支撑呢？我认为高性能、高可用、可扩展的设计绝对是不可或缺的，而且这三点的关系是依次递进的。</p><p>但是在这三个关键的质量属性上，现实的企业又是如何表现的呢？</p><p><strong>为什么技术市场上有很多企业愿意花大量的资金（通常一年都是千万级甚至亿级）购买硬件来保证系统的稳定性，而不愿意花少量的费用（通常是百万级）来做完整的性能项目呢？</strong></p><p>性能项目的价值显然被低估了。为什么会这样呢？这里我给出最主要的两点原因：</p><p>一个原因是，通常业务系统的上线周期中，给技术预留的时间窗口非常有限。一般领导决策、业务分析这些环节就会消耗大量的时间；而到了技术落地的环节，又在架构设计、逻辑设计等层面消耗了大量的时间；等到了实际的代码实现、测试环节呢，它们通常是被认为是体力活的部分，只要堆人就可以堆得出来。虽然我们不同职位的人仍然想奋力实现自己的价值而不断强调自己在做的事情的重要性，但其实你从市场上各职位的薪资情况就可以看得出来自己的实际地位。</p><!-- [[[read_end]]] --><p>不要跟我说虽然你做测试拿得比架构师少，但是干得比他多就比他重要了。重要性不是这么算的，这得看你的职位在一个系统的整个生命周期中处在什么位置。对于测试实施活动来说，因为它处在架构设计、开发之后，所以从流程顺序的角度而言，这是合理的。性能测试实施也不例外。</p><p>但性能工程的思考角度就不一样了，在整个性能工程中，有很多前置的工作。比方说，需要和架构、开发过程紧密结合的工作，需要环境准备、数据准备的工作，需要延伸到运维阶段以环比的工作。这样看来，性能就不仅仅是测试可以覆盖的了。但如果我们只把性能当成“测试”，就注定了时间窗口的局限性，这一点必须承认并接受。</p><p>有些人为了摆脱测试工作的被动并发挥测试的所谓重要性，还提出了“测试左右移”的概念。要我说，左右移听起来是想把一些工作揽到测试这个角色上来。倒不如换个思路，将测试换个名称，比如说叫“巡游质检”之类的，这样往哪移似乎都是可以说得通的。</p><p>其实打破工作流程中对“测试”的局限，根本不是要改变测试的位置，而是要从性能工程的视角来看一个具体的性能项目，不再受到“测试”的局限。这时它在企业中的价值就会体现出来，而且整体的成本并没有增加，因为那些活动都是必须做的，只是之前是被认为兼职着做，而现在当成正式的工作内容了。</p><p>性能项目被低估的第二个原因是，堆硬件确实给一些不缺钱的企业带来了长期稳定的信心，而多数性能测试则做不到这一点。</p><p>有很多企业线上的硬件资源使用长期在10%以下（甚至是5%以下），而不得不等着那用到40%的提心吊胆的一刻。但硬件资源的投入仍然往往被认为是绝对有必要的。而技术人员的成本虽然在整个软件工程中并不算少，但测试工程师绝对是这里面最便宜的一类角色了。从外包市场上可见一斑，一个普通的研发外包叫到三四万应该是常见的，而一个测试外包能到3万就算是绝对的高级了。企业内部在招聘时对测试人员的定价也是同样的逻辑。</p><p>为什么会这样呢？其实简单来说，就是当前的性能测试从业人员不能给人一个系统一定会稳定运行的<strong>信心</strong>。</p><p>这同样依赖于性能项目带给企业的价值。<strong>性能的价值体现是什么呢？就是当你测试了一个系统之后，首先可以给出一个非常明确的能上线不能上线的结论；其次可以给运维非常明确的配置列表，让运维根据你给出的配置来设置生产系统，而且测试场景确实和你的测试结论一致。</strong>这样的性能项目怎么可能让人不放心呢？</p><p>综合以上两点，如果我们能够定义好并发挥好性能项目的价值，在稳定性方面给予企业以信心，那企业自然就会愿意在我们身上多花钱了。</p><h2>稳定性场景的准备工作</h2><p>在运行稳定性场景之前，有几件事情是比较重要的，也是必须要做的：</p><ol>\n<li>检查磁盘空间是不是足够；</li>\n<li>检查系统状态是不是良好；</li>\n<li>检查系统状态数据要不要重置；</li>\n<li>协调各部门做好准备。</li>\n</ol><p>下面我们就按照这四个要点操作一下。我们首先要保证磁盘空间是足够的。</p><p><img src="https://static001.geekbang.org/resource/image/99/f2/99daa5127854714765c162ca48d903f2.png?wh=1920x415" alt="图片"></p><p>这里我们要先来看一遍当前的磁盘空间使用率。从上图可以看到，磁盘还有一些空间可用，但是能支持多长时间的稳定性场景我还不能确定，所以我们要尽量腾出更多的磁盘空间。下面这些可以腾出磁盘空间的动作我建议你都先做一下。</p><p>日志级别在稳定性执行之前，我建议你先调整为Error，以免产生太多的日志。如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/ae/2c/aefc180yy4b30182511cdf42d3fc052c.png?wh=1907x948" alt="图片"></p><p>链路跟踪索引文件也要清理：</p><p><img src="https://static001.geekbang.org/resource/image/65/64/65f3a94e8065f6f7e38daf9bd0377564.png?wh=1920x263" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/29/36/290bffa3a65c0abcb27ca7014f82cd36.png?wh=1920x1018" alt="图片"></p><p>其次，检查系统状态，在我们这个环境中，那就是检查我们在前面提到的全局监控计数器。</p><p>再接着，检查系统状态的重置数据，像MySQL、Redis等的状态数据，我基本在每次执行之前都会刷新一遍，手段就是用flush、truncate等命令或直接重启。</p><p>最后，就是要通知各部门灯光、音效、磨牙棒都准备好，盯好各自的系统和监控视图了。</p><p>但是，现在就可以开始跑了吗？ 别急，我们还要来说一下稳定性场景中的两个关键指标。</p><h2>稳定性场景的两个关键指标</h2><p>这里就涉及到了稳定性的需求问题了，它具体包括：</p><ol>\n<li>稳定性运行时长。</li>\n<li>稳定性要用多大压力运行。</li>\n</ol><p>上节课我们看到TPS能达到1300左右，对吧。接下来，我们要想知道稳定性运行时长，就得给稳定性运行的业务累积量定个小目标，这里我定了个500万。注意哦，这个纯属是我拍脑袋来的。如果你是在自己的项目中，你要知道怎么得到你的稳定性运行目标。</p><p>这一点我在<a href="https://time.geekbang.org/column/intro/100074001">上一个专栏</a>的稳定性章节已经有过详细的说明，这里为了不让你迷路，我也简单说一下。稳定性场景的运行时长，一定不是胡乱猜的，而是根据系统要支持的业务累积量，再结合系统的容量场景结果计算得来的。</p><p>而系统要支持的业务累积量取决于运维周期，有人说现在不都AIOps了吗？还需要人工运维吗？还有运维周期吗？请别那么天真，自动化运维除了在大会和PPT里非常完美之外，能完整又完美落地的企业，不管多大的厂，我还没有见到过。所以你还是乖乖按照老方法把业务累积量算出来就好了，这里我就不多说了。</p><p>再回来看看我的这个示例。我把这个业务累积量定为500万，而前面容量场景告诉了我们TPS能达到1350。那我们的场景运行时长就是：</p><p>$$ \\frac{5000000(业务量） \\div 1350(TPS)}{3600(秒)} \\approx 1(小时)$$</p><p>通过上面的公式，你也可以看出来我是用容量场景的最大TPS来计算的。这是我在RESAR性能工程中定义的。就是说，只要达到业务累积量，稳定性场景可以用容量场景的最大TPS来运行。</p><p>当然你也可以用少一点的TPS来运行，那自然就需要更长的时间了。那这两者有没有区别呢？在我的经验中，从稳定性的角度来说，没有什么区别。如果你要考虑成本、业务特性等等杂七杂八的因素，那估计会有一些区别，但那就是另一个话题了。</p><p>解释清楚这两个稳定性场景的关键前提之后，下面我们就可以来运行了。</p><h2>运行稳定性场景 Round1</h2><p>终于能把场景运行起来了，是不是等这一刻也挺久的了？这里我还是沿用容量场景中的配置，只是把时间设置为1个多小时（只要大于前面计算的时长即可）。</p><p><img src="https://static001.geekbang.org/resource/image/ca/e9/ca9f549a7cdf46c67a67d3a2a5c506e9.png?wh=1499x451" alt="图片"></p><p>真是不动真格不知道，一动真格就掉链子。眼睁睁地看着TPS像西去的太阳就往下落了。</p><p><img src="https://static001.geekbang.org/resource/image/45/e4/45041659f1152d386334a9e7eaa6ede4.png?wh=1849x741" alt="图片"></p><p>这种系统越来越慢的情况，前面我们已经分析很多遍了，就是走RESAR性能工程的性能分析七步法，找到问题的源头。在这里我直接给出分析的结论部分。</p><p>在稳定性场景持续运行过程中，我发现有几个SQL会越来越慢，这里我挑出两个来给你看看。</p><ul>\n<li>SQL1: 第一次观察</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/4f/20/4fa08e4c8891310a2d92ff67a1b04f20.png?wh=1899x439" alt="图片"></p><ul>\n<li>SQL1: 第二次观察</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/e4/1b/e4cf2678de72a50e5cb3878669e4cf1b.png?wh=1908x448" alt="图片"></p><ul>\n<li>SQL1: 第三次观察</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/9f/6f/9f5fc60a9afyyfbcb915fyy6bc63836f.png?wh=1908x444" alt="图片"></p><ul>\n<li>SQL2: 第一次观察</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/2b/75/2bf90c22a840be39d01e68ffb97e4975.png?wh=1910x858" alt="图片"></p><ul>\n<li>SQL2: 第二次观察</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/f8/c1/f87eac825e7e7c14d34dac8624608ec1.png?wh=1913x862" alt="图片"></p><ul>\n<li>SQL2: 第三次观察</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/97/b3/97e188169512ab80fb352a15f4a4c7b3.png?wh=1910x863" alt="图片"><br>\n看到这种SQL越来越慢的情况，我们就可以把慢日志用上了。先来配置一下慢日志。</p><pre><code class="language-java">slow_query_log= 1 \nlong_query_time = 0.1\n</code></pre><p>再到数据库中查一下慢日志的路径。</p><p><img src="https://static001.geekbang.org/resource/image/5f/ea/5fbe5f55f87317c040976259e0535eea.png?wh=460x148" alt="图片"></p><p>然后，用pt-query-digest把慢日志梳理一下，看下慢的SQL有哪些。这里我把我发现的前两个慢的SQL展示在这里。</p><pre><code class="language-java">gaolou@GaoMacPro mysqlslowLog % ./pt-query-digest db-mysql-shadow-7b4cfd69fc-5brxd-slow.log\n\n# A software update is available:\n\n# 1.4s user time, 230ms system time, 36.56M rss, 4.15G vsz\n# Current date: Sun Dec 12 13:23:42 2021\n# Hostname: GaoMacPro\n# Files: db-mysql-shadow-7b4cfd69fc-5brxd-slow.log\n# Overall: 7.37k total, 28 unique, 0.56 QPS, 0.07x concurrency ___________\n# Time range: 2021-12-12T01:33:45 to 2021-12-12T05:12:37\n# Attribute          total     min     max     avg     95%  stddev  median\n# ============     ======= ======= ======= ======= ======= ======= =======\n# Exec time           938s   100ms      1s   127ms   171ms    73ms   110ms\n# Lock time          633ms       0     1ms    85us   144us    49us    66us\n# Rows sent          1.33M       0   6.47k  188.89    4.96   1.03k    0.99\n# Rows examine      10.23M       0 740.27k   1.42k   1.61k   8.46k   1.20k\n# Query size         3.28M       6   1.25k  466.26  874.75  390.87   92.72\n\n# Profile\n# Rank Query ID                            Response time  Calls R/Call V/M\n# ==== =================================== ============== ===== ====== ===\n#    1 0xCE1BA85B4464E7410BF01A7637544A08  371.2299 39.6%  3080 0.1205  0.02 SELECT oms_order\n#    2 0xEA7B7E3B4E7276C29456ABBAA4C32D90  275.2189 29.3%  2236 0.1231  0.04 SELECT oms_order\n#    3 0xB694F8C5658ACE1E9488C66225BD6B97  188.9060 20.1%  1468 0.1287  0.06 UPDATE oms_cart_item\n#    4 0xD732B16862C1BC710680BB9679650648   29.2493  3.1%   147 0.1990  0.14 SELECT oms_cart_item\n#    5 0x56A0B23D82E7DDCA8D6F24101F288802   28.2842  3.0%   235 0.1204  0.00 SELECT pms_product pms_sku_stock pms_product_ladder pms_product_full_reduction\n# MISC 0xMISC                               45.0501  4.8%   204 0.2208   0.0 &lt;23 ITEMS&gt;\n\n# Query 1: 0.29 QPS, 0.03x concurrency, ID 0xCE1BA85B4464E7410BF01A7637544A08 at byte 1963385\n# Scores: V/M = 0.02\n# Time range: 2021-12-12T02:14:47 to 2021-12-12T05:12:37\n# Attribute    pct   total     min     max     avg     95%  stddev  median\n# ============ === ======= ======= ======= ======= ======= ======= =======\n# Count         41    3080\n# Exec time     39    371s   100ms      1s   121ms   148ms    49ms   110ms\n# Lock time     61   392ms    78us     1ms   127us   152us    43us   119us\n# Rows sent      1  15.04k       5       5       5       5       0       5\n# Rows examine  37   3.88M     716   1.71k   1.29k   1.61k  143.73   1.20k\n# Query size    80   2.64M     898     901  899.89  874.75       0  874.75\n# String:\n# Databases    mall\n# Hosts        10.100.4.0 (1035/33%), 10.100.3.0 (1023/33%)... 1 more\n# Users        reader\n# Query_time distribution\n#   1us\n#  10us\n# 100us\n#   1ms\n#  10ms\n# 100ms  ################################################################\n#    1s  #\n#  10s+\n# Tables\n#    SHOW TABLE STATUS FROM `mall` LIKE \'oms_order\'\\G\n#    SHOW CREATE TABLE `mall`.`oms_order`\\G\n# EXPLAIN /*!50100 PARTITIONS*/\nselect\n    id, member_id, coupon_id, order_sn, create_time, member_username, total_amount, pay_amount,\n    freight_amount, promotion_amount, integration_amount, coupon_amount, discount_amount,\n    pay_type, source_type, status, order_type, delivery_company, delivery_sn, auto_confirm_day,\n    integration, growth, promotion_info, bill_type, bill_header, bill_content, bill_receiver_phone,\n    bill_receiver_email, receiver_name, receiver_phone, receiver_post_code, receiver_province,\n    receiver_city, receiver_region, receiver_detail_address, note, confirm_status, delete_status,\n    use_integration, payment_time, delivery_time, receive_time, comment_time, modify_time\n    from oms_order\n     WHERE (  delete_status = 0\n                  and member_id = 651 )\n      order by create_time desc LIMIT 5\\G\n\n# Query 2: 0.17 QPS, 0.02x concurrency, ID 0xEA7B7E3B4E7276C29456ABBAA4C32D90 at byte 1963110\n# Scores: V/M = 0.04\n# Time range: 2021-12-12T01:33:45 to 2021-12-12T05:12:36\n# Attribute    pct   total     min     max     avg     95%  stddev  median\n# ============ === ======= ======= ======= ======= ======= ======= =======\n# Count         30    2236\n# Exec time     29    275s   100ms      1s   123ms   148ms    66ms   110ms\n# Lock time     19   123ms    24us   480us    55us    73us    17us    52us\n# Rows sent      0   2.18k       1       1       1       1       0       1\n# Rows examine  26   2.75M     416   1.70k   1.26k   1.61k  136.22   1.20k\n# Query size     4 165.68k      74      76   75.87   72.65       0   72.65\n# String:\n# Databases    mall\n# Hosts        10.100.2.114 (772/34%), 10.100.4.0 (761/34%)... 1 more\n# Users        reader\n# Query_time distribution\n#   1us\n#  10us\n# 100us\n#   1ms\n#  10ms\n# 100ms  ################################################################\n#    1s  #\n#  10s+\n# Tables\n#    SHOW TABLE STATUS FROM `mall` LIKE \'oms_order\'\\G\n#    SHOW CREATE TABLE `mall`.`oms_order`\\G\n# EXPLAIN /*!50100 PARTITIONS*/\nSELECT count(0) FROM oms_order WHERE (delete_status = 0 AND member_id = 658)\\G\n\n................................\n</code></pre><p>这两个SQL显然都和会员相关，并且都涉及到了Order表，另外我还发现它们带宽用得越来越大。</p><p><img src="https://static001.geekbang.org/resource/image/e3/2d/e36c63d6e7377d36d673b38cf3cdfa2d.png?wh=1819x321" alt="图片"></p><p>也就是说持续时间越长，即便是压力并没有增加，带宽使用得也会越来越大。</p><p>如果业务是固定的话，这种情况显然是不合理的哇。 我仔细考虑了场景的逻辑，觉得应该不会出现越来越大的数据，除非是搞错了。那接下来怎么办呢？</p><p>既然带宽增长是和member_id、Order表有关，那我们就到Order表里按member_id做一个直方图来看看。结果，不看不知道，一看气血直往上涌。表里只有1000个会员的订单。</p><p><img src="https://static001.geekbang.org/resource/image/82/0c/828e5cddb2544fd574f5c7a0c65fea0c.png?wh=1710x421" alt="图片"></p><p>看到这里，如果你是有性能参数化经验的人，大概就可以想到原因了，这是请求的参数中只包括了1000个会员导致的。</p><p>而我们这里是通过录制回放的方式生成的数据，可见录制的源头也只有1000个用户的数据。我们追回去重新查询了数据，发现果然在JMeter中只用了1000个会员的数据。</p><p>没办法，我们只能把参数化数据重做一遍了。这里也要说明一下，经常会有人使用少量的用户模拟容量场景和稳定性场景，这里一定要注意用足够的参数化数据，要不然就只能像我们一样重新再跑一遍了。</p><h2>运行稳定性场景 Round2</h2><p>好了，重做了一遍参数化数据之后，我们再把场景运行起来。我们是在晚上睡觉之前把场景跑起来的，到了第二天一看，好几个容器都起不来了。 这是什么问题呢？</p><p><img src="https://static001.geekbang.org/resource/image/12/99/12edaffbb95c7204e91af845baccbc99.png?wh=735x444" alt="图片"></p><p>再查看一下k8s里的事件，这里我看到有大量因为磁盘不足导致的容器被驱逐的问题。虽然前面我们已经有过很多操作，想要尽可能保证磁盘空间够用，结果还是着了道。</p><p><img src="https://static001.geekbang.org/resource/image/6d/42/6d72e222433a14979bbb0414ca166242.png?wh=1251x569" alt="图片"></p><p>再打开监控工具，发现大部分的数据都展示不出来了。</p><p><img src="https://static001.geekbang.org/resource/image/77/26/770f100560d197604a27fb4544217e26.png?wh=1920x412" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/84/86/84619d33c9c7bf5c54a316874e2b9f86.png?wh=1920x945" alt="图片"></p><p>这就是典型的磁盘不足导致的结果。</p><p>所说，如果你是在执行稳定性场景，一定要计算好磁盘的空间。刚才我已经强调了磁盘空间会导致稳定性失败的情况，但是因为我在执行的过程中没有设置GoReplay的运行时长（-exit-after参数），导致这里还是出了问题，让整个系统都玩不转了。没办法，只有先清理出足够的磁盘空间，再把各种应用启动起来查看数据。</p><pre><code class="language-java">[root@s9 /]# df -h\nFilesystem                                                                                                             Size  Used Avail Use% Mounted on\ndevtmpfs                                                                                                               7.5G     0  7.5G   0% /dev\ntmpfs                                                                                                                  7.6G     0  7.6G   0% /dev/shm\ntmpfs                                                                                                                  7.6G  2.1M  7.5G   1% /run\ntmpfs                                                                                                                  7.6G     0  7.6G   0% /sys/fs/cgroup\n/dev/vda1                                                                                                               40G   16G   23G  41% /\n.................\n172.31.184.230:/nfs/data/default-elasticsearch-master-elasticsearch-master-2-pvc-150ba193-b0c7-4a3b-a303-3d98c94aebdc   99G   89G  5.7G  95% /var/lib/kubelet/pods/291c3569-730d-4efd-a8b8-83fbd2b2ef90/volumes/kubernetes.io~nfs/pvc-150ba193-b0c7-4a3b-a303-3d98c94aebdc\n.................\n172.31.184.230:/nfs/data/default-data-kafka-0-pvc-1dad22ee-9870-41cc-9ad9-3833b7930701                                  99G   89G  5.7G  95% /var/lib/kubelet/pods/27c1eb21-3c98-4a2e-a18a-0ee6c5c98466/volumes/kubernetes.io~nfs/pvc-1dad22ee-9870-41cc-9ad9-3833b7930701\n.................\n172.31.184.230:/nfs/data/default-data-zookeeper-0-pvc-f0af4c78-75e0-4d19-b748-b3471fd994d0                              99G   89G  5.7G  95% /var/lib/kubelet/pods/ec234fbe-7711-4696-82bd-055ada800b8b/volumes/kubernetes.io~nfs/pvc-f0af4c78-75e0-4d19-b748-b3471fd994d0\n.................\n</code></pre><p>从上面的信息可以看到，挂载的NFS都达到了95的磁盘消耗。而k8s默认的情况下，磁盘空间达到75%就会驱逐容器了。</p><p>经过了一轮磁盘空间清理之后，各机器也有了足够的磁盘空间。</p><pre><code class="language-java">[root@s9 mall]# df -h\nFilesystem                                                                                                             Size  Used Avail Use% Mounted on\ndevtmpfs                                                                                                               7.5G     0  7.5G   0% /dev\ntmpfs                                                                                                                  7.6G     0  7.6G   0% /dev/shm\ntmpfs                                                                                                                  7.6G  2.0M  7.5G   1% /run\ntmpfs                                                                                                                  7.6G     0  7.6G   0% /sys/fs/cgroup\n/dev/vda1                                                                                                               40G   17G   22G  44% /\n.................\n172.31.184.230:/nfs/data/default-elasticsearch-master-elasticsearch-master-2-pvc-150ba193-b0c7-4a3b-a303-3d98c94aebdc   99G   61G   34G  65% /var/lib/kubelet/pods/291c3569-730d-4efd-a8b8-83fbd2b2ef90/volumes/kubernetes.io~nfs/pvc-150ba193-b0c7-4a3b-a303-3d98c94aebdc\n.................\n172.31.184.230:/nfs/data/default-data-kafka-0-pvc-1dad22ee-9870-41cc-9ad9-3833b7930701                                  99G   61G   34G  65% /var/lib/kubelet/pods/27c1eb21-3c98-4a2e-a18a-0ee6c5c98466/volumes/kubernetes.io~nfs/pvc-1dad22ee-9870-41cc-9ad9-3833b7930701\n.................\n172.31.184.230:/nfs/data/default-data-zookeeper-0-pvc-f0af4c78-75e0-4d19-b748-b3471fd994d0                              99G   61G   34G  65% /var/lib/kubelet/pods/ec234fbe-7711-4696-82bd-055ada800b8b/volumes/kubernetes.io~nfs/pvc-f0af4c78-75e0-4d19-b748-b3471fd994d0\n.................                                                                                                            \n</code></pre><p>我们再把场景运行起来。这次我们还是老老实实地运行时间短一些，能达到我们的小目标（500万的累积数据量）就可以了。</p><p>这里我还是用JMeter模拟正常的生产流量，GoReplay模拟压测流量。我们一起来看看结果吧：</p><ul>\n<li>JMeter的运行结果：</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/fd/39/fdf29d785bb4b2fa7d64065be1538939.png?wh=1843x732" alt="图片"><br>\n从JMeter的运行结果来看，在近两个小时的时间内，业务累积的请求量超过了102万。TPS稳定，响应时间也很稳定。</p><p>上图中间有一个掉下来的缺口，应该是和某个单节点的服务FullGC有关，这里我不再对它进行具体分析了。因为对稳定性来说，我们关注的是整体的累积业务量。</p><ul>\n<li>GoReplay的运行结果：<br>\n<img src="https://static001.geekbang.org/resource/image/39/42/3953e7a25cde8d6a06cb0689faa1aa42.png?wh=1830x740" alt="图片"></li>\n</ul><p>从TPS的趋势图上来看，TPS非常稳定。但因为GoReplay没有总请求量的数据，我们只有拿它的TPS来和JMeter的TPS进行比较计算。也就是：<br>\n$$102万 \\times \\frac{1000}{150} \\approx 680万$$</p><p>这里之所以用1000来计算，是看到TPS曲线接近1000，这里没有达到容量场景的1350的原因是经过了长时间的运行，数据库中的数据比之前又增加了一些，SQL的执行时间变长了一点。即便TPS也有低的时候，也应该不会小于500万的数据量了。</p><p>再来观察一下这段时间的资源使用情况。</p><p><img src="https://static001.geekbang.org/resource/image/07/b4/07943e6306c2933ff7e4b8f16137cab4.png?wh=1787x384" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/1e/ec/1e0b91dcf197af1eefac0bbbe46d22ec.png?wh=1852x820" alt="图片"></p><p>可以看到，资源也都一直保持良好的状态。这样，我们的稳定性场景就可以顺利完成了。</p><h2>总结</h2><p>好了，这节课就讲到这里。</p><p>稳定性场景是一个系统能否持续运行下去的证据，对于稳定性场景来说，最重要的当然就是稳定。当然，对于稳定的定义，不同的系统也有不同的逻辑。不过通常我们要关注的就是稳定性场景的两个重要指标：<strong>稳定性运行时长和TPS量级。</strong>不管你用多大的TPS量级来运行稳定性场景，业务累积量都是要达到的。</p><p>我们经常听到企业中说“系统要7*24运行，可用性要达到99.99%”，可是达到这样的目标，要用什么样的技术实现来保证呢。只喊一句空话倒也是简单，但如果可用性的百分比只能事后计算，那也意义不大。</p><p>如果你想用稳定性场景来覆盖可用性的需求，其实是可以做得到的，但是，必须要考虑成本。就拿724这种在大部分企业中都只是挂在嘴上而不落地的虚假指标来说，如果想把它变成可以落地的不虚假的指标，就需要把一个系统完整地运行一个星期。你可以想像在参数准备、环境占用、人力保证等各方面的成本投入。至今为止，我还没见过几个企业真正做到过724的稳定性场景。那怎么办呢？结果就是运维背锅呗。这种情况是我们都不愿意看到的。</p><p>刚才说的这些听起来非常完美的指标理论上不是不对，只是成本太高很难实现。毕竟让系统无损地持续下去才是关键，所以我们只要按照今天所讲的逻辑把稳定性场景运行起来，把问题都解决掉，这就很好了。</p><h2>问题</h2><p>学完这节课，请你思考两个问题：</p><ol>\n<li>在你经历过的性能项目中，稳定性场景是如何设计的？</li>\n<li>你觉得稳定性场景在落地过程中最大的难点在哪里？</li>\n</ol><p>欢迎你在留言区与我交流讨论。当然了，你也可以把这节课分享给你身边的朋友，他们的一些想法或许会让你有更大的收获。我们下节课见！</p>',
        article_title: "32 | 稳定性场景：怎样搞定线上稳定性场景？",
      },
      {
        title: "33 | 异常场景：如何模拟线上不同组件的异常场景？",
        id: 474504,
        content:
          '<p>你好，我是高楼。</p><p>这节课，我们来讲讲如何模拟全链路压测的异常场景。异常场景的本意就是通过在场景正常运行时对故障进行模拟，来查看故障产生时业务的响应能力。相比以前的SOA技术架构，全链路压测的异常场景更复杂了一些。复杂点来自于微服务分布式架构的特性、容器化和云基础环境等内容。</p><h2>异常场景有哪些？</h2><p>我们以前做的异常场景基本上是：宕主机、宕应用、宕网卡。而在我们云原生的微服务分布式的全链路逻辑中，需要考虑的异常场景就更多了，我给你画了一张思维导图，你可以参考一下。</p><p><img src="https://static001.geekbang.org/resource/image/26/28/26b4c67e7048322ca43e009a175c3f28.jpg?wh=1920x1402" alt="图片"></p><p>除了异常场景，还有两个词经常被拿来表示故障模拟：混沌工程和非功能测试。</p><p>混沌工程这个词这几年非常流行，想必大家是听说过的。不过它的概念虽然听起来非常高级，经过我的观察，现在我们看到的到混沌工程方向上的开源工具，其实有着一定的局限性。它基本上要实现的就是容器异常、代码异常、应用异常、系统异常这几个方面， 结合我们上面的图来看，你会发现像基础设施级的故障它就模拟不到，所以说混沌工具现在还是有缺失的。</p><p>故障模拟在企业中的另一个叫法就是非功能测试。如果你接触过企业里的非功能测试案例，会有一个直观的感受，那就是故障案例都是模拟的非常重要的故障点。但是！不管是叫异常、还是混沌、还是非功能，我们所能够看到的案例都是有限的几个、几十个，顶多上百个，在企业里能看到上千个案例的非常少。</p><!-- [[[read_end]]] --><p>如果说通过前面我们提到的基准场景、容量场景、稳定性场景就已经可以说明一个系统在正常运行时的性能了，那么异常场景到底应该如何设计才能做到全面覆盖呢？这是一个非常难回答的问题。大部分人可能根本不会从测试的全面性的角度来思考这个问题，而是选择一些看似很重要的案例来执行一下就好了。你可以说这是一种不负责任的做法，当然也可以认为只能是这样了，因为没有一个更好的方式。</p><p>我做性能这些年来，对业务异常的思考也从来没有停止过。借助工作上的一些契机，最近我在尝试做一个之前从来没有做过的事情，那就是把异常场景考虑完整。为了说明我是怎么思考的，我把我们这个项目的架构图又放在了下面。</p><p><img src="https://static001.geekbang.org/resource/image/e8/fa/e855108b770a14c819ac1af6a3c67bfa.jpg?wh=1920x1590" alt=""></p><p>从图中你可以看到，我在一些关键的部分画上了红色的框框。为什么要画这些红框呢？因为如果让我设计异常场景的话，从一个架构的视角来看，我就会从这些角度去设计。但是，只考虑这些就够了吗？如果你是买的云虚拟机，从架构上看，应该是没有盲点了。但是底层的硬件、网络仍然不在掌控范围内。所以如果一个企业决定用云厂商提供的环境，底层的硬件故障就不可能模拟得到了。</p><p>如果你天真地问：“云厂商提供的环境不是可以不考虑硬件吗？由厂商来保证可用性就可以了呀。”那我估计你是没有经历过因为厂商硬件损坏导致的不可逆故障。Netflix刚开始想做混沌工程，就是因为遇到了几次云厂商故障导致的事故，他们这才想到用混沌工程的逻辑来验证系统的稳定性。其实严格点来说，混沌工程就是硬件不在手里不得已而为之的一个思维逻辑。</p><p>现在再回头看看，如果我想把异常故障场景考虑周全，还需要做什么呢？在混沌工程的逻辑之上，我们只要再多考虑一下硬件基础设施的故障就可以了。这样就和我刚才给出的异常场景思维导图对应上了。</p><p>光说不练假把式，只有理论还不行，我们还是要落地才会有真实的效果展现。下面我们就落地一个应用级的故障点来看一下要考虑的东西有多少。</p><p>拿这个专栏用的应用节点（其他各节点也都类似）来说，我们虽然有k8s+docker来保证调度的均衡，但是当节点出现故障的时候，要想评估故障对用户产生的影响，我们还是要从业务代码、JDK、框架等各种角度来模拟更为全面的故障，你可以看看下面我给你画的这张图：</p><p><img src="https://static001.geekbang.org/resource/image/db/7a/dbd5cba81fa1ff76050da281a5f10a7a.jpg?wh=1920x913" alt="图片"></p><p>在具体落地的时候，我们还可以对这张图再进行细化。比如，对业务代码故障，我们可以去模拟方法执行时间长、内存溢出等故障；对容器，我们可以模拟CPU、内存等故障；对网络，我们可以模拟延迟、丢包、抖动、重复包等故障。这样一个个列下去，这张图就会变得非常完整了。</p><p>那说了这么多的故障分析逻辑，我们还是要通过具体的案例来看看异常场景应该如何操作。下面我会用把原理解说和混沌工具的使用结合起来，给你演示一个非常具体的异常场景。在这里我虽然会使用工具，但是我不想强调工具的作用，因为工具是实现思维逻辑的，没有逻辑，工具是没有灵魂的。</p><p>我选择了一个重要的场景进行演示，那就是响应延迟场景。因为很多故障出现的时候，都会表现为响应延迟。比如说方法执行时间长、内存在慢慢泄露、CPU负载高、IO负载高、线程配置不合理等等原因都有可能导致一个服务出现响应延迟。</p><p>下面我就通过模拟完整的网络延迟故障，来让你理解异常场景的具体落地过程和原理。</p><h2>网络延迟故障模拟</h2><p>我们来模拟一下Pod级网络延迟的异常场景。</p><p>首先，我们把场景跑起来。这里为了操作简单，我只用了一个访问首页的接口，毕竟异常场景也不是为了看最大容量。</p><p><img src="https://static001.geekbang.org/resource/image/5e/e9/5e3f8ea1ab3b505e679918yy409f36e9.png?wh=1821x664" alt="图片"></p><p>好，我们把场景跑了一会儿之后，看到TPS已经非常稳定了。下面我们就来模拟一下网络的延迟。这一次我使用ChaosMesh这个工具，如果你对这个工具不熟悉，可以参考一下这几篇文章：</p><ul>\n<li><a href="https://mp.weixin.qq.com/s/9EvKaTiq62OwfpxqyOTXvQ">《混沌工具之ChaosMesh编译安装》</a></li>\n<li><a href="https://mp.weixin.qq.com/s/3O-cYIggprhgzORMvZOcxg">《混沌工程之ChaosMesh使用之一模拟CPU使用率》</a></li>\n<li><a href="https://mp.weixin.qq.com/s/XSIsO2N1sUvnmxMqSOTO8Q">《混沌工程之ChaosMesh使用之二模拟POD网络延迟》</a></li>\n<li><a href="https://mp.weixin.qq.com/s/pWd-R7aZVRaNMFt2wZCMDQ">《混沌工程之ChaosMesh使用之二模拟POD网络丢包》</a></li>\n<li><a href="https://mp.weixin.qq.com/s/KTkPQ9WHgqG11X5y3Fgc5g">《混沌工程之ChaosMesh使用之三模拟POD网络丢包》</a></li>\n<li><a href="https://mp.weixin.qq.com/s/twq-3Pprm14F5BVjBy3SbA">《混沌工程之ChaosMesh使用之四模拟网络Duplicate包》</a></li>\n</ul><p>打开ChaosMesh，创建一个新的实验。</p><p><img src="https://static001.geekbang.org/resource/image/a6/d9/a631c0464dea2613aed962b3486e37d9.png?wh=1017x810" alt="图片"></p><p>选择网络攻击，再选择DELAY，写入Latency 1000ms，提交。</p><p><img src="https://static001.geekbang.org/resource/image/26/5c/2600b8854e63db77ce119d79e1e3fc5c.png?wh=1097x975" alt="图片"></p><p>接着配置范围，选择Portal服务中的三个节点，填入名称，点击持续运行。</p><p>注意，这里先别点提交。我们先到容器里来看一下网络队列有没有配置。上图中的容器是svc-mall-portal-5679f4cdbd-fvqpt，现在进入到这个容器中去。</p><p><img src="https://static001.geekbang.org/resource/image/8f/c5/8f255c6fe452f75cbd5073yy9bc1abc5.png?wh=1513x363" alt="图片"></p><p>下一步点击“Bash”，可以直接进入到这个窗口的Shell中。执行如下命令：</p><pre><code class="language-java">[root@svc-mall-portal-5679f4cdbd-fvqpt /]# tc QDisc ls dev eth0\nQDisc noqueue 0: root refcnt 2 \n</code></pre><p>可以看到，当前容器的网卡QDisc队列中什么也没有配置。</p><p>我们再回到ChaosMesh的界面中点击两次提交，就看到下面这个界面：</p><p><img src="https://static001.geekbang.org/resource/image/d6/f9/d6ab269aeac8b79e28e0f3119c69a2f9.png?wh=1458x129" alt="图片"></p><p>再次进入到这个容器的Shell容器执行命令：</p><pre><code class="language-java">[root@svc-mall-portal-5679f4cdbd-fvqpt /]# tc QDisc ls dev eth0\nQDisc netem 1: root refcnt 2 limit 1000 delay 1.0s\n</code></pre><p>可以看到，这里产生了一个延迟1秒的配置。</p><p>现在我们回到TPS界面看一眼效果。</p><p><img src="https://static001.geekbang.org/resource/image/7b/9e/7b3f69508d24d4e211424e42a60c899e.png?wh=1812x665" alt="图片"></p><p>发现居然所有请求的响应时间都上升了，并且没有继续提升上去。</p><p><img src="https://static001.geekbang.org/resource/image/8f/4c/8fb9b5c3858328388a2ce274647ed54c.png?wh=1466x120" alt="图片"></p><p>我们停止运行这个实验，再回来看看TPS。</p><p><img src="https://static001.geekbang.org/resource/image/b6/33/b66fed20ff7170d499edf82bb57c1433.png?wh=1816x665" alt="图片"></p><p>通过截图可以看到，TPS已经恢复了。</p><p>现在我们来解释一下这个实验的实现原理。为什么我在提交实验之前先去查了一遍网络呢？这是为了让你看到，一开始在网卡排队规则上没有任何的延迟配置，我们是在提交了实验之后才看到了1秒的延迟配置。这就是这个实验的核心。</p><p>那这个QDisc是个什么队列呢？我们来看一张数据传输图。</p><p><img src="https://static001.geekbang.org/resource/image/4c/bc/4cb4d6c85043046be3af50263a99d8bc.jpg?wh=1920x1165" alt=""></p><p>请注意看我在图上标识的红框，这个QDisc就是我们前面的实验操作控制的网卡排队的地方。现在我们来说明一下这个QDisc。</p><blockquote>\n<p>QDisc(排队规则)是queueing discipline的简写，Linux内核需要通过网络接口发送数据包，在发送时都要为网络接口配置QDisc（排队规则），数据包要根据这个规则加入到队列当中。</p>\n</blockquote><p>我们的实验就是通过控制这个排队规则实现网络包的延迟发送的。</p><p>其实你没有工具也照样可以做到这一步，你可以直接使用TC命令来操作QDisc，实现的逻辑是完全一致的。具体的操作你可以参考这篇文章：<a href="https://mp.weixin.qq.com/s/O0OgE51El_rF2EZ02LGjdA">《性能场景之网络模拟》</a>。</p><p>在k8s架构中，要实现上面这个实验的配置文件内容如下：</p><pre><code class="language-java">kind: NetworkChaos\napiVersion: chaos-mesh.org/v1alpha1\nmetadata:\n  name: portaldelay6\n  namespace: default\n  annotations:\n    experiment.chaos-mesh.org/pause: \'true\'\nspec:\n  selector:\n    namespaces:\n      - default\n    labelSelectors:\n      app: svc-mall-portal\n  mode: one\n  action: delay\n  delay:\n    latency: 1000ms\n    correlation: \'0\'\n    jitter: 0ms\n  direction: to\n</code></pre><p>你可以直接创建一个yml文件，把上面的内容放进去，直接执行一下kubectl apply -f &lt;文件名&gt;即可实现。</p><p>所以你看，我们不用特别依赖工具，手工的操作也同样可以做到。像ChaoBlade/ChaosToolkit也都是这样的实现逻辑。</p><h2>Pod驱逐故障模拟</h2><p>下面我们再来看一下Pod驱逐的异常场景。Pod被驱逐应该说是k8s的环境中比较常见的问题之一了。</p><p>我们还是先把场景跑起来，等待TPS稳定了之后再操作。</p><p><img src="https://static001.geekbang.org/resource/image/78/42/78f21c64ff7746baa98byy308bbdc242.png?wh=1826x681" alt="图片"></p><p>这次我们不借助什么混沌工具了，等TPS稳定之后，我们直接用k8s的管理工具实现驱逐。</p><p><img src="https://static001.geekbang.org/resource/image/74/66/74c9606b942c0047416d40fb6e12e066.png?wh=1851x873" alt="图片"></p><p>直接点击驱逐。</p><p><img src="https://static001.geekbang.org/resource/image/d5/b0/d5c0b296c2626c4c0211be6a622f24b0.png?wh=1148x290" alt="图片"></p><p>点击确定，再来看看我们的TPS。</p><p><img src="https://static001.geekbang.org/resource/image/yy/db/yy086a0b3f5fa7bfcc2021acd9e18bdb.png?wh=1814x667" alt="图片"></p><p>可以看到，TPS确实又掉下来了。再检查一下现在被驱逐的容器调度到哪台机器上去了。</p><p><img src="https://static001.geekbang.org/resource/image/51/02/51ca78fd5d1b01fc79b5e9b92db8ab02.png?wh=1490x369" alt="图片"></p><p>看，现在没有s8了，因为我们已经把它驱逐掉了。</p><h2>总结</h2><p>好了，这节课就讲到这里。刚才，我对我认为比较重要和常见的两个异常场景进行了模拟。当然了，除此之外的异常场景还有很多。你可以根据我这节课讲的内容，自已尝试模拟一下其他的异常场景，看一下应用架构的应用策略是否符合业务的要求。</p><p>在微服务分布式架构中，不管系统是不是全链路的，其实异常场景的范围是不会有什么变化的。我们要注意的是，如果你使用的是我们这个专栏的全链路的逻辑，那在做异常场景时，因为真实流量和压测流量走的是同样的服务，所以故障会同时影响到正常流量和压测流量。</p><p>如果你想在生产环境中做故障模拟，我还是提醒你小心行事，避免因为操作失误产生额外的风险。如果你想实现的是灰度发布的逻辑，也就是我们在<a href="https://time.geekbang.org/column/article/462212">第20讲</a>中提到的Service Mesh的发布逻辑，那么因为压测流量和正式流量走的是不同的应用节点，就不会产生相互的影响。但那样的话，也就不是真正的全链路了，你可以把它看成是完全独立的两个应用链路。</p><h2>课后题</h2><p>学完这节课，请你思考两个问题：</p><ol>\n<li>你可以根据项目中的应用架构列出需要测试的异常场景吗？</li>\n<li>如果要模拟CPU负载异常，你能想出几种方式？</li>\n</ol><p>今天是2022年的第一天，希望你有个漂亮的开年。2021已经结束了，但学习不会止步。也欢迎你在留言区继续与我交流讨论。我们下节课再见！</p>',
        article_title: "33 | 异常场景：如何模拟线上不同组件的异常场景？",
      },
      {
        title: "34 | 容量规划：如何精准地对生产系统做容量预估？",
        id: 474925,
        content:
          '<p>你好，我是高楼。</p><p>到这里啊，我们专栏的主体内容已经讲得差不多了，场景也已经跑起来了。这节课，我们就来聊一聊容量规划。</p><p>对于一个线上系统来说，精准地判断系统最大容量是保证系统能稳定运行的前提条件，而对于大部分系统来说，它们的规划和评估都太过粗略，以至于不是浪费了资源就是随时等死，所以容量评估也必然是性能项目要产出的结论之一。这也是全链路线上压测出现的原因。</p><p>对于系统级的容量评估，在当前的技术市场中，除了拿硬件怼上去之外，似乎很难找到什么有效的方法。</p><p>我知道有些人会说，不是有TPC-C、TPC-E…之类的吗？不是有基于CPU算力的XX模型吗？不管你怎么说，在实际的工作场景中，大部分的算法也还是下面这样的：</p><blockquote>\n<p>当一个节点支持100TPS时，有100个同样的节点就大概能支持:<br>\n100 × 100 × 80% ≈ 8000（TPS）</p>\n</blockquote><p>之所以乘以个80%，是因为我们从心理上就觉得，实际场景可以支持的TPS总量好像可以，但又大概率达不到线性增加的程度，因为一定会有性能的损耗。但是这样算来显得也太不专业了，一个复杂的系统怎么能用小学生就能理解的公式来表达呢。确实，这么简单的公式会忽略很多问题，它过于简单粗暴不说，也非常不精准。</p><p>所以，一些架构师在做系统规划的时候会根据TPC-C来计算，下面我们就来说一说这种计算方式，同时也聊一聊磁盘和网络容量的评估方式。</p><!-- [[[read_end]]] --><h2>TPC-C容量评估</h2><p>在讲解TPC-C之前，我们先来了解一下TPC。TPC（Transaction Processing Performance Council）的全称是事务处理性能委员会，它是一个由数十家会员企业组成的非盈利组织。TPC主要负责制定商务应用基准性能测试的标准规范、性能和价格度量，还负责管理测试结果的发布。而所有的发布结果都将由TPC授权的独立审核员审核。</p><p>同时，为了保证结果的公允，TPC还成立了技术咨询委员会（TAB）。如果有人对TPC发布的结果有疑问，他们可以去TAB理论。注意，他们只会发布应用程序性能评估的基准性能测试标准规范，企业的测试代码是可以自己编写的。</p><p>那TPC和 TPC-A、TPC-B、TPC-C、TPC-E有什么关系呢？我们其实可以把它们看成是TPC推出的基准性能测试标准规范的不同版本，TPC-A、TPC-B是前两个版本，TPC-C版本出来之后，就迅速替换了前两个版本。</p><p>TPC-A（1989年发布)主要是对借记卡在局域网和广域网中的性能进行基准评估，它增加了对ACID的要求。</p><p>TPC-B（1991年发布）使用了和TPC-A相同的事务类型，但它取消了TPC-A中的网络和用户交互组件，剩下的是一个批处理事务基准。</p><p>TPC-C（1992年发布）使用的是商品批发销售公司的模型。它主要使用了新订单、支付操作、发货、订单状态查询、库存状态查询这五个事务来执行并发，<strong>TPC-C测试的结果主要有两个指标，也就是每分钟事务数（tpmC）和性价比（Price/Performance，简写为Price/tpmC）。</strong></p><p>而TPC-E（2007年发布）使用的是证券交易所的业务模型，它模拟了12种事务类型，包括交易查询事务、交易执行事务等。</p><p>TPC发布TPC-E的初衷是想让它替代TPC-C的，但是从发布到现在十几年了，看起来他们并没有得到预期的效果，据说只有SQL-Server会刷TPC-E的榜单。</p><p>既然有这么多企业去刷TPC-C的榜单，那它的指标具体是怎么计算的呢？下面我们就来看看。</p><p>TPC-C的tpmC指标计算公式一般是下面这样：</p><p><img src="https://static001.geekbang.org/resource/image/89/85/8919d7cdf6cc29e09f24afc25f7a3985.png?wh=1192x175" alt=""><br>\n我来简单解释一下：</p><ul>\n<li>日均请求量：后台请求数。</li>\n<li>请求峰值系数：这个值从历史经验数据中得来，像有些银行就是2-3之间。</li>\n<li>请求复杂度：这里需要换算一下，看看一个请求约为多少个TPC-C的标准请求。</li>\n<li>预留扩展：以用户数增长、请求量增长来预估系统处理能力的增长。没有固定值，像银行系统有个3-5倍已经挺高的，而互联网系统就不一而足了，预估成百上千倍的增长量的大有人在，这就取决于资本如何运作了。</li>\n<li>资源最佳利用率：通常这个数据为了好看，又为了上下都交得了差，会界定在60%-80%左右，没有严格的规定。</li>\n</ul><p>根据这个公式，我们代入一组数据来算一下：</p><p><img src="https://static001.geekbang.org/resource/image/42/55/42780034e4b3d52db6d867481d1e2455.png?wh=1048x229" alt=""></p><p>上面这些数字代表的内涵跟前面的标准公式是一一对应的，我就不单独解释了。</p><p>在标准的TPC-C基准中，如果我们在一个或固定的几个机器上部署了一个完整的系统，那么通过这个公式，我们就可以计算出一个指标值来。</p><p>你可能会觉得，拿到这么一组数据之后，如果我们想要知道这个值需要多少硬件资源，就可以拿计算出的值和TPC-C标准表来对照了。别人一个主机硬件的TPC-C值如果是900，那么，你需要的就是$\\frac{12,500,000}{900}\\approx139（台）$主机。</p><p>这个计算过程看起来非常合理，又有指标可以参考，感觉没什么难度。但是你在具体的项目中想要计算的时候，却并没有这么简单。原因主要有下面四点：</p><p>首先，你的系统的架构部署和标准的测试系统不一样。这是一个很难平衡的事情，因为每个业务系统都或多或少有一些独特设计，这些根据业务特性所做的部署架构设计会和TPC-C的跑分系统有差异，所以我们不能直接用TPC-C的跑分值来做参考。</p><p>其次，请求复杂度在大部分系统中都是比较难计算的。你可能觉得可以“使用O(n)复杂度表示法”，但是这种表示法在代码方法级别是容易计算的，而到了微服务分布式系统中，由于服务拆分较多导致了服务间调用开始变多，O(n)就很难表达出架构级的复杂度了。</p><p>第三，资源最佳利用率很难匹配到生产系统。根据我的经验，在生产环境中，很多企业投入的硬件资源都远远高于所谓的最佳利用率的值。银行系统不用说，为了保证稳定性，系统资源利用率超过15%的时候都非常少。而互联网大厂也只有在一两个业务峰点的资源利用率会高，但为了保证系统在业务峰点能够正常运行，互联网大厂也几乎是百倍千倍地提前准备冗余的资源。</p><p>第四，峰值系数也是很难确定的。一方面，生产数据拿到的只能是历史数据，要想用这个数据来计算未来的需求，只有一种方法能让计算结果准确，那就是业务模型和业务量都不发生大的变化，但这是几乎没法预测和保障的。另一方面，企业在发展，业务也会发生变化，这时候如果仍然用历史数据来计算tpmC也会不准确。</p><p>鉴于这些原因，虽然TPC-C的基准测试是标准的，基准测试中的数据也是准确的，但到了真实的业务系统中，这个值却只能用来参考。</p><p>如果TPC-C提供的逻辑不是那么精准。那怎么办呢？我们有没有一种更为精准的方式呢？下面我就来聊一聊我认为更为精准的容量评估方式：排队论。</p><h2>排队论容量评估</h2><p>因为排队论完整讲起来需要很多的数学理论知识，再开一个新专栏都不够讲的，所以这里我只是给你提供一个思考的方向和实际落地的demo实现。</p><p>什么是排队论呢？</p><blockquote>\n<p>排队论 (queuing theory) ，是研究系统随机聚散现象和随机服务系统工作过程的数学理论和方法，又称随机服务系统理论，为运筹学的一个分支。</p>\n</blockquote><p>排队论的模型表示为：X/Y/Z/A/B/C。其中：</p><blockquote>\n<p>X：到达时间分布；<br>\nY：服务时间分布；<br>\nZ：线程数；<br>\nA：系统容量限制；<br>\nB：请求源数目；<br>\nC：服务规则。</p>\n</blockquote><p>通常我们认为一个系统的系统容量限制和请求源数目，也就是A和B都是无限大的，因为系统是可以一直提供服务的。那C呢，如果是请求-响应式的同步系统，我们可以认为是先进先出的服务规则；如果是异步情况，就可以考虑平均的服务规则。Z是服务线程数，这个比较容易理解。但是这里我要说明一下，当我们使用排队论算一个服务时，Z可以看作是服务中的线程数；但如果我们想用排队论来看整个微服务分布式架构的容量能力时，应该把Z看作是并行的服务节点个数。X可以看作TPS，Y可以看成是响应时间。</p><p>这样一来，我们就可以把上面的逻辑运用在具体的项目计算过程中了。下面我们就结合demo代码来看一下计算逻辑。</p><p>这里我用R语言来实现，因为在R语言中有一个queuecomputer的开源项目，里面实现了一些基本的算法。下面我给出了实现这个 demo 的代码，代码相应的解释，我也都写到了每行的后面。</p><pre><code class="language-java">library(queuecomputer)\n\nset.seed(4)  #定义随机数种子\nNumberOfRequests &lt;- 40000000 #定义请求数\nlambda_a &lt;- 200/1  #到达率\nlambda_s &lt;- 167/1 #服务率\ninterarrivals &lt;- rexp(NumberOfRequests, lambda_a) #用户或请求呈指数分布\narrivals &lt;- cumsum(interarrivals)         #用户或请求总数\nservice &lt;- rexp(NumberOfRequests, lambda_s) #用户或请求得到的服务呈指数分布\n\n# 服务器个数（也可称为服务线程个数，取决于模型创建在哪个Leverl\nNumberOfServers = 5\n\nQueuingServerN &lt;- queue_step(arrivals = arrivals, service = service, servers = NumberOfServers)  #用到达时间和服务时间来计算响应时间和队列长度\nQueuingServerN\nQueuingServerN_summary &lt;- summary(QueuingServerN) # 计算队列的摘要数据\nQueuingServerN_summary # 打印摘要数据\nQueuingServerN_summary$slength_sum  # 总队列长度\nQueuingServerN_summary$qlength_mean # 平均队列长度\nQueuingServerN_summary$slength_mean # 平均请求长度（系统中的请求个数）\nQueuingServerN_summary$mwt # 系统级平均等待时间\nQueuingServerN_summary$mrt # 系统级平均响应时间\n</code></pre><p>运行这段代码会得到下面的结果（部分内容）：</p><pre><code class="language-java">..................\n&gt; QueuingServerN_summary\nTotal customers:\n 40000000\nMissed customers:\n 0\nMean waiting time:\n 1.28e-05\nMean response time:\n 0.006\nUtilization factor:\n 0.239512129307182\nMean queue length:\n 0.00256\nMean number of customers in system:\n 1.2\n&gt; QueuingServerN_summary$slength_sum\n# A tibble: 17 x 2\n   queuelength    proportion\n         &lt;int&gt;         &lt;dbl&gt;\n 1           0 0.302        \n 2           1 0.361        \n 3           2 0.216        \n 4           3 0.0864       \n 5           4 0.0259       \n 6           5 0.00617      \n 7           6 0.00147      \n 8           7 0.000353     \n 9           8 0.0000854    \n10           9 0.0000213    \n11          10 0.00000489   \n12          11 0.00000126   \n13          12 0.000000230  \n14          13 0.0000000998 \n15          14 0.0000000831 \n16          15 0.0000000358 \n17          16 0.00000000722\n&gt; # 平均队列长度\n&gt; QueuingServerN_summary$qlength_mean\n[1] 0.002556998\n&gt; # 平均请求长度（系统中的请求个数）\n&gt; QueuingServerN_summary$slength_mean\n[1] 1.200118\n&gt; QueuingServerN_summary$mwt # 系统级平均等待时间\n[1] 1.27839e-05\n&gt; QueuingServerN_summary$mrt # 系统级平均响应时间\n[1] 0.006000121\n</code></pre><p>从这个结果来看，我们总共有四千万客户请求，没有失败的请求。因为我们的服务率是167，而到达率为200，所以有一定的等待，还好这个等待时间并不长，只有1.27839e-05，而平均的系统响应时间为0.006000121，也就是6毫秒左右。从这个数据来看，支持四千万的总请求量，在到达率为200，服务率为167的情况下，就需要1.27839e-05+0.006的总时间，把这个时间和你的业务指标相对比，就可以判断出是否满足业务需求了。</p><p>排队论这个逻辑也可以用到生产环境中，前提就是要获得请求数、服务线程个数以及请求到达的分布函数、响应时间的分布函数，这些数据都可以通过日志获得。如果你再发散一下思维，还可以把这个逻辑用做实时计算。比方说，在大压力的场景下，如果我们想把响应时间降到指标以下，可以用它来计算服务线程数。</p><p>除了对性能峰值的综合评估（上面我们说的两种方式），磁盘和网络资源也需要进行评估。</p><h2>磁盘容量评估</h2><p>我们先来看看磁盘的容量评估。</p><p>对于应用程序来说，本地保存日志一般都是循环的，循环策略可以根据文件个数、文件大小、保存时间进行设置，所以相对来说磁盘容量是容易计算的。</p><p>而对于数据库这样需要长期保存数据的地方来说，想要计算磁盘容量就要多加注意了。通常我们会创建一个这样的公式来进行计算：</p><p>$$磁盘容量=原始磁盘容量+\\sum_{r=1}^{单表}（记录长度\\times记录数\\times保存期限）\\times数据库膨胀因子\\times备份因子$$</p><ul>\n<li>数据库膨胀因子：要根据具体的业务量进行评估。不同的业务模型，它们数据库的膨胀速度会有很大差别，而且不同的表的膨胀因子也不一样。</li>\n<li>备份因子：数据库最怕丢失数据，所以备份是必须的，至于要备份几份就得看策略了。有很多重要的系统常常会有至少三个备份，分别存在本地机房、本城机房和远程机房。</li>\n</ul><p>有了这些数据之后，再计算总体需要的磁盘容量空间就比较容易了。但这个公式看似简单，如果你想把所有库的所有表都用这个公式计算一遍，其实也很不容易。</p><p>不过像我们这个专栏的示例项目，因为搭建得比大企业简单很多，所以数据上也会少很多。下面我们就来看看，我们的项目如果跑一分钟会增加多少数据。</p><p>我们先用下面两个语句分别查询数据库表空间和库空间大小。</p><ul>\n<li>查询Mall库的所有表空间大小。</li>\n</ul><pre><code class="language-java">SELECT \n    TABLE_NAME AS \'表名\',\n    CONCAT(ROUND(TABLE_ROWS / 10000, 2), \' 万行\') AS \'行数\',\n    CONCAT(ROUND(DATA_LENGTH / (1024 * 1024), 2),\n            \'MB\') AS \'表空间\',\n    CONCAT(ROUND(INDEX_LENGTH / (1024 * 1024), 2),\n            \' MB\') AS \'索引空间\',\n    CONCAT(ROUND((DATA_LENGTH + INDEX_LENGTH) / (1024 * 1024),\n                    2),\n            \' MB\') AS \'总空间\'\nFROM\n    information_schema.TABLES\nWHERE\n    TABLE_SCHEMA = \'mall\'\nORDER BY TABLE_ROWS DESC;\n</code></pre><ul>\n<li>查询整库空间大小。</li>\n</ul><pre><code class="language-java">SELECT \n    TABLE_SCHEMA,\n    CONCAT(TRUNCATE(SUM(data_length) / 1024 / 1024,\n                2),\n            \' MB\') AS data_size,\n    CONCAT(TRUNCATE(SUM(index_length) / 1024 / 1024,\n                2),\n            \'MB\') AS index_size\nFROM\n    information_schema.tables\nGROUP BY TABLE_SCHEMA\nORDER BY data_length DESC;\n</code></pre><p>运行之前：</p><p><img src="https://static001.geekbang.org/resource/image/27/79/27d6d2ca25d7ce0496f5372db9989679.png?wh=872x670" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/dc/ca/dc34512a061yyfb99bf50f753459c8ca.png?wh=518x224" alt="图片"></p><p>满负载运行一分钟后：</p><p><img src="https://static001.geekbang.org/resource/image/0c/15/0ce6dfe30e18ff62c34e4daf0e973415.png?wh=876x704" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/d0/17/d02c5ded9e08000904e673f2f675e517.png?wh=508x218" alt="图片"></p><p>从Mall库的整体大小来计算，数据库表空间一分钟增加了3.87M。如果是一天呢，那就是近5.4G。这只是数据库空间的增加，像日志之类的还要另算。</p><p>到这里，磁盘容量的评估就完成了。这样根据实际的测试过程计算出的结果是比较靠谱的。</p><h2>网络容量评估</h2><p>接下来我们还要算一下网络容量。</p><p>要想计算网络容量，首先得知道一个系统稳定运行的整体最大TPS。当然了，要想知道这个TPS峰值，你得先保证我们前面讲过的容量场景已经执行通过了。</p><p>因为容量场景已经包括了像业务模型、测试数据、测试环境等等的前提条件，所以当我们得到了系统稳定运行的最大TPS之后，就可以直接用下面这个公式进行计算了：</p><p>$$ 上行带宽 = 最大TPS\\times 每秒上行数据大小$$</p><p>$$ 下行带宽 = 最大TPS\\times 每秒下行数据大小$$</p><p>为什么要分为上下行带宽呢？主要是因为在 ISP 运营商提供的网络服务中，上下行带宽的区别很大。像平时我们家里的宽带，工作人员非常自信地跟你说接入之后就能专享100M的带宽，实际上完全达不到这个值。</p><p>家里带宽不够最多也就是卡得你一个人不舒服，但企业级的带宽不够，那就是严重的生产事故了。我们再对着系统架构图来看一下有哪些带宽是需要计算的。</p><p><img src="https://static001.geekbang.org/resource/image/5d/81/5d33b0b7dac595961c599e211f6eff81.jpg?wh=1920x1593" alt="图片"></p><p>我这次放的架构图里，所有我标识为红点的地方，都是这个系统里需要计算带宽的位置。也就是说，不管是针对外网出口还是只是在内网服务之间，都是需要计算一下带宽的。</p><p>现在有一些企业又做了子网划分，这种情况下需要计算的点就更多了。好在计算公式不会有区别，有区别的只是每个位置TPS和上下行数据的大小，这些是要你通过容量场景统计得来的。</p><p>网络容量评估这一部分的内容就这么多，可以看出来，它的逻辑是比较清晰的。你只要找齐数据，代入公式计算就可以了。</p><h2>总结</h2><p>好了，这节课的内容就讲到这里。虽然刚才我很努力地没有讲一些数学理论（比如说：马尔可夫链），但是如果想要看懂这一讲的内容，还是需要一些数学基础的。</p><p>TPC-C的本意应该是希望能够在业内提供一套标准的基准测试逻辑，通过它来指导系统性能容量规划。但是在具体的项目中，因为有太多的局限，所以我们只能拿它来做个参考。</p><p>相比较而言，如果我们使用排队论来计算容量的话会更加精准。我们的原始数据只要来自于真实的系统，并且通过检验方法（比如说卡方检验、二项检验、K-S检验等）得到了分布函数，就可以根据排队论的逻辑进行系统的容量评估了。</p><p>请注意，这里我并没有代入前面我们提到的各种性能计数器。如果在实际的项目中，我们可以将性能计数器也作为容量规划模型的输入条件，模型将变得更加完整，数据也会更为准确。</p><p>而磁盘空间和网络容量的评估，我们直接根据实际的测试过程来做计算即可。</p><p>关于容量评估我就说到这里，希望能给你一些启发。</p><h2>课后题</h2><p>学完这节课，请你思考两个问题：</p><ol>\n<li>你是如何来做系统级容量评估的？你有没有参照过具体的生产数据？</li>\n<li>你能不能用统计学中的模型（比如差分自回归移动模型ARIMA）来代替排队论呢？</li>\n</ol><p>欢迎你在留言区与我交流讨论。我们下节课见！</p>',
        article_title: "34 | 容量规划：如何精准地对生产系统做容量预估？",
      },
    ],
  },
  {
    chapterTitle: "性能结果/报告",
    children: [
      {
        title: "35 | 压测报告：怎样写出一份让老板满意的报告？",
        id: 475351,
        content:
          '<p>你好，我是高楼。</p><p>在一个性能项目中，我觉得有三类文档是必须要存在的，那就是性能方案、调优报告和性能报告，它们分别代表了我们在项目实施前、实施中、实施后三个阶段的工作表现。而我们只要看看性能报告，大概率就可以知道一个项目做得如何了。</p><p>性能报告是非常重要的一份文档，它不仅要给性能方案中提出的问题一个准确的答复，也就是明确测试目的有没有达到，还要告诉非技术型领导，这个系统能不能支撑正常的业务运行。所以性能报告通常会有两种形式：Word版和PPT版。Word版是给技术人看的，PPT版是给领导看的。</p><p>但是，现在大部分人写的性能报告都只是应付一下流程，除了告诉别人“这个系统我检查完了”，好像也没有什么实质性的作用。为什么这么说呢？原因有下面几点：</p><ul>\n<li>没有明确的可不可以上线的结论。</li>\n</ul><p>这一点至关重要。我收集了几个结论型的描述语句给你看看。</p><blockquote>\n<p>经过多轮的性能测试及调优，系统在部分方面达到了设计需求，可以满足当前部分业务需求，但还未达到最佳性能状态，仍存在可调优的点。</p>\n</blockquote><blockquote>\n<p>测试含风险通过。</p>\n</blockquote><blockquote>\n<p>CPU用了80%、内存用了90%，TPS达到1000，95%响应时间是500ms。</p>\n</blockquote><p>类似这样的描述可以说是数不胜数。总结一下就是只有模糊的描述，没有明确的结论。有人说，CPU使用率80%不算是明确的结论吗？那是肯定不能算的。</p><!-- [[[read_end]]] --><p>结论首先要和方案中的目标对应上，你的结论应该是，目标有没有达到。可惜的是很多方案连写的目标都是模糊的，那就导致结论也是没法精确的。</p><p>另外，结论还要和场景对应上，其中最重要的就是容量、稳定性和异常场景。有人说，咦，你不是一直说四类场景吗？怎么不要基准场景了？ 在结论处，我们要描述的是整个项目对线上的系统运行有什么明确的支撑，我觉得基准场景可描述可不描述。但容量、稳定性、异常是必须要描述的，因为我们的目标就是要回答线上是否能正常运行的问题。</p><ul>\n<li>没有明确的性能上限原因的证据。</li>\n</ul><p>每一个系统都有性能的上限。通常我们给出的是<strong>稳定运行的最大TPS值</strong>。而报告中，要描述出来的是为什么系统只能达到这个TPS值。</p><p>如果是因为指标只需要这么多，那硬件资源还有多少可用的水位；如果是某类硬件资源（CPU、IO、内存、网络、存储等）用到了上限，那就会有非常明确的证据；如果是因为系统设计导致的上限、因为参数配置导致的上限…那也都会有明确的证据。</p><p>唯独没有描述证据是不行的。你要是说“我的系统只能支持1000TPS，但是为啥只能支撑1000TPS，我不知道”，这肯定是不行的。</p><ul>\n<li>没有生产运维的配置数据。</li>\n</ul><p>作为性能项目，在明确上面两点的基础之上，我们还一定要告诉生产运维如何配置。比如说：最大最小线程数、超时时间、熔断限流规则等等各类参数。有多少这样的配置项呢？这就要参考我们的性能分析决策树对应的性能配置树了。</p><p><img src="https://static001.geekbang.org/resource/image/70/7e/70da8df80c48636fe2f3d243fd27d97e.jpg?wh=1920x1369" alt="图片"><br>\n我把配置树的思维导图放在了上面，展开了其中一部分给你参考一下。</p><p>在一个具体的项目中，如果全取默认值，倒也简单了，你可以直接告诉运维都取默认值就可以了。不过但凡有值的变动，你都应该记录下来给到运维。关于这一部分你可以参考我上一个专栏的<a href="https://time.geekbang.org/column/article/379841">第30讲《如何确定生产系统配置？》</a>。 在这个专栏中我们就不展开了，不然就重复了。</p><p>那接下来，关键的问题来了。针对我们这个专栏，压测报告应该怎么写呢？压测的报告中的很多内容跟压测方案是一一对应的，所以我在报告中也会把相应的内容列出来并给出解释。如果你不记得方案了，可以回去看看<a href="https://time.geekbang.org/column/article/428497">第3讲</a>。</p><h2>压测目标</h2><blockquote>\n<ol>\n<li>根据经典的电商下单流程，测试当前系统的单接口最大容量。</li>\n<li>录制真实的线上流量，回放压测流量，充分利用当前服务器资源，找到当前系统的性能瓶颈并优化，最终达到最佳容量状态。</li>\n<li>结合稳定性场景，做到精准的容量规划，给服务做限流降级提供数据上的参考。</li>\n<li>结合异常场景，实践并判断当前系统中的异常情况对线上产生的影响。</li>\n</ol>\n</blockquote><p>显然，上面的每一条我们都是实现了的。第一条对应的是第29讲基准场景；第二条对应的是第30和31讲的容量场景；第三条对应的是第32讲的稳定性场景；第四条对应第33讲的异常场景。</p><h2>压测范围</h2><blockquote>\n<p>通过梳理核心压测链路，即电商下单主流程，可确定压测范围，如下所示：<br>\n<img src="https://static001.geekbang.org/resource/image/92/f3/92ebe3c285b977df01ca8029c4f08bf3.jpg?wh=1920x918" alt="图片"></p>\n</blockquote><p>我们在编写脚本时就是对应这10个接口的。</p><h2>系统架构图</h2><h3>系统技术栈</h3><blockquote>\n<p>微服务框架：Spring Cloud 、Spring Cloud Alibaba<br>\n容器+MVC 框架：Spring Boot<br>\n认证和授权框架：Spring Security OAuth2<br>\nORM 框架：MyBatis<br>\n数据层代码生成：MyBatisGenerator<br>\nMyBatis 物理分页插件：PageHelper<br>\n文档生产工具：Knife4j<br>\n搜索引擎：Elasticsearch<br>\n消息队列：RabbitMQ<br>\n分布式缓存：Redis<br>\nNoSQL 数据库：MongoDB<br>\n应用容器引擎：Docker<br>\n数据库连接池：Druid<br>\n对象存储：OSS、MinIO<br>\nJWT 登录支持：JWT<br>\n日志收集、处理、转发：LogStash<br>\n日志队列和缓冲：Kafka\t<br>\n日志采集：Filebeat<br>\n可视化分析与展示：Kibana<br>\n简化对象封装工具：Lombok<br>\n全局事务管理框架：Seata<br>\n应用容器管理平台：Kubernetes<br>\n服务保护：Sentinel<br>\n分布式链路追踪系统：Zipkin<br>\n基础资源监控： Promethues<br>\n容器级链路监控： Weave Scope<br>\n可视化看板：Grafana</p>\n</blockquote><h3>系统逻辑架构图</h3><p><img src="https://static001.geekbang.org/resource/image/26/b3/260377ac4775cb97ecf9bd6bfce962b3.jpg?wh=1920x1436" alt="图片"></p><h3>系统部署架构图</h3><p><img src="https://static001.geekbang.org/resource/image/2f/4b/2f032901efyy2d9f9e3df8f123e8534b.png?wh=1673x926" alt="图片"></p><p>这些显然就是我们测试的基础信息了，这一点不会有偏差，因为我们的压测就是在这样的架构中做的。</p><h2>硬件环境</h2><p><img src="https://static001.geekbang.org/resource/image/d3/40/d32a2d780e8937350dd47bfdb93c2c40.jpg?wh=1920x3462" alt="图片"></p><p>在压测方案中，我们规划的硬件资源是8台服务器，但我们最后用到的硬件不止8台，中间我们又多加了四台8C16G的机器，所以总共是12台。</p><h2>基础数据</h2><h3>铺底数据</h3><p><img src="https://static001.geekbang.org/resource/image/7d/b5/7d3e9e1e45032b88989ab65723f886b5.jpg?wh=1920x885" alt="图片"></p><p>这是原始的铺底数据，在场景执行的过程中，产生的数据还有很多，在容量评估那一讲，我们也有截图表示。</p><h2>系统改造</h2><p>这里我们要重点说一下了。</p><p><img src="https://static001.geekbang.org/resource/image/08/8f/0828fe104b2e231974e117054502b18f.jpg?wh=1920x1080" alt="图片"></p><p>但是要说明一点，这里提到的时间成本是在你比较熟练的情况下需要的时间。如果需要重新学习这些技术，那估计就不是两三个月的事情了。但是不管怎么说，我觉得全链路的改造成本真的不算高，而且也并不高深。</p><h2>压测设计</h2><p>这里我只摘了压测方案中的关键内容过来。</p><blockquote>\n<p>对于性能场景，我们认为它必须满足两个条件：连续和递增，所以在这次的全链路压测执行过程中，我们会把这两点应用到下面的业务场景中。</p>\n</blockquote><blockquote>\n<p>在 RESAR 全链路压测中，性能场景只需要四类，执行顺序依次为：基准场景、容量场景、稳定性场景和异常场景。<br>\n除了这四类性能场景外，再没有其他类型的场景了。在每一个场景分类中，我们都可以设计多个具体的场景来覆盖完整的业务。</p>\n</blockquote><p>在执行过程中，我是完全按照这个思路来做的，这也是很多性能测试人员纠结的点，他们不知道这四类场景是否可以覆盖住所有的性能需求。但是请不要再纠结啦，在我的职业生涯中，还没有什么需求是可以逃到这四类场景之外的。 反而是那些拿“压力测试”“负载测试”“极限测试”等之类的幌子设置的场景最终无法给出明确的结论。</p><h2>监控设计</h2><p>下面两个图就足够说明我们在这个项目中的监控设计了，并且从文章中给出的各类的监控截图也可以看得出来，图里面的东西几乎都用到了，我们也没有偷工减料。</p><p><img src="https://static001.geekbang.org/resource/image/16/b1/160a542429b73ddf415e508866c46fb1.jpg?wh=1920x809" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/b5/7f/b587489008c4ba1c41dcea5bbf144b7f.jpg?wh=1920x791" alt="图片"></p><p>现在就要来说说输出的内容了。</p><h2>结果整理</h2><p>在结果整理这一步，我是坚决反对用表格的。我认为直接帖压力工具或监控工具的截图即可，因为表格表达不出趋势。这里你可以直接看第29-33讲，把截图帖出来就行。比如：</p><p><img src="https://static001.geekbang.org/resource/image/4e/78/4e0220aa3382d5f4fb7714ef0f1e1578.png?wh=1834x734" alt="图片"></p><p>这一张压力工具的趋势图就可以准确说明容量场景能跑到多大。</p><p>关于资源的截图，因为它太多了，所以我建议你帖几个全局监控的图即可。毕竟这里只是要说明一下，不是在分析过程，不用太详细。</p><h2>结论</h2><p>好，现在重点的来了。针对每个和生产有关的场景，我们要给出最终的结论。</p><h3></h3><h3>容量场景</h3><p>在容量场景中我们知道，系统达到了1650的最大稳定的TPS。而我们一开始的目标是1350。目标显然是达到了，可以<strong>通过</strong>。</p><p><img src="https://static001.geekbang.org/resource/image/f0/e2/f0a3e5b9c34143852e92ayy2066e4fe2.png?wh=746x838" alt="图片"></p><h3>稳定性场景</h3><p>业务累计量我们定了一个500万的小目标，最后跑到了680万。结论显然也是<strong>通过</strong>。</p><p><img src="https://static001.geekbang.org/resource/image/fa/ab/fa7ddeb4a8a80a53320a0c49df0e15ab.png?wh=876x613" alt="图片"></p><h3>异常场景</h3><p>我们只模拟了两个异常的场景，应该说做得非常不全面。对一个专栏来说，为了说明逻辑倒也足够了。但对一个项目来说，那显然是远远不够的。所以在这个专栏的报告中，我认为异常场景的结论是<strong>不通过</strong>。</p><p>在真实的项目中，你可以把我们在异常场景那一节中所提到的各个层级的异常场景都覆盖一遍，只有这样做才算是真正的通过。</p><h2>总结</h2><p>对于性能项目来说，性能测试人员需要给出明确的通过或者不通过的结论，另外还要给出清晰的生产配置建议，关于如何确定配置值，前面我们也给出了相应的链接。</p><p>在这节课所讲的性能报告中呢，我们必须明确说明项目所使用的硬件、软件、数据、压力策略、监控策略等等内容，因为看报告的人没必要再把压测方案找过来跟报告一起对着看，那太麻烦了。</p><p>同时，给出结果整理也是必须的，这是所有执行过程中最精华的部分。虽然你只有一两张好看的截图放在那里，但那背后是一个个辛酸的调优故事。</p><p>最后呢，我们还要给出一个结论，这是我一直强调的。这个结论首先要有数据的直接对比，其次要有能不能上线的直接说明。在报告中，模棱两可的话绝不许出现，没有结论的数据也绝不能出现。</p><h2>课后题</h2><p>​学完这节课，请你思考两个问题：</p><ol>\n<li>你觉得性能报告的重点是什么呢？</li>\n<li>你觉得你写过的性能报告有没有问题呢？如果有，是什么？</li>\n</ol><p>这是我们这一次、也是最后一次思考题，欢迎你在留言区与我交流讨论。</p>',
        article_title: "35 | 压测报告：怎样写出一份让老板满意的报告？",
      },
    ],
  },
  {
    chapterTitle: "结束语",
    children: [
      {
        title: "结束语 | 做能落地的全链路压测项目",
        id: 476112,
        content:
          '<p>你好，我是高楼。</p><p>历时四个多月，正文超过15万字，环境搭建部分超过13万字，在不知道熬了多少个夜之后，28万字终于码完了，全链路压测专栏也到了尾声。这个专栏写得很辛苦，相信你读起来也不轻松，有不少同学跟我说专栏内容太难，哈哈没办法，不难还能叫全链路压测么。</p><h2>为什么要写这个专栏？</h2><p>其实一开始啊，我是非常拒绝写一个全链路压测专栏的，因为它没有超过我之前专栏中写的RESAR性能工程的逻辑。但是我们 7DGroup 小组内部讨论的时候，左泽位同学和李文同学都非常积极，他们都希望能把这样一个热点技术话题给完整地写出来。最后我被说服了，我们讨论出了几个要写的理由：</p><ol>\n<li>\n<p>网上并没有一套完全可落地的全链路压测技术实现，仅有的资料也只是蜻蜓点水似的宏观的方案描述。这一点其实无可厚非，毕竟都是公司内部的资产，每一个字拿出来都是要经过层层审核的。公司需要对内部资料保密，同时也要保证技术先进性。但是这种又要体现公司技术牛逼，又半遮半掩的分享姿态实在是让人感觉不到诚意。而我们如果能够尽已之力把好的技术分享出去，消除这层知识壁垒，我觉得算是好事一桩了。</p>\n</li>\n<li>\n<p>全链路压测的技术方向是个热点话题，大家也都希望能够了解更多细节，所以应该能获得一些关注度。这一点我是持保留态度的，因为我知道文艺片卖好不卖座的道理。虽然这个话题很热，但实际上全链路线上压测要求企业内部各方的协调。哪怕到现在专栏写完了，我也认为要想落地全链路压测，除了专栏当中的技术内容，还有其他很多方面需要准备。</p>\n</li>\n<li>\n<p>多多少少有那么点技术人的执念，觉得这样一个好话题却到处都找不到一个完整的落地逻辑，太可惜了。网上总把全链路吹成“保障生产的核武器”，但看过之后你却根本不知道核武器的组成物质是什么，让人有种隔靴搔痒的感觉。</p>\n</li>\n</ol><!-- [[[read_end]]] --><p>鉴于此，我们决定写这个专栏。同时呢，我们决定全部使用开源产品，让你不仅能看懂，还可以动手实践；如果你想在公司内部推广，成本也可以降到最低。</p><h2>专栏的质量靠的是细节的考究</h2><p>这个专栏不是我一人之功，而是我们 7DGroup 小组之力。从一开始讨论到决定要写这个专栏，再到觉得写不出来，面临断更的危险，再到最后写出来，这四个月，我们就像在坐过山车一样，还是24小时无休的那种。</p><p>在专栏筹备过程中，我们买了云服务器，这一点是个上个专栏不一样的。上一个专栏中，我们是自己弄了一个机架、几个物理刀片机、交换机，然后从物理层开始搭建。而这一次，为了重点体现上层应用的部分，我们直接用了云服务器，这个成本两万打不住。性能项目就是这样，环境的成本是从来都不能敷衍的。</p><p>我们是拿一整个系统来做的改造，从这个专栏的内容中你也可以看到，这就是一个完整的性能项目记录，而不是一个只有理念或者套话的凑字之作。在写的过程中，我们从来都不考虑字数，而是考虑如何把过程体现得更完整，这导致我们最终交付的文稿字数比预期多了60%，这真是加量不加价了。</p><p>全链路压测专栏的重中之重在于改造，所以我们花了大量的篇幅在具体的改造上。写作的过程中，我们也会去查当前的技术市场上都有哪些技术实现，不仅与现在很多大厂做全链路压测的朋友们沟通，还会把他们的技术方案拿出来讨论对比。也就是说，不仅要都了解，还要做落地对比，争取使用最为合理的技术实现。这个过程是痛苦的，通常比了很久才能有一篇产出。</p><p>我还记得有一次在咖啡厅，我们在讨论日志改造那一篇的内容。当时已经写完了一个版本，不过只写了逻辑，并没有具体的落地代码实现。但是我觉得还是不能这样写，这种虚头巴脑的东西写了没什么意义。于是我们立即决定重写一篇，那一天我们三个人用7个小时合作完成了一篇新的日志改造文章，把落地思路和代码实现都写了出来。</p><p>就是在细节上一次次的纠结才有了专栏最终呈现的样子，这也是我们技术能力范围内最好的样子了。</p><h2>如何落地全链路压测</h2><p>聊完专栏的诞生过程，我们可以再看看后续的行动。如果你想在公司内部推广全链路压测，首先要考虑两方面的问题。</p><ol>\n<li>时间成本：这个时间成本主要是技术改造的落地时间、以及和各部门、上下级沟通的时间。这其中，沟通的成本又会远大于落地的时间成本。<br>\n请不要因为领导一句口头上的支持，就盲目去找各团队沟通。你需要非常具体的授权，需要所有人都知道你在做这件事情，并且每个人都不是支持的角色，而是全链路项目中的一员。在大部分企业中，做全链路压测的配套改造，不是技术上难不难的问题，而是部门间的沟通问题。有的部门一上来就表示拒绝承担更多的工作量，再加上一些领导的懒政风格，问题就变得很不好解决了，这是非常现实的事情。</li>\n<li>人力成本：从技术角度来看，全链路压测花在改造上的人力成本并不会很多，你从专栏中估计也能看出来。如果手快的话，我们这样6个系统，应该不会超过1人月，而高级工程师的角色参与要超过4-5个人月，管理角色至少1个人月。当然，前提是所有的权限都在这些人的管控范围之内，他们能处理的内容包括运维、开发、测试、分析、架构等。如果你的系统比较多，并行处理应该也不会有大的时间成本差异，只是人月数会等比例递增。</li>\n</ol><p>有了时间和人力成本的考量之后，具体的技术实现我都已经在专栏中给你展现了，我还把源码链接放在了<a href="https://time.geekbang.org/column/article/467606">第25讲</a>，你完全可以跟着专栏的逻辑去做具体的落地实践。如果后面还有精力的话，我们会尝试把全链路的改造整理成一个SDK，开源出来给所有人使用。</p><p>在全链路压测方案的设计上，你可以参考下图中标红的部分。</p><p><img src="https://static001.geekbang.org/resource/image/e4/c8/e4f673d7607e9f8faeb279dc766af0c8.jpg?wh=1920x1237" alt="图片"></p><p>这些部分的实现都是需要部署能力和开发能力的。如果你是一个纯做测试的，而且也没有开发功底，可能看这个专栏会觉得比较难理解。这其实不是技术思路上的差异，而是对性能工程理解上的差异。</p><p>在我看来，测试人员已经不能再守着测试工具本身，我们还要扩展自己的知识体系。不然不仅测试做不好，性能分析更是做不到。</p><p>从性能工程的角度思考，我认为你需要的技术功底要包括下面这些内容。</p><p><img src="https://static001.geekbang.org/resource/image/30/28/30751f634806ae876d5a5cdd61d24528.jpg?wh=1820x1530" alt="图片"></p><p>当一个团队有了上面的实现方案，又有了技术的支撑，落地全链路压测就是轻而易举的事情了。</p><p>当然了，我在开篇词和正文中也反复说过，要不要推广全链路压测一定要结合企业现状，我们不能做事倍功半的事情。</p><p>我希望通过专栏中实际的改造落地过程，给你展现全链路压测真正的技术难点，把全链路压测的神话拖到地板上摩擦。全链路压测从来都不是法外之地，它也必须遵从性能工程的逻辑。当然，也希望你跟我一样，可以从每个技术细节中体会性能艺术的壮阔和性能分析的博大精深。</p><p><a href="https://jinshuju.net/f/uO5dGy"><img src="https://static001.geekbang.org/resource/image/8d/66/8d4384c388680e230d011f52f4a37866.jpg?wh=1142x801" alt=""></a></p><p>这门课到这里就彻底结束了，感谢你能坚持学完，也希望它能够让你重新认识和了解全链路压测。接下来就看你的了。再见！</p>',
        article_title: "结束语 | 做能落地的全链路压测项目",
      },
      {
        title: "期末考试 |《全链路压测实战30讲》满分试卷，等你来挑战！",
        id: 476976,
        content:
          '<p>你好，我是高楼。</p><p>专栏《全链路压测实战30讲》已经结课了。非常感谢你一直以来的认真学习和支持！</p><p>为了帮你检验自己的学习效果，我特意给你准备了一套结课测试题（可以重复体验:) ）。一共20道选择题，考点都来自我们前面讲到的重要知识。点击下面按钮开始测试吧！</p><p><a href="http://time.geekbang.org/quiz/intro?act_id=1449&exam_id=3660"><img src="https://static001.geekbang.org/resource/image/28/a4/28d1be62669b4f3cc01c36466bf811a4.png?wh=1142*201" alt=""></a></p><p>最后，我还给你准备了一个<a href="https://jinshuju.net/f/uO5dGy">调查问卷</a>。题目不多，大概两分钟就可以填完，主要是想听一下你对这门课的看法和建议。期待你的反馈！</p><p><a href="https://jinshuju.net/f/uO5dGy"><img src="https://static001.geekbang.org/resource/image/8d/66/8d4384c388680e230d011f52f4a37866.jpg?wh=1142x801" alt=""></a></p><!-- [[[read_end]]] -->',
        article_title: "期末考试 |《全链路压测实战30讲》满分试卷，等你来挑战！",
      },
    ],
  },
];
