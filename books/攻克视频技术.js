exports.category = "frontend";
exports.title = "攻克视频技术";
exports.data = [
  {
    chapterTitle: "开篇词",
    children: [
      {
        title: "开篇词｜为什么说视频开发是程序员的一片蓝海？",
        id: 447639,
        content:
          '<p>你好，我是李江，欢迎你跟我一起走进视频和图像的世界。从今天开始，我要用15节课的时间，和你分享视频中的那些核心技术。</p><p>那说起视频，随着以抖音和快手为代表的短视频平台火爆全球，市场对于视频研发人才的需求越来越旺盛。当然还有像随之而来的直播带货、疫情背景下视频会议产品的更新与迭代等等，都给工程师们创造了很多新的机会。</p><p>最简单粗暴的方式是，你可以直接通过各种招聘平台去了解相关的就业形势。就拿Android开发为例吧，现阶段做音视频开发的薪资水平，至少比同资历的Android开发高出30%左右。</p><p><img src="https://static001.geekbang.org/resource/image/7e/fb/7eb645a161768db0573b8d8c4f63bdfb.png?wh=1870x692" alt="图片"></p><p>当然，除了实打实的物质回馈，我想<strong>更大的学习动力还是一些新技术、新应用</strong>，毕竟好奇是人类的天性，也是技术人进阶的驱动力。</p><p>拿我来说吧，我目前是在声网 Agora 担任视频专家，主要从事视频编码和传输方面的工作。但在此之前，我已经积累了8年之久，做过很多新的尝试和探索，收获颇丰。</p><p>早在2013年我还在浙大读硕士的时候，我就开始了视频编码与传输方向的研究。之后一步步转向视频编码的应用、传输策略的优化；再到高阶的视频网络传输、带宽预测、抗丢包等网络对抗技术；再后来就是人脸识别、表情识别和人流量检测等各种有意思的视频AI算法的落地。</p><!-- [[[read_end]]] --><p>总结来说，视频领域的方方面面，从底层原理到上层应用，我都有过深度参与和系统性的沉淀。这也是我开这门课的一个原动力，那就是希望能做更多的分享与输出，把这一路上踩的坑、体验到的技术痛点与乐趣与你分享，让更多有兴趣了解、转型视频开发的同学可以快速涉猎视频技术，而音视频工程师或者相关应用的产品经理也可以借此夯实视频基础、优化视频产品设计体验。</p><h2><strong>学习视频技术究竟难在哪？</strong></h2><p>在我看来，现在就是视频技术最好的时代。一方面，如你所见，当下即是刚需，视频技术的应用已经成为互联网平台的标配；另一方面，未来即是星辰大海，随着AI、元宇宙等技术的发展，视频技术还有无限的想象空间。</p><p>但既然我们前面已经提到了技术痛点这个关键词，那<strong>学习门槛还是有的</strong>。就比如：</p><ul>\n<li>抖音和快手的短视频需要应用图像处理和视频编码技术，如何在保持高画质的情况下，尽量减少视频文件的大小，是很难也很重要的；</li>\n<li>连麦直播则需要懂RTC和直播技术，如何能够保证在各种网络状况下实现超低延时、降低卡顿率是一个非常难的问题；</li>\n<li>视频会议则需要你十分熟悉RTC和转码合流服务等技术，几十上百人的大型视频会议，如何保证流畅度、卡顿率、画质等指标也是十分具有挑战性的。</li>\n</ul><p>尤其对于一些视频技术小白来说，可能看公式都是有难度的。就比如说下面这个公式：</p><p><img src="https://static001.geekbang.org/resource/image/02/64/027c41f5f8ca3d9bd9d2111e9ef42e64.jpg?wh=1300x696" alt=""></p><p>这其实是一个二维DCT变换公式，是用来将图片从空域转到频域的。对于有些基础的同学来说，可能一眼就能看个大概。但是对于之前没有接触过视频编码的同学来说，可能要先从每个字母代表什么学起，然后才能再去搞清楚公式背后的计算逻辑。</p><p>有门槛只是一方面，我认为，<strong>视频技术难上手的另一个重要原因就是它涉及的技术太多了</strong>，这是事实。比如说视频前处理，就涉及到很多信号处理和AI技术；视频的编码算法，则会涉及到信息论和数学；而视频的传输就更多了，包含了各种网络协议和拥塞控制算法。你可以通过以下这张知识图谱去梳理其中的技术环节，既是构建知识网络，也可以查漏补缺。</p><p><img src="https://static001.geekbang.org/resource/image/40/e1/4081cb62214939fc0373793eae0abbe1.jpg?wh=2362x1837" alt=""></p><p>另外，也正是因为这点，目前市面上系统讲解视频的书籍和课程也是比较少的。同时，它与火热的音频技术相比，发展时间更短，所以沉淀下来的资料也不多。</p><h2>课程设计</h2><p>清楚了门槛，后面我们要解决的就是怎么迈过去，推开视频技术这扇大门了。</p><p>我联想到以前在公司内部做技术分享的场景，<strong>复盘问题其实是一种很好的学习方式</strong>。所以在设计课程的时候，我就假想同过去的自己对话。回想那些我曾经困惑的技术点和理论，以及难以解决的工程问题，希望能把那些广杂艰深、让初学者望而生畏的核心技术交付给你。</p><p>那么这门课程的具体内容是怎样的呢？</p><p>我会从视频处理的全链路展开，具体分为图像基础和前处理、视频编码、视频传输和网络对抗、视频封装和播放四个模块。</p><p><img src="https://static001.geekbang.org/resource/image/5e/61/5e154cd7319212970de004ba7b2e6961.jpg?wh=1618x1216" alt=""></p><p><strong>图像基础和前处理：</strong>从视频和图像的基本概念讲起，陆续延伸到视频的颜色空间，尤其是YUV颜色空间，这是视频技术中非常重要的基础知识，同时也是很多视频工程师的学习难点。之后会讨论工程上常用的图像缩放算法及其原理。打好基础，后面事半功倍。</p><p><strong>视频编码：</strong>主要讲解在视频技术中占据重要地位的视频编码技术，重点学习编码算法的底层原理和具体算法的实现，让我们在使用编解码器的同时，也能更深一层，洞察其工作原理。另外，H264码流是视频开发中必学的，所以我也会在这里和你讨论它的结构以及相关工程问题。深入原理，才能快速实践。</p><p><strong>视频传输和网络对抗</strong><strong>：</strong>以实践为主，讲解具体的视频传输和弱网对抗技术，主要包括如何打包视频、预测网络带宽，以及如何在有限的带宽情况下控制好编码器的码率和发送包的速度。之后，我会手把手带你解决两个代表性的视频Bug“花屏和卡顿”，并介绍一下视频会议中的常用技术SVC。</p><p><strong>视频封装和播放</strong><strong>：</strong>介绍常用的视频封装技术，并解决视频工程难题：音视频同步。</p><p>以上就是我要给你分享的重点内容，从最核心的原理、最新的应用到最常见的问题，我相信这门课能帮助你轻松跨入视频技术的大门。</p><p>最后，我想说，技术赋能、常变常新，希望我们都能在时代的洪流中锚定目标与方向。最后，欢迎你在留言区中与我交流，不妨说说你对这门课程的期待，或者你在学习视频技术时的经历和痛点，我会更加有针对性地为你答疑解惑！</p>',
        article_title: "开篇词｜为什么说视频开发是程序员的一片蓝海？",
      },
    ],
  },
  {
    chapterTitle: "图像基础和前处理",
    children: [
      {
        title: "01｜基本概念：从参数的角度看视频图像",
        id: 447685,
        content:
          '<p>你好，我是李江。</p><p>从今天开始，我们会一起来学习一些视频和图像相关的技术。主要包括视频图像的基本概念、图像的缩放处理、视频压缩编码、视频打包传输以及音视频同步等相关知识。</p><p>今天，我们就从视频和图像（视频是由一帧帧图像组成的）的基础知识讲起。掌握了这些之后，我们再讨论如何对图像进行缩放、如何使缩放后的图像更加清晰，以及如何对视频进行编码压缩等就更加游刃有余了。当然了，这些话题更难，但也更有意思，希望我们能有个不错的开始！</p><h2>像素</h2><p>相信你对像素这个概念一点都不陌生。从智能手机市场大火到现在，我们经常能够听到某某最新款手机，多少多少万像素。像素越高，则图像就会越清晰，拍出来的图片就会更逼真。那像素到底是什么呢？</p><p><strong>像素是图像的基本单元，一个个像素就组成了图像。你可以认为像素就是图像中的一个点。</strong></p><p>我们来直观地看看像素是怎么组成图像的。在下面这张图中，你可以看到一个个方块，这些方块就是像素。</p><p><img src="https://static001.geekbang.org/resource/image/5a/ce/5aa82584e3c3ba42e40f7265a53c91ce.jpeg?wh=2472x1034" alt=""></p><p>那一张图片有多少个像素呢？要回答这个问题就需要引出另外一个非常重要的概念——分辨率。</p><h2>分辨率</h2><p>图像（或视频）的分辨率是指图像的大小或尺寸。我们一般用像素个数来表示图像的尺寸。比如说一张1920x1080的图像，前者1920指的是该图像的宽度方向上有1920个像素点，而后者1080指的是图像的高度方向上有1080个像素点。</p><!-- [[[read_end]]] --><p>视频行业常见的分辨率有QCIF（176x144）、CIF（352x288）、D1（704x576或720x576），还有我们比较熟悉的360P（640x360）、720P（1280x720）、1080P（1920x1080）、4K（3840x2160）、8K（7680x4320）等。</p><p>那么<strong>同样一张图像用不同的分辨率表示会有什么不同呢？</strong>我们可以通过以下这组图片来直观感受一下。</p><p><img src="https://static001.geekbang.org/resource/image/af/7e/af3ddc69e258537c348b026acb240f7e.jpg?wh=2108x832" alt=""></p><p>我们可以看到，1x1的时候我们只能看到一个像素，只有一种颜色，根本就不是我们想象中的图像了。而10x10的图像几乎都是糊的，只能看到一点点轮廓。随着图像的分辨率越来越高，图像的细节就越来越清晰。由此，我们可以总结出：</p><ol>\n<li><strong>像素就只是一个带有颜色的小块。</strong></li>\n<li><strong> 图像的分辨率越高，图像就越清晰。</strong></li>\n</ol><p>但从更加专业的角度来说，第2句话不够严谨。原始图像的话，分辨率越高确实会越清晰，但是我们看到的图像往往是经过后期处理的，比如放大缩小，或者磨皮美颜。经过处理过后的图像，尤其是放大之后的图像，分辨率很高，但是它并没有很清晰。</p><p>这是因为放大的图像是通过“插值”处理得到的，而插值的像素是使用邻近像素经过插值算法计算得到的，跟实际相机拍摄的像素是不一样的，相当于“脑补”出来的像素值。</p><p>因此，放大的图像还是会存在偏差，表现出来就是会模糊。我们会在之后的课程中来具体聊聊这个过程是怎么做的。总之，<strong>我们不能简单地认为分辨率数值越高的图像就越清晰</strong>。</p><p>刚才我们在前面还提到，像素就是一个带有颜色的小块，那这个小块到底是怎么组成的呢？这里我们就来讲讲RGB图像像素和位深的概念。</p><h2>位深</h2><p>一般来说，我们看到的彩色图像中，都有三个通道，这三个通道就是R、G、B通道。简单来说就是，彩色图像中的像素是有三个颜色值的，分别是红、绿、蓝三个值。也就是说我们看到的那个带有颜色的块其实是由R、G、B三个值组成的（有的时候还会有Alpha值，代表透明度，我们这里不展开讨论）。</p><p>通常R、G、B各占8个位，也就是一个字节。8个位能表示256种颜色值，那3个通道的话就是256的3次方个颜色值，总共是1677万种颜色。我们称这种图像是8bit图像，而这个8bit就是位深。</p><p>我们可以看到，<strong>位深越大，我们能够表示的颜色值就越多。</strong>因此，图像就可以更精确地展示你拍摄的真实世界。比如现在有10bit图像和12bit图像，8bit图像的每一个像素需要占用3x8总共24个位，3个字节，同理10bit、12bit就会占用更多。</p><p>所以，图像的位深越大，需要的存储空间就会越大，传输这张图像使用的流量就会越多。目前我们大多数情况下看到的图像以及视频还是8bit位深的。</p><h2>Stride</h2><p>接下来我们来看一个特别的概念——Stride。这个Stride不是图像本身的属性，但是视频开发者经常会碰到，也是经常会出问题的一个东西。我们团队在工作中就多次遇到过由于客户没有处理好这个东西，从而导致播放的图像出现“花屏”的情况。</p><p>Stride也可以称之为跨距，是图像存储的时候有的一个概念。<strong>它指的是图像存储时内存中每行像素所占用的空间。</strong>你可能会问，一张图像的分辨率确定了，那一行的像素值不就确定了吗？为什么还需要跨距这个东西呢？其实，为了能够快速读取一行像素，我们一般会对内存中的图像实现内存对齐，比如16字节对齐。</p><p>举个例子，我们现在有一张RGB图像，分辨率是1278x720。我们将它存储在内存当中，一行像素需要1278x3=3834个字节，3834除以16无法整除。因此，没有16字节对齐。所以如果需要对齐的话，我们需要在3834个字节后面填充6个字节，也就是3840个字节做16字节对齐，这样这幅图像的Stride就是3840了。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/24/5e/24c1542183ce2b25116e2257c4558b5e.jpg?wh=1192x991" alt=""></p><p>这个地方你一定要注意，<strong>每读取一行数据的时候需要跳过这多余的6个字节</strong>。如果没有跳过的话，这6个字节的像素就会被我们误认为是下一行开始的2个像素（每个像素R、G、B各占1个字节，2个像素共6个字节）。那这样得到的图像就完全错了，显示出来的就是“花屏”现象，屏幕会出现一条条的斜线。</p><p>所以，不管你去读取还是渲染一张图片，还是说你将这张图片存储下来，都需要设置正确的Stride。很多时候，尤其是不规则分辨率的时候，它和图像的Width（R、G、B的话就是Width x 3）是不一样的。</p><p>有的时候即便图像的Width是一个规则的值，比如说1920或者1280等能被16整除的宽度，图像存储在内存中有可能Stride和Width（R、G、B的话就是Width x 3）也是不一样的，尤其是不同的视频解码器内部实现的不同，会导致输出的图像的Stride不一样。</p><p>所以，一定要在处理图片的时候注意这个Stride值。如果出现一条条斜线的花屏或者说解码后图像的颜色不对的情况，我们需要先确认一下这个Stride值对不对。</p><h2>帧率</h2><p>以上就是图像的基本概念，接下来我们来讲讲视频的一些基本概念。</p><p>前面我们说到，视频是由一系列图像组成的，即“连续”的一帧帧图像就可以组成视频。但事实上，视频中的图像并不是真正意义上的连续。也就是说，在1秒钟之内，图像的数量是有限的。只是当数量达到一定值之后，人的眼睛的灵敏度就察觉不出来了，看起来就是连续的视频了。</p><p>这个<strong>1秒钟内图像的数量就是帧率</strong>。据研究表明，一般帧率达到10～12帧每秒，人眼就会认为是流畅的了。当然，可能会有个体差异。</p><p>通常，我们在电影院看的电影帧率一般是24fps（帧每秒），监控行业常用25fps，而我们声网常用的帧率有15fps、24fps和30fps。你可以根据自己的使用场景来具体设定你想使用的帧率值。</p><p>选择帧率的时候还需要考虑设备处理性能的问题，尤其是实时视频通话场景。<strong>帧率高，代表着每秒钟处理的图像数量会很高，从而需要的设备性能就比较高</strong>。如果是含有多个图像处理过程，比如人脸识别、美颜等算法的时候，就更需要考虑帧率大小和设备性能的问题。同样，也要考虑带宽流量的问题。帧率越大，流量也会越多，对带宽的要求也会越高。</p><h2>码率</h2><p>我们已经知道，视频的帧率越高，1秒钟内的图像数据量就会越大。通常我们存储视频的时候需要对图像进行压缩之后再存储，否则视频会非常大。</p><p>那么压缩之后的视频我们一般如何描述它的大小呢？一般对于一个视频文件，我们直接看视频的大小就可以了。但是在实时通信或者直播的时候，视频是视频流的形式，我们怎么衡量呢？</p><p>这就涉及到我接下来要介绍的概念——码率。码率是指视频在单位时间内的数据量的大小，一般是1秒钟内的数据量，其单位一般是Kb/s或者Mb/s。通常，<strong>我们用压缩工具压缩同一个原始视频的时候，码率越高，图像的失真就会越小，视频画面就会越清晰</strong>。但同时，码率越高，存储时占用的内存空间就会越大，传输时使用的流量就会越多。</p><p>这里请你思考一个问题，同一个原始视频被压缩之后，真的是码率越高，清晰度就越高吗？</p><p>其实准确来说的话，不是。因为视频的压缩是一个非常复杂的过程，之后我们会有好几节课来讲视频压缩的知识。事实上，视频压缩之后的清晰度还跟压缩时选用的压缩算法，以及压缩时使用的压缩速度有关。压缩算法越先进，压缩率就会越高，码率自然就会越小。压缩速度越慢，压缩的时候压缩算法就会越精细，最后压缩率也会有提高，相同的清晰度码率也会更小。</p><p>所以，<strong>并不是码率越高，清晰度就会越高</strong>。</p><h2>小结</h2><p>今天我们学习了图像和视频的基础知识，都很简单但很重要，这里我为你总结了一张图帮助你记忆。</p><p><img src="https://static001.geekbang.org/resource/image/20/ca/20468627e2eccba12119a267c1abbcca.jpg?wh=2546x1406" alt=""></p><p>总结来说，一张图像是由像素组成的，而图像有多少像素则由分辨率来表示。在分辨率之外，存取一副图像还需要特别注意Stride这个东西，它跟分辨率中的Width是不一样的。然后，一帧帧图像组成了视频，我们将每秒中的图像数量称之为帧率。视频编码后每秒的数据量称之为码率。</p><p>这些知识点是我们之后课程的基础，随着我们不断深入学习，还会不断巩固这些概念。</p><h2>思考题</h2><p>现在请你想一想：码率是固定的，还是会变化的？如果是固定的，怎么做到呢？</p><p>这节课到这里就结束了，欢迎留言和我分享你的思考和疑惑，你也可以把今天所学分享给身边的朋友，邀请他加入探讨，共同进步。</p>',
        article_title: "01｜基本概念：从参数的角度看视频图像",
      },
      {
        title: "02｜YUV & RGB：原来图像是这么丰富多彩的",
        id: 449795,
        content:
          '<p>你好，我是李江。今天我们一起来聊一聊图像的颜色空间。</p><p>图像的颜色空间是图像和视频技术里面的一个非常重要的知识点，在图像处理、视频编码等技术中你会经常遇到这个概念。</p><p>而至于它为什么重要，其实也很好理解。在现实世界中，我们的眼睛每天看到的颜色是千变万化的。为了能够更方便地表示和处理这些颜色，不同应用领域就建立了多种不同的颜色空间，主要包括RGB 、YUV、CMYK 、 HSI等（后面两种和这门课程没有关系，因此这里我们不再介绍）。</p><p>在视频技术中，我们经常碰到的颜色空间有两种：RGB和YUV，接下来我们就逐一看一看。</p><h2>RGB</h2><p>RGB相对比较简单。顾名思义，它就是指图像的每一个像素都有R、G、B三个值。RGB是我们平常遇到最多的一种图像颜色空间，比如摄像头采集的原始图像就是RGB图像，且显示器显示的图像也是RGB图像。</p><p>一般来说，<strong>我们的RGB图像，每一个像素都是分别存储R、G、B三个值，且三个值依次排列存储</strong>。比如对于一张8bit位深的RGB图，每个值占用一个字节。但是，需要注意的是RGB图像像素中R、G、B三个值并不一定是按R、G、B顺序排列的，也有可能是B、G、R顺序排列。</p><p>比如OpenCV就经常使用BGR的排列方式来存储图像。所以在存储和读取RGB图像的时候需要稍微注意一下。RGB和BGR的存储方式如下图所示：</p><!-- [[[read_end]]] --><p><img src="https://static001.geekbang.org/resource/image/1b/98/1b4d7c24133df818b0187fdf56898a98.jpeg?wh=1920x1080" alt=""></p><p>虽然RGB比较简单，同时在图像处理的时候也经常会用到。但是在视频领域，我们更多地是使用YUV颜色空间来表示图像的。这是因为R、G、B三个颜色是有相关性的，所以不太方便做图像压缩编码。那YUV究竟是怎么表示图像的呢？它又是如何存储在内存当中的呢？我们接下来就来揭开它的“面纱”。</p><h2>YUV</h2><p>YUV跟RGB类似，也是一种颜色空间，但其种类会更多更复杂些，所以接下来我们会花大量的篇幅去讲解它。</p><p>YUV最早主要是用于电视系统与模拟视频领域。现在视频领域基本都是使用YUV颜色空间。</p><p>跟RGB图像中R、G、B三个通道都跟色彩信息相关这种特点不同，<strong>YUV图像将亮度信息Y与色彩信息U、V分离开来。Y表示亮度，是图像的总体轮廓，称之为Y分量。U、V表示色度，主要描绘图像的色彩等信息，分别称为U分量和V分量</strong>。这样一张图像如果没有了色度信息U、V，只剩下亮度Y，则依旧是一张图像，只不过是一张黑白图像。这种特点有什么好处呢？</p><p>在以前，世界上只有黑白电视机，每一帧电视画面都是黑白的，没有色彩信息。当然黑白电视机也不支持显示彩色图像。后来随着技术的发展，出现了彩色电视机，每一帧画面都是有颜色信息的，那当然我们可以使用RGB、YUV等颜色空间来表示一帧图像。</p><p>但是考虑到兼容老的黑白电视机，如果使用RGB表示图像，那么黑白电视机就没办法播放。这是因为R、G、B三个通道都是彩色的，而Y、U、V就可以。因为黑白电视机可以使用Y分量，Y分量就是黑白图像，而且包含了图像的总体轮廓信息，只是没有色彩信息而已。</p><p>好了，讲了YUV的起源之后，我们来讲讲YUV颜色空间最重要的知识点，那就是YUV的类型和存储方式。</p><p><strong>YUV主要分为YUV 4:4:4、YUV 4:2:2、YUV 4:2:0这几种常用的类型</strong>。其中最常用的又是YUV 4:2:0。这三种类型的YUV主要的区别就是U、V分量像素点的个数和采集方式。</p><p>YUV 4:4:4就是每一个Y就对应一个U和一个V；而YUV 4:2:2则是每两个Y共用一个U、一个V；YUV 4:2:0则是每四个Y共用一个U、V。我们可以通过图片来清晰地看一下三种YUV类型的区别。具体如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/00/ce/00d9109af3b89dbc5ecf1a81230f7bce.jpeg?wh=1920x1080" alt=""><img src="https://static001.geekbang.org/resource/image/f1/a4/f17b300f8c1yyae6eda99f1f746acea4.jpeg?wh=1920x1080" alt=""><img src="https://static001.geekbang.org/resource/image/f7/01/f745382857eefca91c1cb7151c439701.jpeg?wh=1920x1080" alt=""></p><p>总的来说：</p><ol>\n<li>YUV&nbsp;4:4:4，每一个Y对应一组UV。</li>\n<li>YUV&nbsp;4:2:2，每两个Y共用一组UV。</li>\n<li>YUV&nbsp;4:2:0，每四个Y共用一组UV。</li>\n</ol><p>YUV 4:4:4这种类型非常简单，所以存储的方式也非常简单。</p><p>那YUV 4:2:2和YUV 4:2:0这种共用U、V分量的情况，应该在内存中怎么存储呢？下面我就来为你介绍一下。</p><p><strong>YUV存储方式主要分为两大类：Planar和Packed两种</strong>。Planar格式的YUV是先连续存储所有像素点的Y，然后接着存储所有像素点的U，之后再存储所有像素点的V，也可以是先连续存储所有像素点的Y，然后接着存储所有像素点的V，之后再存储所有像素点的U。Packed格式的YUV是先存储完所有像素的Y，然后U、V连续的交错存储。</p><p>下面我们就来看看每一种YUV类型的存储方式是怎么样的。</p><ol>\n<li>YUV 4:4:4</li>\n</ol><p>这种类型的YUV非常简单，因为每一个Y对应一个U、一个V，所以存储的方式也非常简单。例如，4 x 2像素的YUV 4:4:4存储图如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/04/53/0428a59eb7702e4feb9bafd80ac83553.jpeg?wh=1920x1080" alt=""></p><p>可以看到，YUV 4:4:4和RGB图像存储之后的大小是一样的。如果是8bit图像，就是每一个像素点需要占用3个字节。</p><ol start="2">\n<li>YUV 4:2:2</li>\n</ol><p>这种类型的YUV稍微复杂些，每左右两个像素的Y共用一个U和一个V。存储方式主要有以下４种类型。</p><ul>\n<li>YU16（或者称为I422、YUV422P）</li>\n</ul><p>该类型是Planar格式，先存储完Y，再存储U，之后存储V。例如，4 x 2像素的YU16存储图如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/9f/45/9fb26028da7201cdbc7dd7f8dd83b145.jpeg?wh=1912x756" alt=""></p><ul>\n<li>YV16（YUV422P）</li>\n</ul><p>该类型也是Planar格式，先存储完Y，再存储V，之后存储U。例如，4 x 2像素的YV16存储图如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/6b/1e/6b1a9be5614b35bf7c96fe64ed98bc1e.jpeg?wh=1915x775" alt=""></p><ul>\n<li>NV16（YUV422SP）</li>\n</ul><p>这种类型是Packed格式，先存储完Y，之后U、V连续交错存储。例如，4 x 2像素的NV16存储图如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/79/aa/79d68992f9c21eff13d118ccabc145aa.jpeg?wh=1914x761" alt=""></p><ul>\n<li>NV61（YUV422SP）</li>\n</ul><p>这种也是Packed格式，与NV16不同，这种格式是先存储完Y，之后V、U连续交错存储。例如，4 x 2像素的NV61存储图如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/2f/44/2f06f11d9c9dd7a49d268e325b846944.jpeg?wh=1912x755" alt=""></p><p>可以看到，4 x 2像素的YUV 4:2:2只需要16个字节，而RGB图像则需要24个字节<strong>。</strong>也就是说，如果是8bit图像，那么RGB每一个像素需要3个字节，而YUV 4:2:2只需要2个字节。</p><ol start="3">\n<li>YUV 4:2:0</li>\n</ol><p>这是最常见也是最常用的YUV类型。通常视频压缩都是YUV 4:2:0格式的。它是每上、下、左、右4个像素点共用一个U和一个V。存储方式主要分为以下４种。</p><ul>\n<li>YU12（I420、YUV420P）</li>\n</ul><p>这种类型是Planar格式，先存储完Y，再存储U，之后存储V。例如，4 x 4像素的YU12存储图如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/76/16/766ce511056bf5dd3399198398b80016.jpeg?wh=1920x1080" alt=""></p><ul>\n<li>YV12（YUV420P）</li>\n</ul><p>该类型也是Planar格式，先存储完Y，再存储V，之后存储U。例如，4 x 4像素的YＶ12存储图如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/88/71/880114b51649cbea783ee0be883fc971.jpeg?wh=1920x1080" alt=""></p><ul>\n<li>NV12（YUV420SP）</li>\n</ul><p>这种类型是Packed格式，先存储完Y，之后U、V连续交错存储。例如，4 x 4像素的NV12存储图如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/62/45/62689d6a46003ced023b9838ca0a0845.jpeg?wh=1920x1080" alt=""></p><ul>\n<li>NV21（YUV420SP）</li>\n</ul><p>这种也是Packed格式，与NV12不同，这种格式是先存储完Y，之后V、U连续交错存储。例如，4 x 4像素的NV21存储图如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/9e/bd/9ed80ef848c52123fb41413abfa26ebd.jpeg?wh=1920x1080" alt=""></p><p>可以看到，4 x 4像素的YUV 4:2:0只需要24个字节相比RGB图像需要48个字节，存储的大小少了一半。也就是说，如果是8bit图像，RGB每一个像素需要3个字节。而YUV 4:2:0只需要1.5个字节。</p><p>好了，以上就是YUV颜色空间的主要类型和存储方式。这里我用表格给你总结了一下。</p><p><img src="https://static001.geekbang.org/resource/image/e7/88/e7a9fcb9f5800eab1fb0a9ed19066588.jpeg?wh=1920x1080" alt=""></p><h2>RGB和YUV之间的转换</h2><p>我们刚才说到，一般来说，采集到的原始图像、给显示器渲染的最终图像都是RGB图像，但是视频编码一般用的是YUV图像。那么这中间一定少不了两者的相互转换。那RGB如何转到YUV呢？YUV又如何转到RGB呢？</p><p>在讲转换之前，我们先了解Color Range这个东西。对于一个8bit的RGB图像，它的每一个R、G、B分量的取值按理说就是0~255的。但是真的是这样的吗？其实不是的。这里就涉及到Color Range这个概念。Color Range分为两种，一种是Full Range，一种是Limited Range。<strong>Full Range的R、G、B取值范围都是0～255。而Limited Range的R、G、B取值范围是16～235。</strong></p><p>了解了Color Range之后，我们怎么规范YUV和RGB之间的互转呢？其实这也是有标准的，目前的标准主要是BT601和BT709（其实还有BT2020，我们这里不展开讲）。简单来讲，<strong>BT709和BT601定义了一个RGB和YUV互转的标准规范</strong>。只有我们都按照标准来做事，那么不同厂家生产出来的产品才能对接上。BT601是标清的标准，而BT709是高清的标准。</p><p>下面我们来看看这两种标准分别在Full Range和Limited Range下的RGB和YUV之间的转换公式吧。具体如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/19/72/19da3510c564b590f92cf969be01d872.jpeg?wh=1920x1080" alt=""></p><p>从上图我们可以看到每种标准下不同Color Range的转换公式是不同的。所以<strong>在做RGB往YUV转换的时候我们需要知道是使用的哪个标准的哪种Range做的转换</strong>，并告知对方。这样对方使用同样的标准和Range才可以正确的将YUV转换到RGB。</p><p>如果是系统采集出来给到用户的图像就是YUV的话，你也需要获取这个YUV的存储格式、转换标准和Color Range。这样才能保证正确地处理YUV和RGB之间的转换。</p><p>好了，这就是关于RGB和YUV颜色空间的一些知识。在最后我需要再一次强调一下前一节课中讲到的Stride。</p><p><strong>在处理YUV图像的存储和读取的时候，也是有Stride这个概念的。事实上，YUV出问题的情况更多。</strong>在这里举一个例子，比如说一张1283x720的图像，一个Y分量存储按16字节对齐的话应该是每行占用1296个字节，所以每读取一行像素的Y应该是1296个字节，具体如下图所示。千万不要认为是1283个字节，不然就会出现“花屏”。这里一定要注意。</p><p><img src="https://static001.geekbang.org/resource/image/80/57/808b8310bb6dfbabfc3d6fbc2ce88057.jpeg?wh=1920x1080" alt=""></p><h2>小结</h2><p>今天的知识点到这里就讲完了。总结一下，这节课我们主要讲了图像的颜色空间。图像的颜色空间主要有RGB和YUV两种。其中RGB图像每一个像素有R、G、B三个值。而YUV图像有３种类型，其分类如下：</p><ol>\n<li>YUV 4:4:4，一个Y对应一个U和一个V；</li>\n<li>YUV 4:2:2，左右两个Y对应一个U和一个V；</li>\n<li>YUV 4:2:0，上下左右四个Y对应一个U和一个V。</li>\n</ol><p>同时，YUV在存储的时候也有两种大类：一种是Planar格式；一种是Packed格式。其中Planar格式又分为先存U还是先存V两种。而Packed格式是UV交错存储且分为U在前还是V在前两种。</p><p>在图像采集的时候我们一般得到的原始图像是RGB图像，并且渲染的时候最终也是用RGB图像，而在编/解码时用的却是YUV图像。因此，我们需要在RGB和YUV之间互转。转换的标准有两种：一种是BT601；一种是BT709。</p><p>另外，在转换过程中我们还涉及到Color Range这个概念。Full Range的R、G、B三个值的范围都是0～255。而Limited Range的取值范围是16～235。在做转换的时候我们需要选择正确的标准和Color Range。</p><p>最后，我再一次提出了Stride这个概念。在读取YUV图像一行像素的时候一定要区分Width和Stride的区别。Width是原始图像的宽，而Stride是对齐之后的一行像素使用的字节大小。这个一定要注意，千万不要弄错了。</p><h2>思考题</h2><p>通过这节课的学习，现在你知道我们为什么编码的时候使用YUV图像而不使用RGB图像了吗？</p><p>好了，这节课到这里就结束了，欢迎留言和我分享你的思考和疑惑，你也可以把今天所学分享给身边的朋友，邀请他加入探讨，共同进步。</p>',
        article_title: "02｜YUV & RGB：原来图像是这么丰富多彩的",
      },
      {
        title: "03｜缩放算法：如何高质量地缩放图像？",
        id: 452674,
        content:
          '<p>你好，我是李江。</p><p>今天，我们一起来聊聊图像的缩放算法。图像的缩放算法在我们的日常生活中使用非常频繁，只是可能你没有留意到。举个例子，你使用网页或者播放器看电影的时候，经常会开启全屏或者退出全屏，电影的播放画面就会变大，或者变小。这个过程里面就会用到图像的缩放算法。</p><p>事实上，只要视频的原始分辨率和播放窗口的大小不一致，就需要通过缩放处理来使得视频画面适应窗口的大小。比如说，电影分辨率是1080P，播放器的窗口大小是720P，则需要将电影画面从1080P缩小到720P再播放。如果你点击全屏播放，播放窗口变成了4K，则需要将电影画面做放大处理，即放大到4K之后再播放。这就是一个非常典型的图像缩放的例子。</p><p>在视频开发的过程中，图像的缩放就更多了。下面我列举3种用到图像缩放的情形：</p><ul>\n<li>情形1：播放窗口与原始图像分辨率不匹配的时候需要缩放。这和我刚才举的例子是一样的情况。</li>\n<li>情形2：我们在线观看视频时会有多种分辨率可以选择，即需要在一个图像分辨率的基础上缩放出多种不同尺寸的图像出来做编码，并保存多个不同分辨率的视频文件。</li>\n<li>情形3：RTC场景，有的时候我们需要根据网络状况实时调节视频通话的分辨率。这个也是需要缩放算法来完成的。</li>\n</ul><!-- [[[read_end]]] --><p>所以，我们可以看到图像的缩放算法是一个很常用的技术，且它是非常重要的。并且，由于图像的缩放会严重影响我们视觉的主观感受，所以图像缩放算法的选择也是非常重要的。目前图像的缩放算法非常多，其中主要包括最常用的插值算法和目前比较火的AI超分算法。</p><p>由于目前绝大多数图像的缩放还是通过插值算法来实现的，所以我们今天主要来聊聊插值算法。插值算法有很多种，但是其基本原理都是差不多的。<strong>它们都是使用周围已有的像素值通过一定的加权运算得到“插值像素值”</strong>。插值算法主要包括：最近邻插值算法（Nearest）、双线性插值算法（Bilinear）、双三次插值算法（BiCubic）等。那么在一一讲解这些插值算法之前，我们不妨先来聊聊缩放算法的基本原理。</p><h2>缩放的基本原理</h2><p>前面我们讲过，<strong>图像的缩放就是将原图像<strong><strong>的</strong></strong>已有像素经过加权运算得到目标图像的目标像素</strong>。</p><p>什么意思呢？比如说，我们已有图像是720P的分辨率，称之为原图像，我们需要放大到1080P，我们称这个1080P图像是目标图像。目标图像在宽度方向上放大了1920 / 1280 = 1.5倍，高度方向上也放大了1080 / 720 = 1.5倍。</p><p><strong>那怎么通过720P的原图像生成1080P的目标图像呢？</strong>我们先将目标图像的像素位置映射到原图像的对应位置上，然后把通过插值计算得到的原图像对应位置的像素值作为目标图像相应位置的像素值。是不是有点绕？别急，下面我给你举个例子，通过它你就可以更直观地理解这句话的意思了。</p><p>比如说，1080P目标图像中的（0，0）位置就映射到720P原图像的（0，0）位置，取原图像（0，0）位置的像素值作为目标图像（0，0）位置的像素值。目标图像的（1，1）位置就映射到原图像中的（0.67， 0.67）位置。最后，通过原图像已有像素插值得到（0.67，0.67）位置的像素值，并将该像素值作为目标图像（1，1）位置的像素值。</p><p><img src="https://static001.geekbang.org/resource/image/f9/9d/f94f67987afa3c6c6fca937cdf2ce49d.jpg?wh=1280x720" alt=""></p><p>现在你知道了图像放大的大体过程，那图像缩小的过程是怎样的呢？</p><p>同样的我们以720P作为原图像，那么720P缩小到目标图像360P的过程也是类似于图像放大的过程的，这里通过下图描述一下映射的过程，具体就不重复了。</p><p><img src="https://static001.geekbang.org/resource/image/0a/94/0a603907ee36de3956d7e527104d0394.jpg?wh=1324x742" alt=""><br>\n好了，现在我们再回顾一下图像缩放的过程。</p><p>首先是图像放大的过程，对于1080P目标图像中的每一个像素点（x，y），我们只需要将它映射到720P原图像的（x / 1.5，y / 1.5）位置，通过原图像已有的像素值插值得到（x / 1.5，y / 1.5）的像素值就可以了。我们遍历一下目标图像中的每一个像素点位置，都能找到他们在原图像中的映射位置，并通过插值求出映射位置的像素值，这样就可以得到目标图像了，从而也就达到了放大的目的。</p><p>图像缩小的过程也是类似的。对于360P目标图像中的每一个像素点（x，y），我们只需要将它映射到720P原图像的（x * 2，y * 2）位置，通过原图像已有的像素值插值得到（x * 2，y * 2）的像素值就可以了。</p><p>下面我们以更通用的表达式来表达一下缩放过程。</p><p>假设原图像的分辨率是w0 x h0，我们需要缩放到w1 x h1。那<strong>我们只需要将目标图像中的像素位置（x，y）映射到原图像的（x * w0 / w1，y * h0 / h1），<strong><strong>再</strong></strong>插值得到这个像素值就可以了，这个插值得到的像素值就是目标图像像素点（x，y）的像素值</strong>。注意，（x * w0 / w1，y * h0 / h1）绝大多数时候是小数。<strong>这就是图像缩放算法原理的通用表达</strong>。下面是图像放大和缩小的映射过程的示意图。</p><p><img src="https://static001.geekbang.org/resource/image/9b/b6/9b874d563bd457266d451eeab689f2b6.png?wh=3060x1715" alt=""></p><p><img src="https://static001.geekbang.org/resource/image/2d/df/2de3de3d08e72635e8702ffa975024df.png?wh=2706x1516" alt=""></p><h2>三种插值算法</h2><p>到这里我们已经讲完了图像缩放的基本原理，我们注意到，位置映射过程很简单，主要的工作就是如何通过插值算法得到原图像映射位置的像素值。同时，需要说明一下，图像缩放和插值在RGB和YUV颜色空间中都可以进行。因此，我们下面不会对颜色空间做区分。接下来，我们就依次介绍一下三种插值算法，看看它们的具体插值原理是怎样的，以及它们的效果又是怎样的。</p><h3>最近邻插值</h3><p>我们先来聊聊最简单的<strong>最近邻插值算法</strong>。顾名思义，最近邻插值就是：</p><ul>\n<li>首先，将目标图像中的目标像素位置，映射到原图像的映射位置。</li>\n<li>然后，找到原图像中映射位置周围的4个像素。</li>\n<li>最后，取离映射位置最近的像素点的像素值作为目标像素。</li>\n</ul><p>比如说，我们现在要将图像从720P放大到1080P。下面我们给出1080P目标图像中3个像素点（0，0）、（1，0）和（2，2）的最近邻插值过程。</p><ol>\n<li>1080P图像的（0，0）位置的像素，我们映射到720P图像的映射位置就是（0 * 1280 / 1920，0 * 720 / 1080），也就是（0，0）位置，那1080P的（0，0）位置的像素值直接取原图像（0，0）像素点的像素值就可以了。</li>\n<li>对于1080P图像的（1，0）位置的像素，我们映射到720P图像就是（1 * 1280 / 1920， 0 * 720 / 1080），也就是（0.67，0）位置的像素，这个像素需要插值得到。使用最近邻插值的话，（0.67，0）周围的4个像素分别是（0，0）、（1，0）、（0，1）和（1，1），其中距离（0.67，0）最近的位置很明显是（1，0）位置的像素。因此，我们将原图像中（1，0）位置的像素值赋值给目标图像（1，0）位置的像素点。</li>\n</ol><p><img src="https://static001.geekbang.org/resource/image/77/d3/77989efeeb4272e0988ayy9d73e35dd3.jpg?wh=1345x754" alt=""></p><ol start="3">\n<li>对于1080P图像的（2，2）位置呢？同样映射到720P图像，映射位置是（2 * 1280 / 1920，2 * 720 / 1080），也就是（1.33，1.33）位置，其周围4个像素分别是（1，1）、（1，2）、（2，1）和（2，2），很明显（1，1）离（1.33，1.33）位置最近，那我们取原图像（1，1）的像素值赋值给1080P图像的（2，2）位置的像素点。</li>\n</ol><p><img src="https://static001.geekbang.org/resource/image/f9/0d/f90d9655eb3acf415826d7c6d5d4190d.jpg?wh=1352x758" alt=""></p><p>照着这个步骤一个个像素插值下去就可以得到1080P的图像了。是不是很简单？这个过程就是通过目标图像的像素位置，按照缩放比例映射到原图像，然后找到原图像中离映射位置最近的像素点，把它的像素值赋值给目标图像的像素就可以了。</p><p>最近邻插值有一个明显的<strong>缺点</strong>，就是<strong>它直接使用离插值位置最近的整数位置的像素作为插值像素，这样会导致相邻两个插值像素有很大的概率是相同的</strong>。比如说，上面例子中的（1，0）位置和（2，0）位置的像素值是一样的。这样<strong>得到的放大图像大概率会出现块状效应，<strong><strong>而</strong></strong>缩小图像容易出现锯齿</strong>。这是最近邻插值的缺点。但是它也有一个优点，就是<strong>不需要太多的计算，速度非常的快</strong>。</p><h3>双线性插值</h3><p>介绍完了最近邻插值算法，接下来我们将要介绍双线性插值算法。</p><p>双线性插值相比于最近邻插值稍微复杂一些，它也是取待插值像素周围的4个像素，不同的是，它需要<strong>将这4个像素值通过一定的运算得到最后的插值像素</strong>。在开始讲双线性插值的原理之前，我们先来看看双线性插值的基础，也就是线性插值的原理。</p><p>线性插值是在两个点中间的某一个位置插值得到一个新的值。线性插值认为，这个需要插值得到的点跟这两个已知点都有一定的关系，并且，待插值点与离它近的那个点更相似。因此，<strong>线性插值是一种以距离作为权重的插值方式，距离越近权重越大，距离越远权重越小</strong>。</p><p>比如，如下图所示，已知 （x1，y1） 与 （x2，y2）两个点，需求得x对应的y值。</p><p><img src="https://static001.geekbang.org/resource/image/42/35/42211d591931d86f0266290bf7631135.jpg?wh=1280x720" alt=""></p><p>通过线性插值方法，y值的计算公式如下：<br>\n<img src="https://static001.geekbang.org/resource/image/f1/e7/f1e4d19f792a19faa69780dcc96791e7.jpg?wh=1280x720" alt=""></p><p><strong>双线性插值本质上就是在两个方向上做线性插值</strong>。由于图像是两个方向的二维数据，正好适合使用双线性插值算法。下面我们来讲讲双线性插值的具体原理。</p><p><strong>双线性插值其实就是三次线性插值的过程</strong>，我们先通过两次线性插值得到两个中间值，然后再通过对这两个中间值进行一次插值得到最终的结果。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/7f/28/7f16c2af0c92c0ce41b5a595da2d2128.jpg?wh=1280x720" alt=""></p><p>假设我们要插值求的点是p点，其坐标为(x，y)。已知周围4个像素分别是a、b、c、d。我们先通过a和b水平线性插值求得m，再通过c、d水平插值求得n。有了m和n之后，再通过m、n垂直插值求得p点的像素值。计算过程如下：</p><p><img src="https://static001.geekbang.org/resource/image/d4/b5/d43a159dce82ef9131c4d44a6ccd1eb5.jpg?wh=1280x720" alt=""></p><p>我们还是以720P放大到1080P为例，那么1080P图像中的目标像素点（2，2）的双线性插值过程是怎么样的呢？</p><p>首先，将目标像素点（2，2）映射到原图像的（1.33，1.33）位置，对应下面图中的点ｐ。找到（1.33，1.33）周围的4个像素（1，1）、（2，1）、（1，2）和（2，2），分别对应图中的点ａ、ｂ、ｃ和ｄ。</p><p><img src="https://static001.geekbang.org/resource/image/fc/9c/fc70e574058413ac2ddb674a78aa079c.jpg?wh=1352x758" alt=""><br>\n先通过这4个像素插值得到中间像素m和n的像素值。m和n的坐标分别为（1.33，1）和（1.33，2）。通过上面的公式可以求得点p（1.33，1.33）的像素值是：</p><p><img src="https://static001.geekbang.org/resource/image/6c/dc/6c59747b623681425596df5bb75c80dc.jpg?wh=1280x720" alt=""></p><p>插值求得（1.33，1.33）的值之后，将其赋值给1080P目标图像的（2，2）位置的像素点就可以了。这就是双线性插值的过程。</p><p><strong>双线性插值相比最近邻插值运算要多一些</strong>，因此运行时间要长一些，但是相比而言，<strong>插值之后图像效果会好于最近邻插值</strong>。</p><h3>双三次插值</h3><p>下面我们接着来看一种效果相比双线性插值更好一些的插值算法，就是双三次插值算法，也叫BiCubic插值。</p><p>在最近邻插值算法中，我们选择待插值像素周围的4个像素，并取离待插值像素位置最近的像素点权重为1，其余3个点权重为0。在双线性插值算法中，同样选择待插值像素周围的4个像素，并且每个像素以距离作为权重，距离越近权重越大，距离越远权重越小。</p><p>双三次插值算法的基本原理同前两种插值算法差不多，不同的是：</p><p>第一，<strong>双三次插值选取的是周围的16个像素</strong>，比前两种插值算法多了3倍。</p><p>第二，双三次插值算法的<strong>周围像素的权重计算是使用一个特殊的BiCubic基函数来计算的</strong>。</p><p>我们先通过这个BiCubic基函数计算得到待插值像素周围16个像素的权重，然后将16个像素加权平均就可以得到最终的待插值像素了。</p><p><img src="https://static001.geekbang.org/resource/image/db/36/db2e00c747e025346c030069239bf136.jpg?wh=1280x720" alt=""><br>\nBiCubic基函数形式如下：<br>\n<img src="https://static001.geekbang.org/resource/image/70/18/702c29205ca2542fd57a5ffc9961e118.jpg?wh=1280x717" alt=""></p><p><strong>双三次插值的权重值是分水平和垂直两个方向分别求得的，计算公式是一样的，都是上面这个公式</strong>。对于周围16个点中的每一个点，其坐标值为（x，y），而目标图像中的目标像素在原图像中的映射坐标为p（u，v）。那么通过上面公式可以求得其水平权重W（u - x），垂直权重W（v - y）。将W（u - x）乘以W（v - y）得到最终权重值，然后再用最终权重值乘以该点的像素值，并对16个点分别做同样的操作并求和，就得到待插值的像素值了。公式如下：<br>\n<img src="https://static001.geekbang.org/resource/image/51/ba/5111be20665a5cdecd9e08a00b6b11ba.jpg?wh=1280x720" alt=""></p><p>我们还是以720P放大到1080P为例，那么1080P图像中的目标像素点（2，2）的双三次插值过程是怎么样的呢？</p><p>首先，将目标像素点（2，2）映射到原图像的（1.33，1.33）位置，对应下面图中的点ｐ。找到（1.33，1.33）周围的16个像素（0，0）、（1，0）一直到（3，3）。</p><p><img src="https://static001.geekbang.org/resource/image/c0/d2/c0dbfac89b7bdc8ed7d597128d6d73d2.jpg?wh=1280x720" alt=""><br>\n然后，通过BiCubic函数求得每一个点的水平和垂直权重。例如，（0，0）、（1，2）和（3，3）点的水平权重和垂直权重计算方式如下：</p><p><img src="https://static001.geekbang.org/resource/image/ab/9f/ab9b884abe3637a17fe3f7f112e6879f.jpg?wh=1280x720" alt=""></p><p>求出这16个点的水平和垂直权重，两者相乘得到最终的权重值，之后每一个像素用自己的最终权重乘以自己的像素值再求和就是（1.33，1.33）的插值像素值了。将它赋值给1080P图像的（2，2）像素点就可以了。</p><p>我们可以看到，<strong>双三次插值需要计算16个点的权重再乘以像素值求和，相较于前面的最近邻插值和双线性插值计算量较大，但插值后的图像效果最好</strong>。</p><p>好了，我们通过下面几幅图像来对比一下这三种插值算法的效果。我们可以看到：最近邻插值得到的图像有很多块效应，效果最差；双线性插值稍好于最近邻插值一些，但是比较模糊；双三次插值效果最好，对比度也明显好于双线性插值。</p><p><img src="https://static001.geekbang.org/resource/image/94/03/946fb99d19607a8537a451ae6c81c603.jpg?wh=1322x741" alt=""><img src="https://static001.geekbang.org/resource/image/01/9d/01bbf77d405b1443c968667384d9989d.jpg?wh=1312x735" alt=""></p><h2>小结</h2><p>好了，这节课到这里就要结束了。我们来回顾一下今天的学习内容。</p><p>我们主要讨论了图像的缩放算法。图像缩放主要包括两个部分：一个是像素位置映射过程；一个是映射位置像素的插值过程。</p><ol>\n<li>像素位置映射过程</li>\n</ol><p>对于分辨率为w0 x h0的原图像，我们需要缩放到分辨率为w1 x h1的目标图像。我们只需要将目标图像的每一个像素点（x，y）映射到原图像的（x * w0 / w1，y * h0 / h1）位置。一般这个映射位置不是一个整数位置。我们需要通过插值算法得到映射位置的像素值，然后将映射位置插值得到的像素值赋值给目标像素就可以了。</p><ol start="2">\n<li>映射像素的插值过程</li>\n</ol><p>插值过程主要会使用到插值算法。我们今天介绍了最常用的三种插值算法，分别是最近邻插值、双线性插值和双三次插值算法。三种算法的思想和优缺点如下表所示。</p><p><img src="https://static001.geekbang.org/resource/image/b5/1a/b586f35fdc6a5083f4ca9a3129e7yy1a.jpg?wh=1464x990" alt=""></p><h2>思考题</h2><p>现在我有一个思考题留给你。</p><p>双三次插值需要周围16个像素，对于左上角的点，比如（0.5，0.5），它周围不够16个点怎么办呢？</p><p>欢迎你在留言区和我分享你的思考和疑惑，你也可以把今天所学分享给身边的朋友，邀请他加入探讨，共同进步。下节课再见。</p>',
        article_title: "03｜缩放算法：如何高质量地缩放图像？",
      },
    ],
  },
  {
    chapterTitle: "视频编码",
    children: [
      {
        title: "04｜编码原理：视频究竟是怎么编码压缩的？",
        id: 459554,
        content:
          '<p>你好，我是李江。今天我们一起来聊一聊视频编码。</p><p>说到视频，我们首先想到的可能就是占内存。我们知道一个视频是由一连串图像序列组成的，视频中图像一般是YUV格式。现在，我们假设有一个电影视频，分辨率是1080P，帧率是25fps，并且时长是2小时，如果不做视频压缩的话，它的大小是1920 x 1080 x 1.5 x 25 x 2 x 3600 = 521.4G。而我们的电脑一般是500G的硬盘，那就连2部电影都放不下了。如果是在视频通话场景下的话，按照这个大小去传输视频数据，对流量和带宽资源的消耗也是非常大的，并且如此大的数据发送对网络要求也非常高，很显然我们是接受不了的。因此，做视频编码压缩就非常有必要。</p><p>那么，接下来我就深入讲讲视频编码，带你从预测编码、变换编码、熵编码等方面，系统了解下视频编码的原理。相信这节课过后，你会对视频编码有一个全面的了解。</p><h2>视频编码的原理</h2><p>视频编码是对一帧帧图像来进行的。一般我们所熟知的彩色图像的格式是RGB的，即用红绿蓝三个分量的组合来表示所有颜色。但是，RGB三个颜色是有相关性的，为了去掉这个相关性，减少需要编码的信息量，我们通常会把RGB转换成YUV，也就是 <strong>1个亮度分量和2个色度分量</strong>。</p><!-- [[[read_end]]] --><p>另外，人眼对于亮度信息更加敏感，而对于色度信息稍弱，所以视频编码是将Y分量和UV分量分开来编码的。</p><p>而对于每一帧图像，又是划分成一个个块来进行编码的，<strong>这一个个块在H264中叫做宏块</strong>，而在VP9、AV1中称之为超级块，其实概念是一样的。宏块大小一般是16x16（H264、VP8），32x32（H265、VP9），64x64（H265、VP9、AV1），128x128（AV1）这几种。这里提到的H264、H265、VP8、VP9和AV1都是市面上常见的编码标准，下面我会介绍，这里就不再详细讲述。</p><p><strong>图像一般都是有数据冗余的</strong>，主要包括以下4种：</p><ul>\n<li><strong>空间冗余。</strong>比如说将一帧图像划分成一个个16x16的块之后，相邻的块很多时候都有比较明显的相似性，这种就叫空间冗余。</li>\n<li><strong>时间冗余。</strong>一个帧率为25fps的视频中前后两帧图像相差只有40ms，两张图像的变化是比较小的，相似性很高，这种叫做时间冗余。</li>\n<li><strong>视觉冗余。</strong>我们的眼睛是有视觉灵敏度这个东西的。人的眼睛对于图像中高频信息的敏感度是小于低频信息的。有的时候去除图像中的一些高频信息，人眼看起来跟不去除高频信息差别不大，这种叫做视觉冗余。</li>\n<li><strong>信息熵冗余。</strong>我们一般会使用Zip等压缩工具去压缩文件，将文件大小减小，这个对于图像来说也是可以做的，这种冗余叫做信息熵冗余。<br>\n视频编码就是通过减少上述4种冗余来达到压缩视频的目的。接下来我们就一起来慢慢剥开视频编码这个“洋葱”吧。</li>\n</ul><p>对于一个YUV图像，我们把它划分成一个个16x16的宏块（以H264为例），Y、U、V分量的大小分别是16x16、8x8、8x8。这里我们只对Y分量进行分析（U、V分量同理）。假设Y分量这16x16个像素就是一个个数字，我们从左上角开始之字形扫描每一个像素值，则可以得到一个“像素串”。如下图所示：<br>\n<img src="https://static001.geekbang.org/resource/image/6c/16/6c879e4971995d927e9de46616256b16.jpg?wh=1280x720" alt=""><br>\n如果你是程序员的话，你肯定做过一个压缩字符串的编程题目，就是将 “aaaabbbccccc” 压缩成 “4a3b5c”，字符串由13个字节压缩到7个字节，这个叫做<strong>行程编码</strong>。如果我们对图像宏块扫描出来的这个“像素串”做同样的行程编码操作，是不是也有可能减小图像块呢？</p><p>在对“像素串”行程编码之前，我们先回过头来看看另一个行程编码的例子。如果刚才编程题的字符串是 “abcdabcdabcd” 的话，那么编码之后就会是 “1a1b1c1d1a1b1c1d1a1b1c1d”。字符串的大小从13字节变成了25字节，还变大了。</p><p>我们发现如果想要达到压缩的目的，我们必须要使得编码前的字符串中出现比较多连续相同的字符。这对于图像块也是一样的。我们需要使得扫描出来的“像素串”，也尽量出现连续相同的像素值，最好是一连串数字很小（比如0）的“像素串”，因为0在二进制中只占1个位就可以了。</p><p>这个地方你可能会有疑惑，0也是至少要一个字节存储，需要8个位，怎么会是1个位呢？这个有的编码算法是可以做到的，比如指数哥伦布编码，它就可以做到0只占用一个位。事实上，算术编码可以做到一个符号只占用0点几个位，连一个位都不用，这里不详细展开了，感兴趣的话你可以去查阅下资料。</p><p>那我们<strong>如何做到将这串像素值变成有很多0的“像素串”呢</strong>？</p><p>首先第一步，<strong>我们通过减少图像块的空间冗余和时间冗余来接近这个目标</strong>。刚才我们也说到，图像内部相邻宏块之间有很多相似性，并且两张图像之间也有很多相似性。因此，根据图像的这个特点，我们可以在编码的时候进行<strong>帧内预测和帧间预测</strong>。</p><p>帧内预测就是在当前编码图像内部已经编码完成的块中找到与将要编码的块相邻的块。一般就是即将编码块的左边块、上边块、左上角块和右上角块，通过将这些块与编码块相邻的像素经过多种不同的算法得到多个不同的预测块。</p><p>然后我们再用编码块减去每一个预测块得到一个个残差块。最后，我们取这些算法得到的残差块中像素的绝对值加起来最小的块为预测块。而得到这个预测块的算法为帧内预测模式。</p><p><img src="https://static001.geekbang.org/resource/image/69/6a/691de4570939f2a5953c848b697e0c6a.jpg?wh=1326x743" alt=""></p><p>由于这个残差块中像素的绝对值之和最小，这个残差块的像素值经过扫描之后的“像素串”是不是就比直接扫描编码块的“像素串”中的像素值更接近0了？</p><p>同理，帧间预测也是一样的。我们在前面已经编码完成的图像中，循环遍历每一个块，将它作为预测块，用当前的编码块与这个块做差值，得到残差块，取残差块中像素值的绝对值加起来最小的块为预测块，预测块所在的已经编码的图像称为参考帧。预测块在参考帧中的坐标值 (x0, y0) 与编码块在编码帧中的坐标值 (x1, y1) 的差值 (x0 - x1, y0 - y1) 称之为运动矢量。而在参考帧中去寻找预测块的过程称之为运动搜索。事实上编码过程中真正的运动搜索不是一个个块去遍历寻找的，而是有快速的运动搜索算法的。之后我们在帧间预测的课中会详细介绍。</p><p>总之，通过预测得到的残差块的像素值相比编码块的像素值，去除了大部分空间冗余信息和时间冗余信息，这样得到的像素值更小。如果把这个残差块做扫描得到的像素串送去做行程编码，是不是相比直接拿编码块的像素串去做编码更有可能得到更大的压缩率？</p><p><strong>但是我们的目标不只是将像素值变小，而是希望能出现连续的0像素，那怎么办呢？</strong></p><p>这就需要利用我们人眼的视觉敏感性的特点了。我们刚才说了人眼对高频信息不太敏感。因为人眼看到的效果可能差别不大，所以我们可以去除一些高频信息。这个就是接下来我们要讨论的 <strong>DCT变换和量化</strong>。</p><p>为了分离图像块的高频和低频信息，我们需要将图像块变换到频域。常用的变换是DCT变换。DCT变换又叫离散余弦变换。在H264里面，如果一个块大小是16x16的，我们一般会划分成16个4x4的块（当然也有划分成8x8做变换的，我们这里以4x4为例）。然后对每个4x4的块做DCT变换得到相应的4x4的变换块。</p><p>变换块的每一个“像素值”我们称为系数。变换块左上角的系数值就是图像的低频信息，其余的就是图像的高频信息，并且高频信息占大部分。低频信息表示的是一张图的总体样貌。一般低频系数的值也比较大。而高频信息主要表示的是图像中人物或物体的轮廓边缘等变化剧烈的地方。高频系数的数量多，但高频系数的值一般比较小（注意不是所有的高频系数都一定小于低频，只是大多数高频系数比较小）。如下图所示（黄色为低频，绿色为高频）：</p><p><img src="https://static001.geekbang.org/resource/image/b7/ed/b737f23d14a7980d305d16d4136e62ed.jpg?wh=1280x720" alt=""></p><p>这样做完了DCT变换之后，低频和高频信息就分离开来了。由于低频信息在左上角，其余的都是高频信息。那么如果我们对变换块的像素值进行“之字形”扫描，这样得到的像素串，前面的就是数值比较大的低频系数，后面就是数值比较小的高频部分。</p><p>由于人眼对高频信息不太敏感，如果我们通过一种手段去除掉大部分高频信息，也就是将大部分高频信息置为0，但又不太影响人的观感，是不是就可以达到我们最初的目标，即可以得到有一连串0的像素串？这就涉及到量化操作了。</p><p>我们让变换块的系数都同时除以一个值，这个值我们称之为<strong>量化步长</strong>，也就是QStep（QStep是编码器内部的概念，用户一般使用<strong>量化参数</strong> QP这个值，QP和QStep一一对应，你可以自行去网上查询一下转换表），得到的结果就是量化后的系数。<strong>QStep越大，得到量化后的系数就会越小</strong>。同时，相同的QStep值，高频系数值相比低频系数值更小，量化后就更容易变成0。这样一来，我们就可以将大部分高频系数变成0。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/41/4d/41b9be2b425a8925c32bfe008f382f4d.jpg?wh=1280x720" alt=""></p><p>解码的时候，我们会将QStep乘以量化后的系数得到变换系数，很明显这个<strong>变换系数和原始没有量化的变换系数是不一样的</strong>，这个就是我们常说的有损编码。而到底损失多少呢？</p><p>这由QStep来控制，QStep越大，损失就越大。QStep跟QP一一对应，也就是说确定了一个QP值，就确定了一个QStep。所以从编码器应用角度来看，<strong>QP值越大，损失就越大，从而画面的清晰度就会越低</strong>。同时，QP值越大系数被量化成0的概率就越大，这样编码之后码流大小就会越小，压缩就会越高。</p><p>以上就是视频编码的推理过程。总结一下就是，为了能够在最后熵编码的时候压缩率更高，我们希望送到熵编码（以行程编码为例）的“像素串”，是一串含有很多0，并且最好连续为0的“像素串”。</p><p>为了达到这个目标，我们先通过帧内预测或者帧间预测去除空间冗余和时间冗余，从而得到一个像素值相比编码块小很多的残差块。之后我们再通过DCT变换将低频和高频信息分离开来得到变换块，然后再对变换块的系数做量化。由于高频系数通常比较小，很容易量化为0，同时人眼对高频信息不太敏感，这样我们就得到了一串含有很多个0，大多数情况下是一串含有连续0的“像素串”，并且人的观感还不会太明显。这样，最后熵编码就能把图像压缩成比较小的数据，以此达到视频压缩的目的。这就是<strong>视频编码的原理</strong>。</p><h2><strong>编码器的对比及选择</strong></h2><p>说完了编码器的原理，那么常用的编码标准有哪些呢？它们的区别又是什么？我们怎么选择合适的编码器？</p><p>现在市面上常见的编码标准有H264、H265、VP8、VP9和AV1。目前H264和VP8是最常用的编码标准，且两者的标准非常相似。H265和VP9分别是他们的下一代编码标准，这两个标准也非常相似。AV1是VP9的下一代编码标准。</p><p>H264和H265是需要专利费的，而VP8和VP9是完全免费的。由于H265需要付高额的版权费，以谷歌为首的互联网和芯片巨头公司组织了AOM联盟，开发了新一代压缩编码算法AV1，并宣布完全免费，以此来对抗高额专利费的H265。</p><p>目前普通产品还是使用H264最多，而H265因为专利费使用得比较少。VP8是WebRTC默认的编码标准，且WebRTC使用VP8最多。同时，WebRTC也支持VP9和AV1。YouTube使用了VP9和AV1。Netflix也使用了AV1。我们下面来比较一下H264、H265、AV1这三代编码算法的区别。现在你不理解没有关系，等你学完后面的课程会很容易明白下面表格中所涉及的知识的。</p><p><img src="https://static001.geekbang.org/resource/image/b9/a9/b94991907d00bd54743cef34b70e01a9.jpg?wh=1085x467" alt=""><br>\n从上面表格中可以看到，标准越新，最大编码块就越大，块划分的方式也越多，编码模式也就越多。因此压缩效率也会越高，但是带来的编码耗时也越大。所以在选择编码器的时候需要根据自己的实际应用场景来选择，同时还需要考虑专利费的问题。还有一个就是考虑有没有硬件支持的问题。目前H264和H265的硬件支持已经很好了，AV1才刚开始，硬件支持较少，之后会有更多硬编硬件支持。</p><p>我做了个简单的编码清晰度和耗时对比，都是在软件编码下进行的。具体如下表所示。我们可以看到相同码率下，AV1清晰度稍好于H265，而H264最差，但是编码耗时则相反，AV1最高，H265次之，H264速度最快。</p><p><img src="https://static001.geekbang.org/resource/image/c4/22/c48b488cfbd982114a7742f9618c1322.jpg?wh=853x186" alt=""></p><p>所以，如果是在性能比较差的机器上编码，最好使用H264和VP8等速度快的编码器。如果是在比较新的机器上，可以考虑H265编码。中等机器如果支持H265硬编也是可以考虑的。但有一个问题就是H265需要考虑专利费的问题，同时浏览器原生不支持H265编码，所以有这方面需求的，最好不要使用H265，可以考虑使用VP9，甚至可以考虑AV1。另外，由于AV1原生标准就支持屏幕编码的优化，所以屏幕编码场景下可以考虑使用AV1编码。</p><h2>小结</h2><p>总结一下，我们今天主要讲了视频编码的必要性，以及视频编码的原理。<strong>视频编码主要分为熵编码、预测、DCT变换和量化这几个步骤。</strong></p><ol>\n<li>熵编码（以行程编码为例）：视频编码中真正实现“压缩”的步骤，主要去除信息熵冗余。在出现连续多个0像素的时候压缩率会更高。</li>\n<li>帧内预测：为了提高熵编码的压缩率，先将当前编码块的相邻块像素经过帧内预测算法得到帧内预测块，再用当前编码块减去帧内预测块得到残差块，从而去掉空间冗余。</li>\n<li>帧间预测：类似于帧内预测，在已经编码完成的帧中，先通过运动搜索得到帧间预测块，再与编码块相减得到残差块，从而去除时间冗余。</li>\n<li>DCT变换和量化：将残差块变换到频域，分离高频和低频信息。由于高频信息数量多但大小相对较小，又人眼对高频信息相对不敏感，我们利用这个特点，使用QStep对DCT系数进行量化，将大部分高频信息量化为0，达到去除视觉冗余的目的。</li>\n</ol><p>这里你需要注意的是，视频编码实际的步骤是预测、DCT变换和量化，最后是熵编码。经过这几步操作之后，视频中的冗余信息大部分被去除，达到了编码压缩的效果。当然，如何做帧内预测和帧间预测？如何找到最优的预测块？DCT变换和量化又是怎么做的呢？敬请期待我们接下来的课程，我会和你细聊。</p><h2>思考题</h2><p>现在请你思考一下，视频编码过程中，一帧图像能同时进行帧内预测和帧间预测吗？</p><p>你可以把你的答案和感受写下来，分享到留言区，与我一起讨论。下节课再见。</p>',
        article_title: "04｜编码原理：视频究竟是怎么编码压缩的？",
      },
      {
        title: "05｜码流结构：原来你是这样的H264",
        id: 461658,
        content:
          '<p>你好，我是李江。</p><p>上一节课我们一起讨论了视频编码的基本原理。今天，我们就接着来聊聊视频编码的码流结构，这在视频开发工作中是非常重要的。</p><p>视频编码标准其实有很多，比如上一节课讲到的H264、H265、AV1等，但原理大同小异，都是预测、变换、量化和熵编码等几个步骤。H264编码可以说是最常用的编码标准，比较经典，所以这节课我们就以H264为例来讲解码流结构。在掌握了这些之后，迁移学习其它编码标准的码流结构也就简单多了。</p><p>视频编码的码流结构其实就是指视频经过编码之后得到的二进制数据是怎么组织的，换句话说，就是编码后的码流我们怎么将一帧帧编码后的图像数据分离出来，以及在二进制码流数据中，哪一块数据是一帧图像，哪一块数据是另外一帧图像。</p><p>而我们在工程开发中，需要对编码后的数据进行一些解析，以便用于之后的打包。同时我们在打包时也需要判断当前一帧图像数据它的开头和结尾在哪。这些工作的前提就是我们要清楚如何分析编码码流，那么码流结构到底是怎样的，就是当下的学习重点了。</p><p>下面我们就以H264编码为基础，分析一下它的码流结构，并看看它在工程中是如何应用的。</p><h2>H264的编码结构</h2><p>这里有一些前置知识我们需要先了解一下。我们先一起来看几个重要的概念吧。它们之间有这样一条线索，你在接下来的学习中可以重点关注一下，对于你记忆它们也是非常有帮助的。</p><!-- [[[read_end]]] --><p>首先，清楚帧类型是图像的基础；其次，GOP是以其中的IDR帧作为分隔点的；最后的Slice是我们深入帧内部以后的一个重要概念。整个过程，由浅入深。</p><h3>帧类型</h3><p>帧类型相信你在平时的工作中可能已经接触过一部分了，比如说我们可能经常听到视频开发工作者说I帧、P帧之类的。其实在H264中，帧类型主要分为3大类，分别是I帧、P帧和B帧。那么它们之间有什么区别呢？接下来我们就来详细聊聊。</p><p>在<a href="https://time.geekbang.org/column/article/459554">视频编码原理</a>那节课里面，我们讲过为了减少空间冗余和时间冗余，视频编码使用了帧内预测和帧间预测技术，这些都涉及到帧。所以了解帧的类型是很有必要的。</p><p>我们知道帧内预测不需要参考已编码帧，对已编码帧是没有依赖的，并可以自行完成编码和解码。而帧间预测是需要参考已编码帧的，并对已编码帧具有依赖性。帧间预测需要参考已经编码好了的帧内编码帧或者帧间编码帧。并且，帧间编码帧又可以分为只参考前面帧的前向编码帧，和既可以参考前面帧又可以参考后面帧的双向编码帧。</p><p>为了做区分，在H264中，我们就将图像分为以下不同类型的帧。<br>\n<img src="https://static001.geekbang.org/resource/image/6b/8d/6b908464d87e30bf977893ababf7e78d.jpg?wh=1280x392" alt=""><br>\n三种帧的示例图如下所示。例如，从左向右，第一个B帧参考第一个I帧和第一个P帧，第一个P帧只参考第一个I帧（箭头是从参考帧指向编码帧）。</p><p><img src="https://static001.geekbang.org/resource/image/ab/4c/ab75b04921d925f567a92796c992e54c.jpeg?wh=1920x376" alt="图片"></p><p>由于P帧和B帧需要参考其它帧。如果编码或者解码的过程中有一个参考帧出现错误的话，那依赖它的P帧和B帧肯定也会出现错误，而这些有问题的P帧（B帧虽然也可以用来作为参考帧，但是一般用的比较少，所以这里不讨论）又会继续作为之后P帧或B帧的参考帧。因此，<strong>错误会不断的传递</strong>。为了避免错误的不断传递，就有了<strong>一种特殊的I帧叫IDR帧</strong>，也叫立即刷新帧。</p><p>H264编码标准中规定，<strong>IDR帧之后的帧不能再参考IDR帧之前的帧</strong>。这样，如果某一帧编码错误，之后的帧参考了这个错误帧，则也会出错。此时编码一个IDR帧，由于它不参考其它帧，所以只要它自己编码是正确的就不会有问题。之前有错误的帧也不会再被用作参考帧，<strong>这样就截断了编码错误的传递</strong>，且之后的帧就可以正常编/解码了。</p><p>当然，有IDR这种特殊的I帧，也就有普通的I帧。普通的I帧就是指当前帧只使用帧内预测编码，但是后面的P帧和B帧还是可以参考普通I帧之前的帧。但是这里我要说明一下，一般来说我们不太会使用这种普通I帧，<strong>大多数情况下还是直接使用IDR帧</strong>，尤其是在流媒体场景，比如RTC场景。只是说如果你非要用这种普通I帧，标准也是支持的。</p><h3>GOP</h3><p>在H264中，还有一个GOP的概念也经常会遇到，它是什么意思呢？从一个IDR帧开始到下一个IDR帧的前一帧为止，这里面包含的IDR帧、普通I帧、P帧和B帧，我们称为一个<strong>GOP（图像组）</strong>（这是closed GOP，还有一种opened GOP，比较少见，这里不讨论）。</p><p>我们可以看到GOP的大小是由IDR帧之间的间隔来确定的，而这个间隔我们有一个重要的概念来表示，叫做<strong>关键帧间隔</strong>。关键帧间隔越大，两个IDR相隔就会越远，GOP也就越大；关键帧间隔越小，IDR相隔也就越近，GOP就越小。</p><p><img src="https://static001.geekbang.org/resource/image/3d/df/3deac858fff85f0bf3d9c930e6776cdf.jpeg?wh=1920x416" alt="图片"></p><p>GOP越大，编码的I帧就会越少。相比而言，P帧、B帧的压缩率更高，因此整个视频的编码效率就会越高。但是GOP太大，也会导致IDR帧距离太大，点播场景时进行视频的seek操作就会不方便。</p><p>并且，在RTC和直播场景中，可能会因为网络原因导致丢包而引起接收端的丢帧，大的GOP最终可能导致参考帧丢失而出现解码错误，从而引起长时间花屏和卡顿。这一块我们会在之后用单独的一节课来详细讲述。总之，<strong>GOP不是越大越好，也不是越小越好，需要根据实际的场景来选择。</strong></p><p>前面我们讲的是视频图像序列的层次结构，那图像内的层次结构是怎样的呢？</p><h3>Slice</h3><p>这就不得不提到另一个概念了，Slice，也叫做“片”。<strong>Slice其实是为了并行编码设计的</strong>。什么意思呢？就是说，我们可以将一帧图像划分成几个Slice，并且Slice之间相互独立、互不依赖、独立编码。</p><p>那么在机器性能比较高的情况下，我们就可以多线程并行对多个Slice进行编码，从而提升速度。但也因为一帧内的几个Slice是相互独立的，所以如果帧内预测的话，就不能跨Slice进行，因此编码性能会差一些。</p><p>而在H264中编码的基本单元是宏块，所以一个Slice又包含整数个宏块。我们在前一节课中也讲了，宏块MB大小是16 x 16。在做帧内和帧间预测的时候，我们又可以将宏块继续划分成不同大小的子块，用来给复杂区域做精细化编码。</p><p>总结来说，<strong>图像内的层次结构就是一帧图像可以划分成一个或多个Slice，<strong><strong>而</strong></strong>一个Slice包含多个宏块，<strong><strong>且</strong></strong>一个宏块又可以划分成多个不同尺寸的子块</strong>。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/63/16/63f316bf5d1410cdb38334ba7bc9f316.jpg?wh=1280x720" alt=""><br>\n好了，上面都是从概念上来讨论视频编码中的视频序列和图像的层次结构。那有了这些知识之后，接下来我们更进一步，从H264码流的角度来看看这些层次结构具体在二进制码流中是怎样的。</p><h2>H264的码流结构</h2><p>下面我们就以“剥洋葱”的方式来详细地讲解H264的码流结构。先从最外层的码流格式讲起，教你怎么判断视频编码数据的起始；然后再介绍里面的NALU（网络抽象层单元）数据，看看通过它是怎么区分不同的帧类型的；再详细聊聊NALU有几种类型，以及通过什么方式来区分NALU的类型。</p><h3>码流格式</h3><p><strong>H264码流有两种格式</strong>：<strong><strong>一种是Annexb格式</strong></strong>；<strong>一种是MP4格式。</strong>两种格式的区别是：</p><ol>\n<li>\n<p>Annexb格式使用<strong>起始码</strong>来表示一个编码数据的开始。起始码本身不是图像编码的内容，只是用来分隔用的。起始码有两种，一种是4字节的“00 00 00 01”，一种是3字节的“00 00 01”。<br>\n这里需要<strong>注意</strong>一下，由于图像编码出来的数据中也有可能出现“00 00 00 01”和“00 00 01”的数据。那这种情况怎么办呢？为了防止出现这种情况，H264会将图像编码数据中的下面的几种字节串做如下处理：</p>\n<p>（1）“00 00 00”修改为“00 00 03 00”；</p>\n<p>（2）“00 00 01”修改为“00 00 03 01”；</p>\n<p>（3）“00 00 02”修改为“00 00 03 02”；</p>\n<p>（4）“00 00 03”修改为“00 00 03 03”。</p>\n</li>\n</ol><p>同样地在解码端，我们在去掉起始码之后，也需要将对应的字节串转换回来。</p><p><img src="https://static001.geekbang.org/resource/image/8f/b2/8f7b9f988aa439d8ae584eb4561ed1b2.jpeg?wh=1920x311" alt="图片"></p><ol start="2">\n<li>MP4格式没有起始码，而是<strong>在图像编码数据的开始使用了4个字节作为长度标识</strong>，用来表示编码数据的长度，这样我们每次读取4个字节，计算出编码数据长度，然后取出编码数据，再继续读取4个字节得到长度，一直继续下去就可以取出所有的编码数据了。<br>\n<img src="https://static001.geekbang.org/resource/image/94/6d/945b7e9c21e313c1cfcd8ecbe967576d.jpeg?wh=1920x250" alt="图片"></li>\n</ol><p>这两种格式差别不大，接下来我们主要使用Annexb格式来讲解H264码流中的NALU。</p><p>下面，我们剥开“洋葱”的最外层，将起始码去掉，进入“洋葱”的内部，也就是编码数据。这个编码数据就是H264码流的重要部分——NALU。</p><h3>NALU</h3><p>在这节课的开始我们讲了图像分成I帧、P帧和B帧这三种类型的帧。其实除了图像数据，视频编码的时候还有一些编码参数数据，为了能够将一些通用的编码参数提取出来，不在图像编码数据中重复，H264设计了<strong>两个重要的参数集：一个是SPS（序列参数集）；一个是PPS（图像参数集）</strong>。</p><p>其中，SPS主要包含的是图像的宽、高、YUV格式和位深等基本信息；PPS则主要包含熵编码类型、基础QP和最大参考帧数量等基本编码信息。如果没有SPS、PPS里面的基础信息，之后的I帧、P帧、B帧就都没办法进行解码。因此SPS和PPS是至关重要的。</p><p>结合前面我们讲的内容，我们现在可以知道，H264码流主要包含了SPS、PPS、I帧、P帧和B帧。由于帧又可以划分成一个或多个Slice。因此，帧在码流中实际上是以Slice的形式呈现的。所以，<strong>H264的码流主要是由</strong> <strong>SPS、PPS、I Slice、P Slice<strong><strong>和</strong></strong>B Slice组成的</strong>。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/14/d4/145b789d7fd80e415c8c5d7d6e6fe1d4.jpeg?wh=1920x290" alt="图片"></p><p>我们知道了H264码流主要由SPS、PPS和三种Slice组成，那我们<strong>如何在码流中区分这几种数据呢？</strong></p><p>为了解决这个问题，H264设计了NALU（网络抽象层单元）。SPS是一个NALU、PPS是一个NALU、每一个Slice也是一个NALU。每一个NALU又都是由一个1字节的NALU Header和若干字节的NALU Data组成的。而对于每一个Slice NALU，其NALU Data又是由Slice Header和Slice Data组成，并且Slice Data又是由一个个MB Data组成。其结构如下：</p><p><img src="https://static001.geekbang.org/resource/image/df/c0/df6fdacccd55c66d8495cc7c113489c0.jpg?wh=1280x720" alt=""><br>\n在这里，我们重点介绍一下NALU Header。它总共占用1个字节，具体如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/76/26/761aa34e76ed9b449a75c6b3bd226126.jpeg?wh=1586x522" alt="图片"><br>\n其中，</p><ul>\n<li>F：forbidden_zero_bit，占1bit，禁止位，H264码流必须为0；</li>\n<li>NRI： nal_ref_idc，占2bits，可以取00～11，表示当前NALU的重要性。参考帧、SPS和PPS对应的NALU必须要大于0；</li>\n<li>Type： nal_unit_type，占5bits，表示NALU类型。其取值如下表所示。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/86/a7/86c7dbba135911a984224d8b886c2fa7.png?wh=1880x267" alt=""><br>\n有了NALU Type类型表格，那我们解析出NALU Header的Type字段，查询表格就可以得到哪个NALU是SPS，哪个是PPS，以及哪个是IDR帧了。</p><p>这里需要注意一下，<strong>NALU类型只区分了IDR Slice和非IDR Slice</strong>，至于非IDR Slice是普通I Slice、P Slice还是B Slice，则需要继续解析Slice Header中的Slice Type字段得到。我们通过下面两个例子来看看常见的NALU里的NALU Header是怎样的。</p><p><img src="https://static001.geekbang.org/resource/image/3d/a8/3d017ba4005cd602881611772e5203a8.png?wh=1920x608" alt="图片"></p><p>下面我们再来看一个实际码流的例子，看看在实际编码出来的二进制数据中，各种NALU是怎么“放置”在数据中的。下图是我用二进制查看工具打开实际编码后的码流数据。我们可以看到在码流的开始部分是一个起始码，之后紧接着是一个SPS的NALU。在SPS后面是一个PPS的NALU。然后就是一个IDR Slice的NALU和一个非IDR Slice NALU。</p><p><img src="https://static001.geekbang.org/resource/image/77/bb/775e22095450797346e3434f5abdfcbb.png?wh=1220x1256" alt="图片"></p><p>现在，对于码流结构的认知你是不是很清晰了。你也可以去找个H264码流，用二进制查看工具打开它。通过今天学习的知识，你可以试着找出其中的起始码，看看能不能找到SPS、PPS、IDR和非IDR Slice。如果你都能找出来，那恭喜你说明你已经掌握了今天的主要知识点了。</p><h2>常见工程问题</h2><p>好了，在了解了基本的码流结构知识之后，我们来看看如何运用这节课学到的知识去解决工程上常见的一些问题。这里我列举了3个比较典型的问题，如果你有更多问题的话，可以到留言区我们一起讨论。</p><h3>多Slice时如何判断哪几个Slice是同一帧的？</h3><p>我们前面讲过，在H264码流中，帧是以Slice的方式呈现的，或者可以说在H264码流里是没有“帧“这种数据的，只有Slice。但是有个问题是，一帧有几个Slice是不会告诉你的。也就是说码流中没有字段表示一帧包含几个Slice。既然没有办法知道一帧有几个Slice，那我们如何知道多Slice编码时一帧的开始和结束分别对应哪个Slice呢？</p><p><img src="https://static001.geekbang.org/resource/image/ae/64/ae932f9ae0f2de95e1943d98c5a35b64.jpeg?wh=1920x418" alt="图片"></p><p>其实，Slice NALU由NALU Header和NALU Data组成，其中NALU Data里面就是Slice数据，而Slice数据又是由Slice Header和Slice Data组成。<strong>在Slice Header开始的地方有一个first_mb_in_slice的字段</strong>，表示当前Slice的第一个宏块MB在当前编码图像中的序号。我们只要解析出这个宏块的序号出来，</p><ul>\n<li>如果first_mb_in_slice的值等于0，就代表了当前Slice的第一个宏块是一帧的第一个宏块，也就是说当前Slice就是一帧的第一个Slice。</li>\n<li>如果first_mb_in_slice的值不等于0，就代表了当前Slice不是一帧的第一个Slice。<br>\n并且，使用同样的方式一直往下找，直到找到下一个first_mb_in_slice为0的Slice，就代表新的一帧的开始，那么其前一个Slice就是前一帧的最后一个Slice了。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/0d/86/0dbc23dc75c0811a0d83b06916c5aa86.jpeg?wh=1598x1252" alt="图片" title="图片来源于H264标准文档"></p><p>其中，first_mb_in_slice是以无符号指数哥伦布编码的，需要使用对应的解码方式才能解码出来。但是有一个小技巧，如果只是需要判断first_mb_in_slice是不是等于0，不需要计算出实际值的话，只需要通过下面的方式计算就可以了。</p><p><img src="https://static001.geekbang.org/resource/image/2c/69/2c6acyy46cbdf0829da609e281b93c69.png?wh=1470x226" alt="图片"></p><p>这就是多Slice判断一帧的开始和结束的方法。</p><h3>如何从SPS中获取图像的宽高？</h3><p>在编码端编码一个视频的时候，我们是需要设置分辨率告诉编码器图像的实际宽高的。但是解码器是不需要设置分辨率的，那我们在解码端或者说接收端如何知道视频的分辨率大小呢？</p><p>其实，在编码器编码的时候会将分辨率信息编码到SPS中。<strong>在SPS中有几个字段用来表示分辨率的大小。<strong>我们可以</strong>解码出这几个字段并通过一定的规则计算得到分辨率的大小</strong>。这几个字段分别是：</p><p><img src="https://static001.geekbang.org/resource/image/60/3d/603e4da34be2ab8bfc5d24eba83b693d.jpg?wh=1280x574" alt=""><br>\n这几个字段都是通过无符号指数哥伦布编码的，需要先解码出来。解码得到具体值之后，通过以下方法就可以得到分辨率了。注意，pic_height_in_map_units_minus1需要考虑帧编码和场编码的区别，其中场编码已经很少使用了，我们这里不再考虑。<br>\n<img src="https://static001.geekbang.org/resource/image/4c/be/4c9yy45d48234db0da222024653833be.jpg?wh=1280x720" alt=""><br>\n通过上面的方法就可以计算得到图像的分辨率了。</p><h3>如何计算得到QP值？</h3><p>我们在视频编码原理那节课中讲过，量化过程是引入失真最主要的环节。而量化最主要的参数就是QP值，并且QP值的大小严重影响到编码画面的清晰度。因此QP值非常重要。那么我们如何从码流中计算得到QP值呢？</p><p>在PPS中有一个全局基础QP，字段是pic_init_qp_minus26。当前序列中所有依赖该PPS的Slice共用这个基础QP，且每一个Slice在这个基础QP的基础上做调整。在Slice Header中有一个slice_qp_delta字段来描述这个调整偏移值。更进一步，H264允许在宏块级别对QP做更进一步的精细化调节。这个字段在宏块数据里面，叫做mb_qp_delta。</p><p><img src="https://static001.geekbang.org/resource/image/94/e2/9491235aba78da8693e77befabc0bbe2.jpg?wh=1280x388" alt=""><br>\n如果需要得到Slice级别的QP则只需要考虑前两个QP相关字段。如果需要计算宏块QP，则需要三个都考虑。但是宏块QP需要解析整个Slice数据，计算量大。一般我们直接计算到Slice QP就可以了。计算方法如下：</p><p><img src="https://static001.geekbang.org/resource/image/d5/3a/d5b6d75fd0567bca065ddf6631bce83a.png?wh=1328x344" alt="图片"></p><h2>小结</h2><p>这节课我们主要讨论了H264的编码层次结构和码流结构。在一个视频图像序列中，我们将其划分成一个个GOP。<strong>GOP包含一个IDR帧到下一个IDR帧的前一帧中的所有帧。</strong>GOP的大小选择需要根据实际应用场景来选择，<strong>一般RTC和直播场景可以稍微大一些，<strong><strong>而</strong></strong>点播场景一般小一些。</strong></p><p>在H264中，每一帧图像又可以分为I帧、P帧和B帧，而I帧又包含了普通I帧和IDR帧。帧可以划分为一个或者多个Slice，并且<strong>最后帧都是以Slice的方式在码流中呈现</strong>。同时H264码流中除了Slice数据之外，还有<strong>SPS和PPS两个参数集，<strong><strong>分别</strong></strong>用来存放基础图像信息和基础编码参数</strong>。SPS和PPS非常重要，如果丢失了，将无法进行解码。</p><p>每一个Slice和SPS、PPS都是通过NALU来封装的，且NALU含有一个1字节的NALU Header。我们可以<strong>通过NALU Header中的NALU Type来判断NALU的类型</strong>。同时，每一个NALU的分隔有两种方式：一种是<strong>Annexb格式，通过使用起始码分隔</strong>；一种是<strong>MP4格式，通过一个4字节的长度来表示NALU的大小</strong>，从而起到分隔的作用。</p><p>为了帮助你记忆，我们通过下图来总结一下。<br>\n<img src="https://static001.geekbang.org/resource/image/2e/f9/2edfa9c579e43efd2a76e94fa33f83f9.png?wh=1846x1434" alt=""></p><h2>思考题</h2><p>为什么有B帧的时候延时会高？</p><p>你可以把你的答案和思考写下来，分享到留言区，与我一起讨论。下节课再见。</p>',
        article_title: "05｜码流结构：原来你是这样的H264",
      },
      {
        title: "06｜帧内预测：如何减少空间冗余？",
        id: 462268,
        content:
          '<p>你好，我是李江。</p><p>前面几节课我们一起讨论了视频编码的原理以及编码码流结构，从今天开始我们一起来聊一聊具体的编码模式。</p><p>我们知道在视频编码时主要需要减少4个冗余，包括：空间冗余、时间冗余、视觉冗余和信息熵冗余。其中空间冗余就是通过帧内预测的方式来实现的。那帧内预测到底是怎么做到减少空间冗余的呢？</p><p>其实一般来说，<strong>一幅图像中相邻像素的亮度和色度信息是比较接近的，<strong><strong>并且</strong></strong>亮度和色度信息也是逐渐变化的，不太会出现突变</strong>。也就是说，<strong>图像具有空间相关性</strong>。帧内预测就是利用这个特点来进行的。即<strong>帧内预测通过利用已经编码的相邻像素的值来预测待编码的像素值，最后达到减少空间冗余的目的</strong>。</p><p>这里需要注意的一个点就是，我们是通过已经编码了的像素值去预测待编码的像素值。你可能会问，已经编码了的像素值变成码流了，不再是一个个像素了，怎么去预测待编码的像素呢？其实已经编码了的像素是会重建成重建像素，用来做之后待编码块的参考像素的。你可以认为是<strong>已经编码的块会解码成像素用来做参考像素</strong>。</p><p>好了，我们已经知道了帧内预测是怎么减少空间冗余的，那我们还有一个最重要的问题要解答。既然帧内预测是通过相邻像素来预测待编码像素的，那它到底是怎么预测的呢？下面我们就以H264标准来讲解每种帧内预测模式是怎么做预测的。</p><!-- [[[read_end]]] --><h2>不同块大小的帧内预测模式</h2><p>我们在视频编码原理的那节课里面讲过，视频编码是以块为单位进行的。<strong>在H264标准里面，块分为宏块和子块。宏块的大小是16 x 16（YUV 4:2:0图像亮度块为16 x 16，色度<strong><strong>块</strong></strong>为8 x 8）</strong>。在帧内预测中，亮度宏块可以继续划分成16个4 x 4的子块。因为图像中有的地方细节很多，我们需要划分成更小的块来做预测会更精细，所以会将宏块再划分成4 x 4的子块。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/5c/5b/5c0a24c3d52e8yy2de20fb07b58c625b.jpg?wh=1280x446" alt=""></p><p>帧内预测是根据块的大小分为不同的预测模式的。还有一个点就是亮度块和色度块的预测是分开进行的。这里我给你总结一下，主要有以下3点规则：</p><ol>\n<li>\n<p><strong>宏块大小是16 x 16，其中亮度块为16 x 16，色度块为8 x 8</strong>；</p>\n</li>\n<li>\n<p><strong>帧内预测中亮度块和色度块是分开独立进行预测的</strong>，即亮度块参考已编码亮度块的像素，而色度块参考已编码色度块的像素；</p>\n</li>\n<li>\n<p><strong>16 x 16的亮度块可以继续划分成16个4 x 4的子块</strong>。</p>\n</li>\n</ol><p>所以，我们在实际帧内预测的时候就会分为：4 x 4亮度块的预测、16 x 16亮度块的预测、8 x 8色度块的预测（注意亮度8 x 8模式和I_PCM模式很少使用，我们这里不做讨论）。下面我们就开始一个个展开来讨论吧。</p><h3>4 x 4亮度块的帧内预测模式</h3><p>因为4 x 4的块帧内预测模式最多，并且基本包含亮度16 x 16和色度8 x 8的模式，所以我们接下来先从4 x 4亮度块的帧内预测模式讲起。</p><p>4 x 4亮度块的帧内预测模式总共有9个。其中有<strong>8种方向模式和一种DC模式</strong>，且方向模式指的是预测是有方向角度的。好了，那我们接下来就聊聊这9种模式吧。</p><ol>\n<li>Vertical模式</li>\n</ol><p><strong>Vertical模式就是指，当前编码亮度块的每一列的像素值，都是复制上边已经编码块的最下面那一行的对应位置的像素值。</strong>这句有点拗口，别急，等会儿我会给出计算方法和示意图，让你能很容易地理解它。</p><p><strong>Vertical模式得到的预测块同一列中的像素值都是一样的</strong>。该模式得到的块就叫做Vertical预测块。注意，<strong>该模式只有在上边块存在的时候才可用，如果不存在则该模式不可用</strong>。比如图像最上边的块就没有可参考的块存在。</p><p>该模式下预测块像素计算方法如下：</p><p><img src="https://static001.geekbang.org/resource/image/03/e7/03b05d50574c17ed523f9164929019e7.jpg?wh=1280x362" alt=""></p><p>示意图如下：</p><p><img src="https://static001.geekbang.org/resource/image/c4/d8/c4yy0c94ba713d778fd81917238863d8.png?wh=1160x762" alt="图片"></p><ol start="2">\n<li>Horizontal模式</li>\n</ol><p><strong>Horizontal模式就是指，当前编码亮度块的每一行的像素值，都是复制左边已经编码块的最右边那一列的对应位置的像素值</strong>（同样地，下面有计算方法和示意图）。<strong>Horizontal模式得到的预测块同一行的像素值都是一样的</strong>，该模式得到的块就叫做Horizontal预测块。注意，<strong>该模式只有在左边块存在的时候才可用，如果不存在则该模式不可用</strong>。比如图像最左边的块就没有可参考的块存在。</p><p>该模式下预测块像素计算方法如下：</p><p><img src="https://static001.geekbang.org/resource/image/81/91/8185bc4a93669c2a800f5cf0743f7091.jpg?wh=1280x342" alt=""><br>\n示意图如下：<br>\n<img src="https://static001.geekbang.org/resource/image/a1/f5/a15b484807812294b00f1aebcbc942f5.png?wh=1154x750" alt="图片"></p><ol start="3">\n<li>DC模式</li>\n</ol><p><strong>DC模式就是指，当前编码亮度块的每一个像素值，是上边已经编码块的最下面那一行和左边已编码块右边最后一列的所有像素值的平均值</strong>。注意，<strong>DC模式预测得到的块中每一个像素值都是一样的</strong>。DC模式得到的块就叫做DC预测块。</p><p>根据上边块和左边块是不是存在，该模式下预测块像素的计算方法分为以下四种情况：</p><p><img src="https://static001.geekbang.org/resource/image/85/ff/85de3eafb25bff062e821f83f11236ff.jpg?wh=1280x720" alt=""><br>\n示意图如下：<br>\n<img src="https://static001.geekbang.org/resource/image/0y/6b/0yyb8202b3dd85233975ea3cffc13b6b.png?wh=1160x760" alt="图片"></p><ol start="4">\n<li>Diagonal Down-Left模式</li>\n</ol><p><strong>Diagonal Down-Left模式是上边块和右上块</strong>（上边块和右上块有可能是一个块，因为可能是一个16 x 16的亮度块，意思理解就可以）<strong>的像素通过插值得到</strong>。<strong>如果上边块和右上块不存在则该模式无效。</strong></p><p>该模式下预测块计算的方法分为下面两种情况：</p><p><img src="https://static001.geekbang.org/resource/image/a6/06/a6b8b26163ca9d9274377b5735785f06.jpg?wh=1280x412" alt=""><br>\n示意图如下：</p><p><img src="https://static001.geekbang.org/resource/image/11/a1/110b13f5d5af1d119b6f2a559e39f7a1.png?wh=1178x754" alt="图片"></p><ol start="5">\n<li>Diagonal Down-Right模式</li>\n</ol><p><strong>Diagonal Down-Right模式需要通过上边块、左边块和左上角对角的像素通过插值得到。如果这三个有一个不存在则该模式无效</strong>。</p><p>该模式下预测块计算的方法有以下三种情况：<br>\n<img src="https://static001.geekbang.org/resource/image/4f/b7/4f10404d6fc9e3625092d92090aed6b7.jpg?wh=1280x474" alt=""><br>\n示意图如下：<br>\n<img src="https://static001.geekbang.org/resource/image/fc/93/fcbdf7eaec95fea21158b4ce1047ed93.png?wh=1154x760" alt="图片"></p><ol start="6">\n<li>Vertical-Right模式</li>\n</ol><p><strong>Vertical-Right模式是需要通过上边块、左边块以及左上角对角的像素插值得到的</strong>。必须要这三个都有效才能使用，否则该模式无效。</p><p>该模式下预测块计算的方法有以下四种情况：</p><p><img src="https://static001.geekbang.org/resource/image/63/ce/63b391728df65ce7551012753ae331ce.jpg?wh=1280x720" alt=""><br>\n示意图如下：<br>\n<img src="https://static001.geekbang.org/resource/image/c5/bf/c5d488c6a80b9c27ba50048fb4ef4cbf.png?wh=1150x732" alt="图片"></p><ol start="7">\n<li>Horizontal-Down模式</li>\n</ol><p><strong>Horizontal-Down模式需要通过上边块、左边块以及左上角对角的像素插值得到。必须要这三个都有效才能使用，否则该模式无效。</strong></p><p>该模式下预测块计算的方法有以下四种情况：</p><p><img src="https://static001.geekbang.org/resource/image/12/40/120761b8313c18c7433f7708d8b4a140.jpg?wh=1280x720" alt=""><br>\n示意图如下：<br>\n<img src="https://static001.geekbang.org/resource/image/6a/bc/6a3ab22c2570589ac2680c561be0c5bc.png?wh=1172x756" alt="图片"></p><ol start="8">\n<li>Vertical-Left模式</li>\n</ol><p><strong>Vertical-Left模式是需要通过上边块和右上块</strong>（上边块和右上块有可能是一个块，因为可能是一个16 x 16的亮度块，意思理解就可以）<strong>最下面一行的像素通过插值得到</strong>。如果这两种块不存在则该模式不可用。该模式下预测块的计算方法有下面两种情况：</p><p><img src="https://static001.geekbang.org/resource/image/83/66/835d3ec11768f9928098ab2e54556466.jpg?wh=1280x442" alt=""><br>\n示意图如下：<br>\n<img src="https://static001.geekbang.org/resource/image/5e/5d/5e13e4207abd6788b189d9e86669665d.png?wh=1170x774" alt="图片"></p><ol start="9">\n<li>Horizontal-Up模式</li>\n</ol><p><strong>Horizontal-Up模式是需要通过左边块的像素通过插值得到的</strong>。如果左边块不存在，则该模式不可用。</p><p>该模式下预测块的计算方法有下面四种情况：</p><p><img src="https://static001.geekbang.org/resource/image/7b/59/7bbb469c4f050a098b0744ea3cc94e59.jpg?wh=1280x720" alt=""></p><p>示意图如下：<br>\n<img src="https://static001.geekbang.org/resource/image/53/f8/53ddf8a40698cb99e48f103206b149f8.png?wh=1156x764" alt="图片"></p><h3>16 x 16亮度块的帧内预测模式</h3><p>上面我们讲了4 x 4亮度块的9种预测模式，接下来我们来聊一下16 x 16亮度块的预测模式。</p><p><strong>16 x 16亮度块总共有<strong><strong>4</strong></strong>种预测模式。<strong><strong>它们</strong></strong>分别是<strong><strong>Vertical</strong></strong>模式，<strong><strong>Horizontal</strong></strong>模式、DC模式<strong><strong>和</strong></strong>Plane模式</strong>，前面三种模式跟4 x 4的原理是一样的这里就不重复讲解了。16 x 16亮度块的帧内预测的4种模式的示意图如下：</p><p><img src="https://static001.geekbang.org/resource/image/e3/e6/e303d85a11dced7978cedea604f001e6.png?wh=1920x872" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/b0/24/b09910c6bc61b97924a52dcd89c04824.png?wh=1920x885" alt="图片"></p><p>接下来我们介绍一下Plane模式。这种模式相比前面三种模式稍微复杂一些，但是基本原理都差不多。<strong>Plane预测块的每一个像素值，都是将上边已编码块的最下面那一行，和左边已编码块右边最后一列的像素值经过下面公式计算得到的</strong>。</p><p><img src="https://static001.geekbang.org/resource/image/4y/f2/4yyf0416de0dda660b69cb3b59b36ff2.jpeg?wh=2428x1076" alt=""></p><h3>8 x 8色度块的帧内预测模式</h3><p>上面我们讲述了亮度块的帧内预测模式，接下来我们简单介绍一下色度块的帧内预测模式。</p><p>8 x 8色度块的帧内预测模式跟16 x 16亮度块的是一样的，也是总共有4种，分别为DC模式、Vertical模式，Horizontal模式、Plane模式。与16 x 16亮度块不同的是，块大小不同，所以参考像素值数量会不同。但是基本是一致的，这里就不重复了。</p><h2>帧内预测模式的选择</h2><p>现在我们学习了这么多的模式，而每一个块却只能有一种帧内预测模式。那我们怎么确定一个块到底使用哪种模式呢？我们这边先把思路讲一讲，具体细节不展开。等到你把编码基础弄明白了之后可以阅读一下x264的代码，里面有关于具体如何去选择模式的方法。</p><p>以亮度块为例。一个16 x 16的亮度块，我们可以不划分，直接使用4种16 x 16的帧内预测模式，最多得到4种预测块；也可以划分成16个4 x 4的子块，每一个子块最多有9种帧内预测模式。</p><p>对于每一个块或者子块，我们可以得到预测块，再用实际待编码的块减去预测块就可以得到残差块。主要有下面3种方案来得到最优预测模式：</p><ul>\n<li>第一种方案，先对每一种预测模式的残差块的像素值求绝对值再求和，称之为cost，然后取其中残差块绝对值之和也就是cost最小的预测模式为最优预测模式。</li>\n<li>第二种方案，对残差块先进行Hadamard变换（在DCT变换和量化那节课中会介绍），变换到频域之后再求绝对值求和，同样称为cost，然后取cost最小的预测模式为最优预测模式。</li>\n<li>第三种方案，也可以对残差块直接进行DCT变换量化熵编码，计算得到失真大小和编码后的码流大小，然后通过<a href="https://www.jianshu.com/p/6f8be3709003?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation">率失真优化</a>（作为课外内容自行学习，这里不展开讨论）的方法来选择最优预测模式。<br>\n为了让你更容易理解第三种方案，这里我稍微简单介绍一下率失真优化的思想<strong>。</strong></li>\n</ul><p>我们知道预测之后经过DCT变换再量化会丢失高频信息。一般来说QP越大，丢失的信息越多，失真就越大，但是码流大小也越小；反之，QP越小，丢失的信息越少，但是码流大小就越大。这是一个跷跷板。<strong>我们一般会在失真和码流大小之间平衡，尽量找到在一定码率下，失真最小的模式作为最优的预测模式，这就是率失真优化的思想</strong>。</p><p>其实还有很多不同的方案，比如有的为了加速模式选择的过程，率失真计算的时候，只会进行DCT变换和量化，不会进行熵编码。码流大小直接通过QP值估算或者使用预测模式的大小来代替。这些方案都可以，具体看编码器的实现。一般来说，选择过程越精细效果越好，但是速度会越慢。</p><p>通过上面讲的这些方法我们找到了每一个4 x 4块的最优模式之后，将这16个4 x 4块的cost加起来，与16 x 16块的最小cost对比，选择cost最小的块划分方式和预测模式作为帧内预测模式。</p><h2>小结</h2><p>好了，今天的课到这里就要结束了。我们来回顾一下这节课的主要内容。</p><p>我们主要是一起聊了聊帧内预测。我们先详细讲述了有哪几种预测模式，以及各种预测模式的预测块的计算方法。同时强调一点，就是有的模式在参考像素不存在的时候是无效的，也就是不能使用的。这一点需要注意，并不是列举出来的每一个模式都是可以使用的。</p><p>为了方便你记忆，下面我把每一种块和子块支持的预测模式用一个表格来总结一下。</p><p><img src="https://static001.geekbang.org/resource/image/ee/2d/eea93f2ace612181dfe81e34dd5e532d.jpg?wh=1280x720" alt=""><br>\n最后，我们还简单介绍了一下预测模式的选择方法，主要有计算残差块绝对值之和、将残差块做Hadamard变换之后再求和、率失真优化等几种方案来得到cost，然后我们取cost最小的模式作为帧内预测模式。</p><h2>思考题</h2><p>在H264标准里面，视频的第一帧的第一个块应该怎么选择预测模式呢？</p><p>你可以把你的答案留在评论区，和我一起探讨、交流，看看自己的理解是不是正确。下节课再见。</p>',
        article_title: "06｜帧内预测：如何减少空间冗余？",
      },
      {
        title: "07｜帧间预测：如何减少时间冗余？",
        id: 463775,
        content:
          '<p>你好，我是李江。</p><p>上一节课我们一起讨论了帧内预测。帧内预测主要是通过当前编码块它的相邻的已经编码完成的像素预测得到预测块，从而达到减少空间冗余的目的。我们知道在视频编码时主要需要去除4个冗余，包括：空间冗余、时间冗余、视觉冗余和信息熵冗余。那么这节课我们接着之前的课程，继续讲讲如何减少时间冗余。</p><p>提醒一下，这节课难度比较高，涉及的知识点又很多，建议你多看几遍。我也会尽量简化过程，把基本的原理讲清楚。你在这里打好了基础，再去学习更高阶的内容就轻松多了。</p><p>我们已知，视频在1秒钟内有很多帧图像，其通过帧率来表示。一般来说帧率为24fps或者30fps，也就是指，1秒钟会有多达24帧或者30帧图像。</p><p>但是其实在自然状态下，人或者物体的运动速度在1秒钟之内引起的画面变化并不大，且自然运动是连续的。所以<strong>前后两帧图像往往变化比较小，这就是视频的时间相关性</strong>。帧间预测就是利用这个特点来进行的。<strong>通过在已经编码的帧里面找到一个块来预测待编码块的像素，<strong><strong>从而</strong></strong>达到减少时间冗余的目的。</strong></p><p>那么在正式讲解之前，还有三点我需要特别强调一下。</p><p>第一，在帧内预测中，我们是在当前编码的图像内寻找已编码块的像素作为参考像素计算预测块。而帧间预测是在其他已经编码的图像中去寻找参考像素块的。这正是<strong>帧内预测和帧间预测的区别</strong>。</p><!-- [[[read_end]]] --><p>第二，帧间预测是可以在多个已经编码的图像里面去寻找参考像素块的，我们称之为<strong>多参考</strong>。<strong>多参考和单参考（只在一帧图像里面寻找参考像素块）其实底层的原理是一样的</strong>，只是多参考需要多搜索几个参考图像去寻找参考块而已，所以<strong>我们讲解的时候就使用单参考讲解</strong><strong>。</strong>这样既可以简化过程，你也可以更容易掌握帧间编码的基本原理。</p><p>第三，帧间预测既可以参考前面的图像也可以参考后面的图像（如果参考后面的图像，后面的图像需要提前先编码，然后再编码当前图像）。<strong>只参考前面图像的帧我们称为前向参考帧，也叫P帧；参考后面的图像或者前面后面图像都参考的帧，我们称之为双向参考帧，也叫做B帧</strong>。B帧相比P帧主要是需要先编码后面的帧，并且B帧一个编码块可以有两个预测块，这两个预测块分别由两个参考帧预测得到，最后加权平均得到最终的预测块。<strong>P帧和B帧的底层逻辑基本是一样的</strong>。同样为了简化过程，我们这节课<strong>以P帧为例子<strong><strong>来</strong></strong>讲解底层逻辑</strong>。</p><h2>帧间编码</h2><p>好了，接下来我们就以H264标准为基础来聊聊P帧的帧间编码过程吧。就从最基础的块大小开始讲起。</p><h3>块大小</h3><p>上节课，我们讲到了帧内预测有亮度16 x 16、亮度4 x 4和色度8 x 8这几种块。类似地，在帧间预测也一样有不同的块和子块大小。<strong>相比帧内预测，帧间预测的块划分类型要多很多</strong>。宏块大小16 x 16，可以划分为16 x 8，8 x 16， 8 x 8三种，其中8 x 8可以继续划分成8 x 4，4 x 8和4 x 4，这是亮度块的划分。在YUV 4:2:0中，色度块宽高大小都是亮度块的一半。亮度宏块的划分方式如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/00/6f/00f322905ded67f118f45a29ba2ae46f.jpeg?wh=1920x988" alt="图片"></p><h3>参考帧和运动矢量</h3><p>在帧间预测中，我们会<strong>在已经编码的帧里面找到一个块来作为预测块，这个已经编码的帧称之为参考帧</strong>。在H264标准中，P帧最多支持从16个参考帧中选出一个作为编码块的参考帧，但是同一个帧中的不同块可以选择不同的参考帧，这就是多参考。</p><p>通常在RTC场景中，比如<strong>WebRTC中，P帧中的所有块都参考同一个参考帧</strong>。并且<strong>一般会选择当前编码帧的前一帧来作为参考帧</strong>。为什么呢？</p><p>这是因为自然界的运动一般是连续的，同时在短时间之内的变化相对比较小，所以前面的帧通常是最接近当前编码帧的，并且两者的差距比较小。因此，我们比较容易从前一帧中找到一个跟当前编码块差距很小的块作为预测块，这样编码块减去预测块得到的残差块的像素值很多都是0，压缩效率是不是就很高了？</p><p>好了，选择好了参考帧之后，我们还有一个问题。虽然运动变化比较小，但是还是有变化啊，比如说下图中的场景。</p><p><img src="https://static001.geekbang.org/resource/image/22/fe/224110ddf9b98fffe27e5ccd3908f0fe.jpg?wh=1280x720" alt=""><br>\n图中的小车在往前开，树是不动的。我们可以看到车相对于树的距离是变化的。那我们怎么来表示这个变化呢？</p><p>或者从编码的角度来讲，右边图像中橙色块的内容跟左边图像中橙色块的内容基本一样。很明显，如果以左边图像为参考帧的话，那么左边图像中的橙色块就是最合适的预测块，但是左右两幅图像橙色块的位置很明显又不同，而这种不同是因为图像中的小车的运动引起的。因此，为了表示这种变化，我们<strong>用运动矢量来表示编码帧中编码块和参考帧中的预测块之间的位置的差值</strong>。</p><p>比如说上面两幅图像中，小车从前一幅图像中的（32，80）的坐标位置，变化到当前图像（80，80）的位置，向前行驶了48个像素。很明显，如果我们选用（32，80）这个块作为当前（80，80）这个编码块的预测块的话，是不是就可以得到全为0像素的残差块了？这是因为小车本身是没有变化的，变化的只是小车的位置。</p><p>这个位置变化我们怎么表示呢？我们用运动矢量来表示。我们称（32 - 80， 80 - 80）也就是（-48， 0）为运动矢量。我们先<strong>把运动矢量编码到码流当中</strong>，这样解码端只要解码出运动矢量，使用运动矢量就可以在参考帧中找到预测块了，我们再解码出残差（如果有的话），残差块加上预测块就可以恢复出图像块了。</p><p>好了，这就是参考帧和运动矢量。可是还有一个问题：因为我们通过人眼能够看到小车在两幅图像的位置，所以我们可以在参考帧中找到一个与当前编码块相似的块作为预测块，但是<strong>编码器怎么找到这个预测块呢</strong>？它们又没有眼睛。这就是运动搜索算法应该解决的问题啦。我们接下来就来聊聊运动搜索是怎么做的。</p><h3>运动搜索</h3><p>从前面的讨论我们知道，运动搜索的目标就是在参考帧中找到一个块，称之为预测块，且这个预测块与编码块的差距最小。从计算机的角度来说就是，编码块跟这个预测块的差值，也就是残差块的像素绝对值之和（下面我们用SAD表示残差块的像素绝对值之和）最小。</p><p>现在是不是目标就清晰很多了。比如说当前编码块大小是16 x 16，那我们就先去参考帧中找到一个个16 x 16的块作为预测块，并用当前编码块减去预测块求得残差块，然后用我们经常做的绝对值求和操作得到两者之间的差距，最后选择差距最小的预测块作为最终的预测块。</p><p>所以，我们运动搜索的方法就很简单了，就是从参考帧中第一个像素开始，将一个个16 x 16大小的块都遍历一遍。我们总是可以找到差距最小的块。这种方法我们称之为全搜索算法。</p><p><strong>全搜索算法一定可以搜索到最相似的预测块</strong>。但是你有没有发现这种方法有一个特别大的<strong>缺点就是需要逐个像素去遍历每一个块，非常费时间</strong>。由于帧间预测中每一个16 x 16的宏块还可以划分成上面讲的多种不同的子块大小，每一个子块也需要做一遍运动搜索。如果采用这种运动搜索算法的话，那编码一帧的时间将会非常长。</p><p>那有没有速度快一点的搜索算法呢？答案肯定是有的。下面我就来介绍一下常用的两种快速运动搜索算法。</p><p>在讲述快速算法之前我先说明一下，<strong>搜索算法中每一个搜索的点都是搜索块的左上角像素点</strong>。比如说菱形和六边形搜索的几个点都指的是以该点为左上角像素点的块。如果所搜块大小为16x16，则指的是以该点为左上角像素点的16x16的块。</p><ol>\n<li>钻石搜索算法<br>\n<strong>钻石搜索算法又称为<strong><strong>菱形</strong></strong>搜索算法</strong>。顾名思义，就是<strong>以一个菱形的模式去寻找最优预测块</strong>。这里我们以亮度16 x 16的块的运动搜索为例，将算法的思想分为以下几个步骤：</li>\n</ol><p>（1）从搜索的起始点开始，以起始点作为菱形的中心点。首先以该中心点为左上角像素的16 x 16的块作为预测块，求得残差块并求得像素绝对值之和，也就是SAD。之后对菱形4个角的4个点分别做同样的操作求得SAD值。得到最小的SAD值，最小SAD值对应的点就是当前最佳匹配点。</p><p>（2）如果最佳匹配点是菱形的中心点，那我们就找到了预测块了，搜索结束。</p><p>（3）如果最佳匹配点不是菱形的中心点，则用以当前最佳匹配点为中心点的菱形继续搜索，重复之前的步骤直到菱形的中心点为最佳匹配点。</p><p><img src="https://static001.geekbang.org/resource/image/b5/e6/b5c9c40bd4c595367b02yy0e391b19e6.jpg?wh=1280x720" alt=""><br>\n例如，上图中：</p><p>第一步，以绿色点（起点）为中心点，搜索绿色点和旁边蓝色线连接的4个点，得到的最佳匹配点为橙色点，非中心点。</p><p>第二步，再以橙色点为中心点，搜索橙色点和旁边黄色线连接的4个点，最佳匹配点是中心点橙色点，搜索完毕，橙色点为最佳匹配点。</p><ol start="2">\n<li>六边形搜索算法<br>\n六边形搜索跟钻石搜索差不多，只是搜索模式是六边形的。我们还是以亮度16 x 16的块为例来介绍一下，主要有下面几个步骤：</li>\n</ol><p>（1）从搜索的起始点开始，以起始点作为六边形的中心点。求得中心点作为左上角像素的预测块的SAD值。之后对六边形的角上的6个点做同样的操作求得SAD值。得到最小的SAD值，而最小SAD值对应的点就是当前最佳匹配点。</p><p>（2）如果最佳匹配点是六边形的中心点，那我们就用以该点为中心点的菱形和正方形各进行一次精细化搜索。找到中心点、菱形的4个顶点和正方形4个顶点中SAD最小的点作为最佳匹配点。</p><p>（3）如果最佳匹配点不是六边形的中心点，则用以当前最佳匹配点为中心点的六边形继续搜索，重复之前的步骤直到中心点为最佳匹配点。</p><p><img src="https://static001.geekbang.org/resource/image/14/08/14bd07d243052c8946d7b0424cd11f08.jpg?wh=1280x720" alt=""><br>\n例如，上图中：</p><p>第一步，以绿色点（起点）为中心点，搜索中心点和旁边蓝色线连接的6个点，得到的最佳匹配点为橙色点，非中心点。</p><p>第二步，再以橙色点为中心点，搜索橙色点和旁边黄色线连接的6个点，最佳匹配点是是中心点橙色点。</p><p>第三步，再以橙色点为中心点，搜索橙色点和旁边蓝色线连接菱形的4个点，最佳匹配点为黑色点。</p><p>第四步，还是以橙色点为中心点，搜索旁边红色线连接的正方形的4个点，并与菱形搜索得到的最佳匹配点黑色点比较，找到最后的最佳匹配点为红色点，搜索完毕。</p><p>通过上面的快速搜索算法我们就能够得到编码块在参考帧中的最佳匹配点，<strong>以最佳匹配点为左上角像素的块就是预测块</strong>，并且<strong>预测块左上角像素在参考帧中的坐标(x1, y1)与编码块在当前编码帧中的坐标(x0, y0)的差值（x1 - x0, y1 - y0）就是运动矢量</strong>。</p><p>上面两种运动搜索算法都是以搜索形状的中心点为最佳匹配点结束的。但是还有一个问题我们没有解释，那就是搜索的起始点怎么确定呢？</p><p>其实搜索的起始点可以使用当前编码块的左边块、右边块、左上角块和右上角块的运动矢量预测得到。具体预测方法我们会在下面的运动矢量预测小节里面讲解。其总体的思路就是我们认为，一般一个块最大也就16 x 16的大小，而运动的物体一般远大于这个大小，所以相邻块的运动方向大多数是很相似的。因此，<strong>我们一般会通过相邻已经编码块的运动矢量来预测当前块的运动矢量。这个预测的运动矢量也经常用做搜索的起点。</strong></p><p>有了快速运动搜索算法我们就不需要遍历整个参考帧的像素去寻找预测块了，这样速度可以快很多。但是必须要说明一下，就是快速搜索算法也有一个缺点，<strong>它搜索到的预测块不一定是全局最优预测块</strong>，也就是说不一定是最相似的块，有可能是局部最优预测块。</p><p>但是实验数据表明，快速搜索算法相比全搜索算法压缩性能下降非常小，速度却可以提升十几倍到几十倍。所以总的来说，我们可以认为<strong>快速搜索算法是远好于全搜索算法的</strong>，并且一般全搜索算法是不会实际使用的。</p><p>好了，经过上面的讨论，我们已经知道了运动搜索的具体思路了。但是我们还有一个疑问：如果一个物体运动了，比如小车向前行驶了48个像素点，那我们就可以通过运动矢量（-48，0）在参考帧中找到小车。我们前面也说了，小车的运动是连续的，<strong>如果小车向前行驶了48.5个像素点呢？又或者是向前行驶了48.25个像素点呢？</strong>运动矢量选择（-48.5， 0）或者（-48.25，0）吗？可是0.5个像素点是什么样的，0.25个像素点又是什么样的？图像上都没有这种像素点啊，怎么办呢？</p><p>其实没关系的，我们还是可以使用（-48，0）作为运动矢量，只是预测块中的小车位置与我们编码块中的小车位置会相差个0.5或者0.25个像素，得到的残差会大一些，压缩效率稍微低一些，问题也不大。</p><p><img src="https://static001.geekbang.org/resource/image/60/d6/60cf4e9be92eab7f1a8a70c4a4d689d6.jpg?wh=1280x720" alt=""></p><p>比如上面图中的小车，如果直接用编码块减去参考块的话，因为两个块中小车的位置没有完全重合，所以残差块不等于0。这样还是有残差的，因此压缩效率会低一些。</p><p>但是勤劳智慧的人类是不会停止前进的脚步的。为了能够解决这种半个像素或者1/4个像素的运动带来的<strong>压缩效率下降</strong>的问题，我们通过对参考帧进行半像素和1/4像素插值（统称为亚像素插值）的方式来解决。</p><p>什么意思呢？就是<strong>我们用插值的方式将半像素和1/4像素算出来，也当作一个像素</strong>，这样小车向前行驶48.5个像素也好，向前行驶48.25个像素也好，都是可以通过运动矢量找到比较准确的位置的。那亚像素插值具体怎么做呢？我们接下来看一下。</p><h3>亚像素插值</h3><p>亚像素插值归根到底还是插值操作，我们在第3节课里面已经讨论过了一些插值算法。亚像素插值的思想跟前面课里的插值算法的思想是一样的，都是通过已经有的像素点经过一定的加权计算得到需要求得的像素。</p><p>在这里，已经有的像素就是整像素（就是图像本身有的像素称为整像素），需要插值求得的就是半像素和1/4像素。其中半像素通过整像素插值得到，1/4像素又是通过整像素和半像素插值得到的。因此，<strong>我们先通过整像素插值得到半像素，然后<strong><strong>再</strong></strong>通过半像素和整像素插值得到1/4像素</strong>。半像素的插值过程可以通过下面的图示表示：</p><p><img src="https://static001.geekbang.org/resource/image/d5/37/d58f1c91afd0c9d5b5c0f4cae2489637.jpg?wh=1280x720" alt=""><br>\n其中，灰色为整像素点，橙色为水平半像素，黄色为垂直半像素点，绿色为中心半像素点。</p><p>半像素点的插值是以6个整像素点使用六抽头插值滤波器计算得到的，滤波器权重系数为：(1/32, -5/32, 5/8, 5/8, -5/32, 1/32)。具体计算方法如下：</p><p><img src="https://static001.geekbang.org/resource/image/6b/ee/6b23277ec4f2cdca4282178618b7deee.jpg?wh=1280x720" alt=""><br>\n得到了半像素之后，1/4像素就比较简单，由整像素和半像素求平均值得到，其插值过程可以通过下图表示：</p><p><img src="https://static001.geekbang.org/resource/image/5e/6b/5eaf40c83f2653f6879ea691bebe766b.jpg?wh=1280x470" alt=""></p><p>其中，红色点为1/4像素点，具体计算方法如下：</p><p><img src="https://static001.geekbang.org/resource/image/2b/0c/2bd2c2e7ff54df61e9fab907a77ca20c.jpg?wh=1238x470" alt=""><br>\n整个半像素和1/4像素的插值过程可以通过下图表示：</p><p><img src="https://static001.geekbang.org/resource/image/6b/1b/6ba9f30f0f40247118f1410ffa3d371b.jpg?wh=1280x720" alt=""></p><p>插值得到了所有的半像素和1/4像素之后，我们就可以用运动矢量表示0.5个像素和0.25个像素的移动了，就如之前小车向前行驶了48.5个像素。如果只是整像素的图像，那只能表示出向前行驶了48个像素或者49个像素，没办法表示48.5个像素。</p><p>我们用下面的几幅图像来直观地感受一下一个4x4的像素块的半像素图像和1/4像素图像的像素位置。首先来看一幅4x4的像素块的像素位置。</p><p><img src="https://static001.geekbang.org/resource/image/69/db/698d3990bd1198802a0bc7fae810a2db.jpg?wh=1280x720" alt=""><br>\n下面图中的红色的4x4像素表示水平半像素图像。</p><p><img src="https://static001.geekbang.org/resource/image/98/8b/98d938ba868cc3c66d0775a44e23458b.jpg?wh=1280x720" alt=""><br>\n下面图中的红色的4x4像素表示垂直半像素图像。</p><p><img src="https://static001.geekbang.org/resource/image/09/35/0973cf8fb02c5f9f0b4b8df2d677c035.jpg?wh=1280x720" alt=""><br>\n下面图中的红色的4x4像素表示水平、垂直半像素图像。</p><p><img src="https://static001.geekbang.org/resource/image/41/12/414225faf7fb28e0216468c9a6a4c112.jpg?wh=1280x720" alt=""><br>\n下面图中的绿色的4x4像素表示水平1/4像素图像。</p><p><img src="https://static001.geekbang.org/resource/image/d1/d6/d1ed3e489a0952c2150ea95b5c350bd6.jpg?wh=1280x720" alt=""><br>\n下面图中的绿色的4x4像素表示垂直1/4像素图像。</p><p><img src="https://static001.geekbang.org/resource/image/b0/9b/b047a08021fe4427665f69f96861ee9b.jpg?wh=1280x720" alt=""><br>\n下面图中的绿色的4x4像素表示水平、垂直1/4像素图像。</p><p><img src="https://static001.geekbang.org/resource/image/8a/61/8ab0488387a22dfb630896a8fb5c5d61.jpg?wh=1280x720" alt=""><br>\n有了上面的亚像素插值算法，前面的小车运行48.5个像素的问题就可以通过半像素插值在参考帧中插值得到一辆新的小车，从而就可以解决之前预测块和编码块位置不重合的问题。不过，还需要说明一下的就是，<strong>插值得到的小车跟原始的小车的对应像素点的像素值<strong><strong>并</strong></strong>不是完全一样的</strong>，毕竟插值得到的像素点是利用滤波算法加权平均得到的。</p><p>因此，半像素插值得到的预测块并不一定就比整像素预测块的残差小。只是我们多了很多个半像素预测块和1/4像素预测块的选择，所以<strong>我们可以在整像素预测块、半像素预测块和1/4像素预测块里面选择一个最好的</strong>。怎么选择呢？其实是<strong>在整像素运动搜索的基础上，再做一次精细化的亚像素运动搜索</strong>。下面我们就来讲讲亚像素运动搜索的过程是怎样的。</p><h3>亚像素精度运动搜索</h3><p>有了插值得到的亚像素，我们就可以进行亚像素精度的搜索了。一般搜索算法步骤如下：</p><ol>\n<li>先通过快速搜索算法进行整像素运动搜索算法得到整像素的运动矢量（就是我们在运动搜索小节中讲述的内容）。</li>\n<li>对参考帧进行半像素和1/4像素插值。</li>\n<li>以整像素运动矢量指向的整像素为起点，进行钻石搜索算法，分别求得中心点以及上、下、左、右四个半像素点对应预测块的残差块，得到SAD值。取SAD值最小的点为最佳匹配点。</li>\n<li>以半像素运动搜索的最佳匹配点为起点，分别求得中心点以及上、下、左、右四个1/4像素点对应预测块的残差块，得到SAD值，并取最小的点为最佳匹配点。<br>\n通过上面亚像素搜索算法得到的最佳匹配点就可以得到最后的运动矢量了。假设整像素运动矢量为(a0, b0)，半像素最佳匹配点相对于整像素最佳匹配点的运动矢量为(a1, b1)，1/4像素最佳匹配点相对于半像素最佳匹配点的运动矢量为(a2, b2)，则最后运动矢量（a，b）的值的计算方法如下：</li>\n</ol><p><img src="https://static001.geekbang.org/resource/image/a8/cc/a8b1918e08f0c312da319d9a6df20ecc.jpg?wh=1280x266" alt=""><br>\n相当于原先的运动矢量乘以了4，即原先1/4像素的0.25变成了1，0.5像素变成了2，1个像素则变成了4。这主要是因为我们不用小数形式来表示运动矢量。因为浮点型数据会有精度误差，所以我们通过乘以4把它变成整数。</p><p>通过上面的整像素运动搜索和亚像素精度运动搜索，我们就得到了最终的运动矢量了。有了运动矢量之后，我们需要将运动矢量的信息也编码到码流中，并且解码的时候直接取出来用就可以在参考帧中把预测块找出来了。那运动矢量是直接编码到码流中的吗？其实不是的。那是怎么做的呢？接下来我们就来讨论一下运动矢量的预测。</p><h3>运动矢量预测</h3><p>其实，运动矢量跟我们的编码块一样不是直接编码进去的，而是先用周围相邻块的运动矢量预测一个预测运动矢量，称为MVP。将当前运动矢量与MVP的残差称之为MVD，然后编码到码流中去的。解码端使用同样的运动矢量预测算法得到MVP，并从码流中解码出运动矢量残差MVD，<strong>MVP+MVD就是运动矢量了</strong>。</p><p>那运动矢量预测算法是怎样的呢？我们以16 x 16宏块为例通过下图来描述。</p><p><img src="https://static001.geekbang.org/resource/image/fe/82/fe685a1b694b1d9c053088f3597f1f82.jpg?wh=1280x720" alt=""><br>\n其步骤如下：</p><ol>\n<li>取当前编码宏块的左边块A、上边块B、右上块C。如果右上块不存在或者参考帧与当前编码宏块不同（多参考的时候会存在），则使用左上块D替换C，即C = D。</li>\n<li>求得A、B、C块的参考帧有多少个与当前编码块的参考帧相同，记为count。</li>\n<li>如果count &gt; 1，则取A、B、C块的运动矢量的中值（就是A、B、C块运动矢量的3个x和3个y分别取中间值作为MVP的x和y）。</li>\n<li>如果count = 1，则直接将这个块的运动矢量作为MVP。</li>\n<li>如果count = 0，并且B、C都不存在，A存在的话，则直接将A的运动矢量作为MVP。</li>\n<li>如果上述条件都不满足，则取A、B、C块运动矢量的中值。</li>\n</ol><h3>SKIP模式</h3><p>了解了运动矢量的预测算法之后，我们还需要介绍一种比较特殊的模式，也是经常会遇到的一种帧间模式，就是SKIP模式。它的定义是这样的，如果运动矢量就是MVP，也就是说MVD为(0，0)，同时，残差块经过变换量化后系数也都是等于0，那么当前编码块的模式就是SKIP。</p><p>相比于SKIP模式，其它模式要不就是MVD不为0，要不就是量化后的残差系数不为0，或者两者都不为0。所以说<strong>SKIP模式是一种特例，由于MVD和残差块都是等于0，因此压缩效率特别高</strong>。</p><p>比如说P帧中的静止部分，前后两帧不会变化，运动矢量直接为0，而且残差块像素值本身因为几乎没有变化基本为0，只有少部分噪声引起的比较小的值，量化后更是全部变成了0。<strong>这种图像中的静止部分或者是图像中的背景部分大多数时候都是SKIP模式</strong><strong>。</strong>这种模式非常省码率，且压缩效率非常高。因为需要编码的信息非常少，所以单独在这里跟你讨论一下。</p><p>好了，到这里基本的帧间编码的知识我们都有了，接下来我们再来总体过一下P帧宏块的模式选择的过程。</p><h2>帧间模式的选择</h2><p><strong>编码块帧间模式的选择其实就是参考帧的选择、运动矢量的确定，以及块大小（也就是块划分的方式）的选择，如果SKIP单独拿出来算的话就再加上一个判断是不是SKIP模式</strong>。我们主要是确定这4个东西。</p><p>之前的讨论当中我们都是以当前编码帧的前一帧作为参考帧的，也就是说是单参考的，不涉及到参考帧的选择。其实，如果是多参考的话，编码块在选择参考帧的时候只需要遍历每一个参考帧进行块划分，然后再对每一个块进行运动搜索得到运动矢量就可以了。跟单参考相比就是多了一个参考帧遍历的操作。所以<strong>我们这里还是以单参考帧的方式来讲讲帧间模式的选择过程</strong>。</p><p>注意，帧间模式的选择大多数是看编码器的实现的，并且不同编码器实现都会不一样，所以我们只是讲讲其中一种模式选择的思路，具体的细节各个编码器都各不相同。具体选择过程如下：</p><ol>\n<li>首先判断当前宏块是不是可以作为SKIP块（通过相邻已经编码的块是不是存在SKIP块，和当前块使用MVP做运动矢量之后，残差块变换量化后是不是都为0等算法来判断），如果可以作为SKIP块则模式选择结束，不再进行下面的划分了。</li>\n<li>宏块大小为16 x 16。首先不划分宏块，直接使用16 x 16大小的块，在参考帧中进行运动搜索，得到运动矢量和预测块，通过MVP求得MVD，通过预测块求得残差块，并求得残差块的SATD值（残差块经过Hadamard变换之后求绝对值再求和），估计MVD的编码后占用的字节数，将两个值加起来作为cost16x16。</li>\n<li>将16 x 16块划分成4个8 x 8的子块，分别进行运动搜索，并求得每一个8 x 8子块的MVD和残差块，最后分别得到4个子块的cost8x8。</li>\n</ol><p>（1）如果4个8 x 8子块的cost8x8之和小于16 x 16块的cost16x16的话，我们再分别对每一个8 x 8子块划分成4个4 x 4子块，同样分别进行运动搜索，得到每一个4 x 4子块的 cost4x4。</p><ul>\n<li>\n<p>如果4个cost4x4之和小于cost8x8，则将8 x 8块划分成4 x 8和8 x 4两种子块分别求得cost4x8和cost8x4，再根据4个cost4x4、2个cost4x8和2个cost8x4的大小，选择最终的8x8划分的方式，并将对应的cost值更新到cost8x8。</p>\n</li>\n<li>\n<p>否则不划分8 x 8子块。</p>\n</li>\n</ul><p>（2）如果4个8 x 8子块的最新的cost8x8之和还是小于cost16x16的话，则再将16 x 16划分成两个8 x 16和16 x 8子块，并分别求得cost8x16和cost16x8，对比8x8、16x8、8x16 的cost值，并决定最终16 x 16块的划分方式。<br>\n（3）否则的话，不划分16 x 16的块。</p><ol start="4">\n<li>得到了编码宏块的帧间模式之后，我们还需要对编码宏块进行帧内模式的选择。是的，没错。在P帧和B帧中的宏块也是可以使用帧内模式的，所以我们需要看是帧间模式cost更小还是帧内模式cost更小。这也回答了我们在第4节课里留的思考题。如果帧内模式更小则使用帧内模式；如果是帧间模式更小则使用帧间模式。但是一般来说P帧和B帧宏块决策出来绝大多数还是帧间模式的。</li>\n</ol><h2>小结</h2><p>好了，这节课到这里就要结束了。我们现在来回顾一下这节课的知识点。</p><p>我们通过一步步分析如何在参考帧中准确地找到预测块的方式，讲解了帧间预测中最重要的一些知识点。其主要包括以下5个方面：</p><ol>\n<li>宏块的划分。<strong>为了能够更准确的找到预测块，我们可以将16 x 16的宏块继续划分成更小的子块来做运动搜索</strong>。因为图像有的地方静止的背景画面或者平坦的区域可以直接选用最大的块来搜索预测块；而有的地方细节很多，图像中的物体运动方向也各不相同，可能就需要划分成更小的块来做运动搜索。这样每一个块都拥有自己独立的运动矢量，并且得到的预测块更接近于编码块，从而有利于提高压缩效率。</li>\n<li>参考帧和运动矢量。<strong>在RTC场景中我们一般选择单参考，并且一般选择当前编码图像的前一帧作为参考帧。</strong>运动矢量是用来表示参考帧中预测块与编码帧中编码块位置的相对距离的。</li>\n<li>运动搜索。运动矢量是通过运动搜索得到的，而运动搜索是在参考帧中进行的。通常我们会使用钻石搜索和六边形搜索等快速运动搜索算法。一般不会使用全搜索算法。其中钻石搜索算法更简单，步骤更少，所以<strong>如果需要编码速度快，一般选择钻石搜索</strong>。六边形搜索步骤更多，更精细，<strong>要求编码质量高</strong><strong>，</strong><strong>同时对速度要求不高的时候，可以选择六边形搜索</strong>。</li>\n<li>亚像素插值和亚像素精度搜索。光做整像素运动搜索不太能够准确的处理连续运动场景。为了能够处理好这种连续运动的问题，我们对参考帧进行亚像素插值得到半像素和1/4像素图像。然后在整像素搜索的基础上在亚像素图像上做亚像素精度的运动搜索。实验数据证明，<strong>半像素和1/4像素精度的运动搜索相比整像素精度的运动搜索可以明显地提高压缩效率</strong>。</li>\n<li>在最后我们大体讲了一下编码块帧间预测模式的具体选择过程，并单独讲解了一下SKIP模式。<strong>SKIP模式是一种比较特殊的模式，由于MVD和残差块都是等于0，因此其压缩效率特别高。</strong><br>\n最后，我再次强调一下，今天的课程内容非常多。你需要多看几遍才能有比较好的理解。如果你是视频编码的应用工程师的话，掌握其中的思想就可以了；如果你是视频编码算法工程师的话可以先阅读几遍文章，之后再通过阅读编码器具体代码来熟练掌握。实际上，真正的帧间预测过程要比我讲述的内容复杂的多，有很多繁琐的细节，同时每一个编码器的具体实现也不一样，需要你自己课后慢慢学习。</li>\n</ol><p>参考文献：</p><p><a href="https://blog.csdn.net/leixiaohua1020/article/details/45936267">https://blog.csdn.net/leixiaohua1020/article/details/45936267</a>。</p><p>这篇文章中雷霄骅大神针对H264的帧间预测原理和x264中对应的代码做了非常详细地讲解。在这里非常感谢雷神对视频技术做出的贡献，以及无私地分享视频技术知识。雷神也是我学习视频技术路上的导师。在这里向雷神致敬。</p><h2>思考题</h2><p>学完了这节课，我有一个思考题留给你。P帧的第一个宏块的MVP怎么得到呢？</p><p>欢迎你在留言区和我分享你的思考和疑惑，你也可以把今天所学分享给身边的朋友，邀请他加入探讨，共同进步。下节课再见。</p>',
        article_title: "07｜帧间预测：如何减少时间冗余？",
      },
      {
        title: "08｜变换量化：如何减少视觉冗余？",
        id: 464501,
        content:
          '<p>你好，我是李江。</p><p>在前面几节课里面我们对<a href="https://time.geekbang.org/column/article/459554">视频编码的原理</a>、<a href="https://time.geekbang.org/column/article/462268">帧内编码</a>和<a href="https://time.geekbang.org/column/article/463775">帧间编码</a>都做了详细的介绍。我们知道，通过帧内编码可以去除空间冗余，通过帧间编码可以去除时间冗余，而为了分离图像块的高频和低频信息从而去除视觉冗余，我们需要做DCT变换和量化。</p><p>为了让你更透彻地了解视频编码中的DCT变换和量化的原理，在这节课里面，我们会对DCT变换和量化的过程做一个深入的探讨。通过下图你可以很清楚地看到视频编码的过程，并且能够更直观地感受DCT变换和量化在整个视频编码过程中的重要性。</p><p><img src="https://static001.geekbang.org/resource/image/06/78/064a2356cc7bbf4c1c43d2856e186f78.jpg?wh=1280x572" alt=""></p><p>由于DCT变换和量化过程是一个跟数学比较相关的过程，且大多数是数学计算。因此今天的课程中数学公式和计算过程相对会多一些。但是总体来说难度不是很大。</p><p>下面我们就先来讨论一下常规的视频编码中的DCT变换和量化，看看它是怎么去除一部分高频信息来达到去除视觉冗余的。并且，由于H264中用到的DCT变换和量化跟常规的DCT变换和量化有一些区别，其主要在于H264使用整数变换代替常规的DCT变换，并将DCT变换中的一部分计算整合到量化中，从而减少浮点运算漂移问题。因此，我们还会对H264中的DCT变换和量化做一下介绍，最后对比一下H264中的变换和量化与常规的变换和量化的区别。</p><!-- [[[read_end]]] --><h2>常规视频编码中的DCT变换和量化</h2><p>通过前面的学习，我们知道DCT变换和量化的目的是去除视觉冗余。接下来让我们看一下常规视频编码中DCT变换和量化是如何实现这一目的的。</p><h3>DCT变换</h3><p><strong>DCT变换，就是离散余弦变换。它能够将空域的信号（对于图像来说，空域就是你平时看到的图像）转换到频域（对于图像来说，就是将图像做完DCT变换之后的数据）上表示，并能够比较好的去除相关性。</strong>其主要用于视频压缩领域。现在常用的视频压缩算法中基本上都有DCT变换。</p><p>在视频编码原理那节课中我们也讲过，图片经过DCT变换之后，低频信息集中在左上角，而高频信息则分散在其它的位置。通常情况下，图片的高频信息多但是幅值比较小。高频信息主要描述图片的边缘信息。</p><p>由于人眼的视觉敏感度是有限的，有的时候我们去除了一部分高频信息之后，人眼看上去感觉区别并不大。因此，我们可以先将图片DCT变换到频域，然后再去除一些高频信息。这样我们就可以减少信息量，从而达到压缩的目的。</p><p><strong>DCT变换本身是无损的，同时也是可逆的。</strong>我们可以通过DCT变换将图片从空域转换到频域，也可以通过DCT反变换将图片从频域转回到空域。下面我们来看一下DCT变换的公式。</p><p>一维DCT变换公式如下，其中f(i)是指第i个样点的信号值，N代表信号样点的总个数。</p><p><img src="https://static001.geekbang.org/resource/image/b7/34/b767c40657b6758aff71fb148ce4da34.jpg?wh=1280x474" alt=""></p><p>二维DCT变换公式如下，其中f(i, j)是指第(i, j)位置的样点的信号值，N代表信号样点的总个数。</p><p><img src="https://static001.geekbang.org/resource/image/d3/bd/d310a208e95a8d439bb806c6aa10ddbd.jpg?wh=1280x484" alt=""><br>\n一般在编码标准中图像是进行二维DCT变换的，因为图像是个二维信号。但是实际上在代码里面我们经常将二维DCT变换转换成两个一维DCT变换来进行。</p><p>在视频压缩中，DCT变换是在帧内预测和帧间预测之后进行的。也就是说，DCT变换其实是对残差块做的。我们在编码时会将图像划分成一个个宏块，而宏块又可以划分成一个个子块。那DCT变换是在宏块上进行还是在子块上进行呢？</p><p>其实，<strong>通常情况下DCT变换是在4x4的子块上进行的</strong>（也可以在8x8子块上进行，但是只有在扩展profile才支持，由于原理是一样的，因此这里不再展开讨论），即便预测时并没有对宏块再做划分。也就是说，不管宏块有没有被划分到4x4的子块，我们在做DCT变换时，都是在一个个4x4块上进行的。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/00/c4/001a5f7eb9e9a08960c1cc3b8841f8c4.jpg?wh=1280x720" alt=""><br>\n好了，如果我们将上面的DCT变换公式用在4x4的变换块上，则4x4的DCT变换就可以通过下面的4x4的矩阵乘法来表示了。</p><p><img src="https://static001.geekbang.org/resource/image/59/5b/593a45c8e6a4dc69a1038efb7fb6bc5b.jpg?wh=1280x720" alt=""></p><p>为了让你更好地理解DCT变换，我们通过下面的例子来看一下4x4的残差块的DCT变换结果。我们称左上角的系数为DC系数，而其它系数为AC系数。</p><p><img src="https://static001.geekbang.org/resource/image/94/3b/94ec636ddc0fa111a51f7149bc7cc43b.jpg?wh=1280x720" alt=""><br>\n对这个4x4的残差块运用公式，可得：</p><p><img src="https://static001.geekbang.org/resource/image/72/eb/7266275ea219bb6dba2e2866bdbeddeb.jpg?wh=1280x640" alt=""></p><p>我们可以得到下面的图：</p><p><img src="https://static001.geekbang.org/resource/image/e5/c0/e50bf2884c1fc7ec4a753d61f4b8eac0.jpg?wh=1280x492" alt=""><br>\n从上面DCT变换的公式和矩阵表示方式中，我们可以看到，DCT变换的计算过程中涉及到了cos函数。那也就是说计算的过程中一定涉及到了浮点运算。而浮点运算计算速度比较慢。<strong>那有没有什么运算可以将图像块比较快速的转换到频域呢？</strong>答案肯定是有的。前面我们其实已经讲到过，那就是Hadamard变换，也叫哈达玛变换。</p><h3>Hadamard变换</h3><p>在视频编码过程中，Hadamard变换也经常会用到。前面我们在<a href="https://time.geekbang.org/column/article/462268">帧内预测</a>的<a href="https://time.geekbang.org/column/article/462268">率失真优化</a>的模式选择里就讲到过，Hadamard变换可以代替DCT变换将残差块快速转换到频域，以便用来估计一下当前块编码之后的大小。</p><p>其实在H264的亮度16x16帧内预测块和色度8x8预测块中也会使用到Hadamard变换。稍后我们会在H264中的DCT变换和量化部分对它进行简单介绍。下面我们先来看一下Hadamard变换的矩阵表示形式。</p><p><img src="https://static001.geekbang.org/resource/image/37/6c/37ca4e953244ea275e95f5eb03320b6c.jpg?wh=1280x720" alt=""><br>\n你是不是可以看到，Hadamard变换是没有浮点运算的？因此其计算速度很快，并且也能够将图像块从空域变换到频域。因此，我们可以用它一定程度上粗略的代替DCT变换，从而用来简化运算。</p><p>好了，现在我们知道了如何通过DCT变换和Hadamard变换将残差块从空域转换到频域。接下来我们看看在常规视频编码中量化是如何做到通过去除一部分高频信息来最终达到去除视觉冗余的。</p><h3>量化</h3><p>前面我们讲了，我们将图像块变换到频域之后，AC系数比较多，但是一般幅值比较小。并且，我们可以去除一些AC系数，达到压缩图像的目的，同时人眼看起来差距不大。<strong>这个去除AC系数的操作是什么呢？</strong>很明显就是量化了。</p><p>其实<strong>量化的操作并不是针对AC系数去做的，DC系数也同样会做量化</strong>，只是通常情况下，DC系数比较大，从而量化后变换为0的概率比AC系数要小。量化操作其实非常简单，就是除法操作。计算公式如下：</p><p><img src="https://static001.geekbang.org/resource/image/7f/5a/7f020c6yy18d49a13fda06937888cd5a.jpg?wh=1280x358" alt=""><br>\n在量化过程中，最重要的就是QStep（用户一般接触到的是QP，两者可以查表转换）。</p><p>其中，在H264中QP和QStep之间的转换表格如下：</p><p><img src="https://static001.geekbang.org/resource/image/43/ea/4360178571e94yy6d1c916f02282a6ea.jpg?wh=1280x720" alt=""></p><p>通常QStep值越大，DC系数和AC系数被量化成0的概率也就越大，从而压缩程度就越大，但是丢失的信息也就越多。这个值太大了会造成视频出现一个个块状的效应，且严重的时候看起来像马赛克一样；这个值比较小的话，压缩程度也会比较小，从而图像失真就会比较小，但是压缩之后的码流大小就会比较大。</p><p>我们通过一个例子来看一下量化的结果。</p><p><img src="https://static001.geekbang.org/resource/image/bb/0c/bb8783ca8e1b1bd7177459a344fb410c.jpg?wh=1280x484" alt=""></p><p>这就是常规的变换和量化的计算过程。实际上H264里面的变换和量化是这样的吗？原理上是的，但是实际计算过程变了。因为DCT变换过程中涉及到浮点运算，在不同机器上解码会因为精度问题产生漂移导致误差。同样，量化过程有除法运算，大多数时候其结果还是浮点型的数字，在不同机器上解码也会有误差。</p><h2>H264中的DCT变换和量化</h2><p><strong>H264为了减少这种浮点型运算漂移带来的误差，将DCT变换改成了整数变换，DCT变换中的浮点运算和量化过程合并，这样就只有一次浮点运算过程，</strong>以此来减少不同机器上浮点运算产生的误差。下面我们来看看H264中的变换和量化。</p><h3>H264的整数变换和量化</h3><p>我们知道常规的DCT变换的矩阵计算方式如下：</p><p><img src="https://static001.geekbang.org/resource/image/5e/43/5e72913dfa211e19a205c39553db3d43.jpg?wh=1280x720" alt=""><br>\n而在H264中，我们通过下面的推导过程，将DCT变换一步步修改为整数变换。最后H264中的DCT变换就变成了整数变换。其矩阵的计算方式如下：</p><p><img src="https://static001.geekbang.org/resource/image/ff/7f/ff71a57bb82d76493c2bc992107d307f.jpg?wh=1280x720" alt=""><br>\n我们将点乘左边的部分取出来，就是H264中的整数变换了。公式如下：</p><p><img src="https://static001.geekbang.org/resource/image/8f/19/8ff14e94f06766df6cdba1706780c019.jpg?wh=1280x720" alt=""><br>\n我们同样使用上面DCT变换的例子来做一下整数变换。其结果如下：</p><p><img src="https://static001.geekbang.org/resource/image/fe/ec/fe4b65c6e4ba73a15ceacda63651ecec.jpg?wh=1280x472" alt=""><br>\n在前面整数变换里，DCT变换中的点乘部分被拿出来了，这一部分的计算被合并到了H264的量化过程中。因此H264的量化过程如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/3b/f1/3b00268b7f298846b60795b534908ff1.jpg?wh=1280x720" alt=""></p><p>其中，MF我们一般都是通过表格查询得到。表格如下。其中，对于QP大于5的情况，使用QP = QP % 6进行查询。</p><p><img src="https://static001.geekbang.org/resource/image/a3/1e/a37cdd4f5cb2492e658680f950be2b1e.jpg?wh=1280x720" alt=""><br>\n同样，上面整数变换之后，我们用H264的量化公式对其进行量化后得到的结果如下：</p><p><img src="https://static001.geekbang.org/resource/image/cd/7d/cdc4482b0689e8ef6142657a0da3c67d.jpg?wh=1280x468" alt=""></p><p>我们可以看到虽然H264的DCT变换和量化过程跟常规的DCT变换和量化不一样，但是最后量化的结果其实还是一样的。这也是符合预期的。毕竟它们的“目的地”还是一样的，只是“走的路”稍微有些不同而已。</p><p>好了，这就是H264中的DCT变换和量化的基本原理。接下来我们来看看H264各模式块的DCT变换和量化过程具体是怎么样的。</p><h3>H264各模式块的DCT变换和量化过程</h3><ol>\n<li>亮度16x16帧内预测块<br>\n亮度16x16块，首先被划分成16个4x4的小块做整数变换。变换之后将16个4x4小块的DC系数都拿出来，组成一个4x4的DC块，再对这个4x4的DC块进行Hadamard变换。然后，再总体进行量化操作。</li>\n</ol><p><img src="https://static001.geekbang.org/resource/image/e1/e6/e10613350835c3ebb14e001461e7eae6.jpg?wh=1280x346" alt=""></p><ol start="2">\n<li>其它模式亮度块<br>\n对于除亮度16x16帧内预测块之外的其它亮度块，都是直接划分成4x4的块进行整数变换，之后再进行量化操作就可以了。</li>\n</ol><p><img src="https://static001.geekbang.org/resource/image/c7/cd/c7e61f929cdde6d8624ae0be270126cd.jpg?wh=1280x314" alt=""><br>\n3. 色度块<br>\n对于YUV420图像，色度块大小是8x8。我们先将8x8色度块划分成4个4x4的小块做整数变换。变换之后将4个小块的DC系数拿出来，组成一个2x2的DC块，再对这个2x2的DC块进行Hadamard变换。最后总体进行量化操作。</p><p><img src="https://static001.geekbang.org/resource/image/61/16/61c58d94ac755c2c71e8538d772d2b16.jpg?wh=1280x368" alt=""></p><h2>小结</h2><p>我们今天一开始主要讲解了DCT变换的基本原理。DCT变换主要是将图像从空域转换到频域，并将图像的高频和低频信息分离开来。虽然高频信息数据多，但是幅值比较小。这样高频信息在量化的过程中能够比较容易被减少。这样可以比较有效地减少图像的视觉冗余，从而达到压缩的目的。</p><p>接着，我们简单地介绍了一下量化的原理。量化其实就是一个除法操作。通过除法操作就可以将幅值变小，而高频信息幅值比较小，就比较容易被量化成0，这样就能够达到压缩的目的。</p><p>在讲变换的原理的时候，我们还讲到了一个前面提到了好几次的Hadamard变换。Hadamard变换在H264的16x16帧内亮度块和8x8色度块中会被用到。但是Hadamard在率失真优化做模式选择的时候使用的更多。基本上各种视频编码都或多或少会用到它来做率失真优化。</p><p>在H264标准中，我们不会直接使用标准的DCT变换和量化。为了减少多次浮点型运算在解码端产生漂移的问题，H264使用整数变换代替DCT变换。DCT变换中的浮点运算部分跟量化过程进行合并，将两次浮点型运算变成一次，从而减少误差。</p><p>在最后，我们简单介绍了H264标准中不同模式亮度块和色度块的DCT变换和量化的过程。其中需要注意的就是亮度16x16帧内预测块和色度8x8的DC系数会单独拿出来组成一个新的DC块，我们会先对这个DC块进行Hadamard变换之后再做量化操作。</p><h2>思考题</h2><p>为什么我们在率失真优化的过程中会用Hadamard变换之后的块做大小预估？</p><p>你可以把你的答案和疑惑写下来，分享到留言区，与我一起讨论。下节课再见。</p>',
        article_title: "08｜变换量化：如何减少视觉冗余？",
      },
    ],
  },
  {
    chapterTitle: "视频传输和网络对抗",
    children: [
      {
        title: "09｜RTP & RTCP：如何正确地将视频装进RTP中？",
        id: 465503,
        content:
          '<p>你好，我是李江。</p><p>在前面的课程中，我们详细地讲述了视频编码的原理以及预测编码和变换编码的知识。通过这些我们了解了视频编码的基本原理和步骤。同时，我们还用了一节课的时间深入探讨了H264的码流结构，相信你已经清楚了H264码流是什么样的，以及如何从码流中分离出一帧帧图像数据，并学会了如何判断这些帧的类型。</p><p>那么从这节课开始呢，我们就要进入视频传输和网络对抗部分了。我们会在视频编码码流的基础上，讲讲如何将码流打包成一个个数据包发送到网络上，并进一步讨论如何避免在发送的过程中引起网络拥塞，从而保证视频的流畅性。同时，我们会进一步在后面的课程中讲解如何在网络不断变化的时候做好视频码控算法，如何防止视频出现花屏，以及如何尽量减少视频卡顿等非常有难度的实际工程问题。</p><p>这些问题是视频开发过程中经常会遇到且迫切需要解决的重要问题。而解决这些问题的基础就是需要熟悉RTP和RTCP协议，也就是我们这节课的重点。</p><p>接下来我们会分别从RTP协议、RTCP协议和H264的RTP打包方法这三个方面来展开这节课。首先让我们一起来认识一下RTP协议。</p><h2>RTP协议</h2><p>RTP（Real-time Transport Protocol）协议，全称是实时传输协议。它主要用于音视频数据的传输。那它的作用是什么呢？</p><!-- [[[read_end]]] --><p>一般我们在实时通信的时候，需要传输音频和视频数据。我们通常是这样做的，先将原始数据经过编码压缩之后，再将编码码流传输到接收端。在传输的时候我们通常不会直接将编码码流进行传输，而是<strong>先将码流打包成一个个RTP包再进行发送</strong>。</p><p>那为什么需要打包成RTP包呢？这是<strong>因为我们<strong><strong>的</strong></strong>接收端要能够正确<strong><strong>地</strong></strong>使用这些音视频编码数据，不仅仅需要原始的编码码流，还需要一些额外的信息</strong>。比如说：</p><ul>\n<li><strong>当前视频码流是哪种视频编码标准</strong>，是H264、H265、VP8、VP9还是AV1呢？我们知道每种不同的编码标准，其码流解析的方式肯定也不一样。这个就需要通过RTP协议告知接收端。</li>\n<li>当我们知道编码标准了，我们就可以正确地解析码流，并解码出图像了。但是我们又会遇到一个新的问题，那就是<strong>按照什么速度播放视频呢？</strong>这个也需要RTP协议告知接收端。<br>\n这就是RTP协议的一个重要的作用，即告知接收端一些必要的信息。当然RTP协议的作用不止这些，它其实在网络带宽预测和拥塞控制的时候也发挥出了至关重要的作用。我们在之后的课程中会继续讨论，这里就先不讲了，你大体有个印象就可以。</li>\n</ul><p>我们知道RTP包需要附带很多额外的信息，那这些信息在RTP包中是怎么存在的呢？其实RTP包包括两个部分：第一个部分是RTP头；另外一个部分是RTP有效载荷。其中RTP头主要是用来携带前面说的那些额外信息的，等会儿我会详细介绍一下RTP头部每个字段的意义。</p><p>这里我先稍微跟你解释一下另外一个部分，也就是RTP有效载荷。RTP有效载荷，其实就是RTP包里面的实际数据。如果是H264编码打包成RTP包，那有效载荷就是经过H264编码的码流；如果是VP8编码呢，那就是VP8码流。</p><p>接下来，我们重点来看看RTP包的头部。具体如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/14/e6/143a86a6aef7f664beca7268f58e99e6.jpg?wh=1280x426" alt=""><br>\n是不是有点懵，别急，下面我给了一张表格，你可以对照着表格看看RTP包头的每一个字段占用的位数和具体的含义。其中绿色部分是很重要的知识点，需要你重点掌握。</p><p><img src="https://static001.geekbang.org/resource/image/3b/52/3b724acdcb5a69f3c1831591be49ca52.png?wh=1354x1194" alt="图片"></p><p>上面讲的就是RTP头部的主要组成部分。在这里需要单独提一下RTP头部的另外一个比较重要的部分，就是RTP扩展头。从上表我们可以看到，RTP包头有一个扩展头标志位X，当扩展头标志位X为1的时候，说明有RTP扩展头。RTP扩展头由于平时大家很少用看似不怎么重要，但是在RTC场景中，尤其是WebRTC中经常会用到。另外，RTP扩展头我们在带宽预测的时候也会用到。所以建议你也了解一下。</p><p>扩展头主要是用来给用户自定义扩展使用的。因为协议是标准的，但是用户使用场景却是多种多样的，所以RTP需要考虑的比较全面，留了一个扩展头可以让用户根据使用场景和需求，自己定义扩展头，用来传输需要在RTP包中传输的信息。扩展头的格式可以参考<a href="https://www.cnblogs.com/ishen/p/12050077.html">这篇文章</a>，这里就不做过多的展开了。</p><p>好了，以上就是RTP协议的主要知识点。有了RTP协议，我们就能够将码流打包成RTP包发给接收端了。如果你只负责传输RTP包，而不需要管传输过程中有没有丢包，以及传输RTP包的时候有没有引起网络拥塞的话，那你只需要使用RTP协议就可以了。比如说，你选择使用TCP协议传输RTP包的话就可以不用管这些事情，因为TCP协议具有丢包重传、拥塞控制等功能。</p><p>但是通常情况下，我们在传输音视频数据的时候不会使用TCP协议作为传输层协议。这是因为TCP协议更适合传输文本和文件等数据，而不适合传输实时音频流和视频流数据，所以我们通常会<strong>使用UDP协议作为音视频数据的传输层协议</strong>。但UDP协议不具有丢包重传和拥塞控制的功能，需要我们自己实现。那怎么办呢？</p><p>其实，真要做好丢包重传和拥塞控制是非常难的，一节课也讲述不清楚，所以，我们会在接下来的好几节课里详细解释。接下来我们可以先关注下丢包重传和拥塞控制的基础之一，也就是RTP协议的“好兄弟”，RTCP协议。</p><h2>RTCP协议</h2><p>RTCP（Real-time Transport Control Protocol）协议，全称是实时传输控制协议。它是辅助RTP协议使用的。RTCP报文有很多种，分别负责不同的功能。常用的报文有发送端报告（SR）、接收端报告（RR）、RTP反馈报告（RTPFB）等。而每一种报告的有效载荷都是不同的。我们就是通过这些报告在接收端和发送端传递当前统计的RTP包的传输情况的。我们使用这些统计信息来做丢包重传，以及预测带宽。</p><p>不过，我需要再次强调一下，<strong>RTCP协议只是用来传递RTP包的传输统计信息，本身不具有丢包重传和带宽预测的功能，</strong>而这些功能需要我们自己来实现。</p><p>我们上面讲到了RTCP协议有很多种报告，而每种报告其实定义的具体内容都是不一样的。我们这里以RTPFB报告中的NACK报告（丢包提示报告）作为一个例子来看看RTCP协议大概是什么样子的。（RTPFB报告包含了多种子报告，NACK报告只是其中的一种，因为我们后面还会用到这个报告，所以这里我们就先以这个报告为例子。）</p><p>下图就是NACK报告的协议格式。</p><p><img src="https://static001.geekbang.org/resource/image/83/cf/8346b7d062d6e18c1700603a2dd1a5cf.jpg?wh=1280x434" alt=""><br>\n其中，每一个字段在下表中都有详细的解释。</p><p><img src="https://static001.geekbang.org/resource/image/c3/73/c3e2f8185a38c0746be0c0dbd6b52073.jpg?wh=1280x720" alt=""></p><p>从上面的NACK报告我们可以看到，RTCP协议跟RTP不同，它们传递的东西是不一样的。</p><p>我们知道RTP是用来传输实际的视频数据的。它就像一个快递盒，先装好视频，然后填好运送的视频基本信息和收件人信息，最后将视频运送到收件人手上。</p><p>而RTCP协议则像是一个用来统计快递运送情况的记录表。其中的NACK报告就是快递丢件情况的记录表。它记录着哪些快递丢了。发件人收到了NACK之后，可以重新寄一个同样的快递给收件人，防止收件人没有收到快递。在这里也就是将丢失的视频RTP包重传一遍。</p><p>虽然我们只讲了一种RTCP报告，但是其它的报告也是类似的。大多数报告都是用来记录传输信息的。因为数量很多，我们这里就不一一展开了。如果你有兴趣的话，可以查看这个<a href="https://datatracker.ietf.org/doc/html/rfc3550">RFC文档</a>。</p><p>好了，通过学习RTP和RTCP的基础知识，我们了解了RTP包的协议格式和主要负责的功能，也知道了RTCP的协议格式和其主要承担的责任。接下来我们就进入实际工程部分的知识了。</p><p>我相信通过前面课程的学习，你对H264的码流结构已经较为熟悉了，H264是在工程中用得比较多的编码标准，所以这里我们以H264为例来讲讲实际工程开发中，我们怎么将H264码流打包成RTP包。</p><h2>H264 RTP打包</h2><p>我们前面说了，H264码流是放在RTP的有效载荷部分的。因此有效载荷前面的RTP头部跟码流本身是没有关系的，所以我们可以直接先将头部的字段填好就可以。接下来我们需要将H264码流填充到RTP有效载荷中去。</p><p><strong>RTP H264码流打包分为三种方式：分别是单NALU封包方式、组合封包方式、分片封包方式。</strong>顾名思义，单NALU封包方式是一个NALU打一个RTP包；而组合封包方式就是多个NALU打一个RTP包；分片封包方式则是一个NALU分开放在连续的多个RTP包中。下面我们来分别看一下各种打包方式是怎么样的。</p><p>1、单NALU封包方式</p><p>单NALU封包方式非常简单。我们在RTP头部的后面，直接放置NALU数据即可。注意，根据RTP的规定，这里需要<strong>将NALU数据前面的起始码去除</strong><strong>，</strong>不要将起始码也带入RTP包中。其格式如下：</p><p><img src="https://static001.geekbang.org/resource/image/12/91/12961a82dca32ea3f1d760b4674fbf91.jpg?wh=1280x570" alt=""><br>\n为了让你更直观地理解这种打包方式，我给出了打包的示意图。具体如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/3d/9b/3d4845a261ef2f7e0683d8a81522ab9b.jpg?wh=1280x504" alt=""><br>\n这种打包方式适合于单个RTP包小于1500字节（MTU大小）的时候。一般来说，一些P帧和B帧编码之后比较小，就可以使用这种打包方式。</p><p>2、组合封包方式</p><p>组合封包方式稍微复杂一些。它是将多个NALU放置在一个RTP包中。在RTP头部之后，且放置NALU数据之前，我们需要放置一个1字节的STAP-A的头部。其中，STAP-A Header跟NALU Header的格式是一样的，只是Type字段的值不一样。因此，你可以参考H264码流结构课程中NALU小节来理解STAP-A的头部的格式。具体如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/16/c2/16ec78c30e2760fab5a34a74524663c2.jpg?wh=1280x406" alt=""><br>\n其中，Type的取值如下表所示。这里我需要提醒你一下，表中的24和25类型就是STAP组合封包方式。注意，我们这里只讲STAP-A，这是因为STAP-B很少用到。</p><p><img src="https://static001.geekbang.org/resource/image/5a/a6/5a27c25b87c3fcb2be2a396f59ecb2a6.jpg?wh=1280x664" alt=""><br>\n放置完STAP-A Header之后，在每一个NALU的前面我们需要放置一个2字节的size字段，用于表示后面的NALU的大小。之后才是NALU的数据。记住同样需要<strong>去掉起始码</strong>。其格式如下：</p><p><img src="https://static001.geekbang.org/resource/image/bb/c3/bb5498a2d629d5c754920d62d401c5c3.jpg?wh=1280x720" alt=""><br>\n同样地，为了让你更直观地理解这种打包方式，我也给出了打包的示意图。具体如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/9c/c8/9ceee0961afe0be20a20f663fbf932c8.jpg?wh=1280x454" alt=""></p><p>这种打包方式适合于单个NALU很小的时候。因此，我们将多个NALU打包到一起也小于1500字节的时候就可以使用。但是由于一般多个视频帧加到一起还小于1500的情况比较少，所以视频数据的RTP打包一般来说用组合封包方式的情况也很少。</p><p>3、分片封包方式</p><p>分片封包就更复杂一些了，但却是我们经常用到的打包方式。</p><p>它是将一个NALU分开打包在连续的多个RTP包中。因此，我们首先需要一个1字节的FU indicator来表示当前RTP包是不是分片封包方式，再用一个1字节的FU Header来表示当前这个RTP包是不是NALU的第一个包，是不是NALU的最后一个包，以及NALU的类型。</p><p>为什么需要表示是不是第一个包以及是不是最后一个包呢？这是因为一个NALU被分开放在多个RTP包中，我们需要知道哪个是第一个NALU分片，哪个是最后一个NALU分片，以及哪些是中间分片。这样我们才能组成一个完整的NALU。</p><p>那你可能会问，NALU不是已经在NALU Header中有了NALU Type字段吗？为什么FU Header中还要有NALU Type呢？这是因为分片封包时需要去掉NALU Header。因此，我们需要通过FU Header中的NALU Type得到NALU的类型。</p><p>其中，分片封装中的FU indicator跟NALU Header的格式也是一样的，也只是Type字段的值不同，所以我们可以参考组合封包小节中的表格。因为我们一般只使用FU-A，所以接下来讲述的将是FU-A的分片封包方式。另外，FU Header格式如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/d5/63/d50ff5960768966518f9dafdac33a463.jpg?wh=1280x402" alt=""><br>\n这里我简单解释一下各字段的含义：</p><ul>\n<li>S：起始位，占1bit，为1则表示是NALU的第一个RTP包。</li>\n<li>E：结束位，占1bit，为1则表示是NALU的最后一个RTP包。</li>\n<li>R：预留位，占1bit。</li>\n<li>Type：占5bits，表示NALU类型。<br>\n分片打包的格式如下：</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/e3/bd/e3590b35793907d565293bf592c514bd.jpg?wh=1280x560" alt=""><br>\n分片打包的示意图如下：</p><p><img src="https://static001.geekbang.org/resource/image/14/7b/1402a1e69a71d1d9dff7b4f40920907b.jpg?wh=1280x588" alt=""><br>\n这种打包方式主要用于将NALU数据打包成一个RTP包时大小大于1500字节的时候，这是经常使用的视频RTP打包方法。</p><p>好了，以上就是三种打包方式。我们怎么选择使用哪种方式打包呢？一般来说，我们在一个H264码流中会混合使用多种RTP打包方式。一般来说，对于小的P帧、B帧还有SPS、PPS我们可以使用单个NALU封包方式。而对于大的I帧、P帧或B帧，我们使用分片封包方式。当然，你可以根据实际情况进行选择。</p><h2>小结</h2><p>好了，以上就是这节课的主要内容。接下来我们来总结一下。</p><p>首先，我们一起讨论了RTP协议和RTCP协议的主要作用。RTP协议用来封装音视频数据，并且将音视频数据和一些基本信息打包到RTP包中传输到接收端。而RTCP协议则辅助RTP协议使用，其中一个主要的功能就是用来统计RTP包的发送情况，比如说丢包率和具体哪些RTP包在网络发送的过程中丢失了。RTCP包将这些信息收集起来发送给RTP包的发送端。</p><p>然后，我们说明了RTP和RTCP协议是带宽预测和拥塞控制的基础，并且重点强调了RTCP协议本身只统计信息，而带宽预测和拥塞控制算法是需要我们自己实现的，RTCP协议本身并没有这个功能。</p><p>最后，我们介绍了H264的RTP打包方式，总共有三种，分别是单NALU封包方式、组合封包方式和分片封包方式。</p><ul>\n<li>单NALU封包方式，一般适合NALU大小比较小，且打包出来的RTP大小小于1500字节的时候使用。</li>\n<li>组合封包方式，适合多个NALU都很小，且合并在一起打包的RTP包小于1500字节的时候使用。</li>\n<li>分片打包，则适合NALU比较大的情况，且打包成一个RTP包其大小会大于1500字节的时候使用。<br>\n这几种打包方式不是说只能选择一种，在一个RTP流中是可以存在多种打包方式的，即可以混合使用。</li>\n</ul><p>最后再一次强调，这节课和H264码流结构那节课都是非常重要的。它们在实际视频开发的过程中会经常用到，希望你可以熟练掌握。</p><h2>思考题</h2><p>为什么我们在选择RTP打包方式的时候，需要根据NALU大小是不是大于1500字节（MTU）来选择？</p><p>欢迎你在留言区和我分享你的思考和疑惑，你也可以把今天所学分享给身边的朋友，邀请他加入探讨，共同进步。下节课再见。</p>',
        article_title: "09｜RTP & RTCP：如何正确地将视频装进RTP中？",
      },
      {
        title: "10｜带宽预测：3大算法准确预估网络带宽",
        id: 467073,
        content:
          '<p>你好，我是李江。</p><p>上一节我们详细地讲述了RTP和RTCP协议。RTP协议用来封装传输的音视频数据并带上一些基本的信息，而RTCP协议则用来统计这些RTP包的传输情况。RTP和RTCP一般是使用UDP协议作为传输层协议的。因为音视频数据需要比较高的实时性，TCP协议不太适合，所以我们一般使用UDP协议。但是UDP协议没有实现拥塞控制算法。因此，我们使用UDP协议作为传输层协议的话，需要自己实现拥塞控制算法。</p><p>比如说，我们声网就是自己实现了一个全球实时通信网SD-RTN，并研发了Agora Universal Transport（AUT）传输算法。我们的SD-RTN和AUT内部实现了适合不同网络模型的拥塞控制和丢包重传等一整套高质量的传输算法和策略。如果你使用了我们的音视频SDK，则无需自己关注拥塞控制和丢包重传等一系列弱网对抗算法，SD-RTN和AUT会保证你在进行音视频通信时的流畅度和实时性要求。</p><p><img src="https://static001.geekbang.org/resource/image/dd/55/ddd0ac38f97976c5f875f2yy10d0cd55.jpeg?wh=1920x1080" alt=""></p><p>一般情况下，音视频场景中的拥塞控制和丢包重传等算法的基础就是RTP和RTCP协议。我们需要通过RTP包的信息和RTCP包中传输的统计信息来做拥塞控制和丢包重传等操作。因此，我再强调一下，上一节课是我们之后几节课的基础，你需要完全掌握。</p><!-- [[[read_end]]] --><p>今天，我们主要讨论拥塞控制中的带宽预测算法，在之后的课程中我们还会介绍码控算法和丢包重传算法。带宽预测几乎是整个音视频传输和弱网对抗中最重要的环节，也是最难的一个环节。因此，你可能需要多看几遍今天的课程才能完全掌握。</p><p>而且，带宽预测算法涉及到很多阈值设置等细节问题。为了能够更好地带你了解带宽预测的思想，我们在讨论的过程中会将一部分细节省略，这样更加方便你理解核心思想。同时，由于WebRTC在实时音视频中占据绝对的领导地位，它的带宽预测算法非常的成熟，因此，我们今天将以WebRTC的带宽预测算法作为讨论的对象，对其进行深入剖析和探讨。</p><h2>带宽预测</h2><p>带宽预测，顾名思义，就是实时预测当前的网络带宽大小。<strong>预测出实际的带宽之后，我们就可以控制音视频数据的发送数据量。</strong>比如说，控制音视频数据的编码码率或者直接控制发送RTP包的速度，这都是可以的。控制住音视频发送的数据量是为了不会在网络带宽不够的时候，我们还发送超过网络带宽承受能力的数据量，最后导致网络出现长延时和高丢包等问题，继而引发接收端出现延时高或者卡顿的问题。因此，带宽预测是非常重要的。</p><p>而现在的网络中，大多存在两种类型的网络设备：一种是有较大缓存的；一种是没有缓存或者缓存很小的。</p><ul>\n<li>前者在网络中需要转发数据过多的时候，会把数据先缓存在自己的缓冲队列中，等待前面的数据发送完之后再发送当前数据。这种情况就会在网络带宽不够的时候，需要当前数据等一段时间才能发送，因此<strong>表现出来的现象就是网络不好时，延时会加大。</strong></li>\n<li>后者在网络中需要发送的数据过多的时候，会直接将超过带宽承受能力的数据丢弃掉。<strong>这种情况就会在网络带宽不够的时候，出现高丢包的现象。</strong></li>\n</ul><p>通过下面的图你可以很容易理解：</p><p><img src="https://static001.geekbang.org/resource/image/c8/d0/c844c9dbe2a0d958bcb4cb61b19529d0.jpeg?wh=1920x1080" alt=""><img src="https://static001.geekbang.org/resource/image/d8/0f/d89b5fceeb0a2b86c2b2044041e1680f.jpeg?wh=1920x1080" alt=""></p><p>因为互联网中这两种类型的网络设备都存在，为了能够兼顾这两种类型的网络，WebRTC中设计了两个主要的带宽预测算法：一个是基于延时的带宽预测算法；一个是基于丢包的带宽预测算法。下面我们逐一讨论一下。</p><h3>基于延时的带宽预测算法</h3><p>基于延时的带宽预测算法主要是通过计算一组RTP包它们的发送时长和接收时长，来判断当前延时的变化趋势，并根据当前的延时变化趋势来调整更新预测的带宽值。比如说：</p><ul>\n<li>如果延时有明显变大的趋势就说明实际带宽值应该比当前的发送码率要小，则需要降低预测的带宽值。</li>\n<li>如果说延时没有变大，说明当前带宽良好，可能实际带宽值比当前的发送码率还要大，则可以提高预测的带宽值，直到延时有明显变大的趋势再降低预测的带宽值。我们不断这样实时调整更新带宽，来实现带宽预测。</li>\n</ul><p>从上面的讲述我们可以看到，基于延时的带宽预测算法，主要有4个步骤：</p><ol>\n<li>计算一组RTP包的发送时长和接收时长，并计算延时；</li>\n<li>需要根据当前延时和历史延时的大小来计算延时变化的趋势；</li>\n<li>根据延时变化趋势判断网络状况；</li>\n<li>根据网络状况调整更新预测带宽值。</li>\n</ol><p><img src="https://static001.geekbang.org/resource/image/a3/cf/a3f84d6a7a80705551dd7a5f8462becf.jpeg?wh=1911x506" alt=""></p><p>下面，我们一个个步骤来展开详细讲解。</p><p><strong>计算延时</strong></p><p>WebRTC中计算延时的时候是将RTP包按照发送时间来分组的，并且要求当前组中的第一个包和最后一个包的发送时间相差不能大于5ms，而大于5ms则是新的一组的开始。同时，由于UDP会出现包乱序到达的情况，可能导致后面包的发送时间比前面包的还小。为了防止这种情况的发生，我们要求乱序的包不参与计算。</p><p>并且发送端在发送每一个RTP包的时候会记录每一个包的包序号和实际发送时间，并把这些信息记录到一个发送历史数据里面方便之后计算使用（这个发送历史数据好几个计算需要用到，挺重要的）。</p><p><img src="https://static001.geekbang.org/resource/image/e1/17/e1f36fecdcd4a402c84f4458f92cbd17.jpeg?wh=1920x1080" alt=""></p><p><img src="https://static001.geekbang.org/resource/image/51/51/514cb5c090aeb5ec52f830cb09bd9651.jpeg?wh=1920x1080" alt=""></p><p>同时，接收端收到每一个包的时候也会记录包的包序号和实际的接收时间。每隔一段时间就会将</p><p>这些统计信息发送到发送端。现在的WebRTC版本中接收端是通过RTCP协议的Transport-CC报文反馈接收信息的，这个报文主要包含两个信息：</p><ol>\n<li>每一个包序号对应的包是不是接收到了；</li>\n<li>实际的这个包相比前一个包的接收间隔。</li>\n</ol><p>接收端将这些信息组成Transport-CC报文发送给发送端。</p><p>如果发送端收到这个报文，就可以知道每一个RTP包有没有接收到了。如果没有接收到就是丢包了。同时也可以知道没有丢失的RTP包的接收时间。</p><p>发送端就可以根据发送历史数据中各个包的发送时间和Transport-CC报文中计算得到的各个包的接收时间，来计算出前后两组包之间的发送时长和接收时长了。计算方法如下。</p><p><img src="https://static001.geekbang.org/resource/image/af/59/afayy53db3f7041e01dd5bff17dcfc59.jpeg?wh=1920x1080" alt=""></p><p>其中Transport-CC报文格式可以参考下图，具体各字段的解释可以参考这个<a href="https://datatracker.ietf.org/doc/html/draft-holmer-rmcat-transport-wide-cc-extensions-01">RFC文档</a>和<a href="https://blog.jianchihu.net/webrtc-research-transport-cc-rtp-rtcp.html">这篇文章</a>。总的来说，就是通过Transport-CC报文，我们可以计算得到每一个包是不是丢失了，以及没有丢失的每一个包的接收时间。这里我们不做过多展开。</p><p><img src="https://static001.geekbang.org/resource/image/56/e3/563861306be7175ac5a5c8919cba76e3.png?wh=1124x818" alt="" title="图片来源RFC文档"></p><p>有了发送时长和接收时长，我们<strong>将接收时长减去发送时长就是延时</strong>了。</p><ul>\n<li>如果接收时长大于发送时长，延时就大于0，说明当前网络有点承受不了当前的发送数据量，产生了缓存，继而产生了延时。</li>\n<li>如果接收时长几乎等于发送时长，延时就几乎为0，说明当前网络可以承受当前的发送量，因此几乎没有延时。</li>\n<li>如果接收时长小于发送时长，也就是延时小于0呢？这种情况比较特殊，一般出现在之前因为网络带宽不够已经缓存了一部分数据，但是网络在明显变好，从而网络设备快速地将缓存中的数据发送出去的时候。这种情况下就会出现接收时长很短，导致接收时长还小于发送时长，这个时候延时就是一个负数。</li>\n</ul><p>这三种情况具体可参考下图：</p><p><img src="https://static001.geekbang.org/resource/image/e7/db/e79de2971b32e5f1d5492892bbefb9db.jpeg?wh=1920x1080" alt=""></p><p>好了，这就是延时的计算方法。有了延时之后，我们还不能直接使用这个延时来判断网络的好坏，因为网络变化很快而且存在噪声，有的时候延时会因为网络噪声突然变大或变小。因此，<strong>我们需要通过当前延时和历史延时数据来判断延时变化的趋势，来平滑掉网络噪声引起的单个延时抖动。</strong></p><p>说到这里就必须引入一个重要的滤波器了，那就是Trendline Filter。这个滤波器就是用来计算得到延时变化趋势的。</p><p><strong>延时变化的趋势计算</strong></p><p>Trendline Filter中保存了20个最近的延时数据，这些延时数据跟前面直接计算的延时还不完全一样。它们包含了两个部分：一个是当前这个RTP包组所属的Transport-CC报文到达发送端的时间；另一个是经过平滑后的累积延时，它是通过前面计算得到的延时和历史累积延时加权平均计算之后得到的。这样也可以一定程度上防止延时波动太大的问题。</p><p>在WebRTC看来，如果设RTP包组所属的Transport-CC报文的到达时间为x，累积延时为y的话，那么x和y应该是呈线性关系，也就是说y = kx + b。Trendline Filter就是使用20个延时数据，通过线性回归的方法，求得其中的k值，也就是斜率。这个斜率就表示延时变化的趋势。其中线性回归的求解方式用的是最小二乘法。示意图和求解公式如下：</p><p><img src="https://static001.geekbang.org/resource/image/f8/71/f808d5b35039cfd1fb189980e3f65771.jpeg?wh=1084x892" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/27/f2/2735c395c7c7a0c0733061cf39b125f2.jpeg?wh=894x398" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/98/96/98a1b9614be82fbc40a29402d3f44196.jpeg?wh=1584x1352" alt="图片"></p><p>当斜率k &gt; 0时，表示有延时；当k = 0时，表示几乎没有延时；当k &lt; 0时，表示不仅没有延时，反而接收速度更快。</p><p>当然了，我们计算出来的延时变化趋势还不能直接说明网络当前的变化方向。因为网络是变化无常的，不能因为测到延时稍有变大就认为网络变差，延时稍有变小就认为网络变好。我们需要一个根据当前延时趋势和延时阈值，来判断网络是不是真的变好和变坏的模块，这个模块就是我们下一个讨论的重点，叫做过载检测器。</p><p><strong>网络状态判断</strong></p><p>过载检测器有两个主要的工作：一个是通过当前的延时趋势和延时阈值来判断当前网络是处于过载、欠载还是正常状态；一个是通过当前的延时趋势来更新延时阈值，是的，延时阈值不是静态不变的，阈值是跟着延时趋势不断自适应调整的。</p><p>我们下面先来看看网络状态的判断，之后再来讨论一下阈值的更新。</p><p>网络状态的判断其实比较简单。就是将延时趋势k乘以一个固定增益4和包组的数量（包组数量最大是60）作为当前的修改后延时值。将当前的修改后延时值跟延时阈值进行比较，然后根据比较的结果来判断网络状态。步骤如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/8d/d4/8d55ef099f40f3ebf8e74a25f09cdbd4.jpeg?wh=1920x1080" alt=""></p><p>得到网络状态之后，我们会使用当前的修改后的延时值去更新当前的延时阈值。为什么需要这样做呢？</p><p>我认为，延时阈值的更新是因为网络是不断变化的，延时变化也很快，而有的时候延时很大，有的时候又很小。为了防止阈值太大，网络状况检测不够灵敏，同时也防止阈值太小，网络状况检测太敏感了。所以<strong>延时阈值会随着当前的延时做缓慢的调整</strong>。其调整的公式如下：</p><p><img src="https://static001.geekbang.org/resource/image/67/f2/677043013895576352140650b0d0yyf2.jpeg?wh=1640x552" alt="图片"></p><p>有了修改后延时和延时阈值之后，我们就可以计算网络状态了。又根据网络所处的具体状态，我们就可以调整更新当前的带宽值了。而负责更新带宽值的模块就是速率控制器。下面我们就来详细讨论一下如何将网络状态转化到实际带宽的调整上去的吧。</p><p><strong>带宽调整更新</strong></p><p>速率控制器的主要工作就是更新预测带宽值。它里面维护着一个状态机。这个状态机主要是用来干什么的呢？其实，<strong>状态机主要用来根据过载检测器检测到的网络状态和状态机目前所处的状态，来更新状态机的状态的。</strong></p><p>状态机有三个状态，分别是上升、保持和下降状态。当处于上升状态时，速率控制器需要提升带宽值；当处于下降状态时，需要降低带宽值；当处于保持状态时，则不更新带宽值。我们通过下图来看一下过载检测器检测到的网络状态是如何影响状态机状态变化的。</p><p><img src="https://static001.geekbang.org/resource/image/90/99/90dedbd2666a8d402a7bc42a1cfde899.jpeg?wh=1920x1080" alt=""></p><p>我们可以看到，当过载检测器检测到过载时，状态机都切换到下降状态。这个很好理解，因为过载意味着实际带宽值小于发送码率了，需要调低带宽值，继而发送码率因为预估带宽值下降了也会下降。</p><p>当过载检测器检测到正常时，状态机都向上调一个状态。什么意思呢？</p><p>就是如果之前状态机处于下降状态，则更改为保持状态；如果状态机之前处于保持状态，则更改为上升状态；如果是上升状态那就不用变化了。这个也很好理解，因为过载检测器检测到正常，说明当前实际发送数据量还没有达到网络的最大承受量，则可以继续调高预估带宽值，或者之前在降低带宽值的话，就不再继续降低预估带宽值了。</p><p>当过载检测器处于欠载的状态时，状态机全部切换到保持状态。这个有点不好理解。欠载了不是需要直接提高预估带宽值的吗？为什么保持带宽不变呢？</p><p>这是因为WebRTC认为欠载主要发生前面因为过载了在网络设备中缓存了一定数据，之后网络状况变好了的时候，网络设备可以快速的发送完缓存中的数据，从而排空缓存。这个时候不要提高发送码率，等缓存排空之后，因为缓存带来的延时就会接近于0了，这对于降低端到端延时是很有用的，而过载检测器自然就会进入到正常状态。这样状态机就可以切换到上升状态，从而也就可以调高预估带宽值了。</p><p>好了，我们讲解了什么时候需要调高、什么时候需要降低带宽值。但是我们还没有讲到底调高多少以及降低多少。下面我们就来介绍一下具体调节预估带宽的公式。</p><p>带宽调整更新逻辑中，上调带宽逻辑比较复杂。因此，我们先介绍相对简单点的下降带宽调整。下降带宽其实很简单，就是预估带宽等于0.85倍的接收码率。</p><p><img src="https://static001.geekbang.org/resource/image/26/46/269be1c1b3564f8b7381ef8d8fde6b46.jpeg?wh=1901x344" alt=""></p><p>但是下降带宽的时候需要做另外一个事情就是更新当前网络的最大带宽。因为处于下降带宽的过程中，说明当前发送数据量已经达到甚至超过了网络的承受能力。这个时候适合更新网络的最大带宽，将当前的接收码率与之前的最大带宽做加权平均求得当前的最大带宽，并更新最大带宽的标准差。这两个值之后调高带宽的时候需要用。</p><p>这个地方需要提一下，为什么预估带宽和最大带宽都是用接收码率来来计算的，那是因为发送码率可能会超出网络承受能力，不一定能很好的反映真实的网络带宽，相比而言，<strong>接收端的接收码率更能够表示真实的网络带宽。</strong></p><p>在上升带宽的时候，是有两种情况的。</p><p>当前接收码率离最大带宽比较远，具体就是当前接收码率大于最大带宽加上3倍的最大带宽标准差，或者小于最大带宽减去3倍的标准差。这个时候应该快速调高预估带宽，我们使用的是乘性增的方式增加带宽值，就是每隔一秒钟将预估带宽乘以1.08倍。但是为了防止带宽大幅增长，预估带宽不能大于1.5倍的接收码率。</p><p>当前接收码率离最大带宽很近，具体就是当前接收码率大于最大带宽减去3倍标准差，并小于最大带宽加上3倍标准差。这时使用更小的幅度增加带宽，使用加性增的方式增加带宽值。预估带宽是每隔一个RTT（往返时间，由其它模块计算得到，这里不展开讲）增加半个包的大小。如果一个包按照1400字节算的话，那就是每过一个RTT的时间就增加700字节。</p><p><img src="https://static001.geekbang.org/resource/image/d8/9a/d8013889966de404aa2a24710afcd89a.jpeg?wh=1920x712" alt=""></p><p>好了，以上就是基于延时的带宽预测算法。</p><h3>基于丢包的带宽预测算法</h3><p>前面我们讲过，网络设备主要有两种：一种是有较大缓存的；一种是没有缓存或者缓存很小的。有较大缓存的网络设备在遇到数据量太大的时候会把数据放在缓存中，延迟发送。其表现就是网络延时加大。因此这种情况下我们基于延时做带宽预测是比较准确的。</p><p>而没有缓存或者缓存很小的网络设备，在遇到数据量太大的时候是直接将数据丢弃的，这会引起丢包率上升。为了能够更快更准确地做带宽预测，WebRTC针对这种情况设计了基于丢包的带宽预测算法。</p><p>基于丢包的带宽预测算法相比基于延时的带宽预测算法简单很多，没有那么多步骤。<strong>其整体思路就是根据Transport-CC报文反馈的信息计算丢包率，然后再根据丢包率的多少直接进行带宽调整更新。</strong></p><p><strong>丢包率的计算</strong></p><p>接收端会将接收到的每一个包的信息放到Transport-CC报文中，包括每一个RTP包的序号以及这个包有没有接收到。而没有接收到就代表这个包丢失了。这样就很简单了，发送端收到这个Transport-CC报文之后只需要统计这个Transport-CC报文总共有多少个包，以及丢失了多少个包，就可以计算得到丢包率了。</p><p><img src="https://static001.geekbang.org/resource/image/ee/a6/ee56c2c259b1535ffcba1ecaf49047a6.jpeg?wh=822x324" alt="图片"></p><p><strong>带宽调整</strong></p><p>有了丢包率之后，我们就可以做带宽调整了。根据丢包率可以分三种情况。具体如下：</p><ul>\n<li>如果丢包率 &lt; 2 %，认为当前网络状况很好，需要调高带宽值，带宽值等于过去1秒钟所有预测得到的带宽值的最小值 * 1.08；</li>\n<li>如果2% &lt; 丢包率 &lt; 10%，认为当前网络状况正常，不做带宽调整；</li>\n<li>如果丢包率 &gt; 10%，认为网络状况不好，需要降低带宽值，带宽值等于当前预估带宽值 * (1 - 0.5 * 丢包率)。</li>\n</ul><p>如果不太明白，没关系，下面的图能让你一目了然。</p><p><img src="https://static001.geekbang.org/resource/image/cf/86/cf473ee753cc4c10a61980e4d13a4e86.jpeg?wh=1912x740" alt=""></p><p>这就是基于丢包的带宽预测算法。</p><p>现在我们有了基于延时的带宽预测算法，主要用于有大缓存网络设备存在时的带宽预测；同时也有了基于丢包的带宽预测算法，主要用于有小缓存或无缓存网络设备存在时的带宽预测。由于互联网中一般是两种网络设备都有，因此，最终的预估带宽值等于这两者中的最小值。</p><p><strong>到这里，我们的带宽预测算法最核心的部分就完成了。</strong>但是不知道你有没有发现一个问题，就是我们在网络变差的时候，预估带宽会快速的被下调，但是网络变好的时候预估带宽会比较缓慢的上升，同时如果当前发送码率比较小的话，预估带宽还会被限制，不能超过1.5倍的接收码率。</p><p>也就是说，如果我们当前视频处于静止画面的状态，发送的码率会很小。这样预估带宽就很难从一个比较小的带宽调整上去。如果此时画面突然动起来，即便实际网络带宽足够，还是会因为预估带宽不够而限制发送码率，从而导致画面出现模糊和马赛克等问题。</p><p>还有就是程序刚开始启动的时候，预估带宽的爬升也需要慢慢的加上去。但是在做音视频通信的时候，有可能一开始就会出现视频画面变化比较快的情况，这样可能刚开始的时候视频画面就会是模糊的或者有很多马赛克。</p><p>这种问题有没有方法解决呢？答案是有的，下面我们就来讲讲第三种带宽预测的算法，它可以帮助我们快速的探测出当前网络的最大带宽。</p><h3>最大带宽探测算法</h3><p>最大带宽探测算法相对来说思路也比较简单。它的主要过程是：</p><p>第一步，发送端设定一个探测的目标带宽，一般设置为当前带宽的2倍、3倍或者6倍。发送端在发送数据的时候就以这个探测目标带宽的速度快速发送RTP包，一般发送时间（也叫做探测时间）是15ms，同时将这段时间用于探测使用的RTP包信息保存在发送端（前面基于延时的带宽预测的时候，也需要保存发送的RTP包的发送时间等信息，其实都是保存在发送历史数据里面，只是探测的RTP包会多保存一些信息），并给这些RTP包标上是探测包的标记以及探测的cluster_id，每一次探测使用的cluster_id都不同，用于区分每次探测的RTP包，防止多次探测时弄混了。</p><p><img src="https://static001.geekbang.org/resource/image/04/b0/0499e00a594ae7436e8a9bb80a6c60b0.jpeg?wh=1920x1080" alt=""></p><p>第二步，接收端并不关注当前包是不是探测包，而是直接统计每一个包的序号和接收时间，将统计结果组成Transport-CC报文反馈给发送端。</p><p>第三步，发送端接收到Transport-CC报文之后，会看报文中的每一个包是不是探测包，如果是探测包，就从发送端发送的历史数据中，取出其cluster_id和发送时间，并且从Transport-CC报文中得到接收时间，再把这些信息送入到探测带宽计算器中，当探测带宽计算器中相同cluster_id的RTP包信息数量达到一定值之后，就可以计算最终探测到的带宽值了。计算方法如下：</p><ul>\n<li>首先，根据发送的探测RTP包总大小，和探测RTP包的总发送时间，相除就得到了发送端的最大带宽值了。</li>\n<li>然后，再根据接收端接收到的探测RTP包总大小，和探测RTP包的总接收时间，相除就得到了接收端的最大带宽值了。</li>\n<li>之后，取两者中的最小值作为探测到的网络最大带宽值。同时，如果接收端的最大带宽小于0.9倍的发送带宽，说明当前探测目标带宽已经到了最大带宽了，为了保险一些，最后将最大带宽取为接收端最大带宽的0.95倍。</li>\n<li>最后，如果当前过载检测器检测不是在过载状态的话，预估带宽值更新为探测到的最大带宽值。<br>\n具体如图所示：</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/80/0f/80213731d8e28b36c80fa71094fbe30f.jpeg?wh=1920x1080" alt=""><img src="https://static001.geekbang.org/resource/image/be/39/be46e15e04377f85b8b6fcd7df8c0f39.jpeg?wh=1920x1080" alt=""></p><p>这就是最大带宽探测算法，它一般是在程序刚开始启动的时候使用并在程序运行的过程中进行周期性的探测，每隔一段时间定时探测一下。其实如果你自己设计带宽预测算法的话，也可以在实际发送带宽很小的时候探测一下，防止出现因为实际发送码率很小，而无法准确预测网络当前最大带宽值的问题。</p><h2>小结</h2><p>为了帮助你记忆，这里我用图帮你总结了一下这节课的知识点。</p><p><img src="https://static001.geekbang.org/resource/image/8e/69/8ea064dfb262226a239240c22d1f0969.jpg?wh=1598x1222" alt=""></p><p>WebRTC中带宽预测主要分为基于延时的带宽预测算法、基于丢包的带宽预测算法以及最大带宽探测算法。</p><p>基于延时的带宽预测算法主要是解决网络中含有大缓冲网络设备场景的带宽预测。基于丢包的带宽预测算法主要是解决网络中有小缓冲或无缓冲网络设备场景的带宽预测。最终预估带宽等于这两者预测到的带宽值中的最小值。</p><p>同时，为了防止出现发送码率大幅低于实际网络带宽而导致网络带宽预估偏低的问题，我们还引入了最大带宽探测算法，可以周期性的探测网络的最大带宽。如果当前网络不是处于过载状态同时又探测到了最大带宽的话，就将预估带宽更新为探测到的最大带宽。</p><h2>思考题</h2><p>对于视频来说，带宽预测的最终目的是什么？</p><p>你可以把你的答案和感受写下来，分享到留言区，与我一起讨论。我们下节课再见！</p>',
        article_title: "10｜带宽预测：3大算法准确预估网络带宽",
      },
      {
        title: "11｜码控算法：如何控制视频的编码码率？",
        id: 468091,
        content:
          '<p>你好，我是李江。</p><p>上一节课我们讲了带宽预测算法，学习了如何去预测无时无刻不在变化着的网络带宽。准确的预测带宽是实时视频通话技术里面的一个非常重要的环节。</p><p>如果不能够很好地预测出实际带宽，那有可能引起数据超发，导致发送数据量大于实际网络的承受能力，继而引起视频画面的延时和卡顿；也有可能预测的带宽太低，导致发送的数据量远低于实际网络的承受能力，不能很好地利用网络带宽，最终导致视频画面模糊和很明显的马赛克现象。因此，一个好的带宽预测算法是至关重要的。</p><p>当然，好的带宽预测算法还只是开始，如何在预测出带宽之后能够控制数据的发送码率，使其尽量符合当前的网络带宽也是非常重要的。如果你没有做好发送码率的控制，想发送多少数据就发送多少数据的话，那跟没有网络带宽预测是一样的效果。要不就画面卡顿，要不就很模糊。</p><p>因此，这节课我们就接着来讲讲与网络带宽预估算法一样重要的另外一个算法——视频码率控制算法。我们会先简单看一下码控算法的原理和类型，然后再重点讲解其中最难也是在RTC场景中最重要的CBR码控算法。我们会非常详细地剖析CBR算法的原理，讲解它是如何一步步尽量做到恒定码率的。</p><p>通过下面的图你可以清楚地了解码控算法在整个发送端流程中的位置和重要性。</p><!-- [[[read_end]]] --><p><img src="https://static001.geekbang.org/resource/image/39/8d/3907ba972e8bc9f46b6055f533e8708d.jpeg?wh=1913x658" alt="图片"></p><p>好了，下面我们首先来了解一下码控的原理和基本类型。</p><h2>码控的原理</h2><p>码控，顾名思义，就是码率控制，它是编码器的一个重要模块，主要的作用就是用算法来控制编码器输出码流的大小。虽然它是编码器的一个非常重要的部分，但是它并不是编码标准的一部分，也就是说，标准并没有给码控设定规则。我们平时用的编码器的码控都是编码器程序自己实现的。</p><p>那码控的原理是什么呢？<strong>其实码控就是为每一帧编码图像选择一个合适的QP值的过程。</strong></p><p>我们知道当一帧图像的画面确定了之后，画面的复杂度和QP值几乎决定了它编码之后的大小。由于编码器无法决定画面的复杂度，因此，<strong>码控的目标就是选择一个合适的QP值，以此来控制编码后码流的大小。</strong>当然有些码控算法是可以直接外部指定使用哪个QP值去编码的，就不需要编码器的码控算法去做决策了。但是最后的原理是一样的。那接下来我们就来看一下都有哪些码控算法吧。</p><h2>码控的类型</h2><p>常用的码控算法主要有：VBR（动态码率）、CQP（恒定QP）、CRF（恒定码率因子）和CBR（恒定码率）这几种。</p><h3><strong>VBR</strong></h3><p>VBR指的是编码器输出码率随着原始视频画面复杂度的变化不断的变化。通常当画面复杂或者说运动比较多的时候使用的码率会比较高；而当画面比较简单的时候使用的码率会比较低。VBR主要的目标是保证视频画面质量，因此比较适合视频点播和短视频场景使用。</p><h3><strong>CQP</strong></h3><p>CQP很简单就是从头到尾每一个画面都是用同一个QP值去编码。根据我们视频编码的课程可知：</p><ul>\n<li>在画面复杂的时候，残差比较大，相同QP值做量化之后的残差还是比较大的，编码之后的图像大小就会比较大。</li>\n<li>而画面简单的时候，残差很小，同一个QP值量化之后残差可能很小，甚至都为0了，编码之后的大小就会很小。<br>\n其实，我个人觉得CQP是一种特殊的VBR。但要注意的是CQP一般用来衡量编码算法的性能，在实际工程当中不会使用。</li>\n</ul><h3><strong>CRF</strong></h3><p>CRF是x264默认的码控算法。它与CQP不同的是它的QP是会变化的。在画面运动大的时候，它会根据具体算法提高QP值；在画面运动小的时候，它会降低QP值。</p><p>它的思想是：运动很大的时候，人眼不太关注细节，因此QP可以稍微大一点；运动比较小的时候，人眼会将注意力放在细节上面，因此QP稍微小一点。所以相比CQP，CRF能够更省码率一些。但是CRF码控总体上得到的编码后图像的大小，还是随着图像的画面复杂度在变化的。因此，我觉得CRF也算是一种特殊的VBR。</p><h3>CBR</h3><p>另外一种码控算法就是CBR了，它是恒定码率的。<strong>这种码控方式用户需要设置一个目标码率值给编码器。</strong>编码器在编码的时候不管图像画面复杂或简单、运动多或运动少的时候，都尽量使得输出的码率接近设置的目标码率。</p><p>这种方式非常适合RTC场景，因为RTC场景希望编码的码率跟实际预测的带宽值接近，不能超出目标码率太多，也希望能够尽量有效地利用可用带宽，不能太低于目标码率，从而尽量保证编码后图像画面清晰。</p><p>因此，在RTC场景中，我们会将预估带宽分出一定比例给视频数据，并将这部分带宽值当作目标码率设置给编码器。需要编码器的码控算法，能够在各种网络状况下和各种画面变化的情况下，都能使得输出的码率尽量接近于当前预估带宽得到的目标码率。</p><p>相信你光是看到这个描述就知道非常困难了。所以我们前面说了，CBR是很重要但也是非常难的一种码控算法。那CBR到底怎么做到的呢？我们就来详细讨论一下。</p><h2>CBR算法</h2><p>其实，为了实现恒定码率，我们需要做很多个步骤，一步步的将输出码率逼近目标码率，而不是一步到位确定QP就可以实现恒定码率的目标的。所以，我们会分很多级做调整，分别是帧组级、帧级、宏块组GOM（Group of MB）级。具体如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/40/06/40b702afbd985bbf84ab90cb48825306.jpeg?wh=1920x1080" alt="图片"></p><p>具体的操作过程如下：</p><ul>\n<li>先确定帧组级（帧组就是将连续的几个帧组成一组，一般选择8个帧一组）的输出大小尽量接近目标码率。</li>\n<li>然后，确定组内的每一帧具体应该分配多少的大小（称之为目标帧大小），才能保证帧组最后输出的大小可以达到要求。</li>\n<li>接下来，我们再根据这个目标帧大小，确定一个帧级的QP值。</li>\n<li>之后，我们再确定帧内的宏块组（宏块组就是连续的几行宏块组成的一组宏块，一般可以选4行宏块）应该分配多少大小，来保证当前帧最后的输出大小能接近于目标帧大小。</li>\n<li>最后，我们再确定宏块的QP值。<br>\n还有一个很重要的事情，就是我们需要能够保证在不同的画面复杂度和不同的运动程度的情况下，并且输出码率都要尽量接近目标码率的话，我们还需要先计算得到当前帧的复杂度。</li>\n</ul><p>简单来说，这个复杂度是能够大概衡量当前帧在做完预测之后残差值的总体大小的。当然，我们并不是真正去做预测得到残差的，而是通过一些算法近似估算一下残差的大概大小的。因为残差的大小和QP值决定了最后图像编码后的大小。</p><p>同时，在这里说明一下，因为我们主要讲解RTC下的CBR码控，所以我们只考虑I帧和P帧，不考虑B帧。等你理解了这些知识之后呢，你再去学习更复杂的CBR码控算法就会更轻松一些。</p><p>好了，那我们接下来就先讲讲如何计算图像的复杂度，之后我们再依次讨论一下如何在帧组级、帧级、宏块组GOM级别做码控操作，最后得到宏块的QP值。</p><h3><strong>复杂度求解</strong></h3><p>根据帧类型复杂度求解可以分为两种算法：第一种就是I帧的复杂度计算；第二种就是P帧的复杂度计算。</p><p>I帧只做帧内预测，而帧内预测是用编码块周围已编码的像素来预测当前编码块的像素值的。因此，方差是一个比较能够表示I帧复杂度的值。</p><p>因为方差越大，表示帧的内部变化程度越剧烈，而你用周围的像素去预测当前编码块的像素值的话，有很大的可能会产生较大的残差。而方差越小的话，说明帧内部变化比较小，因此周围像素有较大的概率能够比较好的预测出待编码块的像素值。因此，<strong>我们计算I帧的复杂度的时候，是求每一个宏块<strong><strong>的</strong></strong>方差，最后将帧的所有宏块的方差之和作为帧的复杂度</strong>。具体求解过程如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/83/ba/83c4ea155a6d1e15f152887e3655ecba.jpeg?wh=1919x889" alt="图片"></p><p>而P帧，主要是做帧间预测。我们知道，帧间预测就是去参考帧中找一个块来作为当前帧编码块的预测块，因此，我们选择使用将当前帧的宏块减去参考帧对应位置的宏块，求SAD值，并将所有宏块的SAD值加起来作为P帧的复杂度。具体求解过程如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/75/f5/755e9d210c2acf3031c83058f29117f5.jpeg?wh=1920x1080" alt="图片"></p><p>当然，我们会保存记录下I帧和P帧内部每一个宏块的复杂度值，这是因为后面还有地方会使用到。</p><h3><strong>帧组级</strong></h3><p>CBR虽然是恒定码率，但它的意思是保证一段时间内的输出码率接近目标码率，比如说1秒或者几百毫秒，而不是保证每一帧输出都严格接近目标码率的。</p><p>这是因为算法没办法做到每一帧都这么精确。算法是根据一段时间内前面已经编码的结果来调节还未编码帧的QP，从而来达到一组帧的输出大小尽量接近目标码率的。因此，我们在开始的时候，需要根据目标码率来确定帧组的目标大小，之后再确定帧组内每一帧的目标大小。</p><p>我们先根据设定的目标码率和帧率值将两者相除，就可以计算得到每一帧的平均大小。然后我们将帧组的帧数（一般8个帧作为一组）乘以帧的平均大小，就是帧组的目标大小了。</p><p>在编码器刚开始编码的时候，帧组的剩余大小就是帧组的目标大小。当编码帧组中第一帧的时候，我们将帧组的剩余大小除以帧组的帧数，就得到帧组中第一帧的目标帧大小。当帧组中的第一帧编码完成之后，我们需要用第一帧的实际编码后的大小来更新帧组的剩余大小。</p><p>很简单就是将帧组的剩余大小减去第一帧编码后的实际大小。然后，第二帧的目标帧大小就是等于更新后的帧组的剩余大小除以帧组的剩余帧数。随着帧组中的一帧帧不断编码，我们不断更新帧组的剩余大小，不断调整帧的目标大小。</p><p>具体计算过程可以参考下图：</p><p><img src="https://static001.geekbang.org/resource/image/74/db/74f2d475cb72e6728a16cc01c14600db.jpeg?wh=1920x1080" alt="图片"></p><p>你可以很清楚地看到，如果帧组中的前面帧编码后的大小超出平均帧大小的话，后面帧的目标帧大小就会小于平均帧大小，也就是说，前面帧用多了就从后面帧里面扣。同样地，如果前面帧用少了，就补给后面的帧。这样是不是就能保证帧组的最后编码输出码率尽量接近帧组的目标码率了？</p><p>举个例子，就像是你一个月有3000零花钱，平均每天100元。前面10天你已经用了2000了，那后面20天你每天平均只能用50，要省着点花。如果你前面10天只用了500，那后面20天平均每天你可以用125，可以大方点花。帧组分配帧目标大小也是这个道理。</p><h3><strong>帧级</strong></h3><p>有了帧组级别码控中计算得到的目标帧大小之后，我们就能够计算当前帧的SliceQP了（我们这里为了讲述原理尽量简单清晰，只考虑一帧一个Slice，多Slice原理是一致的，就不展开讲了）。那怎么求呢？</p><p>我们根据前面计算得到的当前编码帧的帧复杂度和目标帧大小，再加上前面已经编码完成了的帧的复杂度和编码使用的QStep（与QP一一对应，请参考视频08里面的表格）以及使用这个QStep编码之后实际的编码大小来计算。公式如下：</p><p><img src="https://static001.geekbang.org/resource/image/78/70/78080b6f31a436699b8aaece382ba670.png?wh=1920x896" alt="图片"></p><p>其中I帧和P帧使用不同的公式，因为复杂度的计算方式不一样。</p><p>上面的公式是什么意思呢？其实大体的思想就是：一帧编码后的大小应该是和帧的复杂度成正比的，并且跟帧使用的QStep是成反比的。但是具体成多少比例怎么知道呢？</p><p>其实呢我们不知道，但是我们可以根据前面已经编码好了的帧估算一下。我们先大体计算一下，它们这些帧的复杂度和QStep跟最终的编码大小大概成多少比例。然后再使用这个比例来估算在当前帧的复杂度下，我们大概需要使用多少的QStep能使得输出的大小尽量接近目标帧大小。</p><p>我们通过上面的公式就计算得到了当前编码帧的QStep了，再通过08那节课里面的表格就可以转换成相应的SliceQP了。</p><p>其实，到这里我们就可以用SliceQP值去编码每一个宏块了。比如像VP8编码，它没有宏块级别的QP值，到这里码控就确定了最终QP了。但是H264还可以在宏块级别调整宏块的QP，因此，为了更精细化地调节码率，我们还可以根据已经编码宏块的实际使用的大小来调整未编码宏块的QP。这里就是我们前面提到的宏块组概念了，也就是GOM。</p><h3><strong>GOM级</strong></h3><p>首先，在开始编码一个GOM之前，我们需要计算一下帧的实际剩余大小和帧的目标剩余大小。帧的实际剩余大小是用帧的目标大小减去帧中已编码GOM的实际大小。我们再使用帧的实际剩余大小加上前一个GOM的实际编码大小，减去该GOM的目标大小，就是帧的目标剩余大小。</p><p>这个地方我解释一下，帧的实际剩余大小加上GOM的实际编码大小，就是去掉前一个GOM的目标大小，再减去前一个GOM的目标大小，就是当前的帧目标剩余大小了。</p><p>具体计算过程如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/34/01/348613c46496b9ff1656e291e5e99201.jpeg?wh=1920x1080" alt="图片"></p><p>我们将帧的实际剩余大小除以帧的目标剩余大小：</p><ul>\n<li>如果这个比例大于1，说明我们剩余的大小多了，之后的GOM可以将QP调低一些，我们将后面的GOM中的宏块QP值减去1或者2即可；</li>\n<li>如果这个比例小于1，说明我们剩余的大小少了，之后的GOM的QP需要调高一些，我们将后面的GOM中的宏块QP值加1或者2即可。<br>\n也就是说，通过这个计算之后，我们就得到了GOM中所有宏块的QP值了。然后，我们再根据这个QP值去编码每一个宏块。</li>\n</ul><p>到这里我们还有一个步骤需要做，就是<strong>需要计算一下当前GOM的目标大小，以备下一个GOM编码的时候做GOM级码控计算的时候使用。</strong></p><p>GOM的目标大小是通过当前GOM的复杂度、当前帧剩余GOM的复杂度之和以及帧的剩余大小来计算的。计算公式如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/7f/c5/7f0a2864c5d56ce1edcfc765669920c5.jpeg?wh=1920x387" alt="图片"></p><p>我们是看当前GOM的复杂度占剩余GOM总复杂度的比例来分配目标大小的。其中，GOM的复杂度的值用前面复杂度计算时记录保存的宏块复杂度的值来计算。</p><p>其实，我们还可以通过每一个宏块调整一下QP的方式来做进一步精细化的调节，但是这个内容有点复杂了，等你学好了这节课之后，我们之后有机会再来深入讲解一下。这里就不展开讲解了。</p><p>对于CBR码控算法的整体流程，我用下面的图帮你总结了一下，方便你理解和记忆。</p><p><img src="https://static001.geekbang.org/resource/image/b6/40/b6ff3142562a9a77644e79832ba80240.jpeg?wh=1920x1080" alt="图片"></p><h2>小结</h2><p>这节课我们主要讨论了码控算法，带你了解了一下码控算法的原理和基本类型。码控主要是为每一帧图像确定QP值的过程。如果在图像画面确定的情况下，并且QP值确定了的话，那当前图像编码后的大小就大致确定了，从而编码后的码率大小也基本确定了。</p><p>同时，常用的码控算法主要有CQP、CRF、VBR和CBR。并且，我们还对CBR进行了深入地探讨。我们知道了CBR主要分为：帧组级、帧级和GOM级三个级别的调整，并通过一步步不断精细化的调整最后尽量达到恒定码率的目标。</p><p>你可以通过下面的图来对这节课加强理解和记忆。</p><p><img src="https://static001.geekbang.org/resource/image/40/15/40d69eb2b6479480bf913166c78e0515.jpeg?wh=1917x881" alt="图片"></p><h2>思考题</h2><p>第一个I帧和第一个P帧的QP值怎么确定呢？因为在它们前面没有已经编码好了的I帧和P帧。</p><p>你可以把你的答案和疑惑写下来，分享到留言区，与我一起讨论。我们下节课再见！</p>',
        article_title: "11｜码控算法：如何控制视频的编码码率？",
      },
      {
        title: "12｜Jitter Buffer：拿什么拯救你，花屏卡顿问题",
        id: 469197,
        content:
          '<p>你好，我是李江。</p><p>之前的两节课我们讲述了如何做好带宽预测和码率控制。好的带宽预测算法能够比较准确地预测出实际的网络带宽，而好的码率控制算法能够使得编码输出码率贴合实际网络带宽。这两个算法是视频流能够在各种网络状况下流畅播放的最基础的前提。</p><p>但是在实际情况中，很多时候我们还是会遇到各种各样的卡顿和花屏的问题。相信这两种问题你也是经常遇到，同时也是让你很头痛的问题。</p><p>那么，我们这节课就来讨论一下，一般哪些环节出问题会比较容易出现卡顿，以及哪些环节有问题会比较容易出现花屏。并且，我也会给出一些常用的解决方案。通过这节课的学习，你之后再遇到花屏卡顿问题的时候，可以参考一下，从而能够快速发现问题和找到解决的方法。</p><p>在讲述具体问题之前，先简单介绍一下Jitter Buffer这个模块。它是好几个卡顿和花屏问题的处理模块。Jitter Buffer工作在接收端，主要功能就是在接收端收到包之后进行组帧，并判断帧的完整性、可解码性、发送丢包重传请求、发送关键帧请求以及估算网络抖动的。其中组帧、判断帧完整性、判断帧可解码性、丢包重传、关键帧请求都是这节课的重点。因此，我先在这里重点说一下Jitter Buffer这个模块。Jitter Buffer在接收端所处的位置如下图所示：</p><!-- [[[read_end]]] --><p><img src="https://static001.geekbang.org/resource/image/79/ff/79bfdabc3eb02989fe5ec25d5d616cff.jpeg?wh=1918x447" alt="图片"></p><h2>卡顿问题</h2><p>视频卡顿是在实时视频通话场景中非常重要的一个问题。卡顿率也是实时通话场景中一个非常重要的指标。一般来说，人眼在帧率达到10fps并且均匀播放时就不太能看出来卡顿了。<strong>如果两帧之间的播放时间间隔超过了200ms，人眼就可以明显看出卡顿了。</strong>那一般什么情况下会导致两帧之间的播放时间间隔超过200ms呢？我们下面一个个来分析一下，并给出相应的解决方法。</p><p>接下来我们按照下图中采集到渲染这条链路中每一个可能引起卡顿问题的原因依次来介绍。</p><p><img src="https://static001.geekbang.org/resource/image/b6/b8/b60689222d6ccfdf92b1bb3cb9d47cb8.jpeg?wh=1919x682" alt="图片"></p><h3>帧率不够</h3><p>如果实际采集到的帧率或者设置的帧率本身就只有5fps，即便是均匀播放，两帧之间的间隔也会达到200ms，那么这种情况下肯定会出现卡顿。画面看起来就像是快速播放的PPT。这种情况下相信你能明显地看出来卡顿的原因。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/fa/19/fa167bb1b93f286c7854249181587719.jpeg?wh=1918x915" alt="图片"></p><p>这个问题最好的解决方法当然就是提高帧率了，比如提高到15fps或者更高。当然有的时候采集帧率就是上不来，那我们就要定位一下，采集帧率不高的具体原因是什么。</p><h3>机器性能不够，导致前处理或者编码耗时太长</h3><p>在实时通话场景中，画面是需要实时地做前处理（美颜等操作）并编码之后发送到对端进行解码播放的。如果本身机器性能不够，而画面分辨率又很高，那么这可能会导致前处理一帧或者编码器编码一帧的耗时很高。如下面两张图所示：</p><p><img src="https://static001.geekbang.org/resource/image/d7/f7/d7bbb88e7af5bf7e63ef8fa8ec56dcf7.jpeg?wh=1920x809" alt="图片"><img src="https://static001.geekbang.org/resource/image/c4/83/c4ab10c0602d0ffbb824b9f550f4ec83.jpeg?wh=1914x941" alt="图片"></p><p>这种情况下，即便是采集的帧率很高，但是前处理和编码操作机器处理不过来，从而最后导致两帧被发送出去的间隔也会很高，这时发送到对端，对端就可能会出现明显的卡顿。<strong>这种情况在比较老的手机上特别容易出现。</strong></p><p>当出现这种情况的时候，我们可以在高分辨率的时候尽量使用GPU做前处理，并使用硬件编码或者将软件编码设置为快速档加快处理的速度。GPU做前处理和硬件编码消耗CPU比较小，并且速度更快。软件编码设置为快速档时很多费时间的编码工具都被关闭了，因此可以提高编码的速度。不过这里你需要注意一下，就是这样<strong>压缩率也会下降</strong>。</p><h3>编码器输出码率超过实际网络带宽</h3><p>这种情况是RTC实时通话场景中卡顿问题最常见的根因。当出现的时候往往会引起比较长时间的卡顿，有可能持续1～2秒钟时间。有的时候，网络突然变差，从而网络预估出来的带宽很小，但是实际播放的画面很复杂，且需要的编码码率又比较高，这样就比较容易出现发送码率大于实际带宽的问题。</p><p>我们在<a href="https://time.geekbang.org/column/article/467073">带宽预测</a>的那节课中讲过，当发送码率大于实际带宽的时候，对于有缓冲区的网络设备，它一开始会将包放在缓冲区，且当缓冲区放不下了还是会丢包。</p><p><img src="https://static001.geekbang.org/resource/image/e5/94/e51884ea10b69148c21a7944e60f9d94.jpeg?wh=1917x826" alt="图片"></p><p>而对于没有缓冲区的网络设备，它是直接就丢包。</p><p><img src="https://static001.geekbang.org/resource/image/78/d0/7896f61406dfe0377d8e3319d0d8dbd0.jpeg?wh=1920x752" alt="图片"></p><p>当包被丢弃了，对端就不能完整地恢复出一帧图像了。而我们知道，当一帧图像不能解码，那么之后所有参考它的图像就都不能解码了。并且，在RTC场景中，我们一般使用连续参考的参考帧结构，就是后面的P帧参考它的前一帧，这也就会导致在下一个IDR帧到来之前画面都会卡死。这样卡顿的时间就会很长。如果出现这种问题怎么办呢？</p><p><strong>我们需要对发送码率做严格的限制，防止它超过预估带宽。</strong>这就需要编码器的输出码率要能够贴合预估带宽。也就是说，我给编码器设置多少编码码率，编码器最好就编码出多少码率。</p><p>回顾一下我们在<a href="https://time.geekbang.org/column/article/468091">第11讲</a>中讨论的内容，那是不是我们应该选择CBR的码控算法呢？是的，<strong>在RTC视频通话场景下我们最好选择CBR的码控算法，从而保证输出码率能够比较好地贴合预估带宽。</strong></p><p>如果使用VBR码控算法，编码器的输出码率会随着画面的复杂程度变化，那就会有很大的概率因为画面复杂而出现输出码率超过预估带宽的情况，从而导致对端出现严重的卡顿。而CBR码控算法是你设置多少目标码率，编码器的输出码率就会接近于目标码率。这样，超发的问题就会少很多，相应地对端出现卡顿的概率也会小很多。</p><h3>复杂帧编码后过大或者I帧比较大</h3><p>虽然，我们选择使用CBR的码控来编码可以使得一段时间内（比如说500ms或者1秒钟）的编码输出码率尽量地贴合预估带宽，但是有的时候编码画面变化很大的帧或者需要编码IDR帧的时候，还是会使得编码后这一帧的大小会比较大。如果一次性将这种大帧打包出来的所有包都直接发送到网络中，则会在一瞬间加剧网络的负担，从而容易引起网络丢包，继而引起卡顿的可能。如下面两张图所示：</p><p><img src="https://static001.geekbang.org/resource/image/67/fc/677444364ccf619e213944b318482bfc.jpeg?wh=1919x904" alt="图片"><img src="https://static001.geekbang.org/resource/image/30/03/30a3c1018d04c0fc3a02d5f912d14703.jpeg?wh=1920x1080" alt="图片"></p><p><strong>为了能够减小这种大帧带来的瞬时网络波动，我们可以在编码打包之后、发送之前，加一个平滑发送的模块来平滑地发送视频包。</strong>这个模块在WebRTC中叫做PacedSender（节奏发送器）。那它的工作原理是怎么样的呢？</p><p>PacedSender主要的工作原理就是编码输出的码流打包之后先放到它的缓冲区中，而不是直接发送。之后它再按照预估带宽大小对应的发送速度，将缓冲区中的数据发送到网络当中。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/05/22/05e1184804ef64ef310497b901c12622.jpeg?wh=1920x1080" alt="图片"></p><p>一般PacedSender每隔5ms左右发送一次包，并且它会在内部记录上一个5ms发送周期发送完之后剩余可发送的大小。同时，每隔5ms左右，它计算当前距离上一次发送包的时间差，乘上发送码率得到这段时间可以发送多少大小的数据。然后再加上上一次剩余可发送大小得到本次可发送大小。因为发送的时候是一个个RTP包发送的，而一般一个包差不多就是1500字节，所以上一个剩余可发送的大小可能为负数</p><p>如果本次可发送大小大于0，就从缓冲区中取包发送出去。并且，发送完包之后将剩余可发送大小减去发送包的大小。之后如果剩余可发送大小小于或等于0，则停止发送，并等待下一个5ms发送周期再发送。</p><p>PacedSender是通过控制实际发送码率来平滑发送的，这样能防止编码输出码率超过网络带宽太多，直接将包一次性发送到网络导致卡顿。但是我们要注意，如果编码器输出码率差网络带宽太多，也会导致PacedSender缓冲太多数据包，从而引起延时太长。</p><p>因此，编码器码控还是需要贴合网络预估带宽的。PacedSender大多时候是用来防止一两帧编码后太大引起数据量突增造成丢包。因此，<strong>码控和PacedSender都很重要，它们是一起协作来减少卡顿的。</strong></p><h3>网络本身就有一定的丢包率</h3><p>当然，我们选择CBR的码控同时使用了平滑发送方法，但有的时候网络变化太快了或者我们处在一个无线网络环境下，就是会有一定的丢包概率。那怎么办呢？</p><p>这就要使用我们前面多次讲到过的丢包重传策略了。因为对于视频来说，如果视频帧出现了丢包的话，帧就不完整了，那么当前帧也就不能拿去解码，就可能引起卡顿。如果强行解码，从这一帧开始到下一个IDR帧中间的帧，几乎都会出现解码花屏或者解码错误，而解码错误也会引起卡顿。</p><p>这个知识点你可以参考第05～07这三节课，里面详细讲述了为什么会这样。因此，如果真的出现丢包了，那么我们必须想办法将包恢复。其中，最常用的方法就是丢包重传。</p><p><strong>丢包重传请求策略是在Jitter Buffer里面实现的。</strong>当接收端接收到视频RTP包之后，会检查RTP序列号。</p><p>如果序列号不连续，出现了跳变，也就是说，当前RTP包序列号减去收到的最大RTP包序列号大于1，那么就认为中间的包有可能丢失了，Jitter Buffer就将中间没有收到包的包序号都加入到丢包列表中。因为UDP经常会出现乱序到达的情况，如果中间的包后面到来了，也就是说RTP包序号小于收到的最大RTP包序号，Jitter Buffer就将这个包序号从丢包列表中删除，防止重复传输。</p><p>接收端每隔一定时间将丢包列表组装成RTCP协议中的NACK报文（我们在<a href="https://time.geekbang.org/column/article/465503">第09讲</a>中详细讲述过）发送给发送端，并且我们在<a href="https://time.geekbang.org/column/article/467073">第10讲</a>中也说过，发送端会保存之前的发送历史数据。发送端收到NACK报文之后，就会解出NACK报文中携带的丢失包的序号，并且在发送的历史数据中找出这个包重新发送给接收端。接收端收到包就将丢包列表中的对应序列号删除。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/20/85/20284f58af66d73d2705fc2b2d280985.jpeg?wh=1920x1080" alt="图片"></p><p>但是这里有一个问题就是，有的包重传一次需要一段时间才能到接收端，因为NACK发送给发送端需要时间，重传包传输到接收端也需要时间，中间正好一来一往，差不多一个RTT（往返时间）时间。</p><p>因此，每个丢失包序号发送重传请求之后，下一次需要等一个RTT的时间。如果接收端等待一个RTT的时间后还没有收到对应序号的RTP包，则再次将该序号加入到重传请求中，不能每次NACK请求都把所有丢包列表中的序号加入到报文中，防止重传重复发送，加重网络负担。同时重传也是有次数限制的。如果一个包重传请求发送了好几次，比如说10次，还没有收到，那就不再将该包加入到NACK报文中了。</p><h3>重传也没有收到包</h3><p>一般来说，前面的策略用上了之后，卡顿会小很多。但是，有的时候就是会有极端情况出现。毕竟，网络是千变万化的。如果实在是前面策略都用上了，还是出现了有包没有收到，导致帧不完整，继而导致没有帧可以解码成功的话，那么我们就需要使出最后的大招了，那就是关键帧请求，也叫I帧请求。<strong>I帧请求使用RTCP协议中的FIR报文。这个策略也是工作在Jitter Buffer中的。</strong>具体如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/02/94/02bf046c04153baf2b6a56bbd3e2db94.jpeg?wh=1920x1080" alt="图片"></p><p>前面课中我们讲到过，如果有一个帧解码失败，那之后的帧几乎都将解码失败，直到下一个IDR帧到来。因此，如果有一帧出现了丢包的情况，导致后面的帧都无法解码了，那么接收端这个时候就需要发起一个关键帧请求报文给发送端，发送端收到关键帧请求之后应该立即编码一个IDR帧。这样接收端收到IDR帧之后就可以解码了，而前面不能解码的帧就全部删除掉。同时，将丢包列表清空掉。</p><h2>花屏问题</h2><p>除了卡顿问题，另外一个比较让人头痛的问题就是花屏问题。花屏问题的出现主要有以下几种比较常见的原因。</p><h3>帧不完整</h3><p>前面我们说过，如果帧出现了丢包就送去解码的话，若能解码成功，那肯定会出现解码花屏的问题。尤其是ffmpeg作为解码器的时候，帧不完整也有很大的概率成功解码，但是得到解码后的图像却是花屏的。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/1e/0f/1e8023bf0a3aa59d52yy45f071fbe10f.jpeg?wh=1920x1080" alt="图片"></p><p>因此，<strong>我们在解码一帧数据之前一定要保证帧是完整的。</strong>那怎么保证呢？</p><p>在RTP包里面，RTP头有一个标志位M，表示是一帧的结尾。因此只要收到这个标志位为1的包就代表收到了这一帧的最后一个包。那么如何判断一帧的第一个包有没有收到呢？如果收到了一帧的第一个包，也收到了这一帧最后一个包，那我们就有了一帧的第一个和最后一个包的RTP序列号了。只要中间的序列号对应的RTP包都收到了，那么当前帧就完整了，是不是？</p><p>于是，现在的重点是怎么确定一帧第一个包有没有收到。还记得我们在<a href="https://time.geekbang.org/column/article/465503">第09讲</a>里面RTP打包的时候讲到的：</p><ul>\n<li>在单包模式打包方式的时候，一帧只打一个包，最后一个包就是第一个包。那只要收到最后一个包就等于收到了第一个包，很容易判断，是不是？</li>\n<li>在组合打包方式的时候，一个包里面有好几个帧，那么，只要按照协议将几帧分离开来就可以。这时帧肯定是完整的。因为包丢了的话，这几帧就都丢了，不存在丢掉帧中的一部分，是不是？</li>\n<li>问题在于在分片打包的时候，分片打包一帧会分成好几个包打包，而丢掉了一个包，帧就不完整了。我们知道分片打包时FU Header里面有一个S标志位。这个标志位表示的是第一个包，那我们是不是就可以使用这个标志位来判断是不是收到了第一个包了呢？<br>\n&nbsp;<br>\n答案是：不是！不是！不是！如果使用这种方法来判断完整性的话，那大概率会出现花屏问题。</li>\n</ul><p>这个地方千万要注意，<strong>我们在RTP打包的时候是以Slice为单位打包的，而不是以帧为单位打包的。</strong>因此，前面几种方式都是只能表示一个Slice完整了，而不能表示一帧完整了。因为一帧是有可能有多个Slice的。</p><p>再一次强调前面关于单包、组合打包、分片打包的帧完整性判断都是错误的。<strong>那正确的帧完整性判断应该怎么做呢？</strong></p><p>我们也是在Jitter Buffer中来对帧进行完整性判断的。首先，我们使用前面的方式判断Slice的完整性，保证一个个Slice是完整的。然后，在<a href="https://time.geekbang.org/column/article/461658">第05讲</a>中我们讲到过使用slice_header中的first_mb_in_slice字段，来判断当前Slice是不是第一个Slice。如果这个first_mb_in_slice字段为0，就代表是帧的第一个Slice了。</p><p>我们找到帧的第一个Slice，而Slice也判断了是完整的，再通过RTP头的M标志位判断了帧的最后一个包。如果第一个Slice的第一个包到帧的最后一个包之间的RTP包都收到了，那就代表帧完整了。这是一种方法。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/cf/89/cf2e280e25c5117847fe76df5471ba89.jpeg?wh=1920x1080" alt="图片"></p><p>从上面的图中我们可以看到两个帧都是完整的。而下面图中的帧1是不完整的。</p><p><img src="https://static001.geekbang.org/resource/image/ce/c6/ce791d9a1ed424e2ecf3d0a73ef89bc6.jpeg?wh=1920x1080" alt="图片"></p><p>还有另外一种方法，是WebRTC中在使用的方法，就是将每一个收到的包都排好序放在队列里面。Jitter Buffer收到了当前帧的最后一个包（RTP头的标志位M为1）之后呢，从这个包往前遍历，要求RTP序列号一直连续，直到RTP时间戳出现一个跳变，代表已经到了前面帧的包了。</p><p>如果包序号一直是连续的，那么代表当前帧就是完整的了。因为两帧的时间戳是不可能一样的。这也是一种方法。但是这种方法需要所有包都放在一个队列里面，并且排好序。它没有前一种方法灵活。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/72/07/72b8eb19201989de9acfb4603032c007.jpeg?wh=1920x1080" alt="图片"></p><h3>参考帧不完整</h3><p>当一帧完整了之后，我们是不是就可以将帧送去解码，就不会出现花屏了呢？答案是：不是这样的。</p><p>因为，我们前面强调过很多遍，需要参考帧也是完整的才能送解码，并且参考帧的参考帧也要是完整的才行。如果参考帧不完整或者丢失，会出现如下图所示的花屏。</p><p><img src="https://static001.geekbang.org/resource/image/eb/0e/ebd788d3a6af54813250d147ec6f590e.png?wh=1872x1024" alt="图片"></p><p>那也就是说，如果是连续参考的话，或者说你不知道编码器使用的参考结构的话，你就需要保证从IDR帧开始到当前帧为止所有的帧都是完整的，并且前面的帧都已经解码了，那当前帧才能送去解码。因为只要有一帧没有解码就会出现花屏。具体如下图所示。这部分功能一般也是实现在Jitter Buffer中。</p><p><img src="https://static001.geekbang.org/resource/image/5b/63/5b1e9yy576ea9166b2dac19c1afc4463.jpeg?wh=1920x1080" alt="图片"></p><p>当然，有的时候我们并不一定使用连续参考，比如，我们下一节课会讲到的SVC编码，就不是连续参考的。那就不要求前面的帧都完整才可以解码。或者，你自己设计了参考结构，并不是使用了连续参考的方法做编码的，也不需要要求所有的帧都完整。</p><p>这个时候你需要设计自己的协议告诉接收端什么时候是可解码的，防止出现花屏。如果没有的话，那就当作连续参考处理，防止花屏的出现。因为这种情况花屏一旦出现，到下一个IDR帧到来都会一直花屏，这是不能接受的。</p><h3>YUV格式问题</h3><p>另外一种常见的花屏问题，就是<strong>渲染的时候YUV格式弄错了</strong>。这种问题经常会出现，我们声网的客户就出现了好几次没有处理好这个问题导致的花屏。这种情况有一个特点，就是图像的大体轮廓是对的，但是颜色是有问题的。如下图所示，左图YUV格式是正确的，而右图YUV格式是错误的。</p><p><img src="https://static001.geekbang.org/resource/image/91/74/91a97a7749553a187d5c8d3c0ca5b474.png?wh=1594x814" alt="图片"></p><p>根因是YUV中的Y分量是对的，但是UV是错误的。这种时候你就应该想到，很有可能就是NV12当作NV21处理了，或者I420当作NV21处理了，类似这种YUV格式弄错了的问题。其处理方式也很简单，就是使用正确的YUV格式就对了。渲染或者读取YUV的时候一定不要弄错了YUV的类型。</p><h3>Stride问题</h3><p>最后一种花屏问题，是老生常谈的问题啦。那就是Stride问题。<strong>解码后渲染前一定要处理好YUV的Stride问题，不要和宽度弄混了。</strong>如果出现类似下图的现象的话，去看看你的Stride是不是弄错了吧。</p><p><img src="https://static001.geekbang.org/resource/image/65/57/657939bbb5e0dae0be4ed19c603yy757.png?wh=1512x1096" alt="图片"></p><h2>小结</h2><p>好了，今天的课到这里就要结束了。我们来回顾一下这节课的主要内容。</p><p>我们今天主要介绍了哪些环节出现问题会比较容易引起卡顿和花屏，并给出了相应的解决方法。</p><ol>\n<li><strong>卡顿</strong></li>\n</ol><ul>\n<li>在采集或设置的帧率不够时，会造成两帧之间的时间间隔过长，从而导致卡顿。我们需要提高帧率。</li>\n<li>机器性能不够，导致前处理或者编码耗时太长，从而导致卡顿。我们可以在高分辨率的时候尽量使用GPU做前处理，并使用硬件编码或者将软件编码设置为快速档加快处理的速度。</li>\n<li>编码器输出码率超过实际网络带宽。在RTC视频通话场景下我们最好选择CBR的码控算法，从而保证输出码率能够比较好地贴合预估带宽。</li>\n<li>复杂帧编码后过大或者I帧比较大，如果一次性将这种大帧打包出来的所有包都直接发送到网络中，容易引起网络丢包，继而引起卡顿的可能。为了能够减小这种大帧带来的瞬时网络波动，我们可以在编码打包之后、发送之前，加一个平滑发送的模块来平滑地发送视频包。这个模块在WebRTC中叫做PacedSender。它的主要工作原理就是编码输出的码流打包之后先放到它的缓冲区中，而不是直接发送。</li>\n<li>网络本身就有一定的丢包率，如果视频帧出现了丢包的话，帧就不完整了，那么当前帧也就不能拿去解码，可能引起卡顿。最常用的方法就是丢包重传。丢包重传请求策略是在Jitter Buffer里面实现的。</li>\n<li>重传也没有收到包，导致帧不完整，继而导致没有帧可以解码成功，我们就需要使用关键帧请求，也叫I帧请求。I帧请求使用RTCP协议中的FIR报文。这个策略也是工作在Jitter Buffer中的。</li>\n</ul><ol start="2">\n<li><strong>花屏</strong></li>\n</ol><ul>\n<li>帧不完整。如果帧出现了丢包就送去解码的话，若能解码成功，那肯定会出现解码花屏的问题。因此，我们在解码一帧数据之前一定要保证帧是完整的。记住我们在RTP打包的时候是以Slice为单位打包的，而不是以帧为单位打包的。我们也是在Jitter Buffer中来对帧进行完整性判断的。</li>\n<li>参考帧不完整。我们需要参考帧也是完整的才能送解码。如果参考帧不完整或者丢失，会出现花屏。这部分功能一般也是实现在Jitter Buffer中。</li>\n<li>渲染的时候YUV格式弄错了，也会造成花屏。其处理方式也很简单，就是使用正确的YUV格式就对了。</li>\n<li>Stride问题会造成花屏。我们解码后渲染前一定要处理好YUV的Stride问题，不要和宽度弄混了。</li>\n</ul><p>为了方便你记忆，这里我给出了一张总结图。</p><p><img src="https://static001.geekbang.org/resource/image/20/f6/208c51c78a62c5cd358e31369f3f09f6.jpg?wh=2082x1028" alt=""></p><h2>思考题</h2><p>今天我们的思考题换一种形式，你来说说你遇到过哪些引起卡顿和花屏问题的原因吧。我们一起在评论区相互学习交流解决这类问题的经验。</p><p>你可以把你的问题和疑惑写下来，分享到留言区，与我一起讨论。我们下节课再见。</p>',
        article_title: "12｜Jitter Buffer：拿什么拯救你，花屏卡顿问题",
      },
      {
        title: "13｜SVC：如何实现视频编码可伸缩？",
        id: 470072,
        content:
          '<p>你好，我是李江。</p><p>前面我们用了4节课的时间分别讲述了如何将视频编码码流打包成H264，如何预测网络带宽，如何做好码控来控制视频发送的速率，如何分析视频的花屏和卡顿等问题。基本上循序渐进地将视频传输中最重要的一些知识点都讲解了一遍，并对里面几个重要的算法进行了深入的研究。</p><p>今天，我们再讲述一个视频会议场景中经常会使用的视频编码传输相关的技术——SVC编码，也叫做可伸缩视频编码。它的作用是可以实现在一个码流里面包含多个可解码的子码流，服务器可以根据接收端的网络状况，下发对应码率的码流，从而实现可伸缩性。</p><h2>为什么需要SVC</h2><p>2020年全球爆发新冠疫情，很多公司为了员工的安全，实行在家办公的政策。视频会议一时成为了工作中必不可少的日常工作活动。很多大型公司可能会出现一次几十、上百个人参加视频会议的情况。对于视频会议技术商来说，如何提供几十、上百个人的高质量视频通话技术是一个难题。为什么呢？</p><p>比如说，我和你两个人进行视频通话，我是发送端，网络非常好，你是接收端，网络比较差。发送端和接收端之间的视频通路如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/bb/d2/bb477d3a740c1b5a4a9c41e33c1043d2.jpeg?wh=1920x620" alt="图片"></p><p>在<a href="https://time.geekbang.org/column/article/467073">带宽预测</a>这节课里面我们讲过，由于服务器到接收端的网络比较差，那么最后会引起：</p><ul>\n<li>一组视频RTP包的接收时长很长，而一组视频RTP包的发送时长比较小；</li>\n<li>或者发送端的视频RTP包发送给接收端之后，网络中丢包率很高。</li>\n</ul><!-- [[[read_end]]] --><p>如果不做带宽预测和码控的话最终接收端看到发送端的画面会非常卡。</p><p>当然，我们肯定是会做带宽预测和码控的。遇到这种情况，发送端通过基于延时和基于丢包的带宽预测算法估算出发送端到接收端之间的网络带宽值。得到这个带宽值之后，参考<a href="https://time.geekbang.org/column/article/468091">第11讲</a>，发送端的视频码控算法就会将码率降下来，同时，码率下降引起QP上升，画面质量下降，但是流畅性变好，不会一直卡死。</p><p>这是1对1视频会议场景遇到网络不好时的拥塞控制策略。</p><p>想象一下，现在有10个人参加视频会议，我是主持人，也是视频发送端。我们简化一下场景，我的画面需要发给其他的9个观众，而观众的画面不会发送给其他人。10个人都在自己的家里面办公，每个人的网络状况都不一样。有的人网络非常好，带宽有100M，而有的人网络非常差，只有500～600k，而且还因为使用无线网络丢包率很高。而我的网络比较中等，2M带宽，有线网络丢包率非常小。</p><p>那现在开始视频会议，带宽预测算法开始工作，预测出我和其他9个人之间的链路带宽有2M、1M、500k、800k等很多个不同的带宽大小。假设忽略音频和其它数据占用的码率，所有的带宽都设置给视频的话，那请问现在设置给视频码控算法的目标码率应该是多少？</p><p><img src="https://static001.geekbang.org/resource/image/d5/c5/d51443baf475861fbbc96ddf28ec10c5.jpeg?wh=1920x1080" alt="图片"></p><p>设置为2M？那么带宽只有500k、800k的人看到的画面就是一直卡死的。那我们选择最小值500k不就可以了吗？是的，选择最小值500k，那么所有人都可以流畅地看到画面。可是画面非常糊，质量很差。</p><p>想象一下，如果100人参加视频会议，99个人的网络带宽是100M，一个人带宽是500k，最后我们选择500k的视频码率，合理吗？其他99个人为这一个人的网络不好一直观看质量非常差的画面。这太不公平了。有什么办法能很好地解决这个问题呢？这就是今天的主角——SVC。</p><h2>什么是SVC</h2><p>SVC是指一个码流当中，我们可以分成好几层。比如说分成三层：</p><ul>\n<li>第0层是最底层，可以独立进行编解码，不依赖第1层和第2层；</li>\n<li>第1层编解码依赖于第0层，但是不依赖于第2层；</li>\n<li>第2层的编解码需要依赖于第0层和第1层；</li>\n</ul><p>并且，<strong>第0层质量最低，第0层加第1层次之，三层加在一起的时候质量最高。</strong>注意这里的质量不是直接指的画面质量，而是帧率、分辨率的高低所代表的质量。</p><p>这样分层有什么好处呢？好处就是我们编码一个码流，可以组合出好几个不同的可解码码流出来。比如说上面三层SVC的例子：第0层就是一个可以独立解码的码流；第0层加上第1层也是一个可以独立解码的码流；第0层加上第1层和第2层也是一个可以解码的码流。</p><p>对于网络差的人，服务器给他转发第0层码流对应的RTP包；对于网络中等的人来说，服务器给他转发第0层加第1层码流对应的RTP包；对于网络很好的人，服务器给他直接转发所有层码流的RTP包。这样是不是就对大家都比较公平了。那具体怎么实现SVC分层编码呢？服务器又怎么转发呢？这里我给出我的思路，可供你参考。</p><h2>SVC的分类</h2><p>根据是在帧率上做SVC还是在分辨率上做SVC，我们可以将SVC分为时域SVC和空域SVC两种。接下来我们逐一看一下。</p><h3>时域SVC</h3><p>首先，第一种SVC分层编码方式是我们可以在帧率上做SVC，这种SVC称之为时域SVC。</p><p>帧率上做SVC是什么意思呢？我们在前面的课中讲过，一般我们在RTC场景中选择使用连续参考的参考结构来做编码。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/80/f3/801bc1fa0198e04020b8ef14803cb9f3.jpeg?wh=1920x363" alt="图片"></p><p>这种参考结构非常简单，但是有一个很大的问题就是只要有一帧被丢弃或不完整，就会导致后面的帧都不能解码，强行解码就会出现花屏（可以参考<a href="https://time.geekbang.org/column/article/463775">第07讲</a>和<a href="https://time.geekbang.org/column/article/469197">第12讲</a>）。</p><p>因此，如果是这种编码参考结构的话，就没有可伸缩性了。也就会产生前面多人视频会议的问题。我们把参考帧结构稍微换一下，隔一帧参考一帧，变成一个两层的结构，就可以解决连续参考的问题，如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/8c/5a/8c345746fc50cc31ac9305981e5f9e5a.jpeg?wh=1920x497" alt="图片"></p><p>在图中，帧0是I帧不需要参考，且是第0层的帧。帧1是P帧，参考帧0，且是第1层的帧。帧2是P帧，参考帧0，不参考帧1，是第0层的帧。帧3是P帧，参考帧2，是第1层的帧。一直用这种模式不断地循环下去。</p><p>下面我们再来看一个三层时域SVC编码的参考帧结构，如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/e0/d3/e0dd56dda75da348a203df34633a69d3.jpeg?wh=1920x606" alt="图片"></p><p>在图中，帧0是I帧不需要参考，是第0层的帧。帧1是P帧，参考帧0，与两层时域SVC不同，它是第2层的帧。帧2是P帧，参考帧0，不参考帧1，是第1层的帧。帧3是P帧，参考帧1，是第2层的帧。帧4是P帧，参考帧0，是第0层的帧，帧5是P帧，参考帧4，是第2层的帧。不断按照这个模式循环下去。</p><p>这个就是时域SVC编码。它的优点是什么呢？<strong>它通过调整参考帧结构就能实现分层编码。低层的帧不会参考高层的帧。</strong>如果我们丢弃高层的帧，低层的帧也是可以顺利地完成解码而不会出现花屏的，只是帧率会降低。但是相比连续参考结构中丢失一帧就直接卡死的体验要好很多。</p><p>同时，因为只需要调整一下参考结构，本身常用的编码标准都支持这种参考帧选择的方式，是符合常规标准的。因此，解码器都支持，没有兼容性问题。</p><p>但是它也有缺点。我们在<a href="https://time.geekbang.org/column/article/463775">第07讲</a>中提到过，一般自然运动是连续的，选择前一帧作为参考帧一般压缩率会比较高，因为前后相邻的两帧很相似。<strong>而时域SVC这种跨帧参考的方式会使得压缩率有一定的下降。</strong>两层SVC编码效率大概下降10%，三层大概下降15%。</p><h3>空域SVC</h3><p>下面，我们介绍另一种SVC编码，空域SVC。<strong>空域SVC是在分辨率上做分层。</strong>比如说，我们现在需要编码一个720P的视频。我们分成两层：第0层是360P的分辨率；第0层加第1层是720P的分辨率。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/d3/79/d35b23ccaa472b80f32cc20944229679.jpeg?wh=1920x669" alt="图片"></p><p><strong>空域SVC的优点也是我们可以在一个码流当中分出多个码流出来。</strong>比如说，两层空域SVC，第0层是一个可以独立解码的码流，只是分辨率是360P。第1层依赖于第0层，两个层次加起来是720P分辨率的码流。每个不同的分辨率都对应不同的码率。因此，也可以用来解决多人视频会议的问题，只是丢弃了高层次的层之后分辨率会变小。</p><p>但是我必须要说明一下，<strong>H264、H265、VP8这些常用的编码标准（除了扩展）都是不支持空域SVC的。</strong>因此，市面上的绝大多数的解码器也都不支持空域SVC这种一个码流里面含有多种分辨率的视频码流解码。所以现在很少会使用空域SVC，也很少有编码器实现空域SVC。并且，这种多分辨率的空域SVC相比多个编码器编码不同分辨率的方式，在压缩率上也没有多少优势，而且还不符合常规的标准。</p><p>因此，在WebRTC中直接使用多个编码器编码多种分辨率的方式代替空域SVC。</p><p><img src="https://static001.geekbang.org/resource/image/68/21/688080e0f0bbb7447e04231cd44ebc21.jpeg?wh=1920x621" alt="图片"></p><p>所以，我们接下来不会对空域SVC展开讨论。你可以当作是一个知识点了解一下就可以。</p><h2>时域SVC如何实现可伸缩</h2><p>下面我们再来看一下时域SVC如何做到给不同带宽的接收端转发不同帧率和码率的视频流。当然这个只是我的一些经验之谈。你可以参考一下。</p><p>首先，我们需要一些字段来描述码流中当前帧的层号、帧序号等SVC信息。因为这些字段只有在编码器编码的时候才知道。我们需要在编码出来一帧之后，在RTP包里面打包上这些信息发送给服务器和接收端。为什么需要告诉服务器和接收端呢？我们先来讲讲服务器如何根据网络情况做分层转发策略。</p><p>一般来说，视频会议使用如下的架构做视频数据转发。</p><p><img src="https://static001.geekbang.org/resource/image/f7/c9/f74ae2e58c41e5fdba50d29a35096bc9.jpeg?wh=1920x1080" alt="图片"></p><p>服务器到接收端的链路上，服务器是发送端，在服务器上也需要做带宽预测，预测算法是一样的（可以参考<a href="https://time.geekbang.org/column/article/467073">第10讲</a>）。</p><p>服务器会预测得到每一个接收端和服务器之间链路的带宽值。发送端发送RTP包到服务器，服务器需要通过计算RTP包的大小和当前RTP包所属的帧属于哪一层得到每一层对应的码率。这样服务器在转发的时候，就可以根据到接收端之间链路的带宽值和对应的每一层的码率来选择到底转发几层。</p><p>比如说，视频的码率是2M，时域SVC编码，总共是3层，总帧率是24fps。第一层帧率是6fps，码率是500k；第二层帧率是6fps，码率是500k；第三层帧率是12fps，码率是1M（这里假设码率按帧数平均分配）。</p><p>假设某一个接收端只有600k，那服务器就只转发第一层给它，第二层第三层不转发。另一个接收端有1.5M，那我们就转发第一层和第二层给它，而第三层不转发。还有一个接收端是10M的带宽，我们就转发一二三层给它。这就是时域SVC的服务器转发逻辑。</p><p>这个有一个重要的点就是，服务器如何知道每一个RTP包对应帧所在的层号，以及接收端如何知道当前帧可不可以解码，因为接收端是不知道服务器到底给自己转发几层的码流的。</p><p>这里我们可以参考VP8编码的RTP协议标准。<strong>VP8的RTP协议在RTP头和VP8码流数据的中间还有一个RTP描述头，这个描述头主要用来放帧号、层号等信息的。</strong>具体如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/62/fc/62908b385ab2637b92f80f67192005fc.jpeg?wh=1766x710" alt="图片" title="图片来源VP8的RTP文档"></p><p>其中，几个重要的字段的解释如下：</p><ul>\n<li>I：占1位，表示有没有PictureID字段，为1表示有；</li>\n<li>L：占1位，表示有没有TL0PICIDX字段，为1表示有；</li>\n<li>T：占1位，表示有没有Tid和Y字段，为1表示有；</li>\n<li>M：占1位，表示PictureID字段占7位还是15位，为1表示占15位；</li>\n<li>PictureID：占7位或者15位，表示帧序号；</li>\n<li>Tid：占2位，表示层号；</li>\n<li>TL0PICIDX：占8位，表示当前帧所属的SVC单元，每过一个Tid为0的帧， TL0PICIDX加1；</li>\n<li>Y：占1位，表示当前帧是不是只参考Tid=0的帧。</li>\n</ul><p>服务器可以从RTP描述头得到RTP包对应的层号。这样服务器就可以通过RTP的层号和RTP的包大小来估算每一层的码率了。</p><p>而接收端可以根据帧号、层号和层同步标志位等信息来判断当前帧是不是可以解码，而不用去解码视频码流。</p><p><img src="https://static001.geekbang.org/resource/image/a7/e6/a7d19637f8d089bd5b500d6b279ab3e6.jpeg?wh=1920x791" alt="图片"></p><p>从上图我们可以看到：</p><ul>\n<li>帧0是IDR帧，只要完整了就一定可以解码；</li>\n<li>帧1是P帧，由于它的Y标志位为1，代表它只参考了同一个TL0PICIDX中Tid=0的帧，也就是帧0，因此，只要帧0可解，帧1就可以解码；</li>\n<li>帧2判断逻辑同帧1，只要帧0可解，帧2就可以解码，不依赖于帧1是不是可解；</li>\n<li>帧3也是P帧，但是由于它的Y=0，代表它不是只参考了Tid=0的帧，因此只有同一个TL0PICIDX中前面的帧都可解了才认为是可解的，也就是说只有帧0、帧1、帧2都可解它才可解，这里注意一下，因为帧3可以多参考，它可以同时参考帧1和帧2，只是图中没有画出来；</li>\n<li>帧4是P帧，但是它的Tid=0，因此它只参考前面的帧0，所以只要TL0PICIDX-1的Tid=0的帧可以解码，它就可以解码。也就是帧0可以解码，帧4就可以解码；</li>\n<li>对于帧5判断同帧1，帧6判断同帧2，帧7判断同帧3，一直循环下去；</li>\n</ul><p>我们可以看到帧1、帧3丢弃了的话，并不影响帧0和帧2的可解码性判断。帧1、帧2、帧3都丢失了，也不会影响帧4的可解码性的判断。因此，<strong>我们的服务器就可以通过丢层的方式来实现对不同带宽的接收端下发不同帧率码率的码流了。</strong></p><p>上面是VP8的时域SVC的RTP协议。那H264呢？H264其实在标准的附录G直接定义了SVC的相关字段，也就是说在H264的编码码流里面就可以有SVC信息。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/80/eb/8051abb8d14e0a9f95abc81eb7c8e9eb.jpeg?wh=1920x928" alt="图片" title="图片来源H264标准文档"></p><p>但是由于是附录G的内容，实现这一部分的解码器很少。因此不推荐使用这种方式传递SVC的相关信息。因为这种码流结构很多常规的H264解码器是不支持解码的，通用性不好，所以<strong>我们建议使用RTP扩展头来传输时域SVC的信息。</strong></p><p>我们可以直接使用VP8的RTP描述头的格式，且编码码流还是保持常规标准的码流就可以，这样常规的H264解码器都能解码。服务器和接收端直接从RTP扩展头里面读取相关的SVC信息就可以了。而对于SVC编码，openh264已经实现了最大4层的时域SVC。你可以直接使用openh264就可以实现SVC编码了。</p><h2>小结</h2><p>总结一下，今天我们通过多人视频会议如何设置编码码率的问题引出了为什么需要使用SVC编码。SVC编码可以在一个码流当中包含多个可以解码的子码流，这样服务器就可以根据接收端的带宽转发合适码率的子码流给接收端，从而达到可伸缩性。</p><p>并且，我们还介绍了两种类型的SVC，主要包括时域SVC和空域SVC。在之后，我们对服务器如何做时域SVC码流的转发做了详细的介绍。同时，我们还讨论了如何在RTP协议里面携带SVC信息，用于服务器做转发逻辑和接收端做解码性判断使用。</p><p>我们知道服务器会预测得到每一个接收端和服务器之间链路的带宽值，并通过计算RTP包的大小和当前RTP包携带的层号得到每一层对应的码率。然后，服务器再根据到接收端之间链路的带宽值和对应的每一层的码率来选择到底转发几层。</p><p>最后，接收端再根据RTP包携带的SVC信息来判断帧组完整之后可不可以解码，可以解码才能送解码器，不然就不能送去解码，防止出现花屏。这样我们就实现了可伸缩编码。</p><h2>思考题</h2><p>通过前面的学习，你知道哪些弱网对抗手段？</p><p>欢迎你在留言区和我分享你的思考和疑惑，你也可以把今天所学分享给身边的朋友，邀请他加入探讨，共同进步。下节课再见。</p>',
        article_title: "13｜SVC：如何实现视频编码可伸缩？",
      },
    ],
  },
  {
    chapterTitle: "视频封装和播放",
    children: [
      {
        title: "14｜MP4 & FLV：不要再说AVI了",
        id: 471074,
        content:
          '<p>你好，我是李江。</p><p>前面我们花了很长的时间学习了视频编码和视频传输弱网对抗的知识点。从今天开始我们来学习几个视频封装和播放的知识点。我们先来学习一下什么是音视频封装。之后我们再学习如何做音视频同步。</p><p>其实相比视频编码和传输，音视频封装应该是非常简单的知识点了。而且我们前面还学习过RTP打包，RTP打包音视频数据其实一定程度上也可以算是一种封装。我们今天再介绍两种常用的封装，一种是FLV，一种是MP4，相信你对这两种文件一点儿也不陌生。</p><p>音视频封装其实就是将一帧帧视频和音频数据按照对应封装的标准有组织地存放在一个文件里面，并且再存放一些额外的基础信息，比如说分辨率、采样率等信息。那到底怎么组织这些基础信息还有音视频数据呢？我们接下来先看看FLV是怎么做的。</p><h2>FLV</h2><p>FLV是一种非常常见的音视频封装，尤其是在流媒体场景中经常用到。FLV封装也是比较简单的封装格式，它是由一个个Tag组成的。Tag又分为视频Tag、音频Tag和Script Tag，分别用来存放视频数据、音频数据和MetaData数据。</p><p>下图就是FLV的总体结构图：</p><p><img src="https://static001.geekbang.org/resource/image/38/c0/38cc18a2a824e2001ae4d38818e691c0.jpeg?wh=1920x1080" alt="图片"></p><p>其总体格式图如下：</p><p><img src="https://static001.geekbang.org/resource/image/9b/f0/9b575096e64d54e20a9398828d73f3f0.jpeg?wh=1806x1406" alt="图片"></p><h3>FLV Header</h3><p>其中，FLV Header占用9个字节。前3个字节是文件的标识，固定是FLV。之后的1个字节表示版本。在之后的1个字节中的第6位表示是否存在音频数据，第8位表示是否存在视频数据，其他位都为0。最后的4个字节表示从文件开头到FLV Body开始的长度，一般就是等于9。</p><!-- [[[read_end]]] --><h3>FLV Body</h3><p>在FLV Header之后就是FLV Body了，这就是存放主要数据的地方，放置着一个个Tag。在每一个Tag前面都有一个4字节的Previous Tag Size，表示前一个Tag的大小，方便往回倒。再之后就是具体的Tag了。Tag又是由Tag Header和Tag Data组成，其中Tag Header占用11个字节，格式如上图。</p><p><strong>其中最重要的是时间戳，因为播放的速度还有音视频同步都需要依赖这个时间戳的值。</strong>时间戳占用3～4字节，如果3字节不够的话，则需要使用1字节的扩展时间戳作为时间戳的高8位。还需要注意的一个点就是，时间戳的单位是ms。RTP的时间戳单位是1/90000秒，MP4的时间戳是可以自定义的。这个时间戳的单位也是至关重要的，不要弄错了。</p><p>接下来就是Tag Data数据了。Tag Data有Script、音频和视频。首先来看一下Script Tag的Data。这个Tag存放的是MetaData数据，主要包括宽、高、时长、采样率等基础信息。</p><p>Script Data使用2个AMF包来存放信息。第一个AMF包是onMetaData包。第1个字节表示的是AMF包的类型，一般是字符串类型，值是0x02，之后是2字节的长度，一般长度总是10，值是0x000A。之后就是10字节长度字符串了，值是onMetaData。</p><p>第二个AMF包的第一个字节是数组类型，值是0x08，紧接着4个字节为数组元素的个数。后面即为各数组元素的封装，数组元素为元素名称和值组成的对。常见的数组元素如下表所示：</p><p><img src="https://static001.geekbang.org/resource/image/da/30/da922577ddc53bcac52b1bcccd433130.jpeg?wh=1340x1172" alt="图片"></p><p>音频Tag Data的第一个字节表示音频的编码方式、采样率和位宽等信息，如下图所示。之后就是音频数据了。</p><p><img src="https://static001.geekbang.org/resource/image/4a/5a/4a226cc369df7fd8ee7f469694e47f5a.jpeg?wh=1328x688" alt="图片"></p><p>视频Tag的第1个字节包含了这个Tag的视频帧类型和视频编码方式，格式如下图：</p><p><img src="https://static001.geekbang.org/resource/image/1f/51/1fd4c7db11320f1f163b969f2d411451.jpeg?wh=1342x350" alt="图片"></p><p>对于H264数据，紧接着会有4字节的AVC Packet Type格式，如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/a8/a1/a86a39594d3295f431763959d53487a1.jpeg?wh=1336x452" alt="图片"></p><p><strong>其中最重要的就是CTS。</strong>这个是什么意思呢？这是因为H264有B帧这种类型，涉及到显示时间戳PTS和解码时间戳DTS。前面Tag Header里的时间戳就是DTS，PTS等于DTS + CTS，这个需要注意一下。接下来就是存放具体的视频数据。</p><p>如果AVC包类型是0，则数据格式如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/12/bb/12d630497d4f0e540a764e62dd7f34bb.jpeg?wh=1336x1246" alt="图片"></p><p>如果AVC包类型为1，则数据格式如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/be/81/be79ca64a849568c1e19ce3399750e81.png?wh=1206x324" alt="图片"></p><p>这就是FLV封装。</p><h2>MP4</h2><p>了解了FLV封装之后，我们再来看一下MP4封装。MP4封装相比FLV更常见，但是也更复杂一些。其实它们的基本的思想还是一样的，就是用一个规定的格式组织存放音视频数据和一些基础信息。跟FLV由一个个Tag组成有点类似，MP4由一个个box组成，每一个box存放了不同的数据，而且box里面还可以嵌套着box。</p><p>MP4最外层的box主要有三个，分别是File Type box（ftyp box）、Movie box（moov box）和Media Data box（mdat box）。其中最重要、最复杂的就是moov box了，它里面存放了音视频的基本信息和每一个音视频数据的具体位置。</p><p>还有一点需要说明的就是：在MP4文件中，视频的一帧和音频的一段编码数据称为一个sample。连续的几个sample称之为chunk，而视频的所有sample称为一个视频track，同样音频的所有sample也称为一个音频track。</p><p>因此一般MP4文件是由音频track和视频track组成，而track由sample组成，其中若干个sample组成一个chunk。</p><p><strong>好了，下面我们就来看看比较重要的box吧。</strong>因为，MP4的box特别多，我们不会一个个都介绍，我们只介绍一下比较重要的box。</p><p>每一个box都是由Box Header和Box Data组成。Box Header的结构如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/e8/90/e888133ea73c584a711098869f6f6790.jpeg?wh=1198x572" alt="图片"></p><p>根据Box Header中的type我们将box分为不同类型的box，每一种不同的box对应的Box Data都是不一样。Box Data里面又可以嵌套box。MP4的总体box分布图如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/7f/d1/7f22c603689c17b42098b32eyyf034d1.jpeg?wh=1920x1333" alt="图片"></p><p>首先，ftyp box放在MP4文件的开始，用来表示文件类型，该box的Box Data包含了4字节的主版本（major brand）、4字节的版本号（minor version）和若干个4字节数组组成的兼容版本（compatible_brands）。</p><p><strong>mdat box是MP4的音视频数据存放的地方</strong>。mdat box 基本由头部和数据两部分组成，box type是 “mdat” 的ASCII码值。对于H264来说，是一个个NALU，码流格式使用的是<a href="https://time.geekbang.org/column/article/461658">第05讲</a>里面的MP4格式。这里的NLAU不再包含SPS和PPS，这些数据已经放到moov box里面了，此处NALU类型是图像数据或者SEI数据。</p><p><strong>另一个box就是最重要的moov box，用来存放Metadata信息。</strong>这个box可以放在ftyp的后面也可以放置在文件的最后面。moov box里面会一层层嵌套很多层box。总体嵌套逻辑就是movie里面是track，track里面是sample，多个sample又组成了一个个chunk。</p><p>moov box首先有一个mvhd box（movie header box）主要存放文件的基本信息，比如说MP4文件的创建时间、时间单位、总时长等信息。</p><p>moov box中的另外一个重要的box就是trak box，这个box音频和视频各有一个。具体是音频trak还是视频trak，会在trak box中的mdia box里面的hdlr box中表示出来。</p><p>trak box内部有一个tkhd box（track header box）主要是表示track的一些基本信息，比如说视频的宽高信息和音频的音量信息等。</p><p>trak box还有一个mdia box，它是媒体信息box。它包含了3个子box，一个是mdhd box，一个是刚才提到的hdlr box，一个是最重要的minf box，这个box里面包含了sample的很多信息，这些信息是找到并正确使用音频和视频数据的关键。</p><p>mdhd  box里面最重要的一个值就是时间单位time scale，这个时间单位是sample的时间戳的单位，控制播放速度和音视频同步都需要使用到这个值。</p><p>hdlr box主要包含了track的类型信息，表明是音频还是视频track。</p><p>minf box里面包含了一个stbl box（sample table box），里面存放着可以计算得到每一个chunk的偏移地址、每一个sample在文件中的地址信息和大小、每一个sample的时间戳和每一个视频IDR帧的地址信息。下面我们来详细介绍一下这些box。</p><p>其中，stts box中放置的是每一个sample的时长，这个值是DTS。</p><p><img src="https://static001.geekbang.org/resource/image/6a/75/6a94cf7fcee63b81bd1375f42a206275.jpeg?wh=1342x1250" alt="图片"></p><p>ctts box放置着CTS，也就是每一个sample的PTS和DTS的差值。</p><p><img src="https://static001.geekbang.org/resource/image/58/8d/58b59fc37129abcb6e7398f98cb6ea8d.jpeg?wh=1348x1254" alt="图片"></p><p>stss box中放置的是哪些sample是关键帧。</p><p><img src="https://static001.geekbang.org/resource/image/94/8a/94347b1d78b04a9995yyea1110yy338a.jpeg?wh=1332x1068" alt="图片"></p><p>stsc box中放置的是sample到chunk的映射表，也就是哪些sample属于哪个chunk。</p><p><img src="https://static001.geekbang.org/resource/image/03/a5/034ec8ab5228e2c65240c7fdb2f190a5.jpeg?wh=1338x1446" alt="图片"></p><p>stco box或co64 box中放置着每个chunk在文件中的偏移地址。</p><p><img src="https://static001.geekbang.org/resource/image/5c/ca/5ca7437ed49766cfc827fa75891680ca.jpeg?wh=1332x1104" alt="图片"></p><p>stsz box中放置着每一个sample的大小。</p><p><img src="https://static001.geekbang.org/resource/image/aa/99/aa0b8bef4d0c23da67c1468e92682599.jpeg?wh=1336x1214" alt="图片"></p><p>好了，跟sample相关的box就是这些。</p><h3>工程实践</h3><p>接下来我们结合一个工程问题来实践一下。我们如何计算每一个sample在文件中的具体位置，判断它是不是关键帧，并计算它的具体时间。</p><p>计算sample的具体位置需要使用stco（或co64）、stsc和stsz。我们首先通过stsc将每一个sample属于哪一个chunk计算出来。这样每一个chunk的第一个sample就知道是哪个了。然后我们通过stco和co64就可以知道对应序号的chunk的第一个sample在文件中的地址了。我们再通过stsz查询每个sample的大小，从chunk的第一个sample的地址将中间的sample的大小一个个地加上去就可以得到每一个sample的地址了。</p><p><img src="https://static001.geekbang.org/resource/image/aa/7c/aaac0910d536a50191854cae468b307c.jpeg?wh=1904x852" alt="图片"></p><p>而sample是不是关键帧，我们只需要通过stss对应每一个sample序号查询就可以得到。</p><p><img src="https://static001.geekbang.org/resource/image/54/c5/54fa75cc33d0690c8c206f6bac6b82c5.jpeg?wh=1904x841" alt="图片"></p><p>计算sample的时间我们需要用到stts和ctts。我们先通过stts得到每一个sample的时长，第n个sample的结束时间就是第n-1个sample的结束时间加上第n个sample的时长。但是需要注意一下，这个是DTS，我们还需要通过ctts box得到每一个sample的PTS和DTS的差值。最后每一个sample的PTS就是等于DTS加上CTS。</p><p><img src="https://static001.geekbang.org/resource/image/02/7b/02d684c8ea5e8405fb8f2ce341b90d7b.jpeg?wh=1913x896" alt="图片"></p><p>好了，以上就是MP4封装的主要内容。</p><h2>总结</h2><p>今天，我们主要介绍了一下两种音视频封装格式，分别是FLV和MP4。这两种封装格式是我们工作和生活中经常需要用到的。</p><ul>\n<li>FLV在流媒体场景经常会用到，其实直播RTMP协议和HTTP-FLV协议里面也是用的FLV封装，所以还是很重要的。</li>\n<li>MP4封装就是平时视频文件最常用的封装了，它主要由一个个box组成，其中最重要的就是跟sample有关的box，你需要重点掌握。当然你也不需要背下来，了解主要思想即可，等真正用到的时候查询一下就可以了。</li>\n</ul><h2>思考题</h2><p>好了，今天的课程到这里就结束了。在课程的最后，我给你留一道思考题。请你想一想：为什么FLV相比MP4更适合流媒体？</p><p>你可以在留言区留下你的答案，或者你有任何问题都可以在留言区和我交流，我们下节课再见。</p>',
        article_title: "14｜MP4 & FLV：不要再说AVI了",
      },
      {
        title: "加餐｜基于纯浏览器的视频会议方案探究",
        id: 471899,
        content:
          '<p>你好，我是章韦。目前就职于某知名外企，任职 Technical Leader，主要研究方向为信令控制及媒体传输相关领域。很开心受邀来到李江老师的专栏做一期加餐分享。今天我会围绕“基于纯浏览器的视频会议方案”谈一谈我的想法，期待能给你带来一些启发。</p><p>随着视频编解码技术的快速发展以及带宽成本的逐年降低，视频技术被广泛应用于生产生活中各个不同的领域，而视频会议则是其中最广泛的应用之一。伴随着这几年疫情在全球肆虐，各式各样的商业视频会议软件如雨后春笋，从全球范围来说，除了老牌的 Cisco Webex、微软的 Teams，还有新晋王者 Zoom Meetings, &nbsp;国内来说，不论是阿里还是腾讯，都推出了自己的视频会议软件。</p><p></p><p>那么这节课我们就偏居一隅，聊一聊各大视频会议厂商在基于纯浏览器视频会议上的一些尝试和遇到的问题，以及各主流浏览器在底层接口上提供的一些支持，并对其中涉及到的一些技术栈展开一定介绍。</p><p></p><h2><strong>WebRTC</strong></h2><p></p><p>提到 Web App 和浏览器对媒体传输的支持，首先不得说 WebRTC，典型的基于 WebRTC 的会议系统架构如下：</p><p></p><p><img src="https://static001.geekbang.org/resource/image/b2/8c/b2a338cfbddc5b45992769a3cd29558c.png?wh=1920x947" alt="图片"></p><p></p><p>结合上面的架构图和流程图来看，WebRTC 的 Web App 似乎并不复杂，几乎所有的前后端模块都能找到成熟的开源实现。<strong>然而对于传统的视频会议提供商来说，支持 WebRTC 却并非这么简单。</strong></p><!-- [[[read_end]]] --><p></p><p>首先要考虑到的问题就是 <strong>QoS</strong>。对于成熟的视频会议企业来说，无论是 ARQ、FEC 还是 CC，都会有一套自己私有的网络适应性算法（QoS）框架，&nbsp;但这些经过长时间验证且成熟的网络适应性算法却无法直接用在 Web App 后端的 SFU，因为浏览器端并不支持。</p><p></p><p>更进一步来说，不同的浏览器，其自身的<strong>网络适应性兼容方案</strong>也并不一致，再考虑到各种浏览器版本升级带来的进一步差异，在服务端做兼容各浏览器的网络适应性算法（QoS）其成本显然是相当高的。仅拿拥塞控制（Congest Control）举例来说，至少要分别实现&nbsp;GCC/NADA/SCReAM 算法和&nbsp;REMB/Transport-CC 报文来兼容不同的浏览器。</p><p></p><p>除了服务端的 QoS 问题之外，客户端部分也会有较大的问题，因为 WebRTC 诞生之初想解决的仅仅是 P2P 的媒体传输而并非会议，它提供了基于 P2P 传输简单易用的一揽子方案，但却也因此<strong>没有提供更多底层媒体处理接口</strong>。这就意味着开发者没有办法进一步处理任何的媒体流，进而无法在 Web App 中提供很多富有竞争力的功能，比如各种实时滤镜、虚拟背景、端对端加密等等。虽然 Chrome 开始提供 MediaStreamTrack 的 Insertable Stream 接口可以处理 WebRTC 中的媒体流，但要等到各主流浏览器都开始支持，显然还是路漫漫其修远兮！</p><p></p><p>除此之外，另一个更麻烦的问题是 <strong>WebRTC 中的 TURN/STUN 对于 TCP 443 端口的兼容性问题</strong>。很多企业内部的网络防火墙仅对外开放 TCP 443 端口，且该端口仅允许 TLS 数据包通过，但所有浏览器 WebRTC 的 TURN/STUN 实现都不支持 TLS，这意味我们需要为这种情况做失败转移。而遗憾的是，这里并没有简单的失败转移方案。</p><p></p><h2>失败转移方案</h2><p></p><p>上面我们提到，Web App 为了支持仅开放了 443 端口的企业，不得不实现一套新的失败转移方案。而这套方案需要彻头彻尾的重新实现，其大致架构如下：</p><p></p><p><img src="https://static001.geekbang.org/resource/image/aa/96/aae4704d15750b1e2c55523607d02296.png?wh=1920x1040" alt="图片"></p><p></p><p>从上图我们可以看出，这里我们不再将媒体的处理和传输部分托管给 WebRTC，取而代之的是一个基于 WebWorker 的媒体流水线处理机（WebWorker based Media Processing Pipeline），用来对从本地设备采集到的设备进行编码，并打包成 RTP 包，再通过 WebSocket 信令通道发送给服务端。</p><p></p><p><strong>完整的视频流发送过程是这样的。</strong></p><p>首先通过 getUserMedia（getDisplayMedia）接口从设备获取视频流，再每间隔 33 毫秒（帧率为 30）执行以下流程：</p><ol>\n<li>将此视频流当前帧通过 Canvas API 绘制到画布中；</li>\n<li>通过 Canvas API 读取画布中当前帧的 RGBA 图像；</li>\n<li>将 RGBA 图像数据转化为 YUV 格式；</li>\n<li>将 YUV 格式的图像数据发送到 OpenH264 模块进行编码；</li>\n<li>将编码后的数据包根据不同的 NALUnitType 封装为不同的 RTP Payload；</li>\n<li>将封装后的 RTP Payload 打包为 RTP 包；</li>\n<li>将 RTP 包进一步打成信令数据包；</li>\n<li>将此信令数据包通过信令通道发送到服务端。</li>\n</ol><p>需要注意的是，这里的第 3～6 步都是计算密集型的操作，为了避免阻塞 JS 主线程，需要将其用 WebWoker 包装，使这些步骤运行在非主线程中。同时，第 4 步视频编码操作需要借助WebAsseamly 将第三方的 Video Codec（比如 OpenH264）编译为 Wasm 格式，导出为 JS 模块，方可被前端 App 引用。</p><p></p><p>视频流接收过程如下，每当从信令服务器接收到类型为 RTP 数据包的 PDU 时：</p><ol>\n<li>将此 PDU 解析为 RTP 数据包；</li>\n<li>根据不同的 NALUnitType 将 RTP 数据包中的 RTP Payload 解析出来；</li>\n<li>将解析出来的 RTP Payload 发送到 OpenH264 模块进行解码，得到 YUV 格式的单帧图像；</li>\n<li>将单帧 YUV 图像转为 RGBA 格式的图像；</li>\n<li>通过 Canvas API 将 RGBA 格式的图像绘制到画布中。</li>\n</ol><p>这里的第 3 步同样需要使用 WebWoker 包装，并引用基于 WebAsseamly 的 OpenH264 库，以避免 JS 主线程阻塞。另一个需要注意的问题是，这里的第 4 步和第 5 步，需要使用 WebGL 进行性能优化，否则视频解码渲染的性能可能无法达到预期。</p><p></p><p><strong>这个方案完全移除了对 WebRTC 一揽子解决方案的依赖，并且初步实现了自己的媒体处理流水线。</strong>这就意味着我们可以对视频媒体流进行进一步的定制化处理，从而支持更多面向客户的功能。</p><p>同时因为有了对媒体处理的完全掌控力，我们也可以在浏览器端实现自己的 QoS 方案，且这些方案都在 JS 层实现，可以兼容所有浏览器，从而避免服务端对不同浏览器 QoS 方案的支持。进一步说，将客户端的 QoS 实现策略通过WebAsseamly 移植到 Web App 这边来，也是完全可行的方案。</p><p></p><p>但问题在于，作为失败转移方案，这里的媒体流数据是通过信令通道（WebSocket）进行传输的。在常规情况下，我们希望媒体数据能够走自己单独的数据通道，所以这里我们自然可以选择基于 SCTP（UDP based）传输协议的 DataChannel 作为媒体的传输通道。</p><p></p><p></p><h2>优化方案</h2><p></p><p>上面我们提到，当我们不再依赖 WebRTC 的一揽子解决方案，而是通过 WebAssembly Based OpenH264 Codec 构建自己的媒体处理流水线，并通过 WebSocket 进行传输时，我们一开始提到的无论是 QoS 的兼容性问题，还是客户端的媒体流定制需求，都可以得到满足，客户端还是服务端的架构都变得统一起来。</p><p>基于这种思路，我们可以继续在上面的失败转移方案的基础上，增加额外的基于 SCTP 传输协议的传输通道 DataChannel，以供媒体传输使用。此外，进一步丰富媒体处理流水线，使其能够实现滤镜、虚拟背景等媒体处理需求。最后，再加上统一的可跨浏览器的 QoS 算法，最终的架构图如下：</p><p><img src="https://static001.geekbang.org/resource/image/bf/eb/bf1a3d95f1e6f051123c6e591888aaeb.png?wh=1920x1084" alt="图片"></p><p>增加了如上图中所示的橘色模块以后，我们的系统开始变得丰满起来。<strong>媒体定制处理</strong>加入到了流水线，让我们能跨平台地满足更多的用户功能；完全自己实现的<strong>跨浏览器 QoS 方案</strong>，让前后端的算法变得统一起来，无需再考虑 QoS 的兼容性问题。</p><p></p><p>前端 DataChannel 和后端的 SCTP 的介入，让媒体流在绝大多数情况下，通过 DataChannel（SCTP）进行传输, &nbsp;从而更好地利用 UDP 协议的优势，因为它的可靠性可以得到选择，从而弥补 TCP 按序重发机制在流媒体传输应用中的缺陷。而这里的WebSocket，作为媒体数据传输通道，仅会发生在失败转移的特殊情况下。</p><p></p><p>看到这里，聪明的你一定发现了前端架构演进至此，也已经完全统一了，即正常流程和失败转移的流程除了传输层不同，其他模块完全共享，从而避免了多套解决方案需要同时维护的尴尬。</p><p></p><h2>进一步优化</h2><p></p><p>上述方案实现以后，我们将得到一个完整统一、基于纯浏览器且不受浏览器约束的视频会议系统。但这里是否真的没有任何问题，也没有任何优化空间了呢？</p><p><strong>答案显然是否定的。</strong></p><p>首先是基于 WebAsseamly 的 OpenH264 库的编码效率问题，尤其是在一些低端设备上，会导致 CPU 占用过高，并导致其发送或接收的视频发生卡顿。</p><p>其次就是 SCTP 的传输机制问题，虽然 SCTP 支持多流传输，能缓解队头阻塞的问题，但协议本身无法完全避免队头阻塞。除此之外，SCTP 协议建立连接时需要 4-RTT，其握手策略也可以继续优化，从而进一步减少视频首帧的加载时间。</p><p>Chrome 浏览器率先实现了基于 W3C 的 WebTransport 和 WebCodecs 规范。WebTransport 进一步优化了媒体传输通道，而 WebCodecs 规范则定义了浏览器所提供的更底层的媒体编解码接口。因而<strong>我们可以直接调用浏览器提供的本地视频编解码器</strong>，而不再需要调用 WebAssembly Based OpenH264 Codec 进行视频编解码，从而极大地提高了视频编解码效率。</p><p></p><p>因此，我们对前端架构进一步优化的架构图如下：</p><p><img src="https://static001.geekbang.org/resource/image/cf/52/cff0f4ed7f97b94b2f3c9ab10d9d5c52.png?wh=1920x1088" alt="图片"></p><p>但需要注意的是，由于 WebTransport 和 WebCodecs 规范的提出时间较短，<strong>目前只有 Chrome 浏览器支持</strong>，因此这步优化也只能应用于 Chrome 浏览器，当然 Chromebook 用户也能享受到这两个规范所带来的福利。</p><p></p><p>以上就是这节课的全部内容，其中涉及到的很多技术栈这里只能点到即止，展开的话细节还有很多，感兴趣的同学可以通过文末的链接，获取相关技术的草案或者RFC做进一步了解。</p><ol>\n<li>WebAssembly工具链：<a href="https://emscripten.org">https://emscripten.org</a></li>\n<li>WebWorker：<a href="https://www.w3.org/TR/2021/NOTE-workers-20210128/">https://www.w3.org/TR/2021/NOTE-workers-20210128/</a></li>\n<li>WebTransport：<a href="https://www.w3.org/TR/webtransport/">https://www.w3.org/TR/webtransport/</a></li>\n<li>WebCodecs：<a href="https://www.w3.org/TR/webcodecs/">https://www.w3.org/TR/webcodecs/</a></li>\n<li>WebRTC（RTCDataChannel）：<a href="https://www.w3.org/TR/2015/WD-webrtc-20150210/">https://www.w3.org/TR/2015/WD-webrtc-20150210/</a></li>\n<li>OpenH264 Codec：<a href="https://github.com/cisco/openh264/wiki">https://github.com/cisco/openh264/wiki</a></li>\n<li>拥塞控制（CC）相关协议：<a href="https://datatracker.ietf.org/wg/rmcat/documents/">https://datatracker.ietf.org/wg/rmcat/documents/</a></li>\n</ol><p></p>',
        article_title: "加餐｜基于纯浏览器的视频会议方案探究",
      },
      {
        title: "15｜音画同步：如何让声音和画面手拉手前进？",
        id: 472533,
        content:
          '<p>你好，我是李江。</p><p>在上节课中，我们讲述了音视频封装以及音视频数据是如何装到FLV和MP4文件里面的。这节课我们来讲讲播放这些文件的时候需要用到的一个非常重要的技术——音视频同步，也叫<strong>音画同步</strong>。</p><p>音视频同步是什么呢？它就是指在音视频数据播放的时候，播放的画面和声音是需要同步的，是能对得上的。相信你一定遇到过这种情况，就是看电视、电影或者直播的时候，人的口型和声音是对不上的，这样看起来会让人非常难受，这种问题就是音视频不同步导致的。因此做好音视频同步是非常重要的，当然也会有一定的难度，我们不妨先从一些基础知识讲起。</p><h2>PTS和DTS</h2><p>首先就是PTS和DTS这两个概念。其实，我们在讲音视频封装的时候已经提到过了。</p><p><strong>PTS表示的是视频帧的显示时间，DTS表示的是视频帧的解码时间。</strong>对于同一帧来说，DTS和PTS可能是不一样的。</p><p>为什么呢？主要的原因是B帧，因为B帧可以双向参考，可以参考后面的P帧，那么就需要将后面用作参考的P帧先编码或解码，然后才能进行B帧的编码和解码。所以就会导致一个现象，后面显示的帧需要先编码或解码，这样就有解码时间和显示时间不同的问题了。如果说没有B帧的话，只有I帧和P帧就不会有PTS和DTS不同的问题了。</p><!-- [[[read_end]]] --><p>具体如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/7c/86/7c23be3f62ba07f0ba61f1603afe5a86.jpeg?wh=1920x915" alt="图片"></p><h2>时间基</h2><p>另外一个很重要的概念是时间基。这是面试中经常会问到的知识点，你一定要掌握。</p><p>时间基是什么呢？很简单，<strong>它就是时间的单位</strong>。比如说，编程的时候我们经常使用ms（毫秒）这个时间单位，毫秒是1/1000秒，如果你用毫秒表示时间的话，时间基就是1/1000。再比如说RTP的时间戳，它的单位是1/90000秒，也就是说RTP时间戳的时间基是1/90000。意思是RTP的时间戳每增加1，就是指时间增加了1/90000秒。</p><p>而对于FLV封装，时间基是1/1000，意思是FLV里面的DTS和PTS的单位都是ms。MP4的话，时间基就是在box中的time_scale，是需要从box中读取解析出来的，不是固定的，具体可以参考<a href="https://time.geekbang.org/column/article/471074">第14讲</a>。这就是时间基的概念。</p><h2>音视频同步的类型</h2><p>好，了解了基础知识以后，我们就可以开始学习音视频同步了。音视频同步主要的类型有三种：<strong>视频同步到音频、音频同步到视频、音频和视频都做调整同步</strong>。我们逐一看下。</p><p>首先，视频同步到音频是指音频按照自己的节奏播放，不需要调节。如果视频相对音频快了的话，就延长当前播放视频帧的时间，以此来减慢视频帧的播放速度。如果视频相对音频慢了的话，就加快视频帧的播放速度，甚至通过丢帧的方式来快速赶上音频。</p><p>这种方式是<strong>最常用的音视频同步方式</strong>，也是我们今天讲述的重点，后面我们就会以这种方式来深入探讨其原理。</p><p>其次，音频同步到视频是指视频按照自己的节奏播放，不需要调节。如果音频相对视频快了的话，就降低音频播放的速度，比如说重采样音频增加音频的采样点，延长音频的播放时间。如果音频相对视频慢了，就加快音频的播放速度，比如说重采样音频数据减少音频的采样点，缩短音频的播放时间。</p><p>这里需要格外注意的是，当音频的播放速度发生变化，音调也会改变，所以我们需要做到变速不变调，这个你可以参考另外一个专栏<a href="https://time.geekbang.org/column/intro/100098801?tab=intro">《搞定音频技术》</a>，里面有详细的讲解。</p><p><strong>一般来说这种方式是不常用的</strong>，因为人耳的敏感度很高，相对于视频来说，音频的调整更容易被人耳发现。因此对音频做调节，要做好的话，难度要高于调节视频的速度，所以我们一般不太会使用这种同步方法。</p><p>最后一种是音频和视频都做调整，具体是指音频和视频都需要为音视频同步做出调整。比如说WebRTC里面的音视频同步就是音频和视频都做调整，如果前一次调节的是视频的话，下一次就调节音频，相互交替进行，<strong>整体的思路还是跟前面两种方法差不多</strong>。音频快了就将音频的速度调低一些或者将视频的速度调高一些，视频快了就将视频的速度调低一些或者将音频的速度调高一些。<strong>这种一般在非RTC场景也不怎么使用。</strong></p><h2>视频同步到音频</h2><p>那么接下来我们就深入学习一下最常用的音视频同步，即视频同步到音频，它是怎么工作的。这里我们<strong>参考FFplay的代码实现来讲解其原理</strong>。</p><p>首先，我们使用的时间戳是PTS，因为播放视频的时间我们应该使用显示时间。而且我们需要先通过时间基将对应的时间戳转换到常用的时间单位，一般是秒或者毫秒。</p><p>然后，我们有一个视频时钟和一个音频时钟来记录当前视频播放到的PTS和音频播放到的PTS。注意这里的PTS还不是实际视频帧的PTS或者音频帧的PTS，稍微有点区别。</p><p><strong>区别是什么呢？</strong>比如说一帧视频的PTS的100s，这一帧视频已经在渲染到屏幕上了，并且播放了0.02s的时间，那么当前的视频时钟是100.02s。也就是说视频时钟和音频时钟不仅仅需要考虑当前正在播放的帧的PTS，还要考虑当前正在播放的这一帧播放了多长时间，这个值才是最准确的时钟。</p><p>而视频时钟和音频时钟的差值就是不同步的时间差。这个时间差我们记为diff，表示了当前音频和视频的不同步程度。<strong>我们需要做的就是尽量调节来减小这个时间差的绝对值。</strong></p><p>那怎么调节呢？我们知道，我们可以通过计算得到当前正在播放的视频帧理论上应该播放多长时间（不考虑音视频同步的话）。计算方法就是用还没有播放但是紧接着要播放的帧的PTS减去正在播放的帧的PTS，我们记为last_duration。</p><p>如果说当前视频时钟相比音频时钟要大，也就是diff大于0，说明视频快了。这个时候我们就可以延长正在播放的视频帧的播放时间，也就是增加last_duration的值，是不是视频的播放画面就会慢下来了？因为后面的待播放帧需要等更长的时间才会播放，而音频的播放速度不变，是不是就相当于待播放的视频帧在等音频了？</p><p>反之，如果说当前的视频时钟相比音频时钟要小，也就是diff小于0，说明视频慢了。这个时候我们就缩短正在播放的视频帧的播放时间，也就是减小last_duration的值，是不是视频的播放画面就会加快速度渲染，就相当于待播放的视频帧在加快脚步赶上前面的音频了？</p><p>这里略有点绕，你可以停下来理一理，总之还是很好理解的。</p><p>那具体到底对last_duration加多少或者减多少呢？我们来看看FFplay的代码是怎么做的。</p><pre><code class="language-plain">/* called to display each frame */\nstatic void video_refresh(void *opaque, double *remaining_time)\n{\n    ......\n    if (is-&gt;video_st) {\nretry:\n        if (frame_queue_nb_remaining(&amp;is-&gt;pictq) == 0) {\n            // nothing to do, no picture to display in the queue\n        } else {\n            double last_duration, duration, delay;\n            Frame *vp, *lastvp;\n            /* dequeue the picture */\n            lastvp = frame_queue_peek_last(&amp;is-&gt;pictq); // lastvp是指当前正在播放的视频帧\n            vp = frame_queue_peek(&amp;is-&gt;pictq); // vp是指接下来紧接着要播放的视频帧\n            if (vp-&gt;serial != is-&gt;videoq.serial) {\n                frame_queue_next(&amp;is-&gt;pictq);\n                goto retry;\n            }\n            if (lastvp-&gt;serial != vp-&gt;serial)\n                is-&gt;frame_timer = av_gettime_relative() / 1000000.0;\n            if (is-&gt;paused)\n                goto display;\n            /* compute nominal last_duration */\n            // last_duration是lastvp也就是当前正在播放的视频帧的理论应该播放的时间，\n            // last_duration = vp-&gt;pts - lastvp-&gt;pts。\n            last_duration = vp_duration(is, lastvp, vp); \n            \n            // compute_target_delay根据视频和音频的不同步情况，调整当前正在播放的视频帧的播放时间last_duration，\n            // 得到实际应该播放的时间delay。\n            // 这个函数是音视频同步的重点。\n            delay = compute_target_delay(last_duration, is);\n            time= av_gettime_relative()/1000000.0;\n            \n            // is-&gt;frame_timer是当前正在播放视频帧应该开始播放的时间，\n            // is-&gt;frame_timer + delay是当前正在播放视频帧经过音视频同步之后应该结束播放的时间，也就是下一帧应该开始播放的时间，\n            // 如果当前时间time还没有到当前播放视频帧的结束时间的话，继续播放当前帧，并计算当前帧还需要播放多长时间remaining_time。\n            if (time &lt; is-&gt;frame_timer + delay) {\n                *remaining_time = FFMIN(is-&gt;frame_timer + delay - time, *remaining_time);\n                goto display;\n            }\n            \n            // 如果当前正在播放的视频帧的播放时间已经足够了，那就播放下一帧，并更新is-&gt;frame_timer的值。\n            is-&gt;frame_timer += delay;\n            if (delay &gt; 0 &amp;&amp; time - is-&gt;frame_timer &gt; AV_SYNC_THRESHOLD_MAX)\n                is-&gt;frame_timer = time;\n            SDL_LockMutex(is-&gt;pictq.mutex);\n            if (!isnan(vp-&gt;pts))\n                // 用当前视频帧的pts更新视频时钟\n                update_video_pts(is, vp-&gt;pts, vp-&gt;pos, vp-&gt;serial);\n            SDL_UnlockMutex(is-&gt;pictq.mutex);\n            if (frame_queue_nb_remaining(&amp;is-&gt;pictq) &gt; 1) {\n                Frame *nextvp = frame_queue_peek_next(&amp;is-&gt;pictq);\n                // duration是当前要播放帧的理论播放时间\n                duration = vp_duration(is, vp, nextvp);\n                // 如果视频时钟落后音频时钟太多，视频帧队列里面待播放的帧的播放结束时间已经小于当前时间了的话，就直接丢弃掉，快速赶上音频时钟\n                if(!is-&gt;step &amp;&amp; (framedrop&gt;0 || (framedrop &amp;&amp; get_master_sync_type(is) != AV_SYNC_VIDEO_MASTER)) &amp;&amp; time &gt; is-&gt;frame_timer + duration){\n                    is-&gt;frame_drops_late++;\n                    frame_queue_next(&amp;is-&gt;pictq);\n                    goto retry;\n                }\n            }\n            ......\n            frame_queue_next(&amp;is-&gt;pictq);\n            is-&gt;force_refresh = 1;\n            if (is-&gt;step &amp;&amp; !is-&gt;paused)\n                stream_toggle_pause(is);\n        }\ndisplay:\n        /* display picture */\n        if (!display_disable &amp;&amp; is-&gt;force_refresh &amp;&amp; is-&gt;show_mode == SHOW_MODE_VIDEO &amp;&amp; is-&gt;pictq.rindex_shown)\n            video_display(is);\n    }\n    ......\n}\n\n</code></pre><p>我们再来看看最重要的函数compute_target_delay具体是怎么实现的。</p><pre><code class="language-plain">static double compute_target_delay(double delay, VideoState *is)\n{\n    double sync_threshold, diff = 0;\n    /* update delay to follow master synchronisation source */\n    if (get_master_sync_type(is) != AV_SYNC_VIDEO_MASTER) {\n        /* if video is slave, we try to correct big delays by duplicating or deleting a frame */\n        // get_clock(&amp;is-&gt;vidclk)是获取到当前的视频时钟，视频时钟 = 当前正在播放帧的pts + 当前播放帧已经播放了的时间。\n        // get_master_clock(is)是获取到当前的音频时钟（在视频同步到音频方法的时候），\n       // 音频时钟 = 当前正在播放音频帧的播放结束时间 - 还未播放完的音频时长。\n       // diff等于视频时钟相比音频时钟的差值；\n       // diff &gt; 0 表示视频快了；\n       // diff &lt; 0 表示视频慢了。\n        diff = get_clock(&amp;is-&gt;vidclk) - get_master_clock(is);\n        /* skip or repeat frame. We take into account the delay to compute the threshold. I still don\'t know if it is the best guess */\n        // delay就是last_duration，也就是当前播放帧理论应该播放的时长。\n        // sync_threshold是视频时钟和音频时钟不同步的阈值，就取为delay也就是last_duration的值，并且在0.04到0.1秒之间。\n        // 如果-sync_threshold &lt; diff &lt; sync_threshold的话就不需要调整last_duration了。\n        // AV_SYNC_THRESHOLD_MIN是0.04秒，也就是40ms，\n        // AV_SYNC_THRESHOLD_MAX是0.1秒，也就是100ms，也就是说音视频同步中，最大不同步程度不能超过100ms。\n        sync_threshold = FFMAX(AV_SYNC_THRESHOLD_MIN, FFMIN(AV_SYNC_THRESHOLD_MAX, delay));\n        if (!isnan(diff) &amp;&amp; fabs(diff) &lt; is-&gt;max_frame_duration) {\n            \n            // 如果视频时钟比音频时钟慢了的时间超过了sync_threshold，则将delay（也就是last_duration）减小diff，加快视频的速度。\n            if (diff &lt;= -sync_threshold)\n                delay = FFMAX(0, delay + diff);\n                \n            // 如果视频时钟比音频时钟快了的时间超过了sync_threshold，并且delay（也就是last_duration）太长了，\n            // 大于0.1秒（AV_SYNC_FRAMEDUP_THRESHOLD）的话，\n            // 我们就直接将delay（也就是last_duration）增加一个diff，减慢视频的速度。\n            else if (diff &gt;= sync_threshold &amp;&amp; delay &gt; AV_SYNC_FRAMEDUP_THRESHOLD)\n                delay = delay + diff;\n                \n            // 如果视频时钟比音频时钟快了的时间超过了sync_threshold，并且delay（也就是last_duration）不怎么长的话，\n            // 我们就将delay（也就是last_duration）增加一倍，减慢视频的速度。\n            // 这里和前一个条件处理的不同就在于delay（也就是last_duration）是不是大于AV_SYNC_FRAMEDUP_THRESHOLD，\n            // 上面不直接将delay翻倍应该是delay太大，大于了0.1秒了，超过了不同步阈值的最大值0.1秒了，还不如diff有多少就加多少。\n            // 而这个条件里面delay翻倍而直接不增加diff的原因应该是一般帧率大概在20fps左右，last_duration差不多就0.05秒，\n            // 增加一倍也不会太大，毕竟音视频同步本来就是动态同步。\n            else if (diff &gt;= sync_threshold)\n                delay = 2 * delay;\n        }\n    }\n    av_log(NULL, AV_LOG_TRACE, "video: delay=%0.3f A-V=%f\\n",\n            delay, -diff);\n    return delay;\n}\n</code></pre><p>结合代码我们可以看出，音视频同步并不是完完全全同步的，而是<strong>通过调整正在播放的视频帧的播放时间来尽量达到一个动态的同步状态</strong>，这个状态里面的视频时钟和音频时钟并不是完全相等的，只是相差得比较少，人眼的敏感度看不出来而已。这就是音视频同步的原理。</p><h2>总结</h2><p>今天我们讲述了音视频同步的相关知识。音视频同步主要的任务就是使播放的声音和画面能够对齐同步，防止出现声音和画面对不上的问题。主要的类型有三种，分别是视频同步到音频、音频同步到视频、音频和视频都做调整同步。</p><p>视频同步到音频是指音频的播放速度不需要调节，只调节视频的播放速度。如果视频相对音频快了的话，就减慢视频的播放速度；如果视频相对音频慢了的话，就加快视频帧的播放速度。这种方式是最常用的音视频同步方式。</p><p>音频同步到视频是指视频的播放速度不需要调节，只调节音频的播放速度。如果音频相对视频快了的话，就降低音频播放的速度；如果音频相对视频慢了的话，就加快音频的播放速度。但是需要注意的是，音频速度变化会导致音调改变，所以要保证变速不变调。可由于人耳的敏感度很高，音频的调整更容易被发现，因此这种同步方式难度很高，所以一般不建议你使用它。</p><p>音频和视频都做调整是指音频和视频都需要为音视频的同步做出调整。比如说WebRTC里面的音视频同步就是音频和视频都做调整。整体的思路跟前面两种差不多，音频快了就将音频的速度调低一些或者将视频的速度调高一些，视频快了就将视频的速度调低一些或者将音频的速度调高一些。</p><p>之后，我们对视频同步到音频这种方式做了深入讲解。我们主要是通过计算视频时钟和音频时间之间的差值diff，来调节当前播放视频帧的播放时间last_duration。如果diff大于0，则加大last_duration的值，让视频速度慢下来，等等后面的音频；如果diff小于0，则减小last_duration的值，让视频播放的速度快起来，赶上前面的音频。这就是音视频同步的原理。</p><h2>思考题</h2><p>这节课我们开放讨论，谈谈你在这门课程中的收获吧？或者你还有哪些不懂的知识点都可以说给我听听，如果有必要的话我们还可以做一些针对性的讲解。</p><p>不妨大胆直言，我们畅快交流，留言区见！</p>',
        article_title: "15｜音画同步：如何让声音和画面手拉手前进？",
      },
    ],
  },
  {
    chapterTitle: "结束语",
    children: [
      {
        title: "结束语｜信心比黄金更重要",
        id: 473005,
        content:
          '<p>你好，我是李江。</p><p>我们的课程到这就要结束了，恭喜你已经掌握了视频技术的基础知识。整个学习过程还顺利吗？坦白讲，虽然我想给你呈现的是一门视频通识基础课程，主要讲解一些最基本的视频原理和问题，但是由于视频技术还是有一些门槛的，所以难度相对还是比较高，比如说视频编码和弱网对抗的部分。所以，如果你感觉视频技术不好学习，觉得想放弃，那么大可不必，因为我也是这么过来的。</p><h2>视频技术难是挑战更是机遇</h2><p>视频技术的门槛既是对新学者的一个挑战，也是一个机遇。因为这一块做得有深度的人比较少，所以人才一直是稀缺的。而且<strong>相比其他的软件开发方向来说，视频技术开发者可以做到越老越吃香。</strong></p><p>相信这几年大家经常会焦虑，程序员35岁之后怎么办？会不会没有公司要了？会不会被软件行业淘汰掉？但是，从我的公司声网来看，似乎并不需要担心这个问题，因为我们公司有很多“老专家”，他们都是一大把年纪了（“老专家们”不好意思了哈哈）。</p><p>为什么呢？我想有很大一个原因是因为视频技术比较深比较难，需要长时间的积累。也是因为这样，视频行业相比后台开发、前端开发方向没有那么拥挤，没有那么“内卷”。所以你<strong>大可不必担心学好了视频技术之后没人要，你需要做的就是勇往直前地“攻克视频技术”。</strong></p><!-- [[[read_end]]] --><p>当然，要想学好视频技术也不是一朝一夕的事儿，其涉及的方向有很多，包括视频的采集和渲染、视频的前处理、视频编解码、网络传输和弱网对抗等等。你可以先找一个方向入手，深度钻研，完全掌握之后就以这个方向作为你的<strong>基本盘</strong>，在这里“安营扎寨”，然后向周围慢慢扩张。基本盘是你的“护城河”，它是你最核心的价值。有了核心价值之后，你再<strong>扩充你的“领地”</strong>，学习相关联的知识，逐渐就可以<strong>成为不可替代的人</strong>。</p><h2>好的学习资料</h2><p>那学习视频技术，有哪些好的学习资料呢？这里我给你推荐几个我学习的时候经常会参考的开源软件代码。</p><p><strong>首先是RTC领域的“王者”——WebRTC。</strong>WebRTC在音视频的网络传输和弱网对抗这一方向应该是最完整的参考软件了。它里面有视频编码RTP封包、带宽预测、JitterBuffer、PacedSender等多个模块的具体实现。</p><p>其中，带宽预测模块里有基于延时和丢包的带宽预测实现和最大带宽探测算法；JitterBuffer里面有如何组帧，如何判断帧是不是完整了，如何寻找帧的参考帧判断当前帧是不是可以解码，以及如何使用卡尔曼滤波来预测网络的抖动来平滑播放视频帧，如何做丢包重传策略，如何判断是不是要发送关键帧请求等等；PacedSender里面你可以学习怎样有节奏地发送数据包，防止关键帧或者大P帧引起网络的瞬时拥塞导致卡顿。</p><p>WebRTC还有很多模块都有很重要的参考意义，你可以把它当作一个音视频技术的宝库。</p><p><strong>另外一个音视频领域的“王者”——FFmpeg。</strong>每一个学习音视频技术的人都必须知道FFmpeg，它是学习视频采集播放、视频解码、视频前处理、音视频封装最好的开源项目，也是最全的开源项目。其中，libavcodec主要是音视频编解码相关的实现；libavformat主要是音视频封装和网络协议的实现；libavfilter则是音视频前后处理的实现。相对来说网络上FFmpeg的可参考资料也多一些，学习起来很方便。</p><p><strong>另外，在视频编码领域，我还推荐OpenH264（H264）和libvpx（VP8）。</strong>相比成熟且复杂的x264来说，OpenH264 和 libvpx 更加简单易入门一些，里面的结构更加简单清晰，不会让你觉得每一行代码都像是在“劝退你”。</p><h2>学习方法的一点感悟</h2><p>有了学习资料之后，我还想分享一点我的学习感悟，就聊聊我在中南大学（本科学校）的学习经历吧。<strong>与那些在学习视频技术过程中觉得很困难想放弃的同学共勉！</strong></p><p>我记得我刚入大学不久，就遇到了一个学姐，她被保送了清华大学的研究生。当时我就非常佩服和崇拜学姐，希望有一天能够像她一样也被保送。但是高中的时候我的数学就是弱项，而且你知道的，大一基本上就有一堆数学相关的课程，高等数学、线性代数、概率论啊之类的。就光微积分这一门课就差点劝退我了，因为对于数学不好的我来说实在是太难了，我觉得很痛苦。</p><p>所以我去了学校的心理咨询室找老师聊聊她对我的看法和建议。具体聊的内容我已经忘记了，但是我记得最清楚的就是：老师建议我<strong>不要一开始就想结果，而是去做</strong>。比如说求导，你看一次教材看不懂，就看两次，两次看不懂就看三次，并且不要一直停留在这个还没有弄懂的地方，如果经过四五次你还不懂，可以往前走学习下一个知识点，在学习后面知识点的时候，隔三差五回来再看看这个不懂的地方。总有一天你会把不懂变成懂的。</p><p>在那之后我还真是照着这个方法做的。而且这个方法看起来虽然有点笨，但是等你真正把一个不懂的知识点弄懂了之后，这个经历会不断的正反馈，告诉你再难的知识点你也是可以学会的。</p><p><strong>一次次的把不懂变成懂，就会有一次次的正反馈，激励你去攻克一个个技术难点。</strong>到现在我还是保留了这个学习方法，而且这个学习方法帮助我在大学三年取得了93分的平均成绩（是不是有点炫耀的意思，我觉得多少有一点哈哈），最后顺利保送浙大。</p><p><img src="https://static001.geekbang.org/resource/image/8c/39/8c920de2133bf9f2b661fyya9c8a1939.jpeg?wh=1120x1458" alt="图片" title="为防止被认为吹嘘，附上成绩单，哈哈"></p><p>通过这个经历，我想告诉你的就是，如果你学习视频技术觉得很难的话，先不要放弃，可以反复地查阅资料，反复学习。而且不要死磕一处，可以先往前走，然后中间不断的回过头来学习不懂的地方，最后你会惊喜地发现这些知识点之间都是有联系的，后面所学可以帮助你理解前面的内容，前面的内容又会成为你的积累，帮助你更快接受新的知识。<strong>最终，由点成线，由线组网。</strong></p><p>如果你已经幸运地进入了这个正反馈的过程中，那么恭喜你，你要开启学霸模式了，之后的学习之路会更加通畅。这可能也是大家常说的“<strong>信心比黄金更重要</strong>”吧，其实在学习这件事上也是这样的。</p><p>视频技术学习之路，路遥且艰辛，我与你是一样的，所以不要畏难，也许此刻你的对手已经提前放弃了，你就是最后的少数的幸运儿呢。对吧？</p><p>最后，文末有一份结课问卷，希望你可以花两分钟的时间填写一下。我会认真倾听你对这个专栏的意见或建议，期待你的反馈！</p><p><a href="https://jinshuju.net/f/EfbjrJ"><img src="https://static001.geekbang.org/resource/image/bc/e4/bca1d240295b38d73980fa2fec1fe5e4.jpg?wh=1142x801" alt=""></a></p>',
        article_title: "结束语｜信心比黄金更重要",
      },
      {
        title: "期末测试｜来赴一场满分之约吧！",
        id: 473015,
        content:
          '<p>你好，我是李江。</p><p>《攻克视频技术》这个专栏到今天就完结了。</p><p>为了帮助你检验自己的学习效果，我特别给你准备了一套结课测试题。这套测试题共有 20 道题目，包括 10 道单选题，10 道多选题，满分 100 分，系统会自动评分。点击下面按钮，马上开始测试吧！</p><p><a href="http://time.geekbang.org/quiz/intro?act_id=1316&exam_id=3452"><img src="https://static001.geekbang.org/resource/image/28/a4/28d1be62669b4f3cc01c36466bf811a4.png?wh=1142*201" alt=""></a></p><!-- [[[read_end]]] -->',
        article_title: "期末测试｜来赴一场满分之约吧！",
      },
    ],
  },
];
